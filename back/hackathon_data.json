{
    "hackathons": [
        {
            "title": "SFHacks 2025",
            "location": "San Francisco State University",
            "url": "https://sfhacks2025.devpost.com/",
            "submission_dates": "Apr 04 - 06, 2025",
            "themes": [
                "Beginner Friendly",
                "Social Good"
            ],
            "organization": "SF Hacks",
            "winners": true,
            "projects": [
                [
                    {
                        "title": "RecipeMind",
                        "description": "Your AI companion serving personalized recipes\u2014crafted just for you.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/recipemind"
                    },
                    {
                        "title": "Remi",
                        "description": "Designed to Remember,\r\nMade with Love",
                        "story": "\ud83d\udca1Inspiration: Remi was inspired by our team member's grandma, Jenny. Jenny is diagnosed with stage 3 of Alzheimer's, which means that in a few years, she won't remember us. She won't remember all the birthdays we celebrated together nor the inside jokes we had together... and she is not the only one.7 million Americans aged 65 and older are living with Alzheimer\u2019s disease in 2025.We wanted to address the emotional challenges that come with memory loss, especially for those with Dementia or Alzheimer\u2019s. These individuals often struggle to recognize familiar faces, which can create fear, confusion, and isolation. We wanted to find a way to support memory recall while preserving the dignity and independence of the user. Our goal was to foster connection, comfort, and routine through thoughtful, assistive design.\ud83d\udc40What it does: Remi is a robot companion with a connected web interface that helps users recall their loved ones and shared memories. When it sees a familiar face, it speaks out their name and shares a personalized memory or message. Family members can upload photos, audio, and text memories through the web platform, which sync directly to Remi through both a Mobile Application and TurtleBot4 (Robot). It creates a bridge between technology and empathy\u2014offering routine, familiarity, and warmth.\u2692\ufe0f How we built it: Website- Utilizing Next.js and Firebase, we made an intuitive dashboard for loved ones and caretakers to utilize to input data for Remi to utilize to support for those with Dementia or Alzheimer's.Facial Recognition Server- We set up a Python and FastAPI backend server that uses a YOLOv8 model for facial detection, drawing a bounding box which is then run the facial recognition. For facial recognition, we utilize deepFace to generate and utilize embeddings on all the images uploaded to the Firebase storage from the dashboard to be able to recognize loved ones in real time.Mobile App- Using React Native and Expo, we implemented a mobile application that streams live camera footage to our backend server, allowing us to run our facial detection and recognition models to recognize in real time the individuals in the video feed. Once recognized, information about the person shows up in a minimalistic, easy-to-use UI for those with Dementia or Alzheimers to recall information of their loved ones.Remi (TurtleBot4)- Built on OpenMind's OM1 Agent, we are able to utilize the TurtleBot4 robot to follow the patient and provide reminders / alerts. Remi, the TurtleBot4 robot, will follow around those with Dementia or Alzheimers, notifying them of reminders such as taking medication with ElevenLabs' text-to-speech and Gemini's text generation that is already included within OpenMind's OM1 Agent Architecture. It also provides alerts for when those with Dementia or Alzheimers enter dangerous situations such as leaving the building with supervision, informing loved ones of potential risks.\ud83c\udfc3\u200d\u2642\ufe0f Challenges we ran into: The largest challenge we faced was learning new technologies as we both went outside of our comfort zones:Michelle is a designer who has only worked with Figma, and she went above and beyond implementing her own web app designs into Next.js! This was her first time!Daniel is typically a Full-Stack developer; however, he decided to challenge himself by working with AI / ML models, AI Agents, and Robotics which were all fields foreign to him!Due to exploring new domains, we faced many frustrating blockers, but through staying up all night, we were able to get through them.\ud83d\udc4f\ud83c\udffc Accomplishments that we're proud of: An accomplishment we are proud of is that we both came out of this project with learning new skill-sets. Rather than relying on our strengths, we decided to push the bounds of what we could do and implemented many ideas within such a short time frame.\ud83e\udd29 What we learned: We learned that accessibility isn\u2019t just about physical or visual impairments\u2014it\u2019s about emotional accessibility too. Designing for memory loss taught us to think deeply about familiarity, simplicity, and trust. We also grew our technical skills by combining hardware, machine learning, and UI/UX in a cohesive experience. Most importantly, we learned how powerful design can be in preserving someone\u2019s sense of self.\ud83e\udd16 What's next for Remi: We hope to improve Remi\u2019s facial recognition by training with larger datasets and expanding its ability to learn faces over time. We also want to incorporate voice recognition and conversational AI to support two-way interaction. Beyond families, we see Remi being used in memory care homes and hospitals. Long term, we envision Remi becoming a customizable companion that adapts to the evolving needs of memory care.",
                        "github": "https://github.com/Yatsz/SFHacks2025",
                        "url": "https://devpost.com/software/remi-ft132o"
                    },
                    {
                        "title": "fillosophy",
                        "description": "Agentic document workflows for first-gen immigrants struggling with medical documents. A user uploads a video explaining their situation to power a RAG automation agent to parse and fill documents.",
                        "story": "Inspiration: As children of immigrants, we've experienced firsthand the struggles our parents faced navigating the tedious process of filing medical documents. The complex terminology and bureaucratic language create significant barriers for them. We wanted to leverage the capabilities of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) to build a platform that helps first-generation immigrants complete essential medical paperwork with confidence and ease.What it does: Fillosophy leverages Document Agentic Workflows for end-to-end automation of medical document filing. The user experience follows a simple flow:This approach eliminates language barriers and simplifies what would otherwise be a confusing and stressful process.How we built it: Fillosophy integrates two core agentic features:Leveraged LlamaIndex's LlamaParse library powered by Google GeminiExtracts relevant fields from complex medical documentsIdentifies required information with high accuracy,Implemented Google Gemini 2.0 Pro for video analysis and voice transcriptionConverts native language explanations into structured dataMatches user input to document requirements,Backend: FastAPI (Python)Frontend: Next.js, Tailwind CSS, ShadCNDatabase: MongoDB for secure storage of user documents and videosPerformance: Implemented streaming of LLM completion requests for a seamless user experience,Challenges we ran into: Technical Integration: Despite LlamaIndex documentation indicating support for various file types (.txt, .md, .docx), we discovered through extensive testing that only the PDF parsing functionality worked reliablyWorkflow Complexity: Building a functional document workflow with Gemini and LlamaIndex required significant adaptation and workarounds,Accomplishments that we're proud of: Our proudest achievement is creating an Agentic RAG system capable of handling multiple input types and sources. When users upload videos to complete documents, they can also include multiple supporting materials which our system successfully synthesizes into accurate form completion. This flexible approach enables a much more natural user experience compared to traditional form-filling interfaces.What we learned: Leveraging UI libraries like ShadCN accelerate frontend developmentStrategies for creating effective RAG systems with multiple data sources,What's next for Fillosophy: We plan to expand Fillosophy beyond medical documents to serve other industries with complex paperwork requirements. Our team believes the application can provide valuable assistance to:Law enforcement personnelRestaurant owners and food service businessesSmall business operatorsImmigration servicesTax preparation services,By addressing document complexity across multiple sectors, we aim to make essential processes more accessible to everyone, regardless of language background or familiarity with technical terminology.",
                        "github": "https://github.com/jask1m/fillosophy",
                        "url": "https://devpost.com/software/form-force"
                    },
                    {
                        "title": " AIVue: Control with Vision",
                        "description": "AIVue is a hands-free computer interface that uses eye tracking, voice commands, and AI to empower people with physical disabilities to fully control a computer\u2014no mouse or keyboard needed.",
                        "story": "Inspiration: One of our team members has worked with children who live with physical disabilities. Despite limited fine motor control, many still retain full use of facial muscles and larger muscle groups. This inspired us to imagine a more inclusive way to use computers\u2014one that doesn\u2019t rely on a keyboard or mouse. Our mission: give users complete control using only their head movements, eye gestures, and voice.What it does: AIVue: Control with Vision is a hands-free computer interface that empowers individuals with physical disabilities. It combines:\n    \u2022 Eye and head tracking to move the mouse using just head motion\n    \u2022 Blink detection for left and right mouse clicks\n    \u2022 Speech-to-text voice commands using Vosk\n    \u2022 An intelligent agent powered by Gemini framework principles that understands intent and executes tasks like opening apps or browsing\n    \u2022 A MongoDB cloud backend that logs real-time data\u2014like eye visibility, click events, and head pitch\u2014for debugging and adaptabilityWith AIVue, users can fully operate a computer without using their hands.How we built it: We used a combination of technologies to make AIVue possible:\n    \u2022 Python for core logic and integration\n    \u2022 MediaPipe and OpenCV for real-time face, eye, and head tracking\n    \u2022 PyQt6 for building a clean and intuitive user interface\n    \u2022 PyAutoGUI to control mouse behavior\n    \u2022 Vosk for offline speech recognition\n    \u2022 MongoDB Atlas hosted on Google Cloud to log user actions and enable cloud-based analytics\n    \u2022 Applied Kalman Filters to smooth cursor movements for better user experience\n    \u2022 Built an intent-based system modeled on Gemini\u2019s agentic architecture to enable natural language interactionChallenges we ran into: Accomplishments that we're proud of: What we learned: What's next for  AIVue: Control with Vision:",
                        "github": "https://github.com/MitchTravis0/AIVue",
                        "url": "https://devpost.com/software/aivue-control-with-vision"
                    },
                    {
                        "title": "Secure Sense: A Safety Net for GenAI",
                        "description": "Secure Sense is a GenAI-aware DLP tool that detects and blocks or masks sensitive data in real-time, preventing leaks when anyone uses Gen AI tools like ChatGPT or Gemini. ",
                        "story": "",
                        "github": "https://github.com/nh0397/SF-Hacks",
                        "url": "https://devpost.com/software/secure-sense-a-safety-net-for-genai"
                    },
                    {
                        "title": "Disaster Watch",
                        "description": "Disaster Watch is a unified dashboard that delivers real-time alerts and predictive insights during any disaster, empowering users with clarity when every second counts.",
                        "story": "Inspiration: During a disaster is can be hard to get reliable, centralized information. Alerts and updates are often split between multiple different government agencies, social media, and news outlets\u2014making it confusing and time-consuming for people to stay informed. One of our teammates, Will, experienced this first-hand during the Santa Rosa wildfires and knows how disorienting that chaos can be.We wanted to build something that bridges the information gap during emergencies. Our goal was to create a tool that visualizes disaster data clearly and immediately\u2014something you could pull up in seconds to understand what's happening near you.We were also inspired by:Ultimately, we wanted to create a dashboard that doesn't just inform\u2014it empowers people to make decisions faster and safer during a crisis.What it does: We built a dashboard with a map focused on the city of the user. It aggregates alert information from a database and converts it into pins on the map. It also adds shaded region to simulate a growing fire. There are other components in the dashboard that show historical disasters, susceptibility to certain disaster types in your area, and a weather forecast.How we built it: Bootstrap for frontend, Express for backend, leaflet.js for map, Postgres for the database, hosted on Render, got our domain name ControlPanel.techChallenges we ran into: Version control when multiple people work on one file, Talk about weather (Eric), originally we wanted to set up cloudflare on our project but we ultimately had to abandon the idea.Organizing tasks, and working together and apart while trying not to lose time.Relying on teammates that have more experience that kind of took on sole direction of certain components.,Accomplishments that we're proud of: \"I'm really proud to have contributed to and shipped a project with such a large scope in such a short time period.\" - John\n\"For me, hosting a new project online, and then registering a domain and altering the CNAME and A record to allow DNS to point to our purchased domain name of disasterwatch.tech, that was all new for me. Also, integrating Gemini's API's for multiple parts of our project, the formatting and prompting that needed to be tweaked to get the right output was new for me\" - WillWhat we learned: Version control - keeping stuff up to date and being more coordinated when working on the same file.Be aware of CORS/Javascript issues when reading data from external files.We learned the importance to focus on creating a Minimal Viable Product (MVP) first, then add features if time allows. This was crucial given our time constraints. In the end we were able to add features towards the end, but only after we published an MVP.,What's next for Disaster Watch: We used simulated data for our alerts currently. Ideally , we would use API calls and scapers to get real time data, but obviously the feasibility of doing that is limited by the amount of disasters we feel on a day to day basic. We would also love to have a user base that contributes alerts on the map.",
                        "github": "",
                        "url": "https://devpost.com/software/diaster-watch"
                    },
                    {
                        "title": "ExposeIt: Power to the People, Privacy to the Reporter",
                        "description": "Fear shouldn\u2019t silence justice. ExposeIt enables secure, confidential reporting of crimes & wrongdoings, helping create safer community for all. Let\u2019s work together to protect and empower our society.",
                        "story": "Inspiration: While social platforms exist, posts about serious issues often get buried in noise, and anonymity isn't guaranteed. We realized there\u2019s a need for a focused, secure space where people can raise their voiceswithout fear.That\u2019s what inspiredExposeIt\u2014a platform built toempower anyone to report injustice, stay anonymous, and still be heard. Our goal was to use technology not just to build a tool, but to create a sense ofsafetyandagency, especially for those who often go unheard.What It Does: ExposeItis a secure citizen journalism and anonymous crime reporting platform designed to empower people to speak up without fear. It leveragesAIanddecentralized techto transform raw reports into impactful, verifiable posts\u2014while maintaining user privacy and safety.AI-Powered Report Generation supporting multiple languagesTransforms raw crime descriptions into concise, social-media-style posts using theGemini API, making reports easier to read and share.Decentralized Privacy & ProofEncrypts original reports throughMidnight blockchainwithzero-knowledge proofs, ensuring tamper-proof, anonymous documentation.Emergency Voice Reporting supporting multiple languagesAllows users to record voice notes in urgent situations in any language when typing isn\u2019t feasible, maintaining accessibility.Reputation & Credibility TrackingImplements a community-driventrust scoresystem based on upvotes/downvotes, helping surface credible reporters.User EngagementEnables others to comment, share, and engage with posts, fosteringtransparent community discussions.Evidence SharingSupports image, audio, and video uploads so users can includecrucial evidencewith their reports.Anonymous Reporting to AuthoritiesOffers an option toforward reportsto law enforcement via anonymous email\u2014closing the loop between whistleblowers and action.,How We Built It: We brought ExposeIt to life by combiningmodern web technologies,AI, anddecentralized storage, focused on creating aseamless, secure experience.Built withReactStyled usingTailwind CSSEnhanced withFramer Motionfor smooth animations and transitions,IntegratedAuth0for secure and flexible user login/logout while preserving anonymity when needed,Built withNode.js/ExpressHandles report processing, AI integration, moderation, and database/blockchain interactions,Utilizes theGemini APIto turn raw incident descriptions into structured summaries suitable for sharing,MongoDB Atlas: Manages user profiles, posts, votes, commentsMidnight Blockchain: Stores encrypted original reports with zero-knowledge proofs for immutability and verifiability,Challenges We Ran Into: Accomplishments We're Proud Of: \u2705 Built a fully functionalMVPwithend-to-end anonymous reporting\u2705 Successfully combinedLLMs,blockchain, andencryptioninto one working system\u2705 Balanceduser privacywithpublic transparency\u2705 Made crime reporting feelsafe, focused, and empowering\u2705 Enforced content relevance by filtering out unrelated posts\u2705 Implementedspam detection and flaggingto maintain report integrity\u2705 Addedmultilingual & accessibility supportto reach a wider audience,What We Learned: Developed a deep understanding ofzero-knowledge proofsand their real-world privacy applicationsExplored the strengths/limitations ofLLMswith sensitive contentHoned skills inteam collaboration, adaptability, and rapid development under hackathon pressureLearned how to experiment with new tools likeOpenMind, diving into docs and hands-on testingDiscovered the full potential ofGemini API\u2014especially for summarization, transcription, and prompt optimization,What\u2019s Next for ExposeIt:Power to the People, Privacy to the Reporter: \ud83e\udd1dPartner with NGOs & Law Enforcementto act on verified, anonymized reports\ud83c\udf1fEnhance moderationusing community-driven trust scoring\ud83e\udd16Explore emergency-response robotsfor real-time distress signal response\ud83d\udccdImplement area-wise geolocation crime analyticsfor heatmaps and trend tracking\ud83e\udde0Refine the AI pipelinefor better summaries, voice inputs, and live updates,",
                        "github": "https://github.com/rohan2406/ExposeItt/tree/master",
                        "url": "https://devpost.com/software/exposeit-power-to-the-people-privacy-to-the-reporter"
                    },
                    {
                        "title": "Eqlec.tech",
                        "description": "One line of code for the world. Our custom-built HTML tag helps developers make their site accessible to everyone, regardless of their needs.",
                        "story": "Inspiration: Everyone has different needs. Yet, many websites remain inaccessible for individuals needing more support to access their content. Thus, we wanted to incentivize developers to create accessible code by offering them an easy solution built on one line of HTML code.What it does: We built an npm package that exports a custom HTML tag alongside JavaScript scripts to serve accessible web content to users who may struggle to navigate without it. Users can then log into our system, and we will dynamically serve them accessible content based on their specifications.How we built it: We created a custom HTML tag  that works alongside scripts to serve accessible content rendered by Google Gemini. When properties such as 'enable-large-font' are appended to the tag, our scripts call the Gemini API and serve the resulting HTML content.The user requiring accessible web content directly controls the properties appended to our tag. When developers include our tag, a button pops up on the bottom of their page where the user can configure their accessibility settings. When done, their preferences are saved to a MongoDB database, and the page is updated. After that, the HTML is cached so that when they return, they do not have to wait for a model response and can see their content instantly.Challenges we ran into: When building the express.js and passport.js backend for our website to handle authentication and session management, we encountered many issues with cookies being improperly sent. Thus, user authentication would often fail, and data would not be saved properly.Accomplishments that we're proud of: We are proud of our team's ability to publish an npm package and create a full-stack website in one hackathon. We also did a lot of coding and managed to work through bugs quickly.What we learned: We learned how to create npm packages, use express.js and passport.js, and just a lot more in general about using JavaScript outside of a React environment, given that most of our scripting for our HTML element was in vanilla JavaScript.What's next for Eqlec.tech: We want to expand and include closed captioning of any audio playing in a tab, offer voice interaction with our UI, and include more accessibility options we did not have time to include during this hackathon. We also want to find more time-efficient solutions and direct DOM manipulation for rendering to give users the best experience using our tool.",
                        "github": "",
                        "url": "https://devpost.com/software/eqlec-tech"
                    },
                    {
                        "title": "Wavelly",
                        "description": "Smart Navigation. Safe Journeys. Confident Steps. That\u2019s Wavely.",
                        "story": "Inspiration: We were inspired by the common yet deeply concerning experience shared by many women, feeling unsafe while walking alone, especially at night at an unknown place. Existing navigation tools prioritize speed or distance, but none focus on safety, which is often the top priority for women navigating urban spaces. WAVELLY was born out of the desire to bridge that gap and empower users with smarter, safety-first navigation.What it does: WAVELLY is a navigation app that helps users, especially women, choose the safest walking routes based on real-time safety data. It calculates a Weighted Safety Score for every possible optimal path using parameters like historic crime reports, live foot traffic, street lighting conditions, nearby institutions (schools, fire stations, police stations, hospitals, etc.), and user-reported incidents. It also features a AI call feature powered by Gemini API to deter potential threats. A community incident reporting system to help others stay informed and alert.How we built it: We used the Google Maps API for location tracking, route rendering, and real-time navigation. The custom safety scoring system creates a safety score by processing data from our custom databases having details of crime, foot traffic, and contextual sources. The Gemini API powers the AI call system, creating a convincing simulation of talking to a known person in case of emergencies.Challenges we ran into: Balancing accuracy and sensitivity in the keyword-triggered emergency system.\nCombining multiple safety parameters into a meaningful, real-time score without overwhelming the user.\nSimulating realistic fake calls that feel convincing without actual audio from known persons.\nEnsuring privacy and speed in sharing emergency locations securely.Accomplishments that we're proud of: Successfully integrating Google Maps with safety overlays and custom scoring.\nIntegrated Gemini API for AI calling feature.\nCreating a polished user experience that balances function and discretion.\nDesigning a compelling, community-driven concept that aligns with Tech for Good and Women Empowerment.What we learned: How to leverage navigation APIs beyond traditional use cases.\nThe importance of user psychology in designing safety features.\nHow to merge real-time data, and emergency logic into one cohesive flow.\nThe power of collaborative brainstorming and rapid prototyping in a hackathon setting.What's next for Wavelly: Integrating custom fake call voices and scripted scenarios.\nAdding a beeper/SOS vibration system for subtle emergency signaling.\nSmartwatch and wearable device integration for easier access.\nA Guardian Mode, where friends can track your journey live.\nDeploying on app stores and building partnerships with local safety organizations and city data sources.",
                        "github": "https://github.com/Sapna24Sangmitra/Wavely",
                        "url": "https://devpost.com/software/wavelly"
                    },
                    {
                        "title": "Pill Penny",
                        "description": "Pill Penny helps users find affordable medication alternatives by comparing prices between brand-name drugs and their generic equivalents using a GraphRAG solution with Gemini and Neo4j.\r\n",
                        "story": "Inspiration: We created Pill Penny after witnessing friends and family members struggle with the high costs of prescription medications. Many people don't realize how much they could save by switching to generic alternatives. This information gap became our call to action, driving us to create a tool that democratizes access to medication pricing information and helps people make more informed decisions about their healthcare spending.What it does: Pill Penny is a web application that compares brand-name medications with their generic alternatives, displaying potential cost savings in an intuitive visual format. Users can search for medications, view monthly and annual savings projections, and access detailed information about both brand and generic options. Our interface presents complex pharmaceutical information in an approachable way, helping users understand their options at a glance.How we built it: We developed Pill Penny using a modern tech stack:Frontend: Next.js and React with TypeScript, creating an engaging UI with Tailwind CSS and Framer Motion animationsBackend: Flask API connecting to a dual database system:Pinecone Vector Database: For efficient similarity searches of medication informationNeo4j Knowledge Graph: To model complex relationships between medications, side effects, drug classes, and manufacturersAI Integration: Gemini API using a Retrieval-Augmented Generation (RAG) approach that pulls from both vector embeddings and graph relationshipsArchitecture: Next.js API routes proxy requests between frontend and Flask backend to resolve CORS issues,Our Neo4j knowledge graph is particularly powerful for understanding medication relationships, allowing us to provide insights about similar medications within the same class and potential alternatives beyond direct generics.Challenges we ran into: Our biggest hurdles included resolving CORS issues between our frontend and backend services, optimizing complex animations for better mobile performance, and engineering AI prompts to return consistently formatted medication comparison data. Building and maintaining the Neo4j knowledge graph presented its own challenges, particularly in creating a coherent schema that accurately represents pharmaceutical relationships. We also struggled with creating an intuitive search experience that could handle both partial and exact medication name matches while maintaining a smooth user experience.Accomplishments that we're proud of: We're particularly proud of our visually engaging interface that makes pharmaceutical information accessible to everyone. The neural network visualization creates an engaging backdrop while our comparison cards present cost savings in a clear, impactful way. Our integration of vector search with knowledge graph traversal creates a uniquely powerful RAG system that can answer complex queries about medication relationships. We've successfully created a seamless integration between modern frontend technologies and specialized backend services that work together to deliver a fluid user experience.What we learned: This project expanded our expertise in RAG architecture, knowledge graphs, vector databases, and AI API integration. We gained valuable insights into UI animation psychology and how thoughtful design can make complex medical information less intimidating. Our team developed stronger skills in Neo4j graph modeling and learned effective ways to combine vector similarity search with graph relationship queries. Most importantly, we discovered how technology can bridge information gaps in healthcare and potentially improve people's lives.What's next for Pill Penny: We plan to expand Pill Penny with location-based pharmacy pricing comparisons, insurance coverage integration, and medication interaction warnings leveraging our knowledge graph. We're enhancing our Neo4j implementation to include deeper connections between medications, conditions, side effects, and contraindications. We're exploring adding personalized medication tracking features and developing native mobile applications. Long-term, we envision Pill Penny becoming a comprehensive medication management platform that integrates with healthcare providers' systems to offer a complete view of treatment options and their associated costs.",
                        "github": "https://github.com/ani-sivaa/sfhacks2025",
                        "url": "https://devpost.com/software/pill-penny"
                    },
                    {
                        "title": "Elder Ease",
                        "description": "Not sure what benefits or meds apply to your parents? ElderEase has it covered. Your senior care companion for benefits, health, and well-being.\r\n",
                        "story": "\ud83d\udee0\ufe0f How We Built It: We builtElderEaseas a full-stack AI-powered web app designed to simplify healthcare access for senior citizens. Here's the tech stack that made it possible:\u2699\ufe0fBackend:FastAPIto build a scalable, asynchronous REST API.Google Gemini Pro&Gemini Visionfor all AI-driven features like document summarization, natural conversations, journal title generation, and emotional tone detection.\ud83c\udfa8Frontend:Next.jsfor server-side rendering and routing.Tailwind CSSandFramer Motionto create a soft, elegant, and animated user experience that's friendly for seniors.SpeechRecognition APIandText-to-Speechfor voice input and audio playback.Push Notificationsto remind users of prescription times.\u2601\ufe0fBackend as a Service:AppWritewas used to manage file storage, database, and authentication in a streamlined way.,\ud83e\uddf1 Challenges We Ran Into: \ud83d\udd01Gemini model versioning: Keeping track of different Gemini API versions (Pro vs Vision) and the required prompt formats took some trial and error.\ud83e\uddd3Designing for elderly users: We had to think deeply about spacing, font sizes, icons, and accessibility, making sure the experience was warm and not overwhelming.\u2692\ufe0fBackend reusability: We aimed to keep our backend generic and stateless so it can power future mobile apps or even smart kiosks in clinics or pharmacies.,\ud83d\udcda What We Learned: \ud83e\udd16 How to work withGoogle Gemini APIsfor text and image understanding at scale.\ud83d\udcac Building interactive chat UIs that retainconversation contextand provide helpful follow-ups.\ud83d\udd04 Managing state, routing, and file handling in a full-stack production-grade app.\ud83e\uddcf\u200d\u2642\ufe0f The importance ofsimplicity, empathy, and accessibilitywhen building for a senior audience.,",
                        "github": "https://github.com/Yash170220/Insurance_Backend",
                        "url": "https://devpost.com/software/elder-ease-kewmor"
                    },
                    {
                        "title": "TurtleBot gesture recognition and response",
                        "description": "Allows turtle bot the recognize hand gestures from camera and move accordingly. ",
                        "story": "",
                        "github": "https://github.com/OpenmindAGI/OM1/pull/191",
                        "url": "https://devpost.com/software/turtlebot-gesture-recognition-and-response"
                    },
                    {
                        "title": "MedScanNet",
                        "description": "Created a ML model that can determine if a mammography has a tumor and if the tumor is benign or malignant (cancerous) with 95% accuracy.",
                        "story": "Background: Our inspiration stemmed from a personal journey\u2014one of our team members had a parent diagnosed with breast cancer, igniting a passionate drive to advance Computer-Aided Diagnosis (CAD). Initially focusing on mammography scans for tumor detection, our project evolved to classify tumors as benign or malignant. Expanding further, we integrated chest X-ray analysis to diagnose conditions like Atelectasis, Effusion, and Pneumonia.What it does: Our web application features two distinct functionalities: one for uploading mammography scans and another for chest X-rays. The mammography page detects tumors and classifies them as benign or malignant. Similarly, the chest X-ray page diagnoses conditions from a set of seven possibilities, including Atelectasis and Mass.How we built it: We leveraged PyTorch on Google Colab with A100 GPUs for training our models. Mammography data from CBIS-DDMS and chest X-ray data from NIH and pneumonia-specific datasets were used. Addressing data imbalance, we employed image augmentation techniques such as flipping, rotation, and contrast adjustment.Challenges we ran into: Managing data imbalance posed significant challenges, initially biasing our models towards the majority class. Overcoming this, we learned and implemented image augmentation strategies on the fly during the hackathon.Accomplishments that we're proud of: Achieving a 95% accuracy in classifying mammography scans for tumor presence and expanding to a robust seven-class classification for chest X-rays within the hackathon timeframe showcases our team's efficiency and adaptability.What we learned: Time management emerged as a critical lesson in the hackathon environment, where training ML models under time constraints demanded strategic prioritization and compromise on precision.What's next for Breast Cancer Image Analysis: Future plans involve expanding chest X-ray datasets, exploring advanced loss functions for enhanced accuracy and recall, and integrating more modern techniques to further refine our models.",
                        "github": "",
                        "url": "https://devpost.com/software/breast-cancer-image-analysis"
                    },
                    {
                        "title": "SmartReach",
                        "description": "SmartReach is an AI-powered robotic arm that assists elderly and disabled individuals by recognizing, locating, and delivering objects using voice commands, vision, and intelligent motion control.\r\n\r\n",
                        "story": "Inspiration: Our project, SmartReach, was born out of a desire to create meaningful technological solutions that can directly impact lives\u2014particularly for the elderly and people with disabilities, including those living with Parkinson\u2019s disease. These individuals often face difficulty in performing day-to-day tasks, and we wanted to create a robotic companion that could assist them in a practical, intelligent, and empathetic way.What it does: SmartReach is an assistive robotic system powered by the SO100 robotic arm, integrated with Google\u2019s Gemini AI model, and designed to act as a smart, helpful companion. The system can identify, locate, and retrieve objects on voice command, easing everyday tasks for individuals with limited mobility.Here\u2019s how it works:The SO100 robotic arm is equipped with a Logitech webcam, and all its parts are custom 3D-printed.Using inverse kinematics, the robotic arm calculates the precise joint angles required to reach a given object.When a user gives a command such as, \u201cCan you find the keys on the table?\u201d, the system captures an image using the camera.This image is sent to Gemini AI, which analyzes the photo to identify if the object (in this case, keys) is present.If the object is found and the user follows up with a command like, \u201cCan you give me the keys?\u201d, the robot initiates the grabbing sequence, brings the object to the user, and awaits further interaction.The idea is to have a fully responsive robotic assistant that can intelligently interpret requests and act in the physical world bridging the gap between AI and assistive robotics.How we built it: All mechanical parts for the SO100 robotic arm were 3D-printed, allowing us to fully customize the design.We used STS3215 servos with a single-bus connection, which simplified wiring and reduced the overall hardware complexity.The system was programmed in Python, leveraging ROS 2 (Robot Operating System) to control the robotic arm and integrate AI-based decision-making.Gemini AI was connected to analyze captured images and interpret user commands, acting as the \"brain\" of the system.We developed the inverse kinematics logic to calculate and execute real-time movements for object retrieval tasks.Challenges we ran into: 3D printing proved to be a bottleneck\u2014we had to frequently go back and forth to fix quality issues and reprint defective parts.Our hardware was limited and not high-end, but we found creative solutions to make the most of what we had.Integrating Gemini with real-time robotic controls posed a steep learning curve, but we managed to build a stable communication pipeline between the robot and the AI model.Accomplishments that we're proud of: We always aspired to create something for social good, and we are incredibly proud of the progress we made within such a short time.Despite limited hardware and access to professional tools, we built a fully functional prototype from scratch that surpassed our expectations.Our team took on the challenge of learning completely new technologies, including ROS 2, inverse kinematics, and AI integration\u2014all within the span of a hackathon.We stepped out of our comfort zones and proved that with passion, collaboration, and determination, it's possible to turn an ambitious vision into reality.What we learned: This was our first hands-on experience with building and controlling a robotic arm, and we gained invaluable insights into its mechanics and control systems.We learned how to use ROS 2, a powerful framework for robotic applications.We implemented inverse kinematics from scratch to control precise arm movements.Most importantly, we learned how to bridge the gap between AI and robotics, turning abstract commands into tangible physical actions.What's next for SmartReach: We plan to improve the object recognition accuracy and reduce latency in AI processing.\nWe aim to add voice interaction to create a completely hands-free experience.\nLong-term, we envision SmartReach becoming a commercially viable assistive robot that can live in people\u2019s homes and help with daily tasks\u2014enhancing independent living for the elderly and people with physical disabilities.\nThis project marks a small but significant step in building human-centric robotics, and we are excited to take it even further.",
                        "github": "https://github.com/Atharva2099/SmartReach",
                        "url": "https://devpost.com/software/smartreach"
                    },
                    {
                        "title": "BallinOnABudget",
                        "description": "This app helps users save time and money by optimizing grocery shopping. A user builds a grocery list, the app generates an optimized route of which stores to visit and what to buy at each location.",
                        "story": "Inspiration: The inspiration for this app came from my mom, who meticulously scans weekly ads from local grocery stores, comparing prices and planning what to buy at each store. While this process helps her save thousands of dollars a year, it\u2019s time-consuming and often inefficient in terms of travel. My girlfriend, who relies on government assistance and balances two jobs while attending school, faces a different challenge\u2014she has a limited grocery budget and very little time to spare. I built this app to automate and optimize the grocery shopping process, helping users prioritize either time, money, or both. It\u2019s designed to make smart shopping accessible and practical\u2014especially for low-income individuals and families who need it most.What it does: BallinOnABudget is an intelligent grocery-planning app that helps users save time and money by optimizing where and how they shop. Users can build grocery lists, and the app scans nearby stores for item availability and compares prices. It recommends what to buy at each store based on personalized priorities\u2014like minimizing cost, time, or both\u2014and generates an optimized route. It even maps out the ideal order of store visits to minimize travel distance and time from the user\u2019s starting point to their desired destination, making every grocery trip smarter and more efficient.How we built it: We built the \"BallinOnABudget\" grocery shopping optimization platform as a full-stack application that helps users save money and time on grocery shopping. The application uses MongoDB Atlas as its main database, storing user data, grocery lists, store information, and product pricing. We integrated the Google Maps API for all location-based features, including geocoding addresses, calculating distances between users and stores, visualizing store locations on interactive maps, and providing travel directions. The Gemini API powers our intelligent scraping capabilities, automatically extracting real-time grocery pricing and deals from store websites to provide users with accurate product information. This data powers our three shopping strategies (Money Saver, Balanced Saver, and Time Saver) that help users optimize their shopping trips based on their preferences. The application features a responsive user interface built with React and Tailwind CSS, secure authentication with password hashing using scrypt, and personalized user preferences for location settings and distance filters.Challenges we ran into: During the development of BallinOnABudget, we encountered several significant challenges:Google Maps Integration Issues: We faced persistent problems with map markers not displaying accurately, particularly store locations that would appear incorrectly positioned or entirely missing.Geolocation Accuracy: Obtaining accurate user location data proved challenging, especially with browser permissions and varying mobile device behaviors. We decided to use real-time geolocation.Data Scraping Limitations: Using Gemini API for grocery pricing required extensive prompt engineering to extract structured data consistently across different store websites with varying layouts. Some stores implemented anti-scraping measures that required additional workarounds. We opted to use Puppeteer for data scraping.MongoDB Schema Evolution: As the application grew, we had to manage MongoDB schema changes without breaking existing data.Session Management Complexity: Authentication presented challenges, particularly with session persistence across server restarts and handling expired JWT tokens. We improved security by implementing SESSION_SECRET environment variables and proper session store configuration.Memory Leaks in React Components: The map component caused memory leaks when unmounting, requiring careful implementation of cleanup functions for Google Maps objects to prevent performance degradation and browser crashes.API Rate Limiting: Google Maps API quotas and rate limiting required implementation of caching strategies to reduce API calls and avoid exceeding free-tier limits.Accomplishments that we're proud of: Responsive UI/UX Design: Built a clean, intuitive interface with careful attention to accessibility features and user-friendly interactions.Security Implementation: Implemented robust authentication using secure password hashing with scrypt, proper session management, and environment variable protection for sensitive API keys.Cross-API Orchestration: Successfully coordinated multiple external APIs (Google Maps, Gemini, MongoDB Atlas) into a cohesive system.Kept pushing, not touching grass, and working on this project well into the night.Participated in our first hackathon.What we learned: This takes way more work than we thought!What's next for BallinOnABudget: We want to complete the application and provide a free, mobile-friendly application and start saving people their time and money!Further, we'd like to monetize our application by selling relative and appropriate data- purchases, chosen grocery stores, chosen items- collected from the app to grocery stores to fund a free application for users. We'd also like to tally how much the average user of our app saves per month/year. This could serve to potentially create a subscription-based model for our app that costs $5/month but saves the user hundreds of dollars in a year.We want to implement: \nIntelligent Shopping Strategies: We plan to develop and integrate three distinct optimization algorithms\u2014Money Saver, Balanced Saver, and Time Saver\u2014that will analyze pricing, distance, and time factors to generate personalized shopping plans tailored to user priorities.Seamless Geospatial Integration: Our goal is to create a dynamic, location-aware experience that updates store recommendations based on user proximity and preferences. We\u2019ll include a travel radius visualization to help users easily understand their options at a glance.Automated Price Scraping: We aim to build a robust integration with the Gemini API to extract and organize real-time grocery pricing data from multiple store websites, giving users immediate and accurate information for smarter decision-making.Responsive UI/UX Design: We're planning a clean, intuitive interface optimized for mobile and desktop experiences. The design will emphasize accessibility and include user-friendly features like interactive store cards and simplified map navigation.",
                        "github": "https://github.com/OmeidN/ballinonabudget.git",
                        "url": "https://devpost.com/software/ballinonabudget"
                    },
                    {
                        "title": "GatorGuard",
                        "description": "Eliminate distractions. Enhance focus. Reduce burnout.",
                        "story": "Inspiration: Studies have found that more than half of Gen Z has experienced burnout and/or consistent stress and worry. As college students, we have felt the impact of burnout and stress firsthand. This is why we built GatorGuard, an immersive, AI-powered productivity application that helps users stay focused while also managing their stress and burnout levels.What it does: GatorGuard is a full-stack application that manages browser sessions to ensure that the user is staying on task. Users are provided with three browsing modes: Study Mode (with optional focus areas of Interview Prep or Studying for School), Work Mode, and Leisure Mode. GatorGuard's browser extension analyzes whether the current tab the user is on is appropriate for the current browsing mode, and notifies the user if it is not. We believe this allows users to make sure they stay on task when working/studying, while taking their much-deserved time away from work when they have set aside time to relax.How we built it: Frontend: NextJS, React, TailwindCSSBackend: FastAPIAPIs: Google Gemini, SpeechRecognition API, Spotify APIDatabase: SupabaseChallenges we ran into: Chrome Extension Development: This was our first time building a Google Chrome extension, so we faced a steep learning curve understanding how extensions interact with web pages, background scripts, and browser permissions.Browser Scraping: Extracting data from complex web pages proved challenging due to the sheer volume of information. We had to invest significant time optimizing our AI agent scraping logic and managing dynamic content to ensure fast and reliable data extraction.Accomplishments that we're proud of: It works! We built a fully functionally web application incorporating Google Gemini that enhances focus and filters out irrelevant websites.What's next for GatorGuard: Deployment and gain actual users.",
                        "github": "https://github.com/adarshm11/GatorGuard",
                        "url": "https://devpost.com/software/gatorguard"
                    },
                    {
                        "title": "NOMAD",
                        "description": "Not One More Abandoned & Displaced\u2014NOMAD turns anonymous stories into structured profiles using AI, helping community take immediate and informed action",
                        "story": "\ud83d\ude80 Inspiration: Every day, countless voices go unheard\u2014those displaced, forgotten, or invisible to systems meant to protect them. We realized that if technology can recognize faces, write poetry, and drive cars\u2014why can\u2019t it also listen to the homeless and help?\nNOMAD was born from this belief: Not One More Abandoned & Displaced.\ud83d\udca1 What it does: NOMAD is an AI-powered humanitarian assistant that allows frontline workers to record or upload voice samples from homeless individuals.\nIt then:Transcribes audio using WhisperExtracts structured information (name, location, family, conditions) using NLPPredicts urgency levels using XGBoostVisualizes global data through a dynamic heatmap\nAnd all of this is wrapped in a secure, responsive, privacy-conscious web platform.\ud83d\udee0 How we built it: Frontend: HTML, CSS, JS with Leaflet.js for heatmap renderingBackend: Flask + Python with MongoDB Atlas for persistenceAudio: OpenAI\u2019s Whisper for transcriptionNLP: Gemini API for structured data extractionML: XGBoost model predicting urgency from encoded featuresBonus: JWT Auth, audio recording in-browser, mock alerts for social worker/NGO outreach\ud83e\uddf1 Challenges we ran into: Converting raw voice into actionable, structured data wasn\u2019t easyHandling inconsistent user inputs across different formatsTraining a usable model with meaningful urgency scoresEnsuring privacy and ethics while dealing with sensitive dataJuggling ML, NLP, UI/UX, and full-stack architecture within 36 hours!\ud83c\udfc6 Accomplishments that we're proud of: Built a fully functional end-to-end platform in under 2 daysSeamlessly integrated AI, ML, and geospatial data into a single flowDelivered real-time urgency predictions with meaningful impactCreated something that could genuinely help those forgotten by societyAnd perhaps most importantly\u2014gave voices back to the voiceless.\ud83d\udcda What we learned: Working with real-world humanitarian problems requires deep empathyML is powerful\u2014but useless without ethical and contextual designTranscription and NLP pipelines are messy, but extremely rewardingHow to hustle, fail, and innovate\u2014together\u2014as a team with purpose\ud83d\udd2e What's next for NOMAD: Real-time outreach integrations (NGO dashboards, city alerts)Multi-language support for transcription + extractionMental health triage and recommendation systemsAnalytics dashboard for civic and social organizationsOffline capabilities for field workersPartnering with shelters, hospitals, and relief efforts to deploy NOMAD where it\u2019s needed most",
                        "github": "",
                        "url": "https://devpost.com/software/nomad-g72fbz"
                    },
                    {
                        "title": "Canvas Auto- submit",
                        "description": "Chrome extension that auto submits the uploaded assignment when the due date and time hits.",
                        "story": "Inspiration: Canvas Auto-Submit highlights a real and related problem faced by students. Students often uploads files for submiss but forget to submit it. This earns them nothing but a 0 due to human error.,What it does: Canvas Auto-Submit is designed to make canvas submission better by enabling the feature of auto submission. Suppose a student is working on paper and the students uploads the draft. If the student is not able to finish the final draft or forgets about it, then at least the first draft will be submitted by its own when the deadline hits. It relieves last moment stress and anxiety and helps students focus on the better things without worrying about deadlines.,How we built it: We built it by making a chrome extension.A popup UI (html, CSS and JavaScript) for user input and settingsA background scripts which manages the alarms and auto submissionsA concept script which operates Canvas Assignment page to display any triggersUsed chrome.storage.sync to secure user data,Challenges we ran into: The biggest challenge was getting the API . It asked for admin access. We got the error when we thought we were almost done. We could have implemented it better if we had the needed API but I believe that the ideas was a success and it could be highly useful for students.,Accomplishments that we're proud of: This was our first time participating in Hackathon. We are proud that we could implement the chrome extension for the very first time. We were able to get the exact time submission details. Because of Canvas not giving API, it got difficult to get dummy courses list for our project.,What we learned: We were able to learn a lot about API's which for us is a big success. We learned about DOM manipulation and how to add more functionality to certain websites. We also learned about Open Authentication as we came across the big error of Canvas's API.,What's next for Canvas Auto-submit: Next Auto- submit is looking for OAuth and better views and functionality to reduce student's burden.,",
                        "github": "https://github.com/AkshPatel63/Canvas-extension-.git",
                        "url": "https://devpost.com/software/canvas-autosubmit"
                    },
                    {
                        "title": "Cultural Hotspot",
                        "description": "Cultural Hotspot, an app for rediscovering locations and identities of cultural past through present communities.",
                        "story": "Inspiration: Living in the United States, we\u2019re surrounded by an incredibly diverse population. However, we noticed that many second-generation immigrants sometimes feel disconnected from their cultural roots. This inspired us to buildCultural Hotspot\u2014a website dedicated to helping people reconnect with their heritage by discovering local cultural spots and communities. We wanted to create something that shows people that culture is all around us, just waiting to be explored.What It Does: Cultural Hotspot lets users explore nearby cultural places through an interface inspired by Google Maps. You can test it out by clicking the demo, which highlights various cultural hotspots in your area\u2014be it a small grocery store, a cultural center, or a community event.How We Built It: We built the project using:Next.jsfor the full-stack web application frameworkTypeScriptfor type safety and better developer experienceTailwind CSSfor responsive and clean stylingGoogle Maps APIto integrate location-based servicesWe made our plan and divided tasks based on strengths and learning goals, which helped keep the workflow smooth even with different experience levels.Challenges We Ran Into: One of the biggest challenges we faced was time management, especially since two of our members were relatively new to coding. Balancing learning with building under time pressure was tough, but it also pushed us to support each other and grow as a team.Accomplishments We're Proud Of: We\u2019re especially proud of our interactive map. It\u2019s fun, intuitive, and visually engaging\u2014something we worked hard on to make sure it adds real value to the user experience.What We Learned: We learned that there\u2019s always a cultural space nearby\u2014you just have use our website. Building this project opened our eyes to the richness of communities that exist even in places we thought we knew well.What\u2019s Next for Cultural Hotspot: We plan to:Improve the UI for a more polished experienceExpand the demo with more data and featuresAdd user-submitted spots and reviews,This is just the beginning. Cultural Hotspot has the potential to grow into a platform that celebrates and connects cultures across neighborhoods and generations.",
                        "github": "https://github.com/dev-alto/sfhacks-2025",
                        "url": "https://devpost.com/software/cultural-hotspot"
                    },
                    {
                        "title": "Deepify",
                        "description": "Evidence-based study strategies guide to help students deeply learn any topic in any level they want.",
                        "story": "Inspiration: Our goal is to help people study better with scientifically proven techniques such as active recalling and deep learning.What it does: Course ManagementSyllabus Processing - Upload your course syllabus (PDF/DOCX) and get an AI-generated study roadmapCustom Courses - Create and manage your own coursesProgress Tracking - Monitor your learning journey with visual progress indicators\n\ud83e\udde9 Smart QuizzesPre-lecture Quizzes - Test your knowledge before diving into topicsNotes-based Quizzes - Generate quizzes from your own notesHandwritten Notes Analysis \ud83d\udcdd - Take photos of your handwritten notes and let AI generate quizzesSpaced Repetition \u23f0 - Smart scheduling of review questions based on your confidence ratings\n\ud83d\udcdd Note TakingFlexible Formats - Add notes as text, upload PDFs/DOCXs, or take photos of handwritten notesContent Extraction - Automatic text extraction from uploaded documentsImage Support - Store and view images alongside your notes,How we built it: Frontend\n\u269b\ufe0f Next.js\n\ud83c\udfa8 Tailwind CSS\n\ud83e\uddf0 TypeScript\n\ud83d\udcca React components for interactive UI\nBackend\n\ud83d\udc0d FastAPI\n\ud83d\uddc4\ufe0f MongoDB Atlas\n\ud83e\udd16 Google Gemini AI\n\ud83d\udcc4 PDF & DOCX processingChallenges we ran into: implementing geminiAccomplishments that we're proud of: having a complete project with real life use cases that can help people learn betterWhat we learned: writing full stack app.What's next for Deepify: \ud83d\udcca Analytics dashboard with learning insights\n\ud83d\udc65 Collaborative study groups\n\ud83d\udd04 Integration with learning management systems\n\ud83d\udcf1 Mobile app version",
                        "github": "https://github.com/willi301/sfhacks2025",
                        "url": "https://devpost.com/software/deep-learner"
                    },
                    {
                        "title": "QuitBet",
                        "description": "QuitBet is an AI-powered assistant that helps users quit gambling by tracking behavior, analyzing spending, and offering daily support.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/quitbet"
                    },
                    {
                        "title": "Venus Postal Service",
                        "description": "Navigate a floating vertical city to deliver parcels in hazardous conditions. Use sorcery to gain an edge, but beware\u2014your power sparks fear among the inhabitants. Pick your routes wisely!",
                        "story": "Inspiration: The hackathon's theme sparks my imagination of a public service that we depend on yet seldom appreciate.\nAt least a few things serve as my reference:articles and subreddits on the conditions of US Postal Service, FedEx, and United Parcel ServiceKiki's Delivery ServiceJak II RenegadeDeath Stranding,What it does: You play as a postman who delivers and collect parcels on behalf of various clients in a large 2D sidescrolling map teaming with vertical structures and platforms. The gamespace provides the player with a few modes of transportation such as your trusty broom, a railcar, and a VTOL bike that relies on fueling stations for sustained use. You may also be assigned to fix parts of your transportation infrastructure. Be cautious of the thick atmosphere that becomes more toxic and incendiary the closer you are to surface level. Bypass bandits and hungry creatures that will eat your packages but be prepared to fight back if necessary. No bystander besides a fellow sorcerer will help you in an emergency.How we built it: I draw a UML diagram for implementing a polymorphic state machine and various subsystems the player will interact with to organize a list of features I can implement within the project's scopeChallenges we ran into: Accomplishments that we're proud of: What we learned: What's next for Venus Postal Service:",
                        "github": "",
                        "url": "https://devpost.com/software/venus-postal-service"
                    },
                    {
                        "title": "Ta.ai ",
                        "description": "A teaching assistant powered by AI to ease the process of creating study material for students.",
                        "story": "Inspiration: Our inspiration comes from witnessing our teachers/professors struggle with creating effective study material for students.What it does: This program takes study material and outputs an ai generated study guide or tests.How we built it: We used the Gemini API, supabase database, Nextjs Stack, and Vercel to deploy.Challenges we ran into: The database auth was very hard to configure and figure out.Accomplishments that we're proud of: We were able to successfully integrate Gemini API.What we learned: We learned how to use the Gemini API and how to use the Supabase Auth feature.What's next for Ta.ai: Next, we want to make it a community platform for teachers anywhere.",
                        "github": "",
                        "url": "https://devpost.com/software/ta-ai-f98xio"
                    },
                    {
                        "title": "Safe Walk",
                        "description": "Worried about walking alone at night? Traditional maps ignore safety. SafeWalk shows real-time crime data to guide you on the safest route home\u2014because getting there safe matters most.",
                        "story": "Inspiration: Have you ever walked alone at night and wondered if you were a potential victim of an untimely crime?We all know that San Francisco isn\u2019t the safest city. Every day, thousands of people walk home alone, unsure if their route is truly safe.Traditional maps prioritize distance, time, and traffic \u2014 but completely ignoresafety.We wanted to change that.That\u2019s why we builtSafeWalk\u2014 a real-time, crowd-sourced web application that helps pedestrians navigate safer routes by integrating live 911 data, user-submitted alerts, and intelligent route planning.What it does: SafeWalkhelps users find thesafest walking routesacross San Francisco by:Fetching real-time 911 dispatch data (e.g., assaults, robberies, suspicious activity)Analyzing multiple route options and highlighting the ones withminimal danger zonesAllowing users to submitcrowd-sourced alertsof suspicious activityVisualizing all safety incidents and route risk levels directly on an interactive map,How we built it: Frontend: React with Tailwind CSS for a responsive, clean UIMap & Routing: Leaflet.js for map rendering, Google Maps Directions API for route generationBackend: Flask API that fetches and processes live 911 incident dataData: Live feeds from San Francisco\u2019s 911 dispatch API, filtered and mapped using proximity analysisAI Feature: Chatbot guide to help users with questions about safety,Challenges we ran into: Parsing and filteringreal-time 911 dataeffectivelyMatching incidents to thecorrect route segmentsEnsuringmobile responsivenessand Google Maps autocomplete compatibilityVisual clarity when displaying large amounts of location-based data,Accomplishments that we're proud of: Successfully integratingreal-time safety datainto a functional mapBuilding a fullsafe route recommendation engineImplementing a workingalert feed, panic button, and custom route assistantDesigning amobile-first, user-friendly UI in less than 48 hoursHelping make city navigation more accessible, secure, and informed,What we learned: How to integrate real-time geospatial datasets with front-end mapsThe power ofproximity-based filteringfor personalized route planningHow to combine safety, utility, and accessibility in UX designWorking effectively in a team under tight deadlines,What's next for Safe Walk: Addinglive community alerts, AI-based risk prediction, and SMS alert optionsPartnering with local law enforcement or city services for real-world deploymentImproving the routing engine withweighted risk scoringfor even smarter recommendations,",
                        "github": "https://github.com/Dextron04/SafeWalk",
                        "url": "https://devpost.com/software/safe-walk-0427ew"
                    }
                ],
                [
                    {
                        "title": "Fake News Detector",
                        "description": "Our Fake News Detector is a smart tool that helps you quickly tell whether a news article is real or fake!",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/fake-news-detector-60cjez"
                    },
                    {
                        "title": "Foresight",
                        "description": "AI Rescue Platform to Locate Missing Children in Minutes, Not Hours.",
                        "story": "\ud83d\udccc Inspiration: According to theU.S. Department of Homeland Security, a child goes missing every40 secondsin the United States.Despite the magnitude of this crisis, the vast majority of cases remain unsolved\u2014mainly due to a lack of fast, actionable information for first responders. Without timely intel, cases can take weeks, months, or even years to resolve.\ud83d\udd0d What It Does: Foresightis an AI-powered platform that radically transforms how missing persons\u2014especially children\u2014are located.Processeslive CCTV footagein real-time (under 2 seconds latency)UsesYOLOv11to detect humans and extract features like clothing, age, accessoriesIntegrates withGoogle Geminito generate natural-language descriptions(e.g., \u201cteenager in blue hoodie holding a backpack\u201d)Displays alert matches against scraped Amber Alert databases,\ud83d\udee0\ufe0f How We Built It: FastAPI+Uvicorn\u2014 Lightweight, real-time API serverYOLOv11\u2014 Human and object detection from video framesGoogle Gemini\u2014 Attribute recognition & description generationOpenCV\u2014 Frame parsing and filteringMongoDB Atlas\u2014 Cloud-based video metadata and vector storage,Next.js\u2014 Blazing fast web interfaceTailwindCSS\u2014 Utility-first stylingShadCN\u2014 Modern UI component libraryLeaflet\u2014 Interactive camera mapFramer Motion\u2014 Smooth animations and transitions,\ud83e\udde9 Interface Features: Foresight integrates a Retrieval-Augmented Generation (RAG) model for chatting with the system and narrowing search parameters intuitively.Foresight scrapes real-time Amber Alert databases and lets users query for missing children by physical traits or metadata, instantly surfacing related cases.\u2699\ufe0f Challenges We Faced: Reducing pipeline latency to keep detection under 2 secondsHandling multi-stream video ingestion on the front endAligning object detection output with semantic search relevance,\u2705 Accomplishments We're Proud Of: Matched live video frames to user prompts and photos within thefirst 5 hoursDesigned an intuitive, clean interface for high-pressure search operationsFully optimized AI search pipeline for fast, meaningful visual results,\ud83d\udcda What We Learned: How to optimizeGeminito extract meaningful visual details from live videoHow to usevector searchfor probabilistic matching in real-world framesBest practices for building a real-time visual intelligence dashboard,\ud83d\ude80 What\u2019s Next for Foresight: Expand camera network access across California through municipal partnershipsIntegratereal-time 911 call transcription and sentiment detection,- Further optimize semantic visual search for even faster matching: Thanks for reading! <3",
                        "github": "https://github.com/vietnguyen2358/Foresight",
                        "url": "https://devpost.com/software/watchdog-24cj5r"
                    },
                    {
                        "title": "MoveMate",
                        "description": "MoveMate is a voice-guided workout app for the visually impaired, using audio cues and body tracking to help users exercise safely and hands-free.",
                        "story": "Inspiration: We wanted to create something inclusive \u2014 a fitness app that works for people who are visually impaired. Most workout apps rely on screens, which just isn\u2019t accessible. So we built MoveMate to give people the ability to work out safely, confidently, and independently using only their voice and movement.What it does: MoveMate is a voice-guided workout web app for the visually impaired. It uses audio instructions, voice commands, and real-time body tracking to help users complete basic exercises like jumping jacks, squats, and knee raises \u2014 all hands-free and screen-free.How we built it: We used HTML, CSS, and JavaScript for the frontend. For voice input, we used the Web Speech API. For pose detection and body tracking, we integrated TensorFlow.js with MoveNet. The UI was designed with accessibility in mind: high contrast, large fonts, and audio-first flow.Challenges we ran into: Making sure browsers allowed audio to autoplay without user interactionDelaying speech recognition until audio instructions were finishedDesigning for people who can\u2019t rely on visualsHandling real-time body tracking without making things too complex or distracting,Accomplishments that we're proud of: Fully working voice navigationReal-time form trackingA clean, accessible, and audio-based designA hands-free experience built for a community that\u2019s usually left out,What we learned: We learned how to design without relying on visuals, how to work with browser limitations for voice and audio, and how much thought goes into truly accessible experiences.What's next for MoveMate: Add more exercises and difficulty levelsLet users track their progressTest with real users in the visually impaired community and iterate from there,",
                        "github": "https://github.com/Armankd1/SFHacks2025",
                        "url": "https://devpost.com/software/movemate-3hrcj5"
                    },
                    {
                        "title": "Eventur",
                        "description": "Eventur is a swipe-based web app that helps people discover, create, and join local events and communities.",
                        "story": "Inspiration: Post-COVID, our generation is more digitally connected than ever\u2014but also more physically disconnected from real-world communities. We saw a need for a simple mobile app that makes it easier to create and discover events in one place. Most event apps today feel cluttered, confusing with fragmented features across different applications. We wanted something that feels clean, simple, and community-driven\u2014something that shows one event at a time, like a story feed, so users can actually focus and feel drawn to show up.What It Does: Eventur allows people to:Create, discover, and host local eventsSwipe through curated, AI-tagged eventsExpress interest or skip with a familiar swipe interactionRSVP and track events they\u2019re attendingGrow into or build new communities over time\nOur goal is to make it easy to connect through shared interests\u2014starting with individuals, and scaling into lasting communities.,How We Built It: Frontend: HTML, CSS, JavaScriptBackend: Golang (Go) using the Gin web frameworkDatabase: MongoDB Atlas for storing users, events, preferencesAuthentication: HTTP Basic Auth using Gin middlewareHosting: Deployed on Cloudflare Pages with .tech domain\n-Tags : Ai Gemni,Challenges We Ran Into: Implementing authentication and MongoDB integration from scratchGo's Gin middleware for authentication was very barebones\u2014Marc faced issues with session management and access logic under pressureLearning and using Go, HTTP Auth, and MongoDB Atlas for the first timeTime constraints while juggling multiple tracks (business Pitch, Database , and AI , .tech)Making UX decisions under a compressed schedule while ensuring functionalityNo one in our group had used Go before.,Accomplishments That We're Proud Of: Built and deployed a hosted MVP within 48 hoursImplemented swipe, event filtering, and account functionalityConducted interviews with 14 users to shape our solutionValidated our business model and user needLearned new technologies as a team and kept each other motivated\u201cWhen HTTP Auth finally worked, it was the best moment of the weekend.\u201d \u2013 Marc,What We Learned: How to work across roles\u2014frontend, backend, design, research\u2014under pressureHow to use Go, MongoDB, and deploy a full-stack app from scratchThe importance of user research in guiding features and pitch directionHow to present a business case, pivot features based on feedback, and deliver both a working prototype and a compelling story,What's Next for Eventur: Improve our authentication and user session systemAdd AI integration for automatic event tagging and description generationEnhance the UX and visuals for swipe interactionget feedback with local colleges and hobby communities,",
                        "github": "",
                        "url": "https://devpost.com/software/eventur"
                    },
                    {
                        "title": "VR_OM_Lift",
                        "description": "VR_OM_Lift is a VR-controlled forklift prototype that boosts operator safety and remote access using Unity, Raspberry Pi, and real-time video streaming.",
                        "story": "Inspiration: VR_OM_Lift was born out of a desire to improve operator safety and accessibility. I wanted to create a system where someone recovering from injury \u2014 or living with physical limitations \u2014 could still control industrial equipment remotely, like from home.As president of BitBots (our hardware club) and VP of the 3D printing team, I\u2019ve helped others build. But this time, I wanted to test myself \u2014 solo \u2014 across electronics, embedded software, Unity, and VR integration.The concept worked beautifully\u2026 until our university Wi-Fi tried to stop me from using my own Quest headset. Lesson learned: security policies are the true final boss.What it does: VR_OM_Lift is a remote-controlled forklift powered by a Raspberry Pi, driven via VR input using a Meta Quest headset. It receives directional and lift commands via sockets and streams live camera footage to the operator in real time.The system also supports a fallback PC controller for debugging or non-VR use.How we built it: This prototype includes:A Raspberry Pi motor server (DC + stepper motor control)A USB camera with MJPEG streamingA Unity app using OpenXR to map VR controls to JSONA Python GUI controller for keyboard controlA full custom circuit using L298N + ULN2003 motor driversCustom 3D-printed parts (with more in progress)The code, rig, and wiring were built from scratch during the hackathon.Challenges we ran into: Getting Unity + Meta Quest + OpenXR input mappings to workRaspberry Pi GPIO signal timing and stepper motor limitsSchool Wi-Fi completely blocking local device communication :'(Manually setting USB camera paths when they change at bootAccomplishments that we're proud of: Building a full robotic system from hardware to VR interface in a weekendCreating a stable motor control + safety system with lift limitsImplementing real-time video streaming with fallback controlTesting everything independently with working demos and GUIWhat we learned: Unity + XR bindings are way more nuanced than expectedThe Raspberry Pi GPIO is powerful, but timing mattersHaving both VR and non-VR control paths is a lifesaver during debuggingGood documentation + bash scripts = less 2AM troubleshootingWhat's next for VR_OM_Lift: Re-integrating VR now that we\u2019ve learned how to dodge Wi-Fi blockersAdding stereo cameras for depth and better visibilityIntegrating a HUD inside Unity with a live video feedBuilding a full-size (or at least bigger) warehouse telepresence robo",
                        "github": "https://github.com/Jacob9610/VR_OM_Lift#",
                        "url": "https://devpost.com/software/vr_om_lift"
                    },
                    {
                        "title": "BrowseAble",
                        "description": "Tech for Good. AI for all. Because the Web Should Work for Everyone!",
                        "story": "Inspiration: When we encountered the \"Tech for Good\" theme and the Accessibility track, we asked ourselves: Is AI truly for everyone? For neurodivergent individuals\u2014those with ADHD, Dyslexia, Sensory Processing Disorder, and Blindness\u2014the web can be a maze of challenges. Yet, how often do we see web content that truly addresses their unique needs? The answer: rarely, if ever.That's where BrowseAble comes in. Our Chrome extension is designed to transform the web into a more inclusive space for people with diverse neurotypes. It\u2019s built on the simple but powerful idea that accessibility should go beyond just being a feature \u2014 it should be foundational. With BrowseAble, we're asking: What if AI could empower individuals by making the web truly adaptable to their needs?Imagine a world where someone with ADHD can engage with web content free of distractions, or where someone blind can instantly hear the heart of a website in a simple, clear spoken format. Picture a dyslexic user reading effortlessly through a layout and font tailored specifically to their style. BrowseAble makes all of this possible.But we didn\u2019t stop there. BrowseAble also serves the caretakers of neurodivergent individuals. Our caregiver dashboard is a powerful tool designed to manage multiple users and their preferences\u2014particularly for those who may struggle to operate technology independently. This feature enables caretakers to easily track users' activities, preferences, and interactions with the web.The dashboard doesn\u2019t just stop at management; it also offers AI-driven suggestions for activities and tasks that would benefit each user based on their unique needs. Moreover, it provides helpful resources that can assist caregivers in carrying out those activities more effectively. By offering real-time insights into user behavior, BrowseAble ensures that both users and caretakers are empowered with the information they need to foster better engagement and support. This additional layer of functionality creates a more informed and seamless experience for everyone involved.Inspired by the belief that technology should be designed for everyone, not just the majority, BrowseAble is our answer to the question: Can AI create a more universally accessible web? Our mission is to make the internet a place where all users\u2014regardless of neurotype\u2014can feel seen, heard, and empowered.What it does: BrowseAble is a Chrome extension that adapts web content based on users\u2019 neurotypes. It transforms cluttered, overwhelming web pages into focused, digestible layouts tailored to ADHD, Dyslexia, and Blind users.For ADHD: Distraction-free layout, bulleted breakdowns, and visual anchors.For Dyslexia: Font and spacing adjustments with simplified sentence structures.For Blind: Clear narrations using Chrome TTS and simplified text summaries.,It also includes acaregiver dashboardthat lets caretakers:Manage multiple users.Set preferences and instructions per user.Use AI for personalized insights and activity suggestions based on each user\u2019s neurotype.,How we built it: BrowseAble is a Chrome extension that adapts web content based on users\u2019 neurotypes. Once the user selects their preferred neurotype (ADHD, Dyslexia, or Blind), the background script sends the page\u2019s content to Gemini API, which analyzes it and generates a custom DOM structure. The content script then injects this personalized layout into the webpage as a dynamic overlay. For Blind users, we use Chrome\u2019s TTS API to read the content aloud, making it more accessible. This approach ensures that each neurotype gets the most suitable web experience, enhancing engagement and accessibility.The Caregiver Dashboard allows caretakers to manage multiple users, set preferences (e.g., simplify UI, declutter), and choose modes like ADHD or Autism. It provides AI-driven insights into user activities, recommending helpful tasks and resources. The dashboard uses the same technologies to monitor and support neurodivergent users, ensuring both users and caretakers have the tools needed for a more inclusive web experience.Challenges we ran into: Understanding the unique needs of neurodivergent users was a key challenge. To create a solution that truly benefits individuals with ADHD, Dyslexia, and Blindness, we first conducted research and testing to understand how these conditions impact web navigation. Using the Google Gemini API, we analyzed web content and identified the most effective ways to adapt it for each neurotype, applying JavaScript to dynamically adjust the user interface in our extension.Once the requirements were clear, we focused on crafting the right prompts to guide Gemini in generating accurate content modifications. The key here was precision\u2014ensuring Gemini would provide relevant layout adjustments and simplifications that aligned with each neurotype\u2019s needs. The content script passed these tailored modifications into the page, creating an adaptive browsing experience for each user.Another significant challenge was ensuring we could reliably extract the right content from various websites. Using web scraping and DOM manipulation, we filtered out irrelevant elements, such as ads and footers, ensuring only the most important content was passed to Gemini. This allowed us to generate personalized, accessible, and distraction-free layouts for users across diverse websites.Accomplishments that we're proud of: Created a fully functional, neurotype-aware Chrome extension in under 48 hours.Built a caregiver dashboard that supports multiple users and personalized accessibility settings.Used Google Gemini to dynamically rewrite web content based on cognitive profiles.Delivered adaptive overlays and voice narration for Blind users using Chrome\u2019s TTS API.Proved that AI can create real-time, tailored accessibility experiences without hardcoding every page.,What we learned: Building BrowseAble has been a transformative experience that redefined how we approach accessibility and AI. When we started, we didn\u2019t just want to build a tool \u2014 we wanted to create something that could genuinely make a difference, especially for neurodivergent individuals. It was this mission that led us to leverage cutting-edge technologies like Gemini, a generative AI tool, to reshape the way we think about accessible web browsing.What we learned early on was that accessibility isn\u2019t just a set of features \u2014 it\u2019s a mindset. And technology, like Gemini, is key to making that mindset a reality. Gemini allowed us to adapt web content in ways that address the unique needs of people with ADHD, dyslexia, and visual impairments, helping us break away from the traditional tech solutions that don\u2019t always cater to these diverse needs. The experience taught us that AI isn\u2019t just for traditional problem-solving; it\u2019s a powerful tool for reshaping user experiences based on specific needs, and it's especially useful in making the web truly adaptable. This approach also pushed us to think about AI for all, as technology should empower everyone, regardless of neurotype.An important lesson came when we created dynamic layouts using AI. It wasn\u2019t just about throwing technology at the problem \u2014 it was about understanding the specific challenges each user faces. By crafting the right prompts for Gemini, we learned the importance of user-centric design in AI. It\u2019s a delicate balance: while AI can generate amazing solutions, it\u2019s the human understanding of the user that makes the results truly impactful. This underscored the idea that accessibility is not a one-size-fits-all solution.We also took a step further with the creation of the caregiver dashboard, which allows caretakers to manage and track multiple users. This taught us that accessibility can extend beyond just the individual \u2014 it can support the people who help care for them too. By providing AI-driven insights and personalized suggestions, we were able to improve both the users' experience and their caretakers' ability to provide better support.Ultimately, BrowseAble was an exercise in pushing the boundaries of what technology can do. The use of Gemini in such a dynamic way is an example of how emerging tech can drive real-world change, showing that AI can be used to solve accessibility challenges in ways we never thought possible before. We realized that tech for good isn\u2019t just about solving problems\u2014it\u2019s about rethinking how technology can be inclusive and work for all. This project reinforced our belief that AI, when paired with empathy and user understanding, can lead to groundbreaking solutions.What's next for BrowseAble: Add more neurotypes and expand to motor/learning disabilities.Integrate with learning platforms (Google Classroom, Coursera) for accessible education, schools, caregivers, and accessibility-focused orgs.Build real-time caregiver dashboards with co-browsing and guidance tools,Built With: Gemini API (Google Generative AI)\u2014 DOM generation based on neurotypeChrome Extensions APIs (manifest v3)JavaScript (ES6)HTML5 + CSS3Chrome TTS API\u2014 for speech output in Blind modeFirebase(for storing user preferences and onboarding content),Try it Out: GitHub Repository:https://github.com/aprajita27/BrowseAbleCaregiver Portal:https://browseable-586fa.web.app/Youtube Demo:https://youtu.be/lRPK5BmfzVA,",
                        "github": "https://github.com/aprajita27/BrowseAble",
                        "url": "https://devpost.com/software/browseable"
                    },
                    {
                        "title": "HerVoice",
                        "description": "HerVoice is an chatbot designed to empower women in STEM. It provides an neutral, anonymous space for mentorship, emotional support, and navigating workplace challenges.",
                        "story": "Inspiration: HerVoice was inspired by those moments when you\u2019re not sure if you should speak up or just let it go. It\u2019s made to be a supportive, smart sidekick that helps you figure things out without the pressure.What it does: HerVoice is a private, no-judgment chatbot for anybody that can benefit from a wise ear at a critical moment. You can talk through tricky situations, ask anything, and get clear, supportive guidance\u2014all totally confidential.How we built it: Using Streamlit, PostgreSQL, LangChain, Google GeminiChallenges we ran into: Learning about strict types with LangGraph, learning about Pydantic classes helpedAccomplishments that we're proud of: we made it all the way to San Francisco!! We learned how to manipulate postgres datastores containing containing Google models/text-embedding-004 embeddingsWhat we learned: we learned how to get structured data from an LLMWhat's next for HerVoice: public access would be awesome! chromadb implementation would be great to move away from a local postgres database",
                        "github": "https://github.com/jovalie/sf-hacks-2025",
                        "url": "https://devpost.com/software/hervoice-rxq8a7"
                    },
                    {
                        "title": "Formi: Automating Immigration",
                        "description": "Immigration forms are complex but attorneys are expensive. Formi provides quick and simple DIY document completion through plain language questions at the fraction of what our competitors charge.",
                        "story": "Inspiration: We are a team of international students that have personally felt the burdens of the USCIS complicated bureaucratic process. As such, we deeply resonate with other immigrants who have limited English capabilities and limited access to support resources. Formi was our vision of the intersection of technology and positive social impact.What it does: Over 50 million immigrants file immigration documents annually in the US alone, forming a $12B+ underserved market. These forms are often complex for immigrants, especially those with limited Out platform aims to provide a quick, simpleHow we built it: We developed an interactive prototype mockup of our platform using Figma. Our team heavily focused on understanding the problem, ideating a realistic and impactful solution, and doing market research because we were targeting the Innovation & Entrepreneurship track.Challenges we ran into: Performing market research in a field that is not well documented and coming up with a differentiated idea that strongly aligned our goals were the largest challenges we faced. Through long and thorough discussions, we finally came up with this idea that we were satisfied with.Accomplishments that we're proud of: Coming to this hackathon empty handed and leaving less than 3 days later with a well-formed business idea and pitch is certainly a feat our team celebrates. Moreover, we've learnt many things together from overcoming our challenges and strengthened our team bonding throughout the event.What we learned: We've acquired the skills necessary to run through all the preliminary steps of designing and refining a product idea: ideating, prototyping, iterating and in-depth researching. Additionally, we've also learnt how to create a powerful and persuasive business pitch by leveraging concision, clarity and confidence.What's next for Formi: Automating Immigration: With a solid business foundation in mind, building the first iteration of our platform with code would be the next logical step we plan to pursue.",
                        "github": "",
                        "url": "https://devpost.com/software/formi-automating-immigration"
                    },
                    {
                        "title": "Ecolocator",
                        "description": "We are the AirBnB for institutions looking to composting and Recycling.",
                        "story": "Inspiration: We all care about the environment. We see throughout our community and beyond, waste management is an issue. We specifically aim to reach out to the institutions around the bay that generate the most waste.: What it does: Through this app, we bridge the gap between the companies and institutions that want to compost and the composting facilities, aiming to build a strengthened community toward the effort of living a sustainable life.: How we built it: We used ReactJS as our foundation for this project. We also used Gemini for the AI chatbot that aims to guide institutions who use our app and inform them more about composting and sustainability if they need more information. We also used React Leaflet for the map that shows different composting facilities around the Bay Area. Finally, we used MongoDB to handle the database. Therefore, companies and facilities can both sign up and login to their accounts.: Challenges we ran into: We had a difficult time with configuring the database with MongoDB. We also had trouble with configuring the AI bot. However, through our perseverance, we were able to configure these successfully.: Accomplishments that we're proud of: We are proud of using new tools that we never used before, such as MongoDB and Gemini AI. We are happy to learn new things.: What we learned: We learned how to configure a database using MongoDB, and we learned how to configure a chatbot using Gemini AI. We also learned the importance of team work, communication, and perseverance. Despite some of our ideas being different, we decided to compromise and come up with a solution that all three of us agreed on. Additionally, we also didn't give up despite all the challenges we faced building this website. We stayed optimistic through the difficult portions, and as a result, we developed our project successfully.: What's next for Ecolocator: Ecolocator will continue to expand out of the Bay Area. It will spread through other regions of California, other states throughout the US, and eventually other countries and companies there who are also looking to build a strong sustainable community. Thus, we will also translate to other major spoken languages like Spanish, Mandarin, and French.:",
                        "github": "https://github.com/davidcayapan/EcoLocator/tree/main",
                        "url": "https://devpost.com/software/ecolocator"
                    },
                    {
                        "title": "OpenMind OM1",
                        "description": "We integrated AiNex and Muto Hexapod with OM1 for real-time video interaction using remote access, gesture detection, and LLMs!",
                        "story": "InspirationWe wanted to enhance the interactivity of the HiWonder AiNex humanoid robot as well as the Muto HexaPod by integrating it with OpenMind's OM1 platform, enabling responsive, video-driven communication powered by AI.What it doesThe system enables AiNex to detect human presence via its onboard camera, interpret visual input using a Large Language Model (LLM), and respond with physical gestures or text. It supports real-time video streaming and remote operation.How we built itWe set up a persistent SSH connection using Tailscale, installed OM1 in a Docker container on the AiNex and Muto Hexapod's Raspberry Pi, configured the robot with a JSON5 config file, and launched the full ROS stack on boot to initialize sensors, camera, and app logic.Challenges we ran intoOvercoming school network restrictions required using a mobile hotspot.\nResolving Docker's internet access issues through manual DNS configuration.\nAligning ROS1 and ROS2 environments for full system compatibility.Accomplishments that we're proud ofSuccessfully integrated a multimodal AI runtime on a humanoid robot.\nEnabled real-time, camera-based interactions using LLMs.\nAutomated the robot\u2019s full boot and interaction stack.What we learnedWe deepened our understanding of ROS architecture, containerized AI systems, VPN-based remote networking, and real-time robotic perception powered by AI language models.What's next for OpenMind OM1We plan to expand AiNex and Muto HexaPod's capabilities with more advanced gestures, voice integration, multi-agent collaboration, and deployment to other robots beyond AiNex and Hexapod.",
                        "github": "",
                        "url": "https://devpost.com/software/openmind-om1"
                    },
                    {
                        "title": "NotedAI",
                        "description": "NotedAI: Turn conversations into insights and actions - AI-powered transcription and summarization that lets you focus on listening, not note-taking, while automatically scheduling your meetings.",
                        "story": "Inspiration: During our studies and professional meetings, we were constantly torn between active participation and taking comprehensive notes. This struggle led us to create NotedAI, a solution that leverages AI to handle note-taking so users can be fully present in their conversations.Our team observed that while audio recording tools exist, they often lack intelligent processing capabilities to make the content immediately useful. Additionally, enterprise users require heightened security for sensitive discussions. These insights shaped our vision for NotedAI.What it does: NotedAI is an AI-powered web application that:Captures audio through direct recording or file upload.Transcribes speech into text using Google Speech-to-Text.Generates concise, bullet-pointed summaries using Gemini AI.Enables users to ask questions about the content with AI-powered answers.Secures private conversations with enterprise-grade encryption (Midnight integration).Provides a personalized dashboard to manage and search all sessions.Offers wellness tips based on usage patterns to promote better meeting habits.Automatically detects meeting scheduling mentions in conversations and creates Google Calendar events.How we built it: NotedAI is built with a modern tech stack focusing on security, scalability, and user experience:Backend: We developed a Node.js/Express server with RESTful APIs for session management, transcription, and AI processing.Frontend: We created a responsive React application with Tailwind CSS for a clean, intuitive interface.Database: MongoDB Atlas provides flexible document storage with search capabilities.Authentication: We implemented Google OAuth for secure, seamless user authentication.AI Integration: We leveraged Google's Speech-to-Text API for accurate transcription and Gemini 1.5 Pro for summarization and question-answering.Security: For enterprise users, we integrated with Midnight for encrypted storage of sensitive transcripts.Wellness: We developed an analytics system that tracks usage patterns and provides personalized wellness recommendations.Calendar Integration: We integrated with Google Calendar API to automatically extract meeting times from transcripts and create calendar events without user intervention.Challenges we ran into: Real-time Audio Processing: Implementing efficient audio capture and processing directly in the browser required overcoming various browser-specific limitations and permissions.Summary Quality: Generating concise yet comprehensive summaries required careful prompt engineering with Gemini API to ensure the most important information was captured.Enterprise Security: Simulating the Midnight secure storage integration required implementing proper encryption while maintaining a seamless user experience.Contextual Question Answering: Teaching Gemini to answer questions specifically based on the transcript context, rather than its general knowledge, required careful parameter tuning.User Experience: Balancing feature richness with simplicity was challenging - we wanted to offer powerful capabilities without overwhelming users.Natural Language Understanding: Accurately detecting and parsing meeting scheduling information from natural conversation required sophisticated pattern recognition and contextual analysis.Accomplishments that we're proud of: Creating a fully functional end-to-end solution that addresses a real problem we face.Successfully integrating Google Speech-to-Text for accurate transcription across various accents and audio qualities.Implementing the Gemini AI integration that delivers remarkably useful summaries and contextual answers.Designing an intuitive UI that makes complex functionality accessible.Building enterprise-grade security features with proper encryption.Implementing intelligent automation that turns conversation into calendar events with no manual steps.What we learned: Through this project, we gained valuable experience with:Modern authentication patterns and security best practices.Working with audio processing in web applications.Prompt engineering for generative AI models.Designing systems with both consumer and enterprise use cases.Creating responsive web interfaces that handle complex workflows.What's next for NotedAI: We have an exciting roadmap ahead:Browser extension for capturing audio directly from video calls.Mobile application for on-the-go recording.Team collaboration features for shared access to transcripts.Advanced analytics to identify meeting patterns and suggest improvements.Integration with calendar applications for automatic meeting recording.Additional language support for global accessibility.",
                        "github": "https://github.com/rutujanemane/NotedAI",
                        "url": "https://devpost.com/software/notedai"
                    },
                    {
                        "title": "StudyBuddy Connect",
                        "description": "StudyBuddy Connect \u2013 Smart learning support powered by AI and real people, bridging the education gap one student at a time.",
                        "story": "Inspiration: As a first-generation, low-income student, I often faced barriers in my academic journey that stemmed not from a lack of motivation, but from a lack of access. Growing up, I didn't have the resources many of my peers took for granted\u2014reliable internet, private tutoring, or structured academic support outside of school.Just before graduating high school, I learned about a peer tutoring program run by my school. I joined as a volunteer tutor, excited to help others, but I noticed something unexpected: there were actually more tutors than tutees. This showed me that the issue wasn\u2019t a lack of people willing to help\u2014it was that the program's scope was too narrow.That\u2019s what sparked the idea for StudyBuddy Connect: what if we could build a platform that expanded beyond a single school or district, allowing students anywhere to receive help from both AI-powered tools and a wider network of volunteer tutors? It would allow us to utilize the goodwill of volunteers more effectively, while also using AI to fill in the gaps.What It Does: StudyBuddy Connect is a hybrid learning platform designed to support students through both AI-driven assistance and real human interaction. The platform includes:AI Study BuddyAn always-available virtual tutor powered by OpenAI and Gemini that helps students with homework, explanations, and study tips.Peer Tutor MatchingConnects students with volunteer tutors based on subject expertise, availability, and academic goals.Q&A ForumA moderated space where students can ask questions and get responses from both tutors and the broader community.Progress TrackingPersonalized dashboards that monitor learning progress, study streaks, and completed sessions.Achievement SystemA reward system that encourages consistent learning and community participation through badges and milestones.AI-Powered Educational GamesText-based RPGs and interactive mini-games powered by Gemini, designed to reinforce knowledge in a fun and immersive way.,How We Built It: Frontend:Built with React.js and styled using Chakra UI for a responsive and accessible interface.Backend:Firebase Functions handle real-time operations, user management, and server-side logic.Database:Firebase Firestore stores user data, tutor sessions, forum posts, and progress tracking metrics.Authentication:Firebase Auth provides secure login and role-based access for students, tutors, and admins.AI Integration:Integrated OpenAI API and Gemini for AI tutoring features and custom educational games.Game Features:Built educational RPGs using Gemini\u2019s narrative capabilities to simulate real-world scenarios and reinforce key academic topics.,Challenges We Ran Into: Balancing Supply and Demand in Tutor MatchingCreating a system that efficiently matches tutors and students while maintaining fairness and low wait times.Managing AI Costs and ResponsivenessAI services can be expensive, so we had to implement rate limits and fallback mechanisms to manage cost and maintain performance.Ensuring AccessibilityThe platform needed to perform well even on low-end devices and in areas with unreliable internet access.Driving EngagementIt was important to design features that made the experience enjoyable and habit-forming without distracting from learning outcomes.,Accomplishments That We're Proud Of: Successfully built a platform that bridges the gap between AI support and human mentorship.Created a real-time, scalable tutor-student matching system.Developed interactive AI-powered learning games that make academic content engaging.Prioritized inclusivity and accessibility in both design and implementation.,What We Learned: Building technology for underserved communities requires careful consideration of access, engagement, and equity.Full-stack development using Firebase allowed for rapid iteration and scalability.Gemini\u2019s AI capabilities can be used not only for direct tutoring but also for creating educational narratives that deepen understanding.Designing with empathy and user feedback results in more impactful features and a better user experience.,What's Next for StudyBuddy Connect: Mobile and Progressive Web App SupportLaunching a fully responsive version for smartphones and tablets.Smarter PersonalizationUsing learning data to personalize AI responses, game content, and tutor recommendations.Tutor Verification and FeedbackAdding tutor reviews, training resources, and certification options to improve quality.Offline Mode and Data SyncAllowing students to download lessons and access AI features offline with sync capabilities.Multilingual and Global ExpansionOffering language support and region-specific content to help more students around the world.,",
                        "github": "https://github.com/dsanh14/StudyBuddyConnect",
                        "url": "https://devpost.com/software/studybuddy-connect"
                    },
                    {
                        "title": "NutriAI",
                        "description": "NutriAI builds smart, personalized meal and hydration plans based on your body, lifestyle, and needs\u2014even adapting to dietary restrictions or what\u2019s in your kitchen.",
                        "story": "Inspiration: We were inspired by how complicated and overwhelming meal planning can be\u2014especially for people managing dietary restrictions, medical conditions like diabetes, or limited food access. Our goal was to create something smart and flexible that helps users make healthier choices with less effort.Brandon, a member of our team, was born with a congenital kidney condition called Hydronephrosis, which causes a buildup of urine in the kidneys. This condition increases the risk of urinary tract infections, kidney stones, and potentially kidney disease later in life. Growing up, his parents constantly reminded him to drink water to help dilute urine and prevent fluid buildup\u2014but like many kids, he often forgot. He pitched this idea to Justin and Aaron, the two other members of the team and they added the idea of also including a macronutrient planner, as they all share similar interest in physical activities. They realized there was an additional challenge in maintaining a high-protein diet, which can add further strain to kidneys if not balanced with proper hydration. Combining this with our busy lifestyle, we realized how helpful a personalized app that could schedule water intake reminders\u2013something that could also be connected with a smartwatch\u2013and adjust hydration and protein goals based on anyone's diet, activeness, and most importantly morbidity. Aaron, another member of our team has always had trouble gaining weight and with this new program it should help him be able to bulk up more to succeed in his body transformation. Justin, the final member of the team has been big into dieting and nutrition. However he always found it hard keeping track and staying on schedule when dieting. With this program he hopes to get back on his fitness goals.What it does: NutriAI is an AI-powered tool that creates personalized daily nutrition and hydration plans. Based on a user\u2019s gender, age, weight, height, and activity level, NutriAI generates:A recommended daily water intakeTarget calorie and macronutrient goals (carbs, fats, proteins)A full-day meal plan that aligns with those goalsCustomizable and editable plans using the Gemini APIIngredient-based planning (builds meals using only the foods you have)Optional support for dietary restrictions like diabetes,What we have right now: Generates a recommended water intake scheduleComputes daily nutrition goals: calories, carbs, fats, and proteinsBuilds meal plans that can be assigned to specific days of the weekAllows editing and regenerating of plans using the Gemini APIUses USDA\u2019s FoodData Central\u2014a dataset of 500,000+ food products\u2014filtered and compiled into a streamlined CSV file for efficient processingBuilt an early version of a mobile app frontend usingKivy, making NutriAI accessible beyond the terminal,How we built it: We used:Pythonto manage input logic, calculations, and file handlingGemini APIfor generation and revision of meal plansFoodData Centraldataset for nutrition informationKivyto build a cross-platform mobile app frontendVS Codefor development,We designed prompts for Gemini that allow flexible user interactions, such as tailoring plans for diabetes or adjusting meals to pantry inventory.Challenges we ran into: Cleaning and filtering a massive dataset to ensure high-quality nutrition dataTuning Gemini prompts to be accurate, consistent, and medically safeDesigning flexible logic for real-world inputs, including unit conversions and missing dataIntegrating frontend elements with backend logic in a way that feels smooth and user-friendly,Accomplishments that we're proud of: Built an end-to-end working prototype that generates complete, customized nutrition plansCreated support for both dietary conditions and limited ingredient inputBegan developing an app withKivyto bring the experience to mobileSuccessfully integrated AI with real-world data in a meaningful and practical way,What we learned: How to effectively prompt-tune large language models for user-specific tasksBest practices for managing and using large datasets with real-world nutrition dataHow to collaborate quickly and efficiently under a tight timelineHow to build mobile apps usingKivyand connect frontend and backend logicWays to design user-first health tech tools that prioritize adaptability,What's next for NutriAI: Finish building theKivy-based mobile appfor a fully interactive experienceEnable account creation and save progress across daysExpand cultural food options and recipe variety,",
                        "github": "https://github.com/brandonllum/NutriAI/tree/main",
                        "url": "https://devpost.com/software/nutriai-io3z6q"
                    },
                    {
                        "title": "EduAI",
                        "description": "Making homework help accessible to grade school students by providing and step-by-step solutions. EduAI user interface makes it easy for students of all ages to navigate and learn.",
                        "story": "\ud83c\udf1f Inspiration: We were inspired by the growing need for personalized and accessible educational tools that adapt to different learning styles. Traditional classrooms often fall short for students who benefit from interactive, step-by-step guidance or who have learning differences. We set out to build a platform that fuses the power of AI with gamified learning\u2014making education not only more accessible, but also more engaging and adaptable.\ud83e\udde0 What it does: Our project has two main components: the Tutor section and the Gamification section.The Tutor section includes three AI-powered tutors: one for homework help, one for simplifying complex topics, and one tailored for accessibility and diverse learning needs. These tutors provide step-by-step guidance, encouraging critical thinking instead of just giving answers.The Gamification section includes interactive educational games:Charades, where users guess academic concepts using AI-generated clues.Detective Game, where players solve subject-related mysteries by answering questions.,Together, these features offer an engaging and adaptive learning experience that makes education fun, effective, and inclusive.\ud83d\udee0\ufe0f How we built it: To build the Tutor section, we used the Gemini API for all three tutors, designing each one with targeted prompt engineering. We implementedstart_chatfrom Gemini to maintain conversational context, allowing tutors to reference previous exchanges and provide coherent, step-by-step support.For the Charades game, we used prompt engineering with OpenAI to create an interactive AI game host, tracking progress withuser_scoreandai_scorevariables. The Detective Game also used prompt engineering with Gemini, carefully crafting prompts that guided the AI\u2019s narrative and adjusted dynamically based on user input.The front-end was built using HTML, offering a clean, accessible interface that supports the platform\u2019s interactive features.\ud83e\udde9 Challenges we ran into: One major challenge was getting the tutors to retain context during conversations. We solved this by using Gemini\u2019sstart_chatfunction, which allowed for persistent, contextualized dialogue that adapted to users\u2019 prior messages.\ud83c\udfc6 Accomplishments that we're proud of: We\u2019re proud of building a fully functional, AI-powered learning assistant within a short time frame. Combining tutoring with gamified learning created a richer experience for diverse learners. We\u2019re especially proud of the adaptive storytelling in the Detective Game, the clean user interface, and the seamless integration of multiple AI APIs.\ud83d\udcda What we learned: We learned how critical prompt engineering is for crafting effective and interactive AI experiences. We also gained hands-on experience managing conversational context using Gemini\u2019sstart_chat, and learned how to design educational games that are both fun and pedagogically sound. This project challenged us to strike a balance between technical complexity and user experience.\ud83d\ude80 What's next for EduAI: Looking ahead, we plan to add more games, incorporate voice and gesture controls for improved accessibility (using tools like Deepgram and MediaPipe), and introduce an Adventure Mode with AI-generated visuals. We also aim to implement real-time feedback analytics to help students track their progress and reflect on their learning journey.",
                        "github": "https://github.com/cyu60/mentor-matching",
                        "url": "https://devpost.com/software/eduai-em0ud7"
                    },
                    {
                        "title": "Foresight",
                        "description": "See the Past, Shape the Future.",
                        "story": "\ud83c\udf1f Inspiration: Visual impairment affects2.2 billion peoplearound the world, often resulting in struggles with mobility, access to education, and a negative impact on mental health. We strive to bridge the gap by creating anintelligent personal assistantcapable of visual recognition, object memory, and contextual understanding.The idea we had for the app was something capable ofserving as the eyesof visually impaired users\u2014reminding them when and where certain items are, as well as alerting them to any concerning details.\ud83e\udde0 What It Does: Foresightis a personal assistant app designed to help usersremember and interact with their surroundings.By continuously capturing visual data and analyzing it in real-time, it allows users to ask questions like:\u201cWhere did I leave my keys?\u201d\u201cWhat did I do last week?\u201d,Foresight leverages the power ofAI and computer visionto providecontextual memorythat users can rely on in their daily lives.\ud83d\udee0\ufe0f How We Built It: Backend: Built withPython, managing data processing, interaction withGemini, and integration of visual context.AI Integration: ImplementedGeminito help analyze real-world data, enabling accurate, context-driven responses.Database: UsedMongoDBto store chat history, visual data, and user information.API: Implemented withFastAPIfor real-time communication between the app and server.Frontend: Developed usingNext.jsfor a dynamic, responsive interface.Text-to-Speech: IntegratedElevenLabstechnology to provide a human-like assistant voice.,\ud83d\udea7 Challenges We Ran Into: One of the major challenges was ensuringreal-time processingof the live video feed while maintaining app performance.We had to:Manage a continuous stream of image data.Optimize visual recognition for speed and accuracy.,Throughdata consolidationand backend optimizations, we were able to meet these challenges.\ud83d\udcda What We Learned: Building Foresight taught us how to:Seamlessly integratemultiple cutting-edge technologies.Fine-tuneAI-driven visual contextusing Gemini.Efficiently managereal-time data streaming.Balanceperformance and scalability.,\ud83d\ude80 What's Next for Foresight: We aim to:Increase accessibility withvoice-only navigationReal-timealerts for danger and time-sensitive informationEnhancevisual recognition accuracyfor more nuanced environments.Introducepersonalized features, like customizable memory settings.Addpredictive suggestionsbased on user behavior.,",
                        "github": "https://github.com/sf-hacks-2025-project",
                        "url": "https://devpost.com/software/foresight-lz0jt4"
                    },
                    {
                        "title": "Sign Learn ",
                        "description": "An AI-powered web app that teaches ASL through real-time sign recognition, webcam-based practice, and instant feedback\u2014making ASL learning accessible, fun, and inclusive.\u201cSign. Learn. Connect.\u201d",
                        "story": "This project was inspired by a desire to bridge the communication gap between the Deaf and hearing communities. Learning American Sign Language (ASL) can be intimidating or inaccessible for many, especially without hands-on guidance. We wanted to build something that makes ASL learning approachable, engaging, and truly interactive\u2014with no special equipment or prior experience required.The fundamentals of computer vision and how it can be applied to gesture recognitionHow machine learning models like Random Forest can classify hand signs effectivelyReal-time webcam integration using OpenCV and MediaPipeBuilding accessible web apps that are intuitive and inclusive,Hand Detection: Used Google\u2019s MediaPipe to track hand landmarks from the webcamSign Classification: Trained a Random Forest model to recognize ASL alphabet signs based on hand landmark positionsInterface: Developed a Python-based GUI for desktop, and are now working on converting it to a browser-based app for broader accessibilityFeedback Loop: Implemented real-time prediction display and user prompts to help learners practice and improve,Ensuring high accuracy across diverse lighting conditions and hand sizesDistinguishing similar-looking ASL signs (like G and H)Creating a responsive, real-time system without lagDesigning a user-friendly interface that works smoothly on the web,",
                        "github": "https://github.com/hw528/SignLearn",
                        "url": "https://devpost.com/software/sign-learn-vl1a36"
                    },
                    {
                        "title": "EchoLens",
                        "description": "Enhancing spatial and social awareness for the hard of hearing. ",
                        "story": "Inspiration: Deaf and hard-of-hearing (HoH) individuals often face barriers in accessing the full spectrum of audio cues around them\u2014everything from casual conversations to critical alerts like alarms or someone calling their name. Our team was inspired by a vision of an accessible future where no one is left out of their environment due to hearing loss. We wanted to combine spatial awareness, emotional understanding, and practical utility to create something that truly empowers communication and awareness.What It Does: EchoLens translates real-world audio\u2014both spoken and environmental\u2014into accessible visual information. It performs real-time speech-to-text transcription with emotional tone detection, classifies environmental sounds (like alarms, door knocks, or footsteps), and displays directional cues to inform users where the sound originated from. It enhances both spatial and social awareness for Deaf/HoH users.What We Learned: This hackathon challenged us to explore the capabilities of accessible, multimodal computing:AI Integration:Whisper API for real-time transcriptionGemini AI for emotional tone enhancementYAMNet for environmental sound classificationDeepface (optional) for visual emotion cuesMultimodal fusion for contextual awarenessMongoDB used for AI memory and user personalization capabilities,Real-Time Audio Processing:Spatial sound mapping with PyRoomAcousticsAudio capture and streaming with SounddeviceCanvas API for live waveform renderingWhisper and SpeechRecognition coordination,Frontend and UX Design:Material UI theming for accessibilityFramer Motion for intuitive animationsDynamic transcription panel with emotional taggingDirectional sound map for spatial feedback,Technical Collaboration:Flask API for communication with the frontendReal-time backend-to-frontend syncIntegrating multiple APIs and services under pressureDesigning for inclusivity in every interface decision,Most importantly, we learned how immersive, accessible design can empower communities and reshape how we think about sound.How We Built It: React.js with Material UI for a responsive and accessible interfaceFramer Motion for smooth animations and transitionsCanvas API for real-time waveforms and particle visualizationsCustom themes for dark/light mode supportReact Router for dynamic page navigation,Flask framework for high-performance REST API endpointsTensorFlow, YAMNet, and Google Gemini API for multimodal audio and emotion analysisSpeechRecognition and Whisper API for real-time transcriptionSounddevice and PyRoomAcoustics for audio capture and spatial modelingOpenCV and Pillow for visual processing,YAMNet for sound event classificationGemini AI for emotional and contextual inferenceOpenCV and Deepface for visual emotion cuesMultimodal fusion combining audio and visual cues for enhanced environmental understanding,PyMongo for MongoDB integrationPython-dotenv for secure environment management,Interactive sound map with directional indicatorsDynamic transcription panel with emotion taggingCustom animations for different sound types and intensitiesLight mode feature,Challenges We Ran Into: Real-Time Processing:Balancing performance with accuracy in live audio environments was a significant challenge.Emotion Detection:Sarcasm and emotional nuance are difficult to quantify and detect programmatically.Spatial Sound Mapping:Translating acoustic direction into intuitive visuals required experimentation with layout and interaction.Frontend-Backend Communication:Synchronizing live visualizations with backend predictions in real time was technically demanding.Accomplishments We're Proud Of: Created a live spatial sound visualizer with directional indicatorsBuilt a responsive interface that transcribes speech and detects emotions simultaneouslyEnabled detection and contextual labeling of common household/environmental soundsSuccessfully integrated a multimodal AI model (Gemini) into a user-facing tool.Learnt and integrated several new technologies.,What's Next for EchoLens: Custom sound training so users can label personal sounds (e.g., a pet\u2019s bark)Add haptic feedback for physical alertsImprove speaker identification and dialogue grouping,Mobile deployment for on-the-go accessibilityExtend support to wearable tech like AR glasses,Our goal is to make audio accessibility smarter, more intuitive, and available anytime, anywhere.",
                        "github": "https://github.com/Abdullah0x0/sf-hacks",
                        "url": "https://devpost.com/software/echolens-oujfr0"
                    },
                    {
                        "title": "TopAIcal",
                        "description": "TopAIcal is an AI-powered web app designed to be a smart companion for skin health, especially for people of color, our AI can detect common skin concerns.",
                        "story": "Inspiration: Our inspiration for TopAIcal came from the desire to create a tool that can simplify and automate the process of image analysis using machine learning. We wanted to build a solution that could process images, extract relevant information, and present it in an easy-to-understand format. The integration of AI for inference, cloud-based infrastructure for scalability, and a user-friendly frontend were key components of our vision.What it does: TopAIcal is an AI-powered image analysis tool that uses a deep learning model to process images and provide valuable insights. It allows users to upload images, processes them through a trained PyTorch model, and returns results in real time. The application is built with a frontend in Vue.js, a Flask backend for the inference, and leverages cloud services like AWS for file hosting and Google Cloud for embedding content via iframes.How we built it: We built TopAIcal by combining several technologies:Frontend:We usedVue.jsto develop an interactive and responsive user interface, displaying the captured image and result from the model.Backend:Flaskserved as the backend, providing the necessary endpoints to handle image uploads and communicate with the trainedPyTorchmodel for inference.Machine Learning:OurPyTorchmodel was trained using an 8-GPU rig, allowing us to efficiently process large datasets and make predictions.Cloud Integration:We usedAWS S3to host and serve static files like images andGoogle Cloudfor embedding iframe content to enhance the functionality of the application.Domain Management:We managed our domain usingControlPanel.techto set up the DNS and connect the domain to our AWS S3 bucket for hosting.,Challenges we ran into: Model Deployment:Deploying the trained PyTorch model for real-time inference was challenging. Integrating the model with Flask and ensuring smooth communication between the frontend and backend required some trial and error.Cloud Configuration:Configuring the cloud services, particularly AWS S3 for static hosting and Google Cloud for iframe integration, involved a steep learning curve. We had to troubleshoot issues with DNS records and ensure the S3 bucket was configured correctly.Performance Issues:Optimizing the performance of the model, especially with large image datasets, was a hurdle. We needed to ensure that our 8-GPU rig was fully utilized and that the model inference was fast and efficient.,Accomplishments that we're proud of: Deploying the application using cloud services like AWS and Google Cloud, ensuring scalability and ease of access.Configuring the domain and DNS settings through ControlPanel.tech, which simplified the process of pointing the domain to the right resources.Building an intuitive and responsive user interface with Vue.js that makes the process of uploading and analyzing images seamless for users.,What we learned: Vue.jsprovided a great way to build a dynamic frontend that could interact with the backend efficiently.Flaskmade it easy to expose APIs and integrate the PyTorch model for real-time inference.PyTorchgave us the tools to build a powerful machine learning model, and we learned how to optimize it for performance using an 8-GPU rig.Cloud serviceslike AWS and Google Cloud simplified the process of hosting and scaling our application, giving us valuable experience with infrastructure management.ControlPanel.techmade managing DNS and domain configuration much easier, saving us time and effort during deployment.,What's next for TopAIcal: Moving forward, we plan to improve the model's performance and accuracy by training it on a larger and more diverse dataset. We also want to expand the application's capabilities by adding more analysis features and supporting a wider variety of image formats. Additionally, we aim to enhance the user interface, making it even more intuitive and adding more customization options. Finally, we plan to integrate more cloud-based tools to improve",
                        "github": "https://github.com/rpmorata/SFhacks2025-AI-Project",
                        "url": "https://devpost.com/software/topaical"
                    },
                    {
                        "title": "AIna - Your Intelligent Ally for an Accessible Web",
                        "description": "AIna is your AI-powered co-pilot for visual web accessibility\u2014analyzing websites, grading compliance, and guiding improvements to build a safer, more inclusive internet for everyone.",
                        "story": "Inspiration: Over2.2 billion peopleglobally live with a visual or cognitive disability, yet96.3%of the top 1 million websites still fail basic accessibility checks (WebAIM, 2024). From low-contrast text and unlabeled elements to flashing visuals that can trigger seizures, accessibility is still an afterthought on most of the web.We builtAInato change that \u2014 making web accessibility smarter, automated, and truly user-centered.What it does: \ud83e\uddeaSite Scanner: Input any URL to trigger a deep scrape of the site\u2019s code and design.\ud83e\udde0AI-Powered Analysis: We useGoogle's Gemini Flash 2.0to evaluate HTML, both inner and outer CSS and visual components, and provide accessibility suggestions.\ud83e\uddfeAccessibility Grade: Get an intuitive, badge-based score out of 7, based on WCAG 2.1.\ud83d\udccbDetailed Feedback: Actionable improvement tips generated by AI.\ud83d\uddc2Rankings Dashboard: A public catalogue of scanned websites (stored in MongoDB Atlas).\u2699\ufe0fAccessible Mode: High-contrast mode toggle for our own UI, built-in for demo accessibility.\ud83d\udee1\ufe0fComing Soon: Blockchain-based accessibility certificates for sites that pass compliance.,How we built it: Frontend: React + TypeScriptBackend: Node.js, Express, JSDOM (for HTML/CSS parsing)Database: MongoDB Atlas on AWSAI Integration: Gemini Flash 2.0 API for evaluationStyle: Accessibility-first responsive design with dark/light/contrast toggles,Challenges we ran into: Scraping inconsistent or bloated web structuresCreating a scoring system that\u2019s both objective and interpretablePrompting Gemini for consistently helpful and accurate suggestionsBlockchain integration caused some major bugs, so we will add it later,Accomplishments that we're proud of: \ud83d\ude80 Built a working full-stack accessibility platform over the weekend\ud83e\udd1d Seamlessly integrated real-time AI accessibility grading\ud83d\udcca Developed a live public ranking dashboard\ud83c\udfa8 Delivered a fully accessible interface \u2014 including a toggleable high-contrast mode,What we learned: Web scraping and DOM parsing bring real engineering challengesHow to build full-stack platformsHow to deploy cloud-based databases and MongoDB AtlasHow to use GeminiAPI and efficiently prompt engineerGreat accessibility starts with intentional design, not retroactive fixesCostco pizza tastes pretty good at the right time,What's next for AIna - Your Intelligent Ally for an Accessible Web: \ud83e\uddecSmarter AI Suggestions: Fine-tuning Gemini prompts for specific roles (e.g., developer vs. user).\ud83d\udd12Blockchain Certifications: Verifiable tokens for sites that meet compliance.\ud83e\uddd1\u200d\ud83e\udd1d\u200d\ud83e\uddd1User Stories and Testing: Partnering with people who use assistive tech to test real-world usability.\ud83c\udf0dBrowser Extension: Scan websites on-the-fly as you browse.\ud83d\udce2Advocacy: Empowering developers and users alike to push for a more inclusive web \u2014 one scan at a time.,",
                        "github": "https://github.com/aimogeniusOfficial/sfhacks_frontend",
                        "url": "https://devpost.com/software/aina-your-intelligent-ally-for-an-accessible-web"
                    },
                    {
                        "title": "Syll.ai",
                        "description": "Let\u2019s be real, nobody actually reads the whole syllabus. So we built a website that scrapes syllabi and gives you just the important bits. From emails to midterms, Syllai has got you covered.",
                        "story": "",
                        "github": "https://github.com/DanielYRoh/syllabus-scraper",
                        "url": "https://devpost.com/software/syll-ai"
                    },
                    {
                        "title": "Backlogz",
                        "description": "Backlogz transforms idle time into learning by generating concise, sourced audio summaries on topics you choose, enabling efficient, on-the-go knowledge acquisition.",
                        "story": "Inspiration: Do you have a endless list of things you want to read or learn? They pile up! During SF Hacks 2025, we thought, what if you could justlistento summaries of those topics whenever you have a spare moment? From there, Backlogz was started, an idea to turn that overwhelming list into easy, listenable learning bites using AI.What it does: Backlogz lets you quickly add topics you're interested in. Then, just hit 'Quick Play'. The app grabs a random topic, asks Google's Gemini AI to whip up a short, conversational summary, and reads it out loud using your phone's text to speech. Think of it like a personalized mini podcast feed made from your own curiosity list.How we built it: We built this during the hackathon using React Native and Expo, which are fantastic for building mobile apps quickly. TypeScript helped keep things organized. The magic happens by calling the Gemini API to generate the scripts and using Expo Speech to play them back. For now, topics are just saved locally on the device.Challenges we ran into: Getting the AI call, the waiting time, and the speech playback to work together smoothly in React Native within the hackathon time limit was the trickiest part. Making sure the AI summaries sounded natural also took a few tries with the prompts.Accomplishments that we're proud of: Getting the core idea working! It actually takes a text topic, generates an AI summary, and plays it back cleanly. Seeing that main loop function smoothly in a mobile app built over these two days felt pretty great. The 'Quick Play' is simple but genuinely useful.What we learned: Definitely learned a lot about juggling asynchronous tasks in React Native like API calls and speech. We got real hands on experience calling a generative AI like Gemini from an app and figuring out how to prompt it effectively. Expo's tools for things like text to speech were also cool to work with.What's next for Backlogz: Next steps would involve enhancing the existing features, such as adding more robust editing capabilities to the notes section. A big priority is adding cloud backup (likely using Firebase) so topics and notes aren't just stored locally. Longer term, features like better topic organization (tags, folders), user accounts for syncing, and maybe even summarizing web articles directly from a URL could be explored.",
                        "github": "https://github.com/ChesterCaii/back-logz",
                        "url": "https://devpost.com/software/backlogz"
                    },
                    {
                        "title": "Safe Haven",
                        "description": "Safe Haven connects donors to local safehouses in need and helps vulnerable people find aid fast. It features real-time updates, donation tracking, and a Gemini-powered support chat.",
                        "story": "Inspiration: Initially, while we were discussing ideas, we talked about what kinds of things we can build that will help people. The first thought that came to mind was helping homeless people since one of us volunteers for our local church, helping to serve homeless people. Regarding design, I'd say the biggest inspirations were other fundraising websites like GoFundMe. We took inspiration from its design and made our own.What it does: Users will be able to access our website, create an account, and fill out a form. The form will ask them what type of items they are willing to donate. If they choose to donate money through their credit or debit card, a set of credit/debit card info will appear. Then, users can choose locations that are available at the moment (more coming soon) and then submit.How we built it: We built this website's front end using HTML, CSS, and Javascript. For the backend, we used Node.js / Express.js, and for the database, we used PostgreSQL.Challenges we ran into: Some of the challenges we ran into were minor CSS styling or HTML tag missing issues for the front end. For the backend, some issues were that not all members were able to get the server running. For that, we had to make sure everyone had the necessary libraries installed on their devices. Then, we also ran into some issues when it came to pushing our code into GitHub (merge conflicts, forgetting to git add, etc.).Accomplishments that we're proud of: Honestly, based on the timing of the hackathon and since this group was formed during the hackathon, we are proud of what we have built. Many of us are new to some of the languages on the tech stack for this project, such as PostgreSQL and Node.js / Express.js. So, based on the challenges and the results, we are proud of what we were able to accomplish within 48 hours.What we learned: We learned many new things about the tech stack and how to use them in different aspects of our project. We also learned about teamwork and communication, which was important. We learned to ask more questions to our group members and not be afraid to ask them.What's next for Safe Haven: The next thing for Safe Haven is an improved version of the AI chatbot, a live map to view the locations, business account availability, etc.",
                        "github": "https://github.com/thamizarasus/safe-haven.git",
                        "url": "https://devpost.com/software/safe-haven-r9io4v"
                    },
                    {
                        "title": "ShieldUp",
                        "description": "Stand strong. Stay informed.",
                        "story": "",
                        "github": "https://github.com/alexavvega/2025sfhack",
                        "url": "https://devpost.com/software/shieldup"
                    },
                    {
                        "title": "MindChat",
                        "description": "A mental health chatbot that blends real-time empathy with reliable information. Using Google's Gemini API and RAG the backend is built with Flask and MongoDB Atlas, and the UI is built using react.",
                        "story": "",
                        "github": "https://github.com/merxgrc/sfhacks-mental-health-chatbot/tree/main",
                        "url": "https://devpost.com/software/mindchat-k50wac"
                    }
                ],
                [
                    {
                        "title": "Stewdio.tech",
                        "description": "Stewdio is an open source and highly extensible version control suite for audio projects. From educational recordings to legal transcripts, Stewdio ensures your work is safely archived and preserved.",
                        "story": "Inspiration: As a team of music producers, audio engineers, and tech enthusiasts, we noticed a glaring gap in how the audio world preserves and manages its history. While GitHub has revolutionized version control for software and even education, audio files\u2014whether it's music, podcasts, legal audio records, or sound design\u2014still lack robust tools for tracking and archiving changes. This sparked a question: Why doesn\u2019t version control exist for audio in the same way it does for code?We envisioned a world where audio creators could trace their work, collaborate safely, and maintain a complete historical record of their sound projects. Enter Stewdio.tech\u2014an open source ecosystem built to bring version control to the world of audio.What it does: Stewdio.tech is a version control and audio archiving tool designed for music producers, sound designers, and audio engineers. It enables users to version, sync, and collaborate on audio projects with the same precision and history-tracking that software developers enjoy.Its core features includeAdditional Features: How we built it: We used a modern tech stack centered around performance and extensibility:Challenges we ran into: Accomplishments that we're proud of: Built a working, Git-style versioning system for audio in under 48 hoursCreated a custom binary diff engine that minimizes storage while maximizing traceabilityDeveloped a sync server that enables real-time collaboration on audio projectsIntroduced a completely new paradigm for how audio creators can manage and preserve their work,What we learned: Deepened our understanding of how version control systems like Git operate under the hoodLearned how to manipulate and analyze binary audio data for patching and reifying file historiesGained practical experience integrating CLI tools with web technologies and server-side synchronizationDiscovered the untapped potential of applying software engineering tools to the audio production space,What's next for Stewdio.tech: We see Stewdio.tech as the beginning of a much larger movement. Future plans include:Digital Audio Workstation Plugin IntegrationsSeamless plugins for Logic Pro, Ableton, FL Studio, and others\u2014bringing version control directly into the producer\u2019s workspace.Extended Format SupportSupporting more audio formats, multitrack sessions, and project containers (e.g., .als, .logicx).Team Collaboration ToolsArchive & Compliance FeaturesTools for legal, journalistic, or academic use cases\u2014where audio chain-of-custody and authenticity are crucial.,Our vision is to make Stewdio.tech the GitHub for audio\u2014democratizing project management, preserving creative history, and fostering true collaboration in sound.",
                        "github": "https://github.com/water-sucks/stewdio",
                        "url": "https://devpost.com/software/stewdio"
                    },
                    {
                        "title": "NBA Player Guessing Game",
                        "description": "Login or sign up to guess the NBA player based off of blurred images and other trivia.  ",
                        "story": "Inspiration: I wanted to use this hackathon to build my first full stack project. I have some experience with databases and python but almost no frontend or full stack experience. I also wanted to build something I was interested in.What it does: You are presented a blurred image of an NBA player and you have to guess it. After an incorrect guess you are given a hint. High scores are saved.How we built it: We had to get the database set up, we used Cloudinary to store the blurred and unblurred images for the players. We also had to set up the login screen and made sure that it saved the player's info if they already have an account.Challenges we ran into: One challenge was sorting the data. We used 2 APIs to get the data, one had player stats and the other had player profiles. The player profiles API returned so much information that we had to parse. \nThe whole front end was a struggle since neither of us ever used Flask or have done any front end development.Accomplishments that we're proud of: I'm proud that we got the database working correctly and are able to save user logins.What we learned: We learned how to create a cluster in MongoDB Atlas. The basics of using Flask (routes, reading/writing to the DB, etc.).What's next for NBA Player Guessing Game: Making it look better and adding more players. Make user passwords encrypted in the DB lol.",
                        "github": "https://github.com/Mystic2122/SFHacks25",
                        "url": "https://devpost.com/software/nba-player-guessing-game"
                    },
                    {
                        "title": "Venty",
                        "description": "Your safe space to vent, reflect, and grow.",
                        "story": "Inspiration: We built this app because we realized that in moments of anger, stress, disappointment, people can often react impulsively and regret it later on. In an ideal world, we would be able to \"press pause\", and think about our actions, have time to self reflect, and then act accordingly. Often time people act in a certain way and then realize that they were caught up in the moment, and that if they had the chance to talk to their past selves, they would advise them to remain calm.Venty gives the user a chance to do such a thing, by allowing me to revisit my thoughts and concerns. The goal is to take a step back, examine emotions, and self-reflect. Over time, the chatbot learns thinking patterns and becomes a kind of \"future you\", gently guiding you to making more sound decisions.What it does: prompts you to vent in the chat screencomputes a sentiment analysis score, finds context words/entitiesclusters vents into threads so bot \"remembers\" past advice given and user's input on that topicgenerates a response based on the score as well as similar \"threads\" with similar context and entities.schedules follow ups that ask reflective questions based on sentiment score. Timing of notification changes based on score, for example, a more negative score would require a faster follow up whereas a more positive entry would warrant a followup later in the day. Follow ups occur in 1hr, 6hr, and 24hr intervalsoffers mini quiz on preferred persona of bot (mother figure, friend, etc.) and preferred values at different emotion states.,How we built it: Frontend: Expo and React Native with Expo Router for file-based navigation and notifications for local remindersBackend: FastAPI in Python, Gemini API for prompt engineering and summariesData: MongoDB Atlas, to store thread IDs, input, output, summary of user input, topic, context metadata, sentiment score,Challenges we ran into: We had lots of issues figuring out endpoints and getting code to work on our individual phones.Expo doesn't offer push notifications in their development build, so we had to resort to local notifications for the purposes of our projectembedding similarities were too similar in the beginning so the user could complain about two separate issues and they would be classified as similar vents and given same thread IDs.,Accomplishments that we're proud of: We created an end-to-end reflective loop that takes in a vent, then creates a summary, then schedules a follow up/check in by looking back at the memoryWe implemented customized personas as well as allowing user to input in what the value so that the bot not only learns from previous follow ups, but also takes into account who the user wants to be as a person.Good UIfunctioning app with frontend, backend, storage, and notificationsno git problems, no merge issues,What we learned: How important useRouter is in React NativeRAG, combing embeddings to add to memoryusing expo notifications for timed + context-aware check ins,What's next for Venty: more testing, the most effective way to test the app is by using it for a couple of days and being very genuine and intentional with the messages the user is sending to the bot.Adding more resources within the website for mental health purposesadding visuals so the user can understand were a bulk of their issues come from, and how much progress they are making by using the app.Model fine tuning,",
                        "github": "https://github.com/varunpalanisamy/Venty",
                        "url": "https://devpost.com/software/reflectin"
                    },
                    {
                        "title": "RentSpiracy",
                        "description": "Protecting immigrants & students from rental scams.",
                        "story": "\ud83d\udca1 Inspiration: As international students and first-time renters ourselves, we saw countless stories of immigrants and international students falling for predatory rental scams. Many leases are written in confusing legal jargon or foreign languages, and background checks on landlords are nearly impossible. We builtRent-Spiracyto protect vulnerable communities - especially people of color, international students, immigrants, and first-generation renters - by making leases understandable and scammers accountable.\ud83e\udde0 What It Does: Rent-Spiracy is an Gemini-powered web app that helps renters detect scams and understand leases, with multilingual support and tools built for accessibility. Users can:Select the language they speak (currently supports 8 - English, Bengali, Hindi, Chinese, Korean, Swahili, Arabic, Spanish)Upload a lease (PDF/image) and get a simplified, translated analysis of the lease in their chosen languageReceive scam and red-flag clause detection comparing to the state lawsGet scam-likeliness score 0-100 and riskiness scoreGet followup questions to ask to the landlord about the leaseFind region-specific, language-matching tenant lawyersSearch or report scam landlords (with evidence handling),\ud83d\udee0\ufe0f How We Built It: We built Rent-Spiracy as a full-stack app using:Frontend: Next.js 15 + Tailwind CSS (Vercel), built for accessibility with dark mode and keyboard/screen reader supportBackend: FastAPI (Python), containerized with Docker and deployed via RenderDatabase: MongoDB Atlas, storing suspect landlords, reports, and multilingual lawyer profilesAI: Gemini API for clause summarization and translation; prompt-engineered for legal clarity and scam detection,\ud83e\uddd7 Challenges we ran into: One of our biggest technical and human challenges was training Gemini to meaningfully simplify and analyze lease documents. Most leases are dense, inconsistent, and full of legalese. Words like \"indemnify,\" \"arbitration clause,\" or \"right of entry\" may be harmless in one lease but suspicious in another depending on how they\u2019re phrased. This meant we couldn\u2019t just prompt the model to \u201csimplify\u201d or \u201ctranslate.\u201d We had to teach it how to interpret context.We iterated through dozens of prompts to get Gemini to not just summarize, butspot specific risk indicators- such as vague payment terms, hidden renewal clauses, or restrictions that disproportionately harm tenants. Often, the model would either oversimplify (losing critical details) or flag generic clauses as scams. To fix this, we broke down the pipeline:First, we extracted raw text from leases (OCR preprocessing, removing headers, footers, and non-content text)Then, we chunked clauses by sections - e.g., payments, maintenance, rights, dispute resolutionEach chunk was passed into Gemini withtargeted prompts: one for simplification, one for red-flag analysisFinally, we used rule-based post-processing tocross-reference flagged clauseswith a curated list of known scam patterns,This architecture helped us reduce hallucinations, minimize false positives.\ud83c\udfc6 Accomplishments that we're proud of: We didn\u2019t want Rent-Spiracy to be just another AI wrapper or frontend demo. We wanted it to work forreal people- many of whom don\u2019t speak English, have low digital literacy, or don\u2019t trust systems that rely on Google or Meta. This led us to build our ownself-contained multilingual fallback system, decoupled from third-party translation APIs.It started with realizing that lease analysis isn\u2019t just a translation problem - it\u2019s alocalizationandtrustproblem. If a user uploads a lease in Hindi or Bengali and gets AI output in English legalese, we\u2019ve failed them. But relying on Google Translate introduces bias, latency, and privacy concerns. So we built:Custom translation dictionariesand rule-based grammatical transformations for 8 languages (on both frontend and backend)A fallback pipelinewhere if Gemini translation fails or rate-limits, users still get accurate, fast results from our local engineContinuous testing with real users who speak Hindi, Korean, and Bengali to tune output readability and clarityUI-level design decisions that prioritized readability over design flash - larger font sizes, full keyboard nav, RTL support, high-contrast dark mode,Current accessibility supports include - Skip-to-content links and keyboard navigation, ARIA live regions for screen reader announcements, Offline detection with visible indicators, Reduced motion support for vestibular disorders, Improved color contrast and text spacing, Touch-friendly mobile optimizations, Semantic HTML with proper ARIA attributes, Keyboard-navigable UI components, PWA support with manifest.json, Focus management for screen readers, High contrast mode support, Loading state indicators with reduced animations, Cognitive accessibility improvements, Responsive design for low bandwidth environmentsThis system isn't flashy, but it\u2019sresilient. It means Rent-Spiracy can work in areas with poor connectivity, without sharing user data externally, and with translations tuned forlegal clarity, not marketing speak. We learned that accessibility is about dignity. And that required engineering effort.\ud83d\udcda What we learned: Early in the build, we thought accessibility meant adding alt text, ARIA labels, and contrast-compliant colors. But when we tested the app with users whose primary language wasn\u2019t English (I convinced my aunt to spend 3 hours with me today testing this app lol)\u2014or who were unfamiliar with legal documents entirely\u2014our assumptions started breaking. The UI looked clean to us, but it wascognitively inaccessible.We started asking: what happens if the user is stressed, reading in their second or third language, and only has a phone to work with? This shifted how we approached engineering.We redesigned components withsemantic chunkingof text\u2014grouping clauses into \u201cwhat you pay,\u201d \u201cwhat they can do,\u201d and \u201cwhat you can\u2019t do.\u201d This wasn't just visual; it changed the way we structured our API responses and AI prompts. We also added inline definitions for legal terms, not just as hover-tooltips (which don\u2019t work well on mobile) but as expandable blocks readable by screen readers.That process taught us thataccessibility is a debugging framework\u2014when your interface works for the most overloaded, under-resourced user, it will work better for everyone else too. And in our case, that user was often a person of color, an immigrant, or a first-generation renter trying to navigate an exploitative system in silence.\ud83d\ude80 What's next for RentSpiracy: We realized that Rent-Spiracy doesn\u2019t just analyze leases\u2014it captures stories. Every time someone flags a suspicious landlord, every time a clause gets labeled \"exploitative,\" it reflects real harm. But right now, that knowledge is siloed to the user. We want to change that by turning Rent-Spiracy into acommunity-sourced database of rental scams, organized by region, landlord identity, and document type.Imagine you\u2019re a Nigerian student in Queens seeing a listing on Craigslist. You upload the lease to Rent-Spiracy, and it tells you: \u201c\u26a0\ufe0f This lease matches two other scam patterns reported by users in your area. Here\u2019s what to watch for.\u201d That\u2019s not just helpful\u2014that\u2019snetworked protection.To build this, we\u2019ll use MongoDB Atlas to link new scam reports with known flags via fuzzy matching on lease text and leaser metadata. Reports will be anonymized but structured for public analysis. And we\u2019ll surface recurring scam tactics\u2014like fraudulent application fees or invasive entry rights\u2014to empower not just individuals, butcommunities of renters, especially in underrepresented zip codes where legal help is scarce.Hope you had a great time reading through this.\ud83c\uddfa\ud83c\uddf8English: Thank you\ud83c\uddea\ud83c\uddf8Spanish: Gracias\ud83c\udde8\ud83c\uddf3Chinese (Simplified): \u8c22\u8c22 (Xi\u00e8xi\u00e8)\ud83c\uddee\ud83c\uddf3Hindi: \u0927\u0928\u094d\u092f\u0935\u093e\u0926 (Dhanyavaad)\ud83c\uddf0\ud83c\uddf7Korean: \uac10\uc0ac\ud569\ub2c8\ub2e4 (Gamsahamnida)\ud83c\udde7\ud83c\udde9Bengali: \u09a7\u09a8\u09cd\u09af\u09ac\u09be\u09a6 (Dhonnobad)\ud83c\udf0dSwahili: Asante\ud83c\uddf8\ud83c\udde6Arabic: \u0634\u0643\u0631\u0627\u064b (Shukran),(Tashrique Ahmed)[www.tashrique.com]\nBrian\nArpan",
                        "github": "https://github.com/tashrique/Rent-Spiracy",
                        "url": "https://devpost.com/software/rentspiracy"
                    },
                    {
                        "title": "NATSU AI",
                        "description": "A compassionate AI companion designed to support and enrich the lives of senior citizens\r\n\r\n",
                        "story": "",
                        "github": "https://github.com/chicogac/NATSU_AI",
                        "url": "https://devpost.com/software/natsu-ai-bqav6p"
                    },
                    {
                        "title": "RecovAI",
                        "description": "RecovAI is an AI-powered recovery assistant that provides personalized care plans and supportive chat to help surgery patients heal smarter, safer, and with confidence at home.",
                        "story": "Inspiration: We were inspired by the struggles many elderly patients face after surgery, particularly the lack of continuous guidance once they leave the hospital. Many of these patients lack caretakers at home and are left to manage complex recovery routines on their own, which can lead to further complications, confusion, and slower healing. We wanted to create a tool that supports recovery both physically and emotionally, especially for those without consistent access to caregivers or follow-up care.What It Does: RecovAI helps surgery patients recover at home by generating personalized daily wellness schedules based on their dietary needs, restrictions, and recovery goals. It also features a supportive AI chatbot that offers encouragement, answers questions, and provides tailored guidance throughout the healing process.How We Built It: We built RecovAI using a full-stack approach. The frontend was developed with React, HTML, and CSS to create a clean, responsive user interface. On the backend, we used Node.js and Express to manage routes and connect to a MongoDB database for storing patient data. We integrated Google's Gemini API using Python to power the conversational chatbot and provide empathetic, AI-driven support. Flask was also used during the initial prototyping phase of our AI integration. JavaScript tied everything together across the stack, allowing smooth communication between the frontend and backend components.Challenges We Ran Into: One major challenge was getting the Gemini API to work with our current API key \u2014 we initially tried to use gemini-pro with the official SDK, but ran into compatibility issues due to API access restrictions and had to pivot to using chat-bison-001 via the MakerSuite endpoint. Another challenge was designing a dynamic scheduling system that could adjust in real-time based on patient input like dietary restrictions and recovery stages, while keeping the UX clean and intuitive. Balancing personalization with simplicity required several design iterations.Accomplishments That We're Proud Of: We're proud of building a fully functional AI assistant that personalizes care in a meaningful way, and especially to those who need it the most. From customizing wellness schedules based on individual needs to offering compassionate conversation, RecovAI brings together empathy and technology in a way that feels truly supportive. We\u2019re also proud of overcoming technical challenges and delivering a polished, end-to-end solution in such a short amount of time.What We Learned: Through building RecovAI, we learned how to integrate large language models into a real-world application and adapt quickly when APIs didn't behave as expected. We deepened our understanding of full-stack development, especially how frontend and backend components communicate securely and efficiently. We also gained experience working with cloud databases, handling user input dynamically, and designing user interfaces that prioritize both functionality and empathy. Most importantly, we learned how to turn a health-focused idea into a working product that could genuinely improve the patient recovery experience.What's next for RecovAI\nIn the future, we aim to expand RecovAI into a more comprehensive post-operative care platform. We plan to integrate voice input to make the tool more accessible for patients with limited mobility or visual impairments. A companion mobile app is also in development to ensure patients can access their care plans on the go. Additionally, we hope to implement a clinician dashboard that allows healthcare providers to remotely monitor patient progress, review chatbot interactions, and update recovery plans in real time. Further improvements include adding multilingual support, medication reminders, and the ability to sync with wearable health devices to provide more accurate, responsive care based on real-time vitals. Long-term, we envision partnering with hospitals and clinics to integrate RecovAI into official discharge processes, bridging the gap between in-hospital care and home recovery.",
                        "github": "https://github.com/anayrshukla/SFHACKS2025",
                        "url": "https://devpost.com/software/recovai"
                    },
                    {
                        "title": "SafetySnap",
                        "description": "SafetySnap: Preventing OSHA violations before they cause harm. Upload, analyze, protect! ",
                        "story": "\ud83d\udea7 The Problem: Construction sites are hazardous environments where safety violations can lead to serious injuries, fatalities, and costly penalties. Identifying OSHA violations early is crucial, but:Safety managers can't be everywhere at onceWorkers may not recognize potential hazardsDocumentation and reporting are often cumbersomeViolations frequently go unnoticed until inspections or incidents,\ud83d\udca1 Our Solution: SafetySnapempowers construction workers to be proactive safety advocates by allowing them to:\u2728 Key Features: \ud83d\ude80 How It Works: \ud83d\udee0\ufe0f Technology Stack: Frontend: Next.jsBackend: SupabaseAI Integration: Google Gemini APIAuthentication: SupabaseStyling: Tailwind CSS,\ud83d\udcbb Usage: Sign up for an account or log inUpload a photo of a potential safety concern  1Review the AI analysis for OSHA violationsGet recommended actions to address any identified issuesSave and share the results with your team,\ud83d\udd2e Flow Diagram: What We Learned!: \u2022 How to integrate advanced AI vision models (Google Gemini) to analyze construction site images for safety hazards\u2022 Techniques for creating an intuitive user interface that requires minimal training for construction workers\u2022 Methods for translating complex OSHA regulatory requirements into actionable safety recommendations\u2022 Approaches to build offline-capable web applications for use in remote construction environments\u2022 Strategies for balancing accuracy with speed when processing safety-critical image analysis\u2022 The importance of user-centered design when creating tools for non-technical industries\u2022 How to effectively present technical solutions to address real-world problems with significant human impact\ud83d\udc65 The Team: SafetySnap was created during a hackathon byMahdi,Cole, andNoah.\ud83d\udcdc License: This project is licensed under the MIT License \u2013 see theLICENSEfile for details.",
                        "github": "https://github.com/Cole-Hartman/SafetySnap",
                        "url": "https://devpost.com/software/safetysnap"
                    },
                    {
                        "title": "Resource Overflow",
                        "description": "Resource Overflow is a platform that helps underserved populations find and access local community resources using natural language queries. ",
                        "story": "Inspiration: The Resource Overflow was inspired from challenges that underserved populations face when trying to access critical services. Many people in need don't know where to find help and what kind of help exists. Additionally, existing resource directories are often difficult to navigate. I wanted to create a solution that uses technology to connect people with the resources they need in a simple way that prioritizes accessibility and utility.What it does: Resource Overflow is a platform that helps underserved populations find and access local community resources using natural language queries. Users can describe their needs in everyday language (e.g., \"I need help finding food for my family\"), and the system processes these requests to identify and recommend appropriate resources.The platform includes:Natural Language Search: Using Gemini API, the system analyzes user queries to find out information like required services, location preferences, and accessibility requirements.Resource Database: A MongoDB Atlas database stores data about community resources, including services offered, location data (with GeoJSON support for spatial queries), hours of operation, eligibility requirements, accessibility features, and contact information.Resource Similarity Recommendations: The system leverages MongoDB's querying capabilities to find and recommend similar resources that might address related needs, helping users discover services they might not have explicitly searched for.Community Contribution: A submission form allows community members and service providers to add new resources to the platform, ensuring the database stays current and comprehensive.User-Friendly Interface: An accessible and responsive design that\u2019s easy to understand and follow.,How We built it: Resource Overflow was built using a modern web development stack with several key technologies:Backend DevelopmentNode.js and Express: RESTful API(s) with distinct routes for resources, categories, search, and feedback.MongoDB Atlas: The database includes a JSON schema design with GeoJSON for location data, along with indexing for efficient text and geospatial searches.Mongoose: Models for resources, categories, search logs, and user feedback with proper validation and relationships.,Frontend DevelopmentVanilla JavaScript: Implemented Client-side logic without heavy front-end frameworks to ensure performance and accessibility. Needed to maximize the hackathon allotted time.Custom CSS: Integrated TailwindCSS\u2019s styles customized some.Interactive Features: The UI includes dynamic content loading, geolocation, resource details, and resource recommendations.,Natural Language ProcessingGemini API Integration: Integrated Google's Gemini API to process natural language queries and extract structured search parameters.Query Analysis Service: The implementation interprets user queries to identify categories, locations, and special requirements.Fallback Mechanisms: Built fallback search methods so that it\u2019d work even when the AI service is unavailable.,Resource Similarity FeatureSimilarity Matching Algorithm: Developed an algorithm that identifies related resources based on categories, services, and text similarity.Recommendation Display: Each resource card shows similar resources that users might also need without needing to search for it.,Database SeedingUsed a seeding script that populates the database with sample data.Each resource includes structured data for hours, location, services, and contact information using MongoDB's document model.,Challenges we ran into: Accomplishments that I\u2019m proud of: What we learned: What's next:",
                        "github": "https://github.com/loofsan/SF-Hacks2025",
                        "url": "https://devpost.com/software/resource-overflow"
                    },
                    {
                        "title": "MathSetAI",
                        "description": "Generates quiz practice problems and provides AI feedback from Gemini to help the user excel in algebra, geometry, and calculus.",
                        "story": "Inspiration: As I've tutored several students in various mathematics subjects, I've often realized how tedious it can be to compile teaching materials. Additionally, as a student, when studying for tests, it can be stressful to keep finding new problems when the study guide may not be enough to grasp many concepts. My goal with this, then, was to create something that could automatically create a variety of math problems and provide tutoring for difficult problems.What it does: The user starts by choosing how many problems they want to solve and the type of problems. Next, the program randomly generates questions for the user to solve. The program checks the user's answers and uses Gemini AI to talk with the user about how to solve the question and where they probably made a mistake.How we built it: I started by building many templates for problems for each field of study. Then, using randomized variables, I could use the templates to generate many more problems. Once I got a good basis of templates, I started working on incorporating Gemini AI to read and compare the user's answer to the actual answer for the question and return an appropriate response. The final thing I worked on was creating a UI to bring it together. For this, I created a simple GUI for the problems to take in the user's answers and give the Gemini feedback.Challenges we ran into: Incorporating the Gemini API into my project was a pretty substantial challenge. Having never programmed with an API and with almost no experience in Python, it took me a long time to figure out how to get and receive responses between the program in Java and Gemini in Python. This was also my first time using a GUI. I didn't know what I was doing with it or how to make it look nice. In general, I also struggled with my limited knowledge of Java and the different tools available to me, which I had to find.Accomplishments that we're proud of: I'm proud of the final product despite how rough and unfinished it is. I was able to successfully set up Gemini API in my program to get AI responses. I'm also proud of the fact that I made a working GUI that does nearly everything I want it to. My initial plan was to create a website using HTML and JavaScript, but I decided to challenge myself about halfway through by building a GUI for my application.What we learned: I came into the hackathon with 2 goals: to learn how to build a real project much bigger than anything I've built in the past and to learn how to use the Gemini API to get AI responses. I learned much more about those 2 than I anticipated. I struggled with them, but I also crushed my initial expectations of how much I would learn and get done. Additionally, I learned a great deal about the tools available in Python and Java, including how to use Swing to create GUIs.What's next for PiQuiz: The project I came up with is incomplete, missing several types of problems. My first item is to create more templates to generate problems. Alongside that, I would like to introduce a selectable difficulty for problems. Next, I want to clean up my code by finding better solutions, making it more legible, and adding more error-handling. Afterwards, I want to clean up the UI, making it look nicer as well as introducing new ways to interact with it through multiple-choice questions, interactive diagrams for geometry, a basic calculator, and maybe even a way to plot graphs. The final thing I would like to do with it is expand the topics to all elementary and high school math courses. As I learn more math over the years, I would like to keep adding new courses.",
                        "github": "https://github.com/bkallencf/MathTutor.git",
                        "url": "https://devpost.com/software/piquiz"
                    },
                    {
                        "title": "PagePal",
                        "description": "Bringing Books to Life, One Chat at a Time",
                        "story": "PagePal was inspired by the theme \"Tech for Good\" and a desire to make books more interactive and accessible. Many people find it difficult to engage with the deep content of books, whether due to personal struggles or time constraints. We wanted to create a platform where people could have a personalized conversation with books, exploring ideas and learning from them in an engaging and dynamic way. Whether it's helping those in need of self-help guidance, those wanting to recall specific details from sci-fi novels, or people wanting a religious or motivational boost, PagePal aims to provide valuable knowledge in a more humanized and approachable manner.AI-Powered Conversations: We deepened our understanding of how AI, specifically the **RAG modelandChatGPT API, can be used to create conversational interfaces that are engaging and contextually relevant.Backend Integration: We learned how to integrate **FlaskwithMongoDB Atlasfor seamless data storage and retrieval, ensuring that our application can scale and handle large datasets effectively.Prompt Engineering: One of the biggest takeaways was the importance of crafting personalized prompts for each genre of books. We had to ensure that the responses felt authentic, as if the book itself was speaking to the user.Project Management: Through effective **communication and planning, we ensured that the roles were well-distributed across the team to utilize everyone's strengths, which helped us stay on track and make decisions swiftly.,Frontend: We built the frontend using **React, **TypeScript, and **Vite. The user interface allows users to log in, select genres, browse books, and chat with them.Backend: The backend was built with **Flask. We implemented the **RAG modelwith theChatGPT APIto handle dynamic user interactions. Book data is stored and indexed inMongoDB Atlaswith vector search to quickly retrieve relevant information.Ingestion: We used **Python scriptsfor data ingestion. We ingested book content, chunked it into meaningful sections, and indexed it for efficient retrieval.Custom Prompts: Each book genre, like **Self-Help, **Biography, **Cookbooks, etc., was given its own tailored prompts to make the responses feel humanized and personalized.,Building PagePal was a rewarding journey. We learned how to integrate AI with traditional backend technologies and bring a book to life with interactive conversations. While we faced challenges, our commitment to usingsoftware engineering best practicesensured that we built a solid, scalable solution. We\u2019re excited to see where this project can go and how it can help people engage with knowledge in new and innovative ways.Goal:Introduce personalized book recommendations based on user preferences and previous interactions, allowing for a more customized browsing experience.Challenges:Building a recommendation engine based on user history and interaction would require advanced data analytics and was a complex feature to implement.Goal:Implement audio-based interactions, allowing users to speak with books instead of typing. This could provide a more natural and immersive conversational experience.Challenges:Integrating speech-to-text and text-to-speech capabilities into the existing system was outside the scope of our current hackathon timeline but is something we plan to explore.Goal:Allow users to select and interact with multiple books at once, enabling a multi-book conversation. For example, a user could ask two books, like a biography and a self-help book, the same question and compare their perspectives.Challenges:This would require a more complex conversation management system that can track multiple book contexts simultaneously, which wasn't fully implemented in this hackathon due to time\u00a0constraints.Goal:Incorporate blockchain technology to ensure decentralized ownership of books and Verifiable Credentials (VCs) for users, showing that they\u2019ve successfully learned specific concepts or insights from books.Challenges:The integration of blockchain with our current database structure is a complex task and was not feasible within the time frame of this hackathon.",
                        "github": "https://github.com/shahtirth07/PagePal",
                        "url": "https://devpost.com/software/pagepal-3nmati"
                    },
                    {
                        "title": "Nutrisor",
                        "description": "Do you want to save time and also take care of your body? Our application is perfect to know about your own personalized supplements to upgrade your health within seconds",
                        "story": "Inspiration: Imagine a future where you no longer waste time and money obtaining health advice. As our bodies begin to show signs of wear, especially in our late 20s and early 30s, it becomes critical to choose the right dietary supplements to maintain your vitality. Our application cuts through the noise by offering personalized supplement recommendations and clear, accessible insights on each product's benefits. Empower yourself to take control of your health\u2014reclaim your time, save money, and unlock the energetic, vibrant life you deserve.What it does: Our app begins with a personalized questionnaire that dive into your unique health profile. By using your response data, our app will deliver customized dietary supplement recommendations.How we built it: Our backend is built with FastAPI, which handles user requests and connects to a MongoDB database for data storage and retrieval. Gemini fetches and processes this data to generate personalized supplement recommendations. Finally, the recommendations are rendered on an HTML page using CSS and JavaScript.Challenges we ran into: Everyone of our team are beginners so we had a lot of difficulties.Conceptualization: Refining our ideas while considering key selling points.Data Integration: Connecting JSON data to our database.Database Familiarity: Gaining proficiency in MongoDB.API Integration: Learning how to effectively call and utilize APIs.Frontend Development: Implementing dynamic actions in HTML using CSS and JavaScript,Accomplishments that we're proud of: Developed a questionnaire form that captures user input.Converted user input into JSON and uploaded it to our MongoDB database.Integrated Gemini to fetch data from the database and display the output on the front end.,What we learned: Setting up a backend server and connect it to a database.Importing user data as JSON and upload it to the database for Gemini to retrieve.Implementing routes for efficient navigation,What's next for Nutrisor: We are aiming to use Healthkit and get realtime health data from apple watchProcess that data with Gemini and obtain more accurate supplement recommendations with brands rated in ranks.,",
                        "github": "https://github.com/sk4rm/sfhacks-2025",
                        "url": "https://devpost.com/software/nutrisor"
                    },
                    {
                        "title": "panorama",
                        "description": "Get the whole picture.",
                        "story": "Panorama: Our project was born from the realization that navigating today's polarized news landscape is increasingly challenging. We wanted a tool that provides a clear, balanced view of important topics, helping users escape echo chambers by presenting sources from across the political spectrum.We observed how easy it is to become insulated within specific media bubbles. Our inspiration stemmed from a desire to enhance informed decision-making by encouraging exposure to diverse political viewpoints. This led us to create a multi-partisan news aggregator that promotes transparency and balanced information.We used React for our front-end, ensuring a clean, responsive, and intuitive user experience. Our backend is powered by Python's FastAPI, enabling rapid and efficient API development. We leveraged pplx Sonar to identify and collect relevant sources spanning different political perspectives. To scrape articles and gather essential metadata, we implemented Llama Index, streamlining the organization of content.For user authentication and data persistence, including search histories and bookmarked articles, we integrated MongoDB. This allowed personalized experiences, enabling users to revisit topics and save content effortlessly.One significant challenge was efficiently managing data retrieval from diverse sources while maintaining speed and accuracy. Another obstacle involved seamlessly integrating multiple APIs and ensuring coherent data flow between the front-end and back-end. Addressing privacy and security for user data, particularly search histories and bookmarks, was also essential.We learned the importance of clear architecture planning, particularly when integrating diverse technologies like pplx Sonar and Llama Index. Handling user authentication and database management deepened our appreciation for robust security practices. Overall, this project highlighted the value of teamwork, clear communication, and agile development methods to overcome complex integration challenges.",
                        "github": "https://github.com/arnavsurve/panorama",
                        "url": "https://devpost.com/software/panorama-pmocv8"
                    },
                    {
                        "title": "EquiBot",
                        "description": "EquiBot is a mobile chatbot that helps California residents with legal and policy issues. EquiBot is designed to use accurate material and information taken straight from California laws/acts.",
                        "story": "",
                        "github": "https://github.com/Hrios05/EquiBot",
                        "url": "https://devpost.com/software/equibot"
                    },
                    {
                        "title": "Talk It Out",
                        "description": "An AI-powered conflict resolution simulator to help develop your soft skills.",
                        "story": "Inspiration: Our inspiration for Talk It Out came from personal experiences with conflict resolution scenarios, where we often felt unprepared to handle real-life situations effectively. We recognized a significant gap between theoretical knowledge and practical application opportunities for developing these essential soft skills.What it does: Our web app simulates realistic conflict resolution scenarios where users interact with an AI co-worker through voice. As you speak into your microphone, our program transcribes your message and feeds it to our carefully prompt-engineered AI, which responds dynamically to create an authentic conversation experience. Moreover, our AI also considers nonverbal gestures too, as communication isn\u2019t solely about words, via facial recognition software through the webcam. After each session, the AI provides specific feedback based on established frameworks like Nonviolent Communication and the Thomas-Kilmann Conflict Management Model.How we built it: We built Talk It Out using Next.js and React for our frontend development, while integrating OpenAI and ElevenLabs APIs to create realistic simulations that help users improve their social and conflict resolution skills in a practical setting.Challenges we ran into: Our main challenge was fine-tuning prompts to ensure the AI would engage in natural conversation while providing actionable, constructive feedback that users could immediately apply to improve their conflict resolution abilities.Accomplishments that we're proud of: We're proud of rapidly bringing this concept to life and creating a tool that addresses a real need for practical soft skills development in a safe, accessible environment.What we learned: Through this project, we gained valuable insights into prompt engineering complexities and learned how to integrate multiple APIs seamlessly to create a cohesive user experience that feels natural and educational.What's next for Talk It Out: We plan to expand our scenario library to cover diverse conflict types and personalities, and implement a backend system that enables personalized learning paths by tracking user progress and tailoring feedback over time.",
                        "github": "https://github.com/emilioece/sfhacks",
                        "url": "https://devpost.com/software/talk-it-out-ydk79r"
                    },
                    {
                        "title": "Sign2Me",
                        "description": "An AI-powered web app, utilizing computer vision, machine learning, and Google Gemini to create an accessible American Sign Language (ASL) learning platform. ",
                        "story": "Inspiration: We were inspired by the theme of Tech for Good and the desire to make learning American Sign Language (ASL) more accessible, interactive, and fun. Communication is a fundamental human right, yet there are still major gaps in access between hearing individuals and the Deaf/Hard of Hearing community. We wanted to build a tool that not only teaches ASL in a beginner-friendly way but also uses AI to support and guide users throughout their learning journey.What it does: Sign2Me is a web app that helps users learn and practice ASL letters through:Real-time hand tracking with MediaPipeASL gesture recognition powered by a custom-trained machine learning modelInteractive practice sessions that test users on random signsAI-generated feedback using Google\u2019s Gemini API to coach users as they signResponsive UI built with React and Tailwind CSS,The app tells you which letter to sign, uses your webcam to track your hand, and gives you feedback to help improve your accuracy. Once the correct sign is detected, you're prompted to move on to the next letter!How we built it: We used a full-stack architecture:Frontend: Built with React, styled with Tailwind CSS, and hosted on VercelBackend: A Flask server that:Processes live webcam input via MediaPipePredicts the signed letter using a custom-trained ML modelUses Gemini (Google Generative AI API) to provide real-time, sign-specific feedback,Deployed using RailwayMachine Learning:Trained a classifier on our own custom-collected samplesFocused on x/y coordinate data from MediaPipe hand landmarks to improve generalization and reduce 3D noise,Challenges we ran into: Model Confusion: Letters like \u201cC,\u201d \u201cG,\u201d and \u201cO\u201d were visually similar and hard to distinguish\u2014especially with variations in hand size, angle, and lightingReact + Tailwind + Webcam: Integrating the MediaPipe webcam stream broke some Tailwind/PostCSS configurations, leading to a full rebuild of our frontend environmentTiming Feedback Logic: Designing a system that could provide non-intrusive real-time coaching (not just after a correct answer) required careful state management and prompt designAccomplishments that we're proud of: Successfully integrated a real-time ML pipeline between React and FlaskBuilt a beautiful, accessible UI that provides visual user feedbackUsed Gemini AI not just for basic prompts, but to dynamically help users improve based on their actual sign inputCreated a working MVP that helps users learn ASL letters in a fun, encouraging wayWhat we learned: How to deploy and connect a full-stack web app with React + FlaskHow to train and tune a MediaPipe-based ASL recognition model using minimal features (just x/y coordinates)How to use frameworks like React and TailwindHow to prompt-engineer Gemini for contextual guidance based on model outputsHow to balance UX, accessibility, and technical functionality for a more inclusive appWhat's next for Sign2Me: We\u2019d love to keep building on this foundation! Some next steps include:Expanding the model to support two-handed signs and dynamic gestures like J and ZAdding user login and streak tracking to encourage habit-buildingIntroducing multi-language UI options for broader accessibilityCreating lesson-based modules with progressive difficultyExploring voice-to-sign translation for real-time communication",
                        "github": "https://github.com/brandon-kf-lee/Sign2Me",
                        "url": "https://devpost.com/software/sign2me"
                    },
                    {
                        "title": "WheelScore",
                        "description": "Introducing Metrics for Building Accessibility: Using AI Agents to Compute Wheelchair Coverage Scores",
                        "story": "",
                        "github": "https://github.com/aneesh6214/WheelScore/tree/main",
                        "url": "https://devpost.com/software/wheelscore"
                    },
                    {
                        "title": "GradeAssist",
                        "description": "GradeAssist uses AI to create rubrics and instantly grade assignments, saving instructors time and giving students fast, detailed feedback for better learning outcomes. ",
                        "story": "",
                        "github": "https://github.com/usatie/sf-hacks-2025",
                        "url": "https://devpost.com/software/gradeassist"
                    },
                    {
                        "title": "Kalai",
                        "description": "Kalai is an online art auction platform where artists sell their work and donate a portion of proceeds to meaningful causes \u2014 turning creativity into impact.",
                        "story": "Inspiration: Art ispowerful\u2014 it sparks emotion, tells stories, and challenges norms. But we asked ourselves: What if art could also give back? That question became the spark forlink, a platform where artists can showcase their creations and donate a portion of their auction proceeds to charitable causes. It's wherecreativity meets impact.What it does: Kalai is a full-stack web platform thatempowersartists to list their work for auction, engage with art lovers, andsupport charities. Artists can create profiles, upload artwork, set auction parameters, and choose a cause to support. Buyers can bid on pieces, explore artist profiles, and follow auctions in real time.How we built it: Kalai was built using:Frontend: React + TypeScript with Vite, styled using TailwindBackend: Node.js, Express, MongoDB (Mongoose)AI Integration: Google Gemini for generating artwork descriptionsAuthentication: JWT-based auth system with protected routesCloud Deployment: Render for backend and Vercel for frontend deployment,Challenges we ran into: One of our biggest challenges was to implement areal-time bidding systemto allow buyers to see live updates during an auction \u2014 bid placements, countdown timers, and dynamic UI transitions.We explored usingWebSockets via Socket.IO, and while we managed to establish bid broadcasting between clients and the server in a test environment, several obstacles emerged:Handlingconcurrent usersand race conditions when two bids arrive simultaneouslyManagingauthenticationover socket connections to ensure only verified users could participateBuilding a smooth, reactivefrontend animation flowduring active auctions,Due to these complexities, we chose not to deploy the real-time features to production just yet. Instead, we opted to finalize the auction creation flow and focus on stability and core features.Real-time bidding is our next major milestone, and we\u2019ve already laid the technical foundation to get there soon.\ud83d\udcf8 Image Uploads to Base64We needed to allow artists to upload artwork images easily. Managing local file uploads and converting them to base64 strings for backend storage was tricky at first, especially with multiple image support. We solved this by implementing a reusablehandleImageUploadfunction and validating image size and type on the client.\ud83d\udd10 Decoding JWT on FrontendExtracting the authenticated user\u2019s ID from a JWT for associating artworks or auctions caused some hiccups. We used thejwt-decodelibrary and safely extracted theuserIdat the start of each form to simplify downstream logic.,Accomplishments that we're proud of: Building Kalai was about more than just shipping features \u2014 it was about creating an end-to-end platform that empowers artists to sell their work to a global audience. We\u2019re incredibly proud to say:Kalai is now a fully functional, production-ready art auction marketplace.From the ground up, we architected an application where:\ud83e\uddd1\u200d\ud83c\udfa8 Artists caneasily upload their artwork, add auction details, set donation percentages to charities, and receive AI-powered help writing descriptions.\ud83d\udcb0 Buyers can explore artwork, view artist profiles, and soon will be able to place bids on pieces in real-time.\ud83e\udd1d Every auction created on Kalai gives back \u2014 withcharity support built directly into the listing flow, ensuring that art has both beauty and impact.,This wasn\u2019t just a prototype. Kalai includes authentication, database integration, dynamic forms, image processing, and scalable UI components. It\u2019s clean, modular, and already integrated with AI and charity logic \u2014 a product ready for the real world.What we learned: Building Kalai taught us what it really means to take an idea from 0 \u2192 1 \u2014 not just in code, but in listening to users and translating that into meaningful features.Early on, we realized that building something \"cool\" isn\u2019t enough \u2014 it has to beuseful. So we did informal customer discovery on Friday, tapping into our local network of artists at UCSC and we asked:\u269b\ufe0f React + Tailwind UI SystemBuilt reusable, responsive components withreact-hook-formandzodpowering complex, validated form flows.\ud83d\uddbc\ufe0f Image Upload PipelineArtists upload multiple images, which are base64-encoded client-side and sent to MongoDB\u2014no external storage needed yet.\ud83d\udd10 Role-Based JWT AuthUsed token decoding to separate artist and buyer workflows across the stack, enforcing access control with Express middleware.\ud83e\udd16 Gemini AI + Prompt UXIntegrated Gemini via Axios; prompts dynamically combine system templates with user instructions, returning personalized descriptions inline.\ud83e\udde0 WebSocket-Ready Auction LogicLaid foundation for real-time bidding withsocket.io, scoped by auction ID, including emit/listen patterns for reactive updates.,What's next for Kalai: We\u2019re just getting started.Kalai is already usable today\u2014but our vision goes far beyond a simple auction site. We\u2019re building a vibrant, creator-first platform that elevates digital art and social impact simultaneously. Here's what's on our immediate roadmap:We're finalizing oursocket.iointegration to supportlive auction updates, countdowns, and bid placements without refreshing. This will make bidding feel as dynamic as a real gallery room.To enable real transactions, we\u2019re integratingStripe for buyer paymentsandautomated payoutsto artists and charities. We want every auction to feel safe and seamless.Artists will soon haveportfolio pagesshowcasing their past and active pieces, with afollow systemto keep collectors engaged and notified.We're exploring ways to leverage Gemini and embeddings to buildsemantic searchandcollector-style recommendations\u2014think Spotify Discover Weekly, but for fine art.The current UI works on mobile, but we\u2019re redesigning parts of it for atouch-first experience, especially for last-minute bids on the go.",
                        "github": "",
                        "url": "https://devpost.com/software/kalai"
                    },
                    {
                        "title": "Sneky.Tech",
                        "description": "Sneky.Tech is an AI-Powered Study tool. Users upload an audio recording of a lecture, and it will be condensed into an easily digestible summary. You can also generate quizzes based on the lecture! ",
                        "story": "",
                        "github": "https://github.com/RubeculaBee/RubeculaBee.github.io",
                        "url": "https://devpost.com/software/sneky-tech"
                    },
                    {
                        "title": "WalkMate",
                        "description": "Your Smart Mobility Companion",
                        "story": "Inspiration: Traditional mobility tools like walking sticks or basic GPS devices don\u2019t offer enough contextual awareness for blind or visually impaired individuals. We wanted to build something more intelligent\u2014a system that can not only guide users but also recognize people and objects around them, providing a richer, safer, and more independent experience.What it does: WalkMate is a voice-activated smart assistant for the blind and visually impaired. It:Recognizes familiar faces and identifies relationships.Helps dementia patients by providing contextual cues about people nearby.Provides GPS-based navigation with real-time voice updates.Detects nearby obstacles using depth estimation and warns the user.Offers direction-based guidance to nearby objects like chairs, doors, or exits.Optionally chats with users using Google\u2019s Gemini AI for extra support.,How we built it: WalkMate is a multi-threaded Python system that integrates several powerful technologies:Face Recognition: OpenCV + LBPH model trained on local dataset.Object Detection: YOLOv5 with real-time bounding box + Depth-Anything for depth mapping.Voice Commands:speech_recognition+pyttsx3and OpenMind API for bidirectional audio.Navigation: Google Maps API + geolocation withgeopyandgeocoder.Gemini AI(Optional): Text/voice chat integration with Google's generative language model.,We built and connected multiple modules (app.py,map.py,sub.py) with shared speech queues and voice interactions to deliver a unified, real-time assistive experience.Challenges we ran into: Combining multiple real-time inputs like voice, video, and GPS was tricky to synchronize.Depth estimation needed tuning for adaptive object thresholds to avoid false warnings.Voice recognition required filtering and timing logic to reduce misfires.Training the face recognizer for high accuracy while maintaining performance was a challenge.GPS precision limitations on indoor environments restricted full navigation testing.,Accomplishments that we're proud of: Built a real-time, voice-controlled, multi-sensory assistant from scratch.Successfully integrated depth perception with object recognition for smarter alerts.Created a facial memory system that supports dementia patients with context.Developed voice navigation with real-time map data and auditory instructions.Modular and extensible code structure ready for future integration with hardware or wearables.,What we learned: How to integrate voice recognition, computer vision, and depth mapping in a single flow.How to build accessible technology that really considers user context and usability.The importance of modular design for scalability and team collaboration.Hands-on experience with multiple APIs including Google Maps, OpenMind, YOLOv5, Depth-Anything, and Gemini.,What's next for WalkMate: Integrating with wearables like smart glasses or phones for on-the-go usage.Adding live obstacle tracking with LiDAR or IR sensors for real-world testing.Supporting offline maps and localized navigation.Expanding face memory with more contextual cues (e.g., workplace, family).Offering real-time emergency alerts or fall detection features.,",
                        "github": "https://github.com/SujeetJawale/VisionHelp",
                        "url": "https://devpost.com/software/walkmate-ubckqz"
                    },
                    {
                        "title": "VegeeTales",
                        "description": "VegeeTales is a smart, transparent food-tracing system that lets consumers scan a QR code on their produce to instantly see its journey from farm to store.",
                        "story": "Inspiration: My recent focus on improving personal health and eating habits led me to research the nutritional value and potential drawbacks of various foods. A surprising discovery about organic strawberries being sprayed with pesticides labeled as \"organic\" shook my confidence in the produce I buy. This experience sparked an idea: what if consumers could access the entire lifecycle of a product, from farm or factory to store, right from their phone? This hackathon seemed like the perfect opportunity to develop a proof-of-concept for this idea.What it does: VegeeTales is a mobile-based solution that uses QR codes on produce packaging or stickers to provide consumers with an engaging and informative summary of the product's journey. By scanning the QR code, users can access a simplified, readable format of the produce's lifecycle, leveraging powerful Large Language Models (LLMs) to condense complex data into easily digestible content.How we built it: Our team built VegeeTales using a combination of technologies:Flask for web developmentMongoDB Atlas for cloud database storageGemini API to harness the capabilities of fast and powerful LLMsAWS for deploying our application,Challenges we ran into: Two significant challenges we faced were:Determining the optimal data storage approachCrafting effective prompts to generate reproducible, organized, and thoughtful responses from the LLM,Accomplishments that we're proud of: We're proud of several achievements:Successfully deploying our web app to production using AWS EC2Utilizing our free .tech domain, vegeetales.techCreating a functional proof-of-concept that showcases the potential of VegeeTales,What we learned: -Throughout this project, we gained valuable experience in:Integrating multiple technologies to achieve a common goalWorking with LLMs to simplify complex dataOvercoming challenges related to data storage and prompt engineering,What's next for VegeeTales: In the future, we plan to:Refine our data storage approach to ensure scalability and efficiencyExpand our database to include a wider variety of produce and productsEnhance the user experience through UI/UX improvements and additional featuresExplore partnerships with farmers, manufacturers, and retailers to integrate VegeeTales into their supply chainsContinuously update and improve our LLM prompts to ensure accurate and informative responses for consumers.,",
                        "github": "https://github.com/leo-kildani/VeggieTales#",
                        "url": "https://devpost.com/software/vegeetales"
                    },
                    {
                        "title": "OM1 Bounties",
                        "description": "OM1 Bounties",
                        "story": "",
                        "github": "https://github.com/OpenmindAGI/OM1/pull/195",
                        "url": "https://devpost.com/software/om1-bounties"
                    },
                    {
                        "title": "Hello, Alina",
                        "description": "Say hello to Alina\u2014an AI friend just a phone call away, designed especially for elders. No apps or tech skills needed\u2014just call, talk naturally, and Alina can help and make life easier.",
                        "story": "About the Project: A few days ago, as I was walking to the university along Font Blvd, I saw an elderly lady waiting at the stop for the 57 Muni bus. She didn't know how to check the bus arrival times on her phone and asked me when the next bus was coming. Interestingly, I've encountered this same lady multiple times at the same stop around the same time. This incident made me realize that many people, like my own grandparents and even my parents, struggle with using smartphones effectively. My grandparents, for instance, have smartphones but primarily use them just for making phone calls. If they ever need to troubleshoot something or view pictures I send, I always need to guide them step-by-step over the phone. Not everyone has someone available to help them like that.This inspired me to create an AI-based phone service that provides companionship, assistance, and information through simple phone calls, accessible even to those not comfortable with smartphone technology.Through this project, I learned how critical user-centered design is, especially when developing solutions for seniors or individuals who aren't tech-savvy. I explored integrating conversational AI platforms, telephony APIs like Twilio, speech-to-text, and text-to-speech technologies. Additionally, I gained deeper insights into building accessible technology, ensuring ease of use through natural language interactions.I used Twilio's telephony API to handle incoming phone calls and integrated it with a Google's Gemini model to provide a natural, conversational experience. Gemini 2.0 supports speech-to-speech conversations which allows the model to understand the user's emotions and other small but important voice features and respond accordingly. I also integrated APIs to fetch live data like Muni bus arrival times and general web search information, making the AI genuinely useful for daily tasks.",
                        "github": "https://github.com/rehmatsg/helloalina",
                        "url": "https://devpost.com/software/alina-fud8hp"
                    },
                    {
                        "title": "Echo Lens",
                        "description": "Echo Lens empowers web accessibility with cutting-edge image text extraction and multilingual summaries, transforming your browsing experience into effortless discovery.",
                        "story": "Inspiration: Echo Lens was born out of a desire to make digital content more accessible, especially for people in developing countries where standard accessibility guidelines might not always be enforced. The idea was inspired by a close friend who, despite acquiring visual impairment at a young age, never allowed it to hinder his achievements, from winning Olympiads to excelling academically and even exploring creative pursuits like standup comedy. His journey highlighted both the potential of assistive technologies and the limitations of existing screen reading software. This project aims to fill those gaps by not only providing text-to-speech capabilities but also by introducing innovative features like image recognition and multilingual summaries to help users get the essence of long-form content quickly.What it does: Echo Lens is a Chrome extension designed to transform how users interact with digital content. It goes beyond traditional screen reading tools by:Enhanced Accessibility:Offering a more intuitive and enriched reading experience for users with visual impairments.Image Support:Integrating advanced image recognition capabilities to describe visual content that standard screen readers often overlook.Summarization:Using state-of-the-art summarization techniques powered by Gemini to distill lengthy blogs and articles into concise, need-to-know insights.Multilingual Options:Allowing users to select their preferred language so that both the full content and the summary are accessible in multiple languages.,This combination of features is particularly useful for developers and general users alike, especially in regions where access to high-quality assistive technology is limited.How we built it: We built Echo Lens as a browser extension using modern web technologies. The project leveraged:Web Extensions API:To integrate seamlessly into the Chrome browser and provide a smooth user experience.Gemini Integration:For real-time content summarization, ensuring that users receive quick and accurate summaries of extensive web pages.Cross-Platform Tools:To incorporate multilingual support and robust image recognition, making the tool versatile across different languages and content types.,Our development process was agile and iterative. We started with core functionality and gradually integrated advanced features based on user feedback and emerging research in accessibility technologies. The open-source nature of the project on GitHub allowed for community contributions, further enriching the project.Challenges we ran into: Developing Echo Lens wasn't without its hurdles. Some of the major challenges included:Balancing Performance and Functionality:Integrating image recognition and summarization in real-time posed significant performance challenges. We had to optimize our code to ensure that the extension remained responsive, even on resource-limited devices.Multilingual Complexity:Implementing seamless multilingual support required extensive testing and calibration. Ensuring that translations maintained the nuance and accuracy of the original content was a non-trivial task.Accessibility Standards:With accessibility being a central focus, we had to ensure that our tool met high standards and worked reliably across diverse user scenarios. This meant rigorous user testing and iterating on feedback, particularly from users in developing regions where accessibility tools are scarce.,What we learned: Through building Echo Lens, we gained valuable insights:User-Centered Design is Key:Engaging with end-users early in the development process helped us prioritize features that genuinely improve accessibility.The Importance of Performance:Even the most innovative features can be undermined by performance issues. Streamlining our code and optimizing resource usage was crucial.Collaboration Fuels Innovation:Open-source contributions enriched the project, offering diverse perspectives that helped us address unique challenges, especially in multilingual and image processing domains.Accessibility is a Global Need:Our work underscored how critical it is to develop tools that serve communities in developing countries, where access to cutting-edge assistive technologies is limited.,What's next for Echo Lens: Looking forward, there are exciting opportunities to expand Echo Lens:Expanded Language Support:We plan to incorporate additional languages and dialects to make the tool even more accessible globally.Enhanced Customization:Future iterations may include personalized settings for different types of content and user preferences, ensuring a tailored experience.Community-Driven Improvements:We will continue to welcome contributions from developers worldwide, fostering an ecosystem where continuous improvements are driven by real user needs.Integration with Other Platforms:Beyond Chrome, exploring compatibility with other browsers and platforms could widen our reach and impact, further supporting those who need accessible technology the most.,Echo Lens represents our commitment to innovation in accessibility, and we\u2019re excited about the journey ahead. Whether you're a developer eager to contribute or a user in need of better screen reading tools, we invite you to join us in shaping the future of digital accessibility.",
                        "github": "https://github.com/Haaarrrssshhh/EchoLens-sfhacks.git",
                        "url": "https://devpost.com/software/echolens-wpu1xv"
                    }
                ],
                [
                    {
                        "title": "Gobin - Smart Recycle Scanner",
                        "description": "Smarter Recycling, Smarter World.",
                        "story": "Inspiration: The inspiration for Gobin came from having trouble figuring out if the reusable bottles we were handed out in the beginning could be recycled. But also from witnessing how inaccessible recycling guidelines are for people with disabilities. For individuals with visual impairments, cognitive differences, or motor challenges, small recycling labels, inconsistent symbols, and text-heavy instructions create barriers to proper waste disposal. We recognized that traditional recycling systems exclude millions of people, exacerbating environmental harm. Motivated by the principles of universal design, we aimed to create a tool that democratizes recycling knowledge through multimodal, assistive technology, empowering all users to participate in sustainability efforts, regardless of ability. With recycling rates stagnating worldwide and confusion about what can and cannot be recycled, we realized that technology could bridge this knowledge gap. We were motivated to create a solution that empowers individuals to make informed recycling decisions through accessible technology, ultimately contributing to a more sustainable future.What it does: Gobin is an AI-powered accessibility-first recycling assistant that helps users determine the recyclability of items through image recognition designed to break down barriers in waste management. Users simply upload a photo of an object, and Gobin identifies the object and its material composition, analyzes its recyclability with a detailed percentage score, provides specific disposal recommendations and tips, and offers greater insights on biodegradability and bio-recycling potential.How we built it: We built Gobin using the usual modern tech stack:\nFrontend: Next.js with React for a responsive, component-based UI\nStyling: Tailwind CSS and ShadCN for clean, efficient styling\nAI Integration: Google's Gemini 2.0 Flash model for image analysis and recyclability assessment\nHosting and DNS: Netlify for hosting and Tech domain for the DNS domain for our project (https://gobin.tech/)\nDatabase: MongoDB Atlas for storing scan results and global object historyChallenges we ran into: Building Gobin presented several engineering challenges. One of the larger ones was optimizing the image processing pipeline to provide quick results while maintaining accuracy on mobile devices. Setting up MongoDB Atlas and designing an efficient schema for storing scan results was incredibly tricky because the cluster kept timing out. Thankfully that problem went away when I switched from the guest wifi to the hotspot.Accomplishments that we're proud of: We are particularly proud of developing a comprehensive recycling app featuring an intuitive UI, a powerful material identification system, and a vast recycling database.What we learned: This was one of our first times using MongoDB Atlas for building scalable applications and we appreciated the speeds and JSON like approach of the database.What's next for Gobin - Smart Recycle Scanner: We have ambitious plans to expand Gobin's capabilities. We plan to implement community-driven accessibility through crowdsourced alt-text descriptions for obscure objects, and integrating with smart devices for voice assistant compatibility. We also plan to include location-based recycling guidelines, community features, an expanded material database, offline functionality, a mobile app, gamification, integration with smart bins, and business solutions, to make proper recycling accessible to everyone and contribute to a more sustainable future.",
                        "github": "",
                        "url": "https://devpost.com/software/gobin-smart-recycle-scanner"
                    },
                    {
                        "title": "EMO-Bot",
                        "description": "Feel lonely? Too busy have pet? No problem, Emo-bot got you! ",
                        "story": "Inspiration: We\u2019ve always been inspired by those lovable robots from movies\u2014Baymax, R2-D2, WALL-E\u2014the ones that aren\u2019t just machines, but companions with real personality. We wanted to bring a bit of that magic into real life. Everyone deserves a buddy who\u2019s there for them, even if it\u2019s made of plastic and bolts. So we set out to build something that doesn\u2019t just do tasks, but feels alive. A little quirky, a little emotional, and a lot of heart.What it does: EMO-Bot is your personal emotional companion. Whether you\u2019re chilling alone at home or going through a rough day, it\u2019s there for you\u2014no judgment, just vibes. Using real-time emotion detection, it senses your mood and reacts accordingly. If you\u2019re happy, it\u2019ll roll up and celebrate with you. If you\u2019re feeling down, it\u2019ll gently approach and try to comfort you with its little gestures and presence. It doesn\u2019t just recognize how you feel\u2014it responds like a true friend would.How we built it: We started with a dream and a bag of hex bolts. Then came the glue, duct tape, and a 3D printer\u2014we built the hardware with pure determination (and probably too much Red Bull). EMO-Bot was truly a labor of love\u2026 and caffeine.On the tech side:\ud83e\udde0 MediaPipe \u2013 Used Google\u2019s ML framework for face detection, face mesh, and hand tracking to understand the user\u2019s emotional state.\ud83c\udfa5 OpenCV \u2013 Captured and processed real-time video input for emotion recognition.\ud83d\udd27 mBot Firmware Hacking \u2013 We reverse-engineered and modified mBot\u2019s firmware so it could work with our Linux system and compile C++ code smoothly.\ud83c\udf53 Raspberry Pi \u2013 Acted as the robot\u2019s brain, running all the processing and keeping everything on track.It was duct tape meets deep learning\u2014and somehow, it worked.Challenges we ran into: Let\u2019s just say\u2026 hardware is hard.We ran into all sorts of issues trying to get everything working together. The Raspberry Pi 5 turned out to be a bit of a power-hungry monster, and powering it efficiently was a constant struggle. Building a robot as software engineers felt like doing surgery with a wrench\u2014we had to figure out balance, structure, how the bot should look, and how to hold it all together (spoiler: duct tape helped, but only so much).One of the biggest challenges was hacking around the mBot\u2019s closed-source Arduino firmware. Instead of using their existing tools, we scrapped it and built our own custom firmware from scratch to make it work with Linux and C++.It was chaotic, messy, and held together by sheer willpower (and screws we kept losing).Accomplishments that we're proud of: EMO-Bot can accurately detect human emotions and gestures using just a camera and MediaPipe, and it actually responds in a way that feels meaningful. Whether you\u2019re smiling or frowning, it vibes with you and reacts in kind. We even gave it a cute little robot voice that makes it feel more alive.But the biggest win? We hit our original goal: build a robot that could feel like a friend. And we did that. That\u2019s something we\u2019re genuinely proud of.What we learned: Hardware is hard\u2014but super rewarding when it finally works.Emotion detection isn\u2019t just about fancy models; it\u2019s about tuning it to real-world, messy human behavior.Duct tape can fix many things\u2026 but not everything.Building something that \u201cfeels alive\u201d is more about personality than perfection.Bringing lipstick to a hackathon is weirdly useful for facial detection calibration.Red Bull is not a substitute for sleep (but it does give you wings\u2026 for a few hours).Collaboration, hacking things together, and laughing through failure is what makes hackathons awesome.\nUse whatever you have with you, it works!!What's next for EMO-Bot: Through this project, we realized there\u2019s a real need out there\u2014for people who feel lonely, who can\u2019t afford a pet, or simply don\u2019t have the time to take care of one. EMO-Bot could be the perfect alternative: a low-maintenance companion that still brings emotional warmth and connection.Our next goal is to take EMO-Bot beyond the hackathon and turn it into a real product. We want to refine its design, make it more responsive, and accessible to anyone who needs a little emotional support in their day-to-day life. Whether it\u2019s for kids, the elderly, or busy young adults, we believe EMO-Bot can be that little friend that makes life feel just a bit less lonely.",
                        "github": "https://github.com/codejapoe/SFHacks",
                        "url": "https://devpost.com/software/emo-bot"
                    },
                    {
                        "title": "ICUthere",
                        "description": "ICUthere uses empathetic voice AI to streamline hospital triage, cutting ER wait times, easing staff burnout, and improving care quality for patients.",
                        "story": "\ud83e\udde0 Background: ER teams juggle life-or-death decisions under intense pressure, often relying on outdated or manual triage processes. This leads to longer wait times, inconsistent prioritization, and staff burnout.ICUthereis an AI-assisted triage platform that analyzes patient inputs \u2014 like speech, facial expression, and symptoms \u2014 to recommend urgency levels and surface high-risk cases instantly. It empowers healthcare staff with faster decisions, improved care, and reduced legal exposure.\ud83d\udca1 What is ICUthere?: ICUthere enhances ER triage through:\ud83e\uddfeVideo + Audio Patient Intake\u2013 Patients record symptoms via a friendly voice interface\ud83d\udccaTriage Dashboard\u2013 View patients sorted by severity with AI-generated recommendations\ud83d\udea8Urgency Flags\u2013 Instantly highlights critical red-flag symptoms\ud83d\udc69\u200d\u2695\ufe0fStaff Coordination Panel\u2013 Tracks who's on shift and current case assignments\ud83d\udd01Real-Time Queue\u2013 Dynamically updates as new cases come in,\u2728 Essential Features: \ud83e\udd16AI Triage Engine\u2013 Combines voice tone, symptoms, and expressions to recommend care levels\ud83d\udde3\ufe0fEmpathetic Voice Assistant\u2013 Guides patients through check-in calmly and naturally\ud83d\udd04Live Patient Queue\u2013 Sorted by urgency and synced in real time\ud83e\uddd8Burnout Reduction Tools\u2013 Automates repetitive intake questions for staff relief\ud83d\udd0cEHR-Ready Integration\u2013 Built for compatibility with hospital systems,\ud83d\udcda Takeaways: Designing AI for emergency care requires a deep balance between technical precision and emotional sensitivity. We explored how empathy-driven voice tech can ease patient anxiety while supporting medical staff under pressure.Real-time triage powered by AI inputsEmotion-aware voice assistant built with HumeSeamless, intuitive dashboard for ER decision-making,Expand symptom training dataAdd multilingual supportEnhance AI transparency for medical staffImprove HIPAA compliance and patient data security,",
                        "github": "https://github.com/Jeffuz/icuthere",
                        "url": "https://devpost.com/software/deez-7y01ae"
                    }
                ]
            ]
        },
        {
            "title": "LakerHacks",
            "location": "State University of New York at Oswego",
            "url": "https://lakerhacks.devpost.com/",
            "submission_dates": "Apr 19 - 20, 2025",
            "themes": [
                "Beginner Friendly",
                "Machine Learning/AI"
            ],
            "organization": "LakerHacks",
            "winners": false,
            "projects": [
                [
                    {
                        "title": "Study Buddy",
                        "description": "No need to study cruddy \u2014 just use your Buddy!\r\nYour personal AI study partner\u2014turn notes into flashcards, ask smart questions, and study smarter, not harder.",
                        "story": "",
                        "github": "https://github.com/Param-05/StudySnap",
                        "url": "https://devpost.com/software/study-buddy-27isty"
                    },
                    {
                        "title": "QuizInvaders",
                        "description": "Test your vocabulary skills through a space-invader-esque matching game. ",
                        "story": "Inspiration: Gamification is a popular concept that helps connect students to educational facts through their love of games.  QuizInvaders is inspired by Space Invaders, a popular 1970s arcade game, and Quizlet. Currently, I am taking Spanish 101 as required by my language requirements at SUNY Oswego, so I thought this would be a great way to inspire myself to memorize vocabulary.What it does: QuizInvaders is a matching shoot 'em up video game where the user has to match the term with their vocabulary. It can be used to match any two terms together, but for demonstration purposes, I am using it to match Spanish words with their English translation.How we built it: QuizInvaders is made completely on plain HTML, CSS, and JavaScript. No frameworks have been used.Challenges we ran into: One of the main challenges was collision detection. For a strange reason, the left side of any text would not collide with any of the projectiles. After adding a collision box, I realized the collision box started from the center of the word rather than the left edge. After translating the collision over, everything worked smoothly.What we learned: JavaScript can be tricky with its error handling and requires a lot of out-of-the-box manipulation to make things work. While that can be nice for some developers, I do not particularly enjoy having to mess with different components to make a new, simple change. This was a fun way to affirm my belief that I do not like JavaScript for anything that is not related to web design.What's next for QuizInvaders: If I were able to allocate more time towards this project, I would want to addHP System for the UserDifferent stages depending on different sets of vocabularySound effects,",
                        "github": "https://htmlpreview.github.io/?https://github.com/s-Aura-v/QuizInvaders/blob/main/index.html",
                        "url": "https://devpost.com/software/quizinvaders"
                    },
                    {
                        "title": "CSC:GO",
                        "description": "CSC: GO is an educational game that helps students learn by taking them through different combat stages where they must answer questions to progress. The questions are generated using Gemini.",
                        "story": "",
                        "github": "https://github.com/alrey-91/Oswego-hackathon-game",
                        "url": "https://devpost.com/software/csc-go"
                    },
                    {
                        "title": "Voxel",
                        "description": "The AI-powered event management app for Architecture programs to learn event management.",
                        "story": "Inspiration: My sister is an architect, and I\u2019ve seen firsthand how difficult it can be for her to visualize her projects in real time. Her process usually involves creating detailed 3D models and then rendering them to be as photorealistic as possible \u2014 a workflow that\u2019s both time-consuming and lacks real-time interaction. That got me thinking: how might we make it easier for architecture students to visualize their projects, and how might we extend that capability to a broader creative and collaborative space? That question became the inspiration for Voxel.What it does: Voxel is a conceptual app designed to revolutionize how people visualize and interact with physical spaces. It helps users \u2014 especially architects and designers \u2014 scan real-world environments in 3D, interact with them using natural language, and collaborate in real time. The goal is to combine Neural Radiance Fields (NeRF), Language Radiance Fields (LeRF), and photogrammetry to build immersive environments that can be explored and modified intuitively.Imagine asking, \u201cWhere\u2019s the kitchen in this layout?\u201d or \u201cShow me how the space looks with a skylight.\u201d Voxel makes that kind of interaction possible.How we built it: I designed this app from scratch and currently, Voxel exists as a proof-of-concept prototype built in Figma. The prototype explores user flows, wireframes, and the core functionality of the app, including scanning, semantic querying, and layout editing.The technical implementation (in progress) will involve:NeRFs for photorealistic 3D scene generation.LeRFs enhanced with LLMs for language-based querying within 3D spaces.Photogrammetry as a fallback or complementary method for quick scanning.A UX design system built to support both technical and non-technical users.,Challenges we ran into: One of the biggest challenges was ensuring the app was user-friendly \u2014 especially for non-technical users like students who may not be familiar with advanced 3D tools. Designing intuitive interactions around complex technologies like NeRFs and LeRFs required a lot of thinking about simplification.Another challenge was differentiating Voxel from existing apps like Polycam, Luma AI, and RealityScan. While those apps focus heavily on 3D scanning, Voxel aims to stand out by combining spatial understanding with semantic interaction and collaboration \u2014 features that are still largely missing from mainstream tools.Additionally, because many of these technologies are still evolving, we had to work with certain assumptions about performance, scale, and accessibility that will need to be tested in future iterations.Accomplishments that we're proud of: Taking a complex, tech-heavy idea and turning it into a clear, user-centered design.Creating a proof-of-concept that blends AI, spatial computing, and intuitive UX.Identifying real user pain points (inspired by my sister\u2019s experience) and addressing them with innovative thinking.Building a vision that goes beyond 3D \u2014 into intelligent, real-time, and collaborative space interaction.,What we learned: This project opened my eyes to the possibilities of mixed reality, AI-generated environments, and how natural language processing can enhance the way we interact with 3D spaces. I dove deep into the workings of NeRFs, LeRFs, and photogrammetry, and learned how they can work together to create rich, interactive environments.More importantly, I learned how critical usability is when introducing emerging tech to everyday users.What's next for Voxel: Voxel is currently in the design phase, but the next steps include:Building a working prototype using open-source NeRF models and integrating LeRF-like functionality via language models.Testing usability with architecture students and designers to refine interaction patterns.Exploring cloud-based infrastructure for real-time scene generation and collaboration.Eventually releasing a beta version for early adopters to experiment with intelligent 3D design.,Voxel is just getting started \u2014 but its potential to reshape how we design and experience spaces is limitless.",
                        "github": "",
                        "url": "https://devpost.com/software/voxel-sj9vnb"
                    },
                    {
                        "title": "LogiBeasts",
                        "description": "LogiBeasts is a Pokemon style battler crossed with math flash cards. Determine the attack modifier by the accuracy and speed of your answers.",
                        "story": "Inspiration: Inspired by the Pokemon games and the retro educational games of the 90'sWhat it does: LogiBeasts helps students enjoy doing studying math by turning it into a game. It can also help keep advanced students arithmetic skills sharp when they haven't seen a number in problem in yearsHow we built it: This is a browser based game built in SvelteKit and Spring Boot, using WebSockets for two way communication between players.Challenges we ran into: Configuring authentication and making sure cookies are passed correctly is always a challenge. The other main challenge for most of our members was doing frontend development for the first time.Accomplishments that we're proud of: Although we we'ren't able to get many of the animations and sprites into the game due to time constraints, out head of art produced some really gorgeous sprites.What we learned: Frontend development,What's next for LogiBeasts: Getting out art assets into the game, adding security, game balance, and more content!",
                        "github": "https://github.com/LakerHacks25-TCG/tcg",
                        "url": "https://devpost.com/software/logibeasts"
                    },
                    {
                        "title": "Funnel",
                        "description": "Lock in learning.",
                        "story": "--",
                        "github": "",
                        "url": "https://devpost.com/software/funnel-ftj1dp"
                    },
                    {
                        "title": "Scratch Battle",
                        "description": "Turn-based multiplayer video game that implements Scratch blocks in its core gameplay, introducing young players to coding logic through a fun gaming experience experience. ",
                        "story": "Inspiration: scratchWhat it does: scratchHow we built it: from scratchChallenges we ran into: scratching a lotAccomplishments that we're proud of: scratchedWhat we learned: scratchWhat's next for Scratch Battle: scratch 2",
                        "github": "https://github.com/JCxYIS/lakerhacks2025-edublock",
                        "url": "https://devpost.com/software/scratch-battle"
                    },
                    {
                        "title": "act",
                        "description": "Create mental associations through constraints!",
                        "story": "Inspiration: When I learn things it helps to create mental associations with other things. But sometimes I wonder if I'm missing something that would make stuff easier to remember.What it does: Act takes in a markdown file, and will parse it for statements with formats specified in a json file. It will then make a knowledge graph of these statements, and then uses a Subgraph Isomorphism based algorithm to solve the graph, and file possible associations you might have missed.How we built it: I used d-lang to write this so I could learn a new language while working on my project.Challenges we ran into: Subgraph isomorphism is a NP-Complete problem, so this solution depends on the users inferencing rules being extremely simple.Accomplishments that we're proud of: I am glad I was able to get this project working in the time I did, especially since the primary solver algorithm is so specific I couldn't find a paper to help with the implementation.",
                        "github": "https://github.com/bob16795/act",
                        "url": "https://devpost.com/software/act-d4gsav"
                    },
                    {
                        "title": "Education Station",
                        "description": "Get instant feedback on your work! ",
                        "story": "",
                        "github": "https://github.com/AidanHafer/LakerHacks-AIFeedback",
                        "url": "https://devpost.com/software/education-station"
                    },
                    {
                        "title": "Ultimate Addition Tester :gasp:",
                        "description": "Train your child to be the best adder in the entire world!",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/ultimate-addition-tester-gasp"
                    },
                    {
                        "title": "FlashMinds",
                        "description": "Smarter studying. Magical reading. One mission: next-gen learning!",
                        "story": "Inspiration: We were inspired by the idea of making studying more collaborative, more intelligent, and more fun. While tools like Quizlet are great, they often feel passive and repetitive. We wanted to build something that not only helps students study efficiently but also lets them play, compete, and learn together. We also made a tool to make storytelling feel magical again, blending traditional books with modern AR to bring characters to life.What it does: FlashMinds is a web-based flashcard platform that lets users:Create their own flashcard setsInstantly generate flashcards from a topic using AIChallenge friends to multiplayer quiz games using shared decks\nAR BookWhen a specific book page is shown to a webcam, a 3D animated dragon appears,How we built it: Frontend: Built with HTML, CSS, and JavaScriptAI Flashcard Generation: Integrated with Gemini API to generate flashcards based on a user\u2019s input topicMultiplayer Game: Used socket connections (like Socket.IO or Firebase) to enable live sessions between friends using a shared room codeFlashcard Management: Created a system for users to input, edit, and view custom flashcards dynamicallyUnity and Vuforia Engine for AR\n## Challenges we ran intoMaking real-time multiplayer quiz sessions smooth and sync properly across clientsFormatting AI-generated content into usable flashcards without overwhelming the userDesigning an intuitive interface while working under time constraints,Accomplishments that we're proud of: Built a working AI-powered flashcard generator from scratch in under 24 hoursImplemented live multiplayer quiz functionality without requiring user loginsCreated a fun and modern UI that makes the studying experience feel enjoyable.\n-Created a functional AR scene that responds to a real-world imag,What we learned: How to integrate AI into an educational workflow in a helpful and balanced wayHow to manage real-time multiplayer state and user connectionsHow design and UX can dramatically impact the motivation to study,What's next for FlashMinds: Add account-based features so users can save and revisit their decksTrack stats and progress over time with gamified learning streaksIntroduce AI-powered quiz explanations and auto-generated imagesExpand multiplayer modes: time attack, teams, leaderboard tournaments,",
                        "github": "https://github.com/nidhip26/FlashMinds.git",
                        "url": "https://devpost.com/software/flashminds"
                    },
                    {
                        "title": "AI Track Tutor",
                        "description": "Kahoot, but you can generate questions based on your documents and you can race your fellow students",
                        "story": "",
                        "github": "https://github.com/tglynn2/hackathonProject",
                        "url": "https://devpost.com/software/ai-track-tutor"
                    }
                ]
            ]
        },
        {
            "title": "HOF Hacks",
            "location": "HOF Capital ",
            "url": "https://hof-hacks.devpost.com/",
            "submission_dates": "Apr 18 - 19, 2025",
            "themes": [
                "Fintech",
                "Health",
                "Machine Learning/AI"
            ],
            "organization": "Tech@NYU",
            "winners": false,
            "projects": [
                [
                    {
                        "title": "CloudCopilot Lite ",
                        "description": "CloudCopilot lets you voice-command GCP tasks\u2014\u201cDeploy VPC & VM\u201d\u2014and it clicks through live in-browser. It remembers state, logs actions, and secures ops. Cloud like texting a friend.",
                        "story": "Inspiration: The Google Cloud Console is powerful\u2014but dense. Even experienced engineers waste time clicking through nested menus just to complete basic tasks like spinning up a VM or checking billing. Watching fellow hackers get bogged down in repetitive web UI flows sparked a simple idea: what if the Console could drive itself? We envisioned an AI-driven teammate that understands your goal, navigates the Console autonomously, and gets things done\u2014visibly and reliably.What it does: CloudCopilot Lite is a persistent browser agent powered by large language models and Playwright. You run a single Python script, manually sign in to GCP once, and then the AI takes over. Give it plain-English instructions like \u201ccreate a storage bucket named hackathon-2025 in us-central1\u201d and it walks through the Console\u2014just like a human would\u2014clicking, typing, and confirming every step. Crucially, it saves cookies and session state, so the agent can continue where it left off across multiple requests without restarting or re-logging in.How we built it: We built CloudCopilot Lite using thebrowser-usewrapper around Playwright, combined with OpenAI\u2019s GPT-4 models to plan and execute GUI steps. We added persistent browser context management so the agent doesn\u2019t forget previous logins or open tabs. The script handles planning (\"what steps are needed to complete this goal?\") and execution (\"which buttons to click and what text to type?\") using two distinct LLM roles, both operating inside a live browser window. All actions are observable, logged, and reversible for debugging.Challenges we ran into: Login Persistence:GCP doesn\u2019t allow automated logins due to security prompts and 2FA. We solved this by using a headed (non-headless) browser with persistentuser_data_dir, allowing the user to log in once manually and re-use the session in future tasks.GCP\u2019s Dynamic UI:The Console uses lazy-loaded elements and inconsistent selectors. We had to implement an index-based and ARIA-aware element strategy to ensure the agent clicks the right buttons even as the UI changes.Browser Lifecycle Management:Avoiding the overhead of opening new browser instances for each task was key. We configured the system to reuse a single Playwright context and cache state between actions, which dramatically reduced latency and resource usage.,Accomplishments we\u2019re proud of: The agent successfully completed common tasks like provisioning a storage bucket, creating a VM, and reviewing billing data\u2014all without manual intervention after login. In testing, CloudCopilot Lite executed a full GCP bucket creation flow in under 40 seconds\u2014about a quarter of the time it typically takes a human user navigating menus. The architecture is lightweight and requires no external services or servers.What we learned: GUI automation is fragile without persistent memory. Stateful browser agents combined with deterministic planning make LLM-powered automation dramatically more reliable. Visual feedback (i.e., seeing the browser in action) helps build trust in the agent's behavior. We also learned that GCP\u2019s interface, while slick, wasn\u2019t designed for automation\u2014which made DOM resilience and context preservation crucial to success.What\u2019s next: Add headless streaming (e.g., VNC or video) so users can monitor the agent remotelyIntroduce multi-step task memory and simple multi-command pipelinesExtend support to other cloud providers like AWS and Azure using the same persistent-browser patternEnable chat-based interfaces by plugging this into a simple FastAPI backend (future work),CloudCopilot Lite turns the GCP Console into a smart, self-navigating assistant\u2014reducing the burden of clicks and context switching, and giving developers more time to think, build, and ship.",
                        "github": "https://github.com/dpraj007/AI_Cloud_employee",
                        "url": "https://devpost.com/software/cloud-employee"
                    },
                    {
                        "title": "Second Brain",
                        "description": "Second Brain is an intelligent autonomous personal knowledge-base designed to help users organize and recall information effectively. It uses bleeding-edge MCP to smartly integrate with many services.",
                        "story": "Inspiration: The Information Crisis:Knowledge workers spend 9+ hours weekly searching for information they've already seen40% of workday productivity lost to context-switching and information management73% of critical business knowledge never effectively capturedThe average company loses $3M+ annually to this hidden productivity tax,What it does: How we built it: Frontend:Next.jsshadcn/uiTailwindCSSv0.dev,Backend:\n-Next.js\n-Python\n-FastAPI\n-Google Gemini\n-AWS EC2\n-SQLite\n-Several MCP serversTechnical details: Using state of the art agentic AI and MCP technology, we provide integrations with many services, including but not limited to:Google mapsAirbnbWeb scrapingYour personal knowledgebase (SQLite)DatetimeFilesystem,What's next for Second Brain: Technical:Automatically processes meetings, documents, and communications through file uploads and live recordingOptimize database item relationship to improve LLM near-neighbor context fetching and decision making\nNontechnical:Improve UI accessibility and responsivenessCreate a more unique and recognizable visual aesthetic and brand,",
                        "github": "https://github.com/declspecl/second-brain",
                        "url": "https://devpost.com/software/second-brain-kqxlhy"
                    },
                    {
                        "title": "Care Compass",
                        "description": "We are a personalized, multilingual platform helping NYC immigrants and low-income residents find the health, housing, and food support they\u2019re eligible for\u2014clearly, quickly, and safely.",
                        "story": "\ud83e\udde0 Inspiration: It can be difficult to connect New Yorkers struggling with housing insecurity and health care accessibility to resources. Whether one is a recent immigrant experiencing a language barrier, or a single parent working multiple jobs to make ends meet, at-risk individuals face unique challenges to self-advocacy. Knowing exactly where to find help specific to their situation is the first step to acquiring that support. Care Compass allows users to avoid the burden of expending their limited time and energy searching for services and instead swiftly and succinctly delivers it to them.\ud83d\udca1 What it does: Care Compass is a personalized, multilingual web app that helps NYC immigrants and low-income residents find nearby healthcare, housing, and food support they\u2019re actually eligible for.Through a short guided screening, users receive tailored resources, know their rights, and get connected to help\u2014fast.\ud83d\udee0 How we built it: We rapidly prototyped and iterated usingVercel\u2019s v0platform. Despite its low-code nature, we dove deep into the generated code, refining components, connecting toSupabase, and building a dynamic user flow that feels seamless and empowering.\ud83d\udea7 Challenges we ran into: While brainstorming ideas for our project we needed to spend most of our time thinking of the most efficient way to streamline getting resource information to individuals in need, particularly in NYC. We had some trouble narrowing down our ideas for our intended demographic and what that demographic might be looking for in a technology intended to help them.\ud83d\ude80 What\u2019s next for Care Compass: \ud83d\udcf2 SMS integration for reminders and offline-friendly access\ud83d\uddfa Expanded resources across more public services\ud83c\udf0e Scalability to serve communities beyond NYC,",
                        "github": "https://github.com/cnnrdel/HOF-hackathon",
                        "url": "https://devpost.com/software/care-compass-n28i4e"
                    },
                    {
                        "title": "interTabs",
                        "description": "InterTabs is a Google Chrome extension designed to save, organize, and suggest browser tabs intelligently to reduce the repetitive work of finding and opening the tabs each time. ",
                        "story": "Project Summary: InterTabs \u2013 A Smart Tab Management and Suggestion Extension: InterTabs is a Google Chrome extension designed tosave, organize, and suggest browser tabsintelligently. It tackles the everyday chaos of having too many open tabs by providing a structured way to manage them, especially for users who regularly return to the same set of websites for specific tasks.Technology Stack: Frontend: HTML, CSS, JavaScript, jangoBackend: Chrome Extension APIs + local storage for data persistence, Cloud serviceAI Integration: OpenAI API (or equivalent) for generating group names and task-related tab suggestionsUI/UX Design: Custom icons, pop ups, and animationsImpact and Importance: With the rise of multitasking and web-based workflows, users frequently find themselves overwhelmed with dozens of open tabs. This clutter not only slows down performance but also reduces productivity due to the time spent manually searching, reopening, or closing tabs.\ninterTabs solves this by:Key Features: Save Tab Groups: Store all tabs in a window under a group. Each group includes a name (AI-generated or user-customized) and pin/delete options.\nOne-Click Window Launch: Instantly open a saved group in a new window.\nAI-Powered Naming: Automatically suggests a name for the tab group based on content.\nAI Window Creation: Enter a task like \u201cResearching machine learning,\u201d and the AI will open a window with relevant tabs.\nTab Overview: Displays saved groups with details like name, pin/delete actions.\nCustomization: cosmetics, and settings for tab generating amount per session.Development Challenges: Managing Chrome\u2019s Extension API: Handling tabs, windows, and storage across sessions required careful coordination with asynchronous APIs.\nAI Integration: Balancing AI accuracy and speed for naming and tab suggestions while keeping latency low.\nUser Experience: Designing a UI that remains non-intrusive and non-complicated but helpful, especially with pop-up prompts and AI-generated actions.\nTesting Edge Cases: Handling windows with mixed tab types (e.g., local files, extensions, or blank pages), or restoring tab groups after crashes.Scalability: Future versions of interTabs may include:\nCloud sync across devices\nCustomizable routines based on calendar/tasks\nEnhanced AI models for deeper context-based suggestions\nImplement avatars navigating for each function to tour users aroundDeliverables and Demo: Function-by-Function Demo Video:linkWebsite Showcase:linkAccomplishments that we're proud of: We think we created this extension that can really be helpful to us and more people, and create a lot of convenience in people's lives. We also felt proud of the interface animation effects and the UI design, which is very clear, clean, and easing.What we learned: We learned a lot about how to create a Chrome extension and better understanding of its different functions. \nand how to deploy using Vercel conveniently!",
                        "github": "https://github.com/interfinityOfficial/interTabs",
                        "url": "https://devpost.com/software/intertabs"
                    },
                    {
                        "title": "Neptune",
                        "description": "Neptune is an AI-powered concept mapper that transforms scattered notes into an interactive knowledge graph. ",
                        "story": "Inspiration: We were inspired by note-taking apps likeNotion, Obsidian, and Evernoteand how they allow users to link pages to one another. However, our team noticed a common pain point where users often find themselves spending time trying to capture the main idea from lengthy notes and often get lost in how it relates to other ideas. Our team wanted to find a solution that prioritizes an efficient environment that allows learners to have a visual overview of their knowledge base.What it does: Neptune allows users to create an automated and AI-powered concept map of their notes. It draws from the key topics and connects them across notes.How we built it: Our team used a Next.js, FastAPI, and PostgreSQL tech stack. In the backend, we utilized a GPT model, coupled with prompt engineering, to extract keywords from the user's notes. We chose this model because it can efficiently extract the semantic meaning and infer topics based on overall concepts compared to other models. From there, we used the same model to cluster notes under similar topics, and draw relationships between them--generating relationship strength based on the note contents. This is then translated into a graph data structure, which is displayed on the user interface.Challenges we ran into: A roadblock we encountered was deciding which model to use, since many models overlap in strengths and weaknesses. However, with thorough research, we were able to narrow it down to the GPT model for its superior ability to understand natural languages. Another difficulty we experienced was fine-tuning the prompt to get the right amount of generalization across the topics.Accomplishments that we're proud of: Our team is proud of the progress we have made over a short amount of time. It was each member's first time learning about natural language processing techniques, and we were able to build the product we envisioned.What we learned: Our team learned about the array of popular natural language processing models available. We felt that by taking the time to weigh each model's pros and cons, we were not only able to make a sound decision for our project, but also learn about the specific characteristics of each model.What's next for Neptune: A feature we can implement in the future is parameters that allow users to tailor their knowledge map to their preferences. For example, the users can choose how connected or sparse, specific or broad they want their map to be. Another improvement would be to have a customizable map, giving users the freedom to personalize the graph's appearance. Lastly, Neptune has the potential to create mappings from topics to exact sources in the note files, allowing users to quickly navigate through their knowledge base.",
                        "github": "https://github.com/reesedychiao/neptune",
                        "url": "https://devpost.com/software/neptune-6gin8s"
                    },
                    {
                        "title": "AppealRX",
                        "description": "Get mental health support without the paperwork. AppealRX uses AI to simplify claims, so you can focus on healing \u2014 not forms.\r\n\r\nPlease watch both videos for the full demo!",
                        "story": "Please watch both videos for the full demo!: Inspiration: Mental health claims are denied at nearly twice the rate of other medical services. Solo practitioners\u2014who represent 47% of psychologists\u2014often lack the billing staff, legal teams, or infrastructure to effectively handle appeals. As a result, patients are left to navigate complex policy language and administrative processes on their own. We saw an opportunity to build a tool that empowers individuals to advocate for themselves with the support of AI, precedent, and payer-specific policy guidance.What it does: AppealRX helps patients rewrite and improve insurance appeal letters after a denial of mental health services. Users upload their draft, doctor\u2019s note, and any supporting documents. The system analyzes the submission, retrieves similar past appeals and policy rules, and returns a refined draft with specific suggestions.AppealRX also includes a success prediction model\u2014a classifier trained on historical appeal outcomes\u2014which scores each draft\u2019s likelihood of being approved. Together, these tools give users both actionable edits and a clear understanding of how their case compares to successful ones.How we built it: We built the frontend usingTypeScriptand deployed it withVercelfor fast iteration and deployment.On the backend:MongoDBstores user drafts, model outputs, and metadata.Pineconeis used as ourvector database, indexing past appeals, insurer policies, and mental health guidelines.A fine-tunedBERT modelclassifies whether a draft appeal is likely to result in a favorable decision.A customRetrieval-Augmented Generation (RAG)pipeline retrieves relevant legal and clinical language, which is incorporated into our rewriting and suggestion engine.,To manage vector scaling, we route queries by insurance provider and preprocess large PDFs (e.g., insurer policies) to extract and index only relevant content.Challenges we ran into: The biggest challenge was data quality. Medical appeals data was often unstructured and inconsistent. Many documents included the appeal decision in the note body, introducinglabel leakage. We resolved this by applyinglemmatizationand filtering out any sentence containing outcome-related terms like \"denied\", \"approved\", or \"overturned\".We also had to manage the rapid growth of our vector database. As more appeals and documents were added, retrieval became slower. To address this, we built arouting layerthat narrowed search scope based on the user\u2019s insurer. This ensured fast and relevant responses.Finally, working with real policy documents meant preprocessing PDFs to extract key terms (e.g., definitions of medical necessity, exclusions for mental health treatment) and making them accessible to the RAG pipeline.Accomplishments that we're proud of: Built a fully functioning RAG-based appeal assistant in under 36 hours.Fine-tuned a BERT classifier to predict appeal outcomes with strong accuracy.Designed a scalable retrieval system that adapts to user context (e.g., insurer).Created a clean and intuitive user interface that lowers the barrier to action for patients facing denials.,What we learned: We learned how important proper preprocessing is when working with real-world clinical data. We also gained experience in building scalable vector pipelines and routing logic to maintain speed and relevance.From a product perspective, we saw how underserved the space of patient advocacy is\u2014especially in mental health. Many people simply do not appeal because the system is hard to understand and navigate. A small improvement in clarity can dramatically increase someone\u2019s chance of getting the care they need.What's next for AppealRX: We plan to:Expand beyond mental health to other types of medical denials (e.g., chronic conditions, surgery).Partner with advocacy nonprofits and solo practices to pilot real-world use.Add provider-facing tools that allow clinicians to auto-generate appeals on behalf of patients.Build dashboards to track common reasons for denial and recommend best practices for successful appeals.,Ultimately, we aim to make the appeals process less opaque, more fair, and more accessible for everyone\u2014starting with those who need it most.",
                        "github": "https://github.com/wlu03/hofhack",
                        "url": "https://devpost.com/software/appealrx"
                    },
                    {
                        "title": "CrunchMail AI",
                        "description": "Your Gmail, supercharged with AI - smart labeling, voice control, summarization, automated reply  and intelligent organization at your fingertips",
                        "story": "Inspiration: The inspiration for Gmail Smart Labeler came from a universal pain point we observed in professional life - email overload. As students and professionals ourselves, we noticed how much time people spend manually organizing, categorizing, and responding to emails. The statistics were eye-opening: professionals spend nearly one-third of their workweek managing emails instead of focusing on meaningful work. This realization sparked our mission to leverage AI to automate these repetitive tasks.What We Learned: Throughout this project, we gained valuable insights and skills:Deep understanding of Google's OAuth2 authentication and Gmail API integrationHands-on experience with Google's Gemini AI for natural language processingImplementation of semantic search using FAISS and sentence transformersVoice recognition and text-to-speech integrationBuilding scalable Flask applications with proper session managementThe importance of rate limiting when working with AI APIsHandling complex email MIME types and attachments,Auto-categorizes emails using Gemini AICreates and manages intelligent label hierarchiesDetects email priority and importance automaticallyFilters spam, newsletters, and promotionsVoice commands for email operationsAI-generated smart repliesTemplate managementSemantic search across emailsAdvanced filtering optionsAttachment content searchQuick preview capabilitiesEmail volume trendsInteraction patternsProductivity metricsSecure OAuth2 authenticationGmail API integration,How We Built It: Our development process followed these key stages:Challenges We Faced: Future Improvements: We plan to enhance the project with:Advanced workflow automation capabilitiesCustom AI training for organization-specific needsIntegration with popular project management toolsMulti-language supportMobile application development,",
                        "github": "https://github.com/HOF-Hacakathon/Final",
                        "url": "https://devpost.com/software/crunchmail"
                    },
                    {
                        "title": "CampusSync",
                        "description": "CampusSync recommends events based on your vibe, pulling from top campus event sites to match your schedule, interests, and mood, all in one place so you never miss out on what matters.",
                        "story": "TRACK: Freedom from Repetitive Work [Automation & AI]: Our project eliminates the repetitive, time-consuming task of searching across multiple platforms for campus events. By automatically syncing with a user's calendar and aggregating event data from sources like Engage and school-specific sites, we streamline the entire event discovery process. AI is used to generate concise summaries, vibes, and categories for each event, making browsing effortless. Instead of wasting time digging through scattered event listings or missing out entirely, users get relevant events served directly to them,  freeing up their time and mental load for more meaningful experiences.Inspiration: We noticed students constantly juggling between: \nMissing great events due to schedule conflicts, feeling overwhelmed by too many options, and struggling to find events matching their current mood. \nTraditional calendars only shows the user's free time. However,CampusSync tells how to best use your free timebased on your emotional state and cognitive patterns.What it does: AI-Powered Event Matching: Analyzes your mood via text input (\"stressed\", \"energetic\", etc.), Cross-references with your calendar availability, Recommends events that will improve your current state.Time-Aware Suggestions: Considers post-event recovery time, Avoids \"social hangover\" from back-to-back events, Respects your biological prime time (morning person vs night owl).Mental Health Focus:  \"Mood Stabilization Mode\" for stressful periods, Anti-FOMO protection system, Cognitive bandwidth monitoring.,How we built it: We built CampusSync using a combination of:\nNode.js and Express.js to create the backend API for handling user requests and managing data.\nical.js to parse and process iCalendar (.ics) files, allowing us to extract events and identify free slots.\nMongoDB to store user data and free slots, utilizing Mongoose for database management.\nReact.js on the frontend to allow users to upload their calendar data and visualize upcoming events, making it easy to track what\u2019s going on and when.Challenges we ran into: Data parsing: Handling and parsing iCalendar data efficiently was more complex than expected, especially ensuring that all data lines were properly unfolded and parsed correctly.Time zone handling: With users from different time zones, ensuring that events were properly displayed according to each user\u2019s time zone was tricky and required careful management of time zone data.Syncing events: Matching up free slots while preventing overlap with other users\u2019 schedules posed a problem, especially when dealing with group events.,Accomplishments that we're proud of: Seamless integration with Google and Outlook calendars: Successfully enabled users to upload .ics files and sync events from multiple calendar platforms, which worked seamlessly on the backend.Real-time free slot extraction: We developed an efficient algorithm to detect and display free time slots for users, enabling automatic scheduling and minimizing conflicts.User-friendly interface: The frontend was designed to be simple and intuitive, with an easy-to-use calendar interface where users can quickly see their free time and events.,What we learned: Collaborative development: We learned how to effectively combine individual components into a cohesive group project, integrating each member\u2019s contribution seamlessly.Real-world application: This project gave us experience working on a real-life use case with real-time data, from backend processing to frontend UX.Creative and critical thinking: We exercised both creativity and analytical problem-solving to build features like mood-based recommendations and time-aware suggestions.Time-constrained teamwork: We practiced distributing tasks efficiently under tight deadlines, prioritizing features, and managing our workload through collaborative tools and clear communication.,What's next for CampusSync: Mobile app: We plan to expand CampusSync to mobile platforms so students and faculty can have access on-the-go.\nCater not just to students but to anyone\u2014professionals, community organizers, and casual users\u2014by adding templates for business meetings, social gatherings, and personal time\u2011management.\nEvent recommendations: We want to develop a recommendation engine that can suggest events based on user interests, past attendance, or study groups.",
                        "github": "https://github.com/Md905908324/hofhack25.git",
                        "url": "https://devpost.com/software/campussync"
                    },
                    {
                        "title": "SceneCraft.ai",
                        "description": "Studio\u2011grade videos in minutes: AI analyzes your topic, scripts and animates host\u2011avatar dialogues, embeds ads and music\u2014all in one intuitive dashboard. Democratize professional content.",
                        "story": "Creating impactful content is challenging\u2014not everyone has the resources, creativity, or audience reach needed. Recognizing this gap inspired us to revolutionize content creation, aiming to empower anyone and everyone to produce original, engaging, and high-quality videos effortlessly.Our AI-powered platform generates entire video experiences by leveraging generative AI and advanced diffusion models. It analyzes both trending and specific topics, synthesizes engaging transcripts for interactive dialogues between creators and lifelike AI companions, and dynamically integrates advertisements and promotional elements directly into video scripts. Additionally, the platform creates AI-generated 30-second promotional videos, enhancing creators' outreach and engagement.New Feature: The platform also provides background music suggestions tailored to the content theme, helping creators enhance emotional tone and viewer immersion seamlessly.We integrated cutting-edge AI technologies, combining advanced diffusion models for realistic video synthesis, local LLMs and NLP techniques for insightful content analysis, and generative AI models for script creation and dynamic dialogue interaction. By thoroughly analyzing top-performing videos and prominent content creators, we crafted a system capable of autonomously generating contextually relevant and compelling video scripts.We encountered challenges including accurately analyzing diverse internet content, achieving realistic and seamless lip synchronization for AI-generated companions, and ensuring the dynamism and contextual accuracy of the dialogues. Generating realistic movements for the companions, ensuring their natural synchronization with dialogues, and preventing excessive or intrusive movements that might compromise video quality were significant challenges. Balancing creativity with control was also challenging, as we aimed to offer fine-grained customization to creators without overwhelming the user experience.We successfully developed a holistic AI solution capable of independently creating complete, interactive videos featuring realistic AI companions. Achieving precise lip synchronization, natural companion movements, and realistic interactions marked significant milestones. We are especially proud of the system's dynamic ad integration, ensuring monetization seamlessly aligns with engaging content.Through this project, we gained deep insights into the immense potential and limitations of AI-driven content creation. We learned the importance of balancing creativity, technological innovation, and user-friendliness, realizing how crucial fine-tuned control is for content creators.Moving forward, we aim to enhance the platform further by refining realism and interaction capabilities, expanding content personalization features, and exploring new, innovative forms of video content. Using advanced generative AI and diffusion models, we plan to introduce a wider variety of unique companions and diverse companion movements, creating entirely new video formats and experiences. Ultimately, we plan to democratize innovative, high-quality content creation across diverse audiences worldwide.",
                        "github": "",
                        "url": "https://devpost.com/software/scenecraft"
                    },
                    {
                        "title": "SnapPhil",
                        "description": "SnapPhil transforms the job application process from hours to seconds. Our AI-powered Chrome extension automatically fills applications, generates tailored cover letters.",
                        "story": "Inspiration: The idea for SnapPhil was born from personal frustration. After losing my job at quick heal tech, during the economic downturn, I found myself spending 30-45 minutes on each job application, repeatedly entering the same information across different platforms. The breaking point came when I realized I had spent over 20 hours in a single week just filling out forms rather than preparing for interviews or improving my skills.I thought, \"There must be a better way.\" Modern AI technology could surely automate this tedious process. If AI could write essays and generate images, why couldn't it fill out job applications and draft personalized cover letters? This insight sparked the development of SnapPhil \u2014 named after the concept of\"snapping\"through applications withphilosophical precision.What We Learned: How We Built It: SnapPhil was developed as a Chrome extension using a combination of:Frontend: JavaScript, HTML/CSS with a focus on minimal, non\u2011intrusive UIBackend: Node.js server handling the AI processingAI Integration: Custom\u2011trained language models to understand both resume content and job application fieldsData Security: End\u2011to\u2011end encryption for all user dataForm Recognition: Computer vision techniques to identify and interact with various input field types,The development process involved creating an extension that could scan web pages, identify form fields, extract relevant information from uploaded resumes, and intelligently match the right information to the right fields \u2014 all while maintaining a lightweight footprint.Challenges We Faced: Despite these challenges, we've created a tool that transforms hours of tedious work into seconds of automated efficiency, giving job seekers back their most valuable resource:time.What's Next for SnapPhil: Full support for multi\u2011page applicationsEnhanced integration with major job boardsAdvanced analytics to help users optimize their application strategyMobile companion app for tracking applications on the go,SnapPhil isn't just about automation \u2014 it's about empowering job seekers to focus on what truly matters: finding the right opportunity and preparing to shine in interviews.How to use? Quick guide: For first time users:Visit linkhttps://chromewebstore.google.com/detail/SnapPhil%20-%20AI%20Job%20Application%20Assistant/meeggjlhfjliakcccgcjlddpblcedlml?hl=enon desktopAdd to chromeVisit a job siteOpen the extension by clicking extensionIt will popup on pageGo to Settings > signin > create account > close the extension by clicking on pageReopen the extension, it will workAdd your details like resume pdf, name and other details by clicking on edit option on first tab, save.Tapautofill. A small overlay will come in center bottom of page that will send request to AI along with your details and Job listing text available on page. And in 20 \u2013 40 seconds it will start filling the formIt only works for single page Job application. If multiple pages it will still fill the visible section but you will need to run this for all sections by clicking next on job page.,For recurrent users:Visit the job pageOpen SnapPhil extensionThis extension keeps a track of your job applications where you successfully applied using it. You can toggleApplied, Rejected, Offerfor your own reference later. Even the cover letter be available to download as pdf if you ever applied to a job that had cover letter generationContext dumpfeature: This allows user to give any additional details that weren't mentioned in resume. User can also instruct extension for eg: Fill optional details as well, etc. Immigrants can mention laws even though AI will try to estimate it based on resume and even name.Each user has a dynamic range of applications everyday as per server load. It's a limit to provide service to all users or can defer based on the plan user is on with SnapPhil.,How it works?: Scans the page > send data to AI > AI give out appropriate values > extension fill it live in front of userWhat data we store?: Resume, AI generated cover letters (For users to track), User details whatever user gives in context dump, Jobs user has applied. As this is important to accurately fill the applicationsThere are 4 tabs in popup mentioned aboveFor beta usage this server works by estimating how many users are there and how many applications can we fill each day for users (900 approximately) every day combined for all users.",
                        "github": "",
                        "url": "https://devpost.com/software/snapphil"
                    },
                    {
                        "title": "Segment",
                        "description": "Bringing enterprise-grade network security to small businesses through seamless automation",
                        "story": "At any software business, thousands of devices communicate over networks, and while this connectivity fuels innovation and growth, it also introduces vulnerabilities. Large enterprises often manage these risks through costly solutions offered by companies like Cisco or Palo Alto. But what about small and medium businesses (SMBs)? They face the same threats (malware, ransomware, unauthorized access) but lack the budget or expertise to build complex, segmented networks. That gap is what drove our team to create Segment, a solution designed to bring enterprise-grade network segmentation and cyber resilience to businesses of all sizes.We saw that effective network segmentation (keeping certain parts of the network isolated from others) is one of the best ways to contain security breaches. If an attacker compromises one segment, segmentation prevents lateral movement and limits the damage. The problem is that defining and enforcing segmentation rules is hard. It typically requires manual configurations, a deep understanding of both the business structure and the underlying network, and expensive hardware. We wanted to automate that process.Segment is built around a key insight: business policies (like \u201cstudents can only access student resources\u201d or \u201cHR should not access production servers\u201d) can be translated into enforceable network rules. We use NetBox as the starting point, allowing users to design and define network segments and relationships in a centralized, user-friendly platform. From there, we built a Python-based policy translator that pulls this data from NetBox\u2019s API and converts the abstract policies into Zeek-compatible enforcement logic (Python scripts!!). Zeek then acts as our policy enforcer, monitoring live network traffic and generating real-time alerts when communication violates the defined segmentation policies.Our project architecture is a three-stage pipeline:NetBox as Policy Design \u2013 We customized NetBox to serve as our policy design layer. Using custom fields and structured tags, we allowed users to define which network segments (like departments or trust zones) exist and which are permitted to communicate. This created a source of truth for network segmentation policies.Python as Policy Translator \u2013 Our script pulls the segment relationships from NetBox\u2019s API, maps abstract business logic into concrete network rules, and generates Zeek scripts to enforce those rules. We designed the translator to handle a variety of models, including department-based, role-based, and sensitivity-based segmentation.Zeek as Policy Enforcer \u2013 Zeek monitors network traffic in real-time. Our generated scripts tell Zeek what to watch for: for example, if a development workstation attempts to talk to a production database, it flags this as a violation. The alerts are immediate and actionable, helping IT teams intervene before threats can spread.Challenges: One of the hardest challenges we faced was designing a flexible policy model that could capture the diversity of real-world business structures. Some organizations segment by department, others by data sensitivity, others by project teams. Our translator needed to support all of these use cases while remaining user-friendly and robust. There's only so many use cases you can optimize for in a 24 hour hackathon format, but we successfully designed Segment to be customizable as hacker communities develop their own Python scripts for unique security use cases. The future of Cyber is open source.Another major challenge was ensuring that our generated Zeek scripts were syntactically and logically correct. Even a small error could prevent the policy from being enforced or, worse, result in false positives or missed threats. We implemented validation steps to test scripts before deployment and built in support for different network topologies. Too strict of a policy could create unnecessary friction and reduce network performance, while too loose would be ineffective for security. We had to build an engine that could strike that balance and allow users to tune it according to their needs.What we\u2019re most proud of is making security accessible. We built something that doesn\u2019t require a team of security engineers to deploy.Any SMB can use Segment to improve their network hygiene and become more resilient to attacks. And we didn\u2019t stop at building a working prototype\u2014we validated it through real-world use cases, demonstrating how it can detect unauthorized traffic and contain threats by enforcing logical segmentation boundaries.Through this journey, we learned a lot. We deepened our understanding of how business structures shape network communication patterns, how segmentation can serve as a powerful security tool, and how to bridge the gap between policy and implementation. Most importantly, we learned how to build systems that prioritize both security and usability\u2014a rare but necessary combination.Looking forward, we see Segment evolving into a full-fledged segmentation framework.We want to add features like visual policy graphs, machine learning for detecting anomalous traffic, and even automatic remediation for policy violations. Our goal is to help businesses implement the principle of least privilege at the network level,without the need for expensive firewalls or proprietary systems.We want to give all businesses, regardless of size, the tools they need to defend themselves in an increasingly connected world. The delivery plan is where capital comes in, and HOF's talent network, scaling pipelines, can get us to that vision of a future free from repetitive work.",
                        "github": "https://github.com/Abidcat/network-segmentation-automatio",
                        "url": "https://devpost.com/software/segment-a7zod6"
                    },
                    {
                        "title": "AI Resume Builder",
                        "description": "Effortlessly craft standout resumes with AI. Refine your content and customize the format using any Overleaf template\u2014all in one sleek, professional platform.",
                        "story": "Inspiration: We set out to solve the hassle of converting resumes from PDF or DOCX formats into Overleaf templates. Our goal was to build a tool that\u2019s flexible and allows users to switch between different templates effortlessly.What it does: Right now, the app can parse resume PDFs to extract content and generate a resume based on that data. Users also have the option to manually enter their information through the profile section.How we built it: We put the UI together using Vercel\u2019s v0, which made things super fast and slick. For authentication and database stuff, we went with Supabase\u2014it handled everything smoothly. Styling-wise, we leaned on Tailwind CSS and Shadcn to give the app a clean, modern look.Challenges we ran into: Getting Supabase\u2019s policies set up just right took some trial and error. Vercel\u2019s v0 generated a lot of code, so cleaning that up and figuring out what to keep was a bit of a journey. And, of course... merge conflicts. So many merge conflicts \ud83d\ude2dAccomplishments that we're proud of: Successfully resolving merge conflicts caused by some chaotic \"vibe coding\" sessions.What we learned: We learned how to rapidly build beautiful UIs with Vercel\u2019s v0.What's next for AI Resume Builder: We're working on enabling seamless switching between resume templates using any Overleaf template of the user's choice.",
                        "github": "https://github.com/rsc1102/hof_hackathon",
                        "url": "https://devpost.com/software/ai-resume-builder-128t3o"
                    },
                    {
                        "title": "MedFlow",
                        "description": "MedFlow is a chatbot that connects directly to ClinicalTrials.gov, translating complex medical information into clear answers. Upload medical records, and MedFlow helps you discover unaware treatments",
                        "story": "MedFlow: Help When You Need It Most: Ever tried to google a symptom just for it to result in some extreme illness? But that's not it. Imagine when someone you love is sick, you want to move mountains for them, but medical jargon and limited information leave you dependent on whatever your doctor suggests.MedFlow changes this. It's a simple chatbot that connects directly to ClinicalTrials.gov and other research databases, translating complex medical information into clear answers about your loved one's condition. Upload their medical records, and MedFlow helps you discover treatment options your doctor might not know about yet.How It Works: You don't need a medical degree to use MedFlow. The interface is straightforward:\nUpload medical records or enter symptoms and diagnosis information\nAsk questions in everyday language - \"What treatments exist for stage 3 pancreatic cancer?\"\nGet answers based on the latest clinical trials and research, not just standard protocols\nThe chatbot handles the complicated part - searching through thousands of research papers and clinical trials to find relevant options based on your loved one's specific condition, age, medical history, and location.It doesn't replace doctors - it empowers you to have more informed conversations with them. Because sometimes hope exists in a clinical trial your doctor hasn't heard about yet.The Technology (For Those Who are curious): I'm not going to bore you with technical jargon, but for transparency:We built this with modern tools like React and Next.js for a smooth experience\nGPT 4.1 helps translate medical terminology into plain English\nOur FastAPI backend connects directly to ClinicalTrials.gov's API for real-time research updates\nYour data stays private - medical records are processed locally in FAISS VectorDB before extracting relevant information\nWhat matters isn't how we built it, but that it works when someone you love needs every possible option.Why This Matters: Healthcare shouldn't be a world where only medical professionals have the keys. MedFlow breaks down those walls, giving everyone\u2014regardless of background\u2014the tools to understand and participate in health decisions.By connecting patients directly to research opportunities, we're not just helping individuals\u2014we're accelerating medical breakthroughs by bridging the gap between innovative treatments and the people who need them.And perhaps most importantly, we've replaced those intimidating clinical interfaces with something that feels supportive. Exploring health options shouldn't add stress to an already difficult situation.Overcoming Challenges: Creating MedFlow wasn't without its hurdles:We developed a custom shadow system to maintain that comforting aesthetic across different display modes. The swipeable carousel required complex mechanics to feel natural and intuitive. When performance lagged, we reimagined our rendering approach and optimized animations to keep everything running smoothly.Balancing our softer design with accessibility needs meant finding creative ways to enhance contrast and navigation without sacrificing visual comfort. And making the conversational interface feel like talking to a supportive friend rather than a machine took countless iterations.Looking Forward: MedFlow will continue evolving with plans for:Voice input capabilities for hands-free access\nIntegration with wearable devices for real-time insights\nExpanded research connections\nSupport for multiple languages\nPersonalized dashboards for tracking health trendsAt its heart, MedFlow represents a promise\u2014to transform helplessness into agency, confusion into clarity, and isolation into hope. By merging technology with genuine empathy, we're creating a future where everyone has the tools to advocate effectively for their health and the health of those they love.",
                        "github": "https://github.com/TechPertz/MedFlow",
                        "url": "https://devpost.com/software/medflow-l0zu6m"
                    },
                    {
                        "title": "Chatalytics",
                        "description": "Chatalytics tracks Reddit comments & all major news in real time to predict stock moves. Just type a ticker - we\u2019ll show you live sentiment, insights, and price predictions. Start investing smarter.",
                        "story": "Inspiration: \u2b50\ufe0fHistorically, the stock market moves on news and emotion faster than anything else. But while institutional investors have access to expensive tools and proprietary feeds to capitalize on these shifts, everyday investors are often left behind \u2014 forced to scroll through Reddit, news headlines, and Twitter threads just to keep up.\u2b50\ufe0fThat\u2019s why we created Chatalytics \u2014 to quantify those emotional market movements in real time and make that insight available to everyone.\u2b50\ufe0fEvery day, millions of users and journalists publish opinions about companies online. Our platform captures these moments the second they happen, analyzes the content and sentiment, identifies the companies affected \u2014 and predicts the resulting stock price movement, instantly.\u2b50\ufe0fBy automating this entire pipeline with advanced machine learning, we\u2019re not just making faster predictions \u2014 we\u2019re democratizing access to real-time financial intelligence. Whether you're a casual trader or a data-driven investor, Chatalytics gives you institutional-grade insight, without the institutional price tag.What it does: \u2b50\ufe0fChatalytics predicts stock price changes in real time by analyzing online news sentiment, social media conversations, and historical stock price patterns.\u2b50\ufe0fReal-time Monitoring: Automatically captures financial news, Reddit posts, and user commentary the moment they're published.\u2b50\ufe0fSentiment Analysis: Uses advanced NLP models fine-tuned for financial language to evaluate sentiment and relevance.\u2b50\ufe0fCompany Mapping: Links posts to the correct stock ticker, even when users refer to companies by nicknames or symbols.\u2b50\ufe0fPrice Prediction Engine: Combines sentiment signals with historical price data to forecast short-term stock price movement.\u2b50\ufe0fFully Automated Pipeline: From data collection to prediction, the entire process runs without manual input, updating as fast as the internet does.How we built it: Used machine learning algorithm Gradient Boosting Regression Trees to predict the price change of the stockAWS RDS with PostgreSQL for data storageTypescript, React and Vite for front endPython and Flask for back endFully hosted on AWS EC2 (CHECK OUT THE TRY OUT LINK),Challenges we ran into: \u2b50\ufe0f Noise in Online Conversations\u2b50\ufe0fNot every mention of a company is meaningful. We had to develop robust NLP filters and relevance scoring models to distinguish between impactful comments (e.g. an earnings leak) and casual chatter (\u201cI like Apple products\u201d).\u2b50\ufe0fEntity Recognition Across Slang and Abbreviations\nUsers often refer to companies in non-standard ways (e.g. \u201cGOOG\u201d instead of \u201cAlphabet\u201d or \u201c$AAPL\u201d instead of \u201cApple\u201d). Building a custom entity recognition system that could understand tickers, nicknames, and contextual references was a key hurdle.\u2b50\ufe0fLatency in Data Processing\nTo be genuinely \u201creal-time,\u201d we had to optimize our data pipeline to process thousands of new comments per second with minimal lag \u2014 which meant engineering around rate limits, streamlining our ingestion system, and scaling our backend.\u2b50\ufe0fSentiment Ambiguity\nSarcasm, irony, and financial jargon make sentiment analysis especially difficult in this domain. We couldn\u2019t rely on off-the-shelf models. Instead, we fine-tuned transformer-based models (like BERT and FinBERT) specifically for finance-related sentiment.Accomplishments that we're proud of: \u2b50\ufe0fReal-Time Sentiment Engine\nBuilt a fully automated NLP pipeline that captures and analyzes financial news and Reddit posts in under 3 seconds from publication to prediction.\u2b50\ufe0fHigh Signal Precision\nAchieved a 72% accuracy rate in predicting the direction of stock price movement within 30 minutes of high-impact news events.\u2b50\ufe0fCustom Financial Sentiment Model\nFine-tuned a transformer-based model (based on FinBERT) specifically for financial language, outperforming baseline sentiment models by 18%.\u2b50\ufe0fScalable Architecture\nDeployed a production-ready backend capable of processing over 250,000 text data points per day with minimal latency using AWS and Kafka.\u2b50\ufe0fReal-World Backtesting\nRan backtests on over 1M news-comment-price pairs from the past 2 years, demonstrating consistent performance and predictive value.What we learned: \u2b50\ufe0fSentiment Alone Isn\u2019t Enough\nRaw sentiment scores can be misleading. Context \u2014 like the credibility of the source or the stock\u2019s recent volatility \u2014 is crucial for accurate predictions.\u2b50\ufe0fSpeed Matters More Than We Thought\nIn financial markets, timing is everything. Even a 5-second delay in processing news can make the difference between a winning and a losing signal.\u2b50\ufe0fNot All Mentions Are Equal\nA single comment from a top Reddit investor can drive more market movement than 100 random tweets. Weighting sources by influence dramatically improved our model\u2019s performance.\u2b50\ufe0fFinancial Language is Nuanced\nWe learned that off-the-shelf sentiment tools don\u2019t understand finance well. Terms like \u201cbeat,\u201d \u201cshort,\u201d or \u201cburn\u201d have different meanings in this context \u2014 which is why we had to train domain-specific models.What's next for Chatalytics: \u2b50\ufe0fOur vision is to become the go-to intelligence layer for real-time market sentiment \u2014 empowering traders, analysts, and institutions to make smarter decisions, instantly.\u2b50\ufe0fWe believe that in the age of information overload, the edge doesn\u2019t come from having more data \u2014 it comes from making sense of it faster and better than anyone else. Chatalytics will be the platform that transforms raw noise into actionable insights, as they happen.\u2b50\ufe0fExpand Source Coverage\nWe're adding more platforms beyond Reddit and major financial news \u2014 including X (formerly Twitter), Seeking Alpha, and YouTube transcripts.\u2b50\ufe0fLaunch Portfolio Monitoring\nInstead of searching one ticker at a time, users will soon be able to track sentiment shifts across their entire portfolio, with instant alerts.\u2b50\ufe0fInstitutional API\nWe're building an enterprise-grade API to serve hedge funds, quant teams, and trading platforms who want to integrate our real-time sentiment and prediction engine directly into their workflows.\u2b50\ufe0fMobile App Rollout\nA mobile-first version of Chatalytics is in development \u2014 giving retail traders lightning-fast market insights right in their pocket.\u2b50\ufe0fSentiment + Technical Fusion Model\nWe're currently testing a hybrid model that combines our sentiment predictions with technical indicators to improve precision and confidence scoring.",
                        "github": "https://github.com/yaojiejia/hof2025",
                        "url": "https://devpost.com/software/chatalytics"
                    },
                    {
                        "title": "Algora",
                        "description": "Algora scrapes your GitHub, Devpost, and LinkedIn to build a dynamic portfolio, match you with software roles, autofill applications in bulk, and craft custom cover letters\u2014faster.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
                        "story": "Frontend: ReactNext.js,Backend & Automation: SeleniumPuppeteerWeb-scraping pipelines,Database & Auth: Auth0 for secure sign-onPostgres/NoSQL for profile and application storage,Matching & Intelligence: Custom job-matching algorithm with semantic skill parsing,Deployment: Vercel for frontendContainerized backend services,Job hunting today can feel like running a never-ending obstacle course: you juggle half a dozen resumes, fight clunky web forms that refuse to cooperate, and scramble to tailor each application just enough to stand out\u2014only to discover you've missed a crucial field or typo you'll never catch. It's exhausting.Algora transforms this grind into a delightfully simple, almost magical experience. Imagine waking up, grabbing your coffee, and\u2014before your first sip\u2014is reviewing a dashboard of perfectly matched roles, each one handpicked by our skill-matching algorithm from thousands of listings. No more tedious copy-paste marathons, no more anxiety over formatting or overlooked questions. Instead, you breeze through applications in bulk, confident that every field is spot on and every resume truly reflects your best work.But Algora goes beyond mere automation. By scraping your GitHub READMEs, Devpost project write-ups, LinkedIn descriptions, and even local code directories, we craft a dynamic portfolio that breathes life into your achievements. Your strongest projects rise to the top, and Algora weaves those highlights into a personalized cover letter tailored to each role\u2014so you never submit a generic \"To Whom It May Concern\" again.This isn't just time-saving; it's empowerment. Students juggling classes, bootcamp grads racing deadlines, and self-taught developers with brilliant portfolios all finally have a tool that levels the playing field. Suddenly, applying to 30 jobs feels as effortless as sending a text message, and you can focus on what really matters: preparing for interviews, networking confidently, and ultimately landing that dream position.With Algora, you're not just applying\u2014you're presenting a polished, compelling narrative of your career, backed by data and delivered at scale. It's a game-changer that makes job seeking accessible, accurate, and\u2014dare we say\u2014enjoyable.Building Algora's resilient automation platform meant tackling a host of thorny engineering problems head-on.Scraping Challenges:Rich content from GitHub and Devpost wasn't as simple as pointing Selenium at a URLEach site's DOM was in constant fluxInfinite-scroll lists needed careful triggersRate-limits or bot-detection pop-ups threatened to derail our pipelines,Solutions:Developed dynamic wait conditions and fallback selectorsBuilt adaptive scrapers that recover gracefully when elements fail to load or shift position,Security Challenges:Accessing private GitHub repositories and draft Devpost projects securelyPreventing token exposure,Solutions:Built an OAuth-based authentication flow that keeps credentials encrypted on the user's deviceCrafted headless login routines with minimal re-authorization promptsImplemented isolated browser profiles to prevent token leakage,Local Parsing Challenges:Non-standardized open-source codebasesVaried project structures (monorepos, missing metadata, etc.),Solutions:Developed a robust file crawler in Node.jsImplemented language ecosystem detection via package.json, requirements.txt, and file-type heuristicsCreated normalization system for messy data,Autofill & Scaling Challenges:Handling custom dropdown widgets that require user-triggered eventsManaging bulk-apply sessions with concurrent WebDriver instancesAvoiding IP bans,Solutions:Built injection logic for scrolling and click-sequencingImplemented a pooling layer with proxy rotation and strict timeoutsDeveloped extensive end-to-end test suites simulating various form types,",
                        "github": "https://github.com/rayedchow/hof-hacks",
                        "url": "https://devpost.com/software/algora"
                    },
                    {
                        "title": "MediClear",
                        "description": "Decoding healthcare, one bill at a time",
                        "story": "MediClear - Arnav Sastry, Kevin Pei, Nicholai Kudriashov, Maggie Lu\nAccording to a 2024 survey by Flywire, 75% of American patients believe that medical\nbills are too complicated. With rising healthcare costs, the average patient is highly concerned\nwith understanding what exactly they\u2019re paying for with regard to their medical affairs.\nHowever, medical bills are often unintuitive to read, and they\u2019re filled with list items based\ncompletely on medical jargon, difficult for the average person to understand. Moreover, even if a\npatient can decipher a medical bill, they can\u2019t always rely on the information being true.\nAccording to Becker\u2019s Hospital Review, a shocking 80% of medical bills contain errors and\ninaccuracies in some form. Oftentimes, these errors result in patients being overcharged or\ndouble-charged, forcing them to pay thousands out of pocket for fees they don\u2019t truly owe.\nOur solution, MediClear, aims to solve this problem. Using OCR technology, MediClear\nscans images of hospital bills for critical information, such as the line items as well as their\nprices. After that, a chatbot displays the information in the bill and provides a short, simple\nexplanation of each line item. This explanation helps patients understand what exactly they are\nbeing charged for without feeling confused due to complex jargon. It also allows users to easily\nnotice errors, such as procedures that shouldn\u2019t have been charged to them, so that they can take\naction to remedy them. Once the hospital bills have been displayed, the prices of each procedure\nare cross-checked against a data set of average prices and price ranges for the procedure. It is\nthen made noticeable when a procedure\u2019s price is outside its normal range, showing the user that\nthey may have been overcharged for something.\nOverall, our product uses JavaScript for the front end, using the React Native and Node.js\nenvironment. Our backend, namely the OCR scanning and chatbot integration, is built using\nPython. Firstly, we use the Cloudinary API to take and store images. Our OCR technology\nutilizes the Pytesseract and Pillow libraries to scan text in images and extract that data. It stores\nthat information in a text file and uses the OpenAI API to analyze the information. The OpenAI\nAPI is used to display each line item along with an explanation, and then it detects errors\n(repeated line items and line items whose prices are out of the given price range). Finally, we\nused the Flask framework to assist in deploying our application.\nWe ran into quite a few developmental challenges while doing this project. Firstly, we\nwanted to do PDF scanning alongside image scanning, as we anticipated many medical bills\nwould be in the form of PDFs, as well as physical documents that individuals would take\npictures of. We tried to use libraries such as pdfplumber and pdf2image, as well as the PDF\nfunctionality within Pytesseract. However, we were unable to convert the data from PDFs into\ntext files for our use, so we were forced to stick with images. Another major error we\nencountered was finding the necessary data to do our error checks. Initially, we wanted to utilize\nthe Turquoise API to get data about average prices for various medical procedures. However, the\nAPI turned out to be restricted, and we were unable to request an API key to get the necessary\ndata. However, we were able to find a data set that provides the necessary information regarding\nthe normal price ranges and average prices for various medical procedures that we ended up\nusing in the error checks.",
                        "github": "https://github.com/NickRK4/mediscanner",
                        "url": "https://devpost.com/software/mediclear"
                    },
                    {
                        "title": "remote-dev-ai",
                        "description": "Looking to be hired",
                        "story": "GITHUB REPO LINK :https://github.com/yashwanth-alapati/remote-dev-aiDescription: Remote\u2011Dev\u2011AI delivers a fully automated, AI\u2011driven coding assistant that integrates seamlessly into your GitHub codebase via the MCP protocol and a custom GitHub App. By labeling issues withremote-dev-ai, teams invoke our backend\u2014hosted on AWS EC2 and AWS Lambda\u2014that:This model targets the 75% of developer time spent on non\u2011coding, repetitive tasks, slashes weekly issue\u2011resolution costs by 70% (from \\$1,200 in dev hours to \\$350 in AI calls), and provides a globally scalable solution for enterprises and startups alike.Project Technology Stack: GitHub App & MCP Botbuilt in Python, registered as a GitHub App.Hosted onAWS EC2instances running our MCP server and client for persistent connections and low latency,AWS Lambdafunctions triggered by GitHub webhooksLambda invokes the MCP server to fetch issue data and pipeline it into the Backend AI engine,Anthropic accessed via secured API keys\n### Front-endVercel Platform,Project Impact and Importance: Time SavingsDevelopers currently waste ~75% of their workday on non\u2011coding tasks (documentation, debugging, manual workflows).Cost EfficiencyAt 100 issue calls/day, AI costs \u2248\\$350/week vs. \\$1,200/week for a mid\u2011level developer at \\$48.65/hr\u2014a 70% reduction in direct labor cost.Global ScalabilityAs a GitHub\u2011native solution, Remote\u2011Dev\u2011AI deploys instantly across any public or private repo, serving organizations of all sizes worldwide.Democratizing AutomationEmpowers small teams and emerging markets with enterprise\u2011grade automation without large DevOps budgets or specialist hires.,Development Challenges Faced: The GitHub MCP server was open\u2011sourced only weeks before the hackathon, with minimal documentation. Integrating it into a production\u2011grade app required deep protocol analysis and custom extensions.Ensuring AI\u2011generated code aligns with each repo\u2019s conventions demanded a dynamic context loader that fetches README, lint rules, and recent commits before each generation.TEST OUR APP:: https://v0-remote-dev-ai.vercel.app/,TEAM: remote-dev-ai: Yashwanth AlapatiRajath ReghunathManan Prakashkumar PatelRitvik Vasantha Kumar,REFERENCES:: Our front end code by vercel:https://github.com/rrgodhorus/hofhacks-remote-dev-ai-home,",
                        "github": "https://github.com/rrgodhorus/hofhacks-remote-dev-ai-home",
                        "url": "https://devpost.com/software/remote-dev-ai"
                    },
                    {
                        "title": "FinTrack",
                        "description": "manage your finances & get AI powered insights!",
                        "story": "I\u2019ve always believed that budgeting shouldn't be complicated\u2014but most tools either overwhelm users with data or lack personalization. I wanted to build something that not onlysimplifies expense trackingbut also providesactionable financial insightspowered by AI. The idea was to create a clean, intuitive, and intelligent experience that anyone\u2014especially students and early professionals\u2014could use daily.This inspiredFintrack, a lightweight full-stack web app that combines traditional expense tracking with real-time AI analysis.The project uses a modernNode.js + React-based architecture:Built usingBolt (low-code/no-code)to quickly develop UI components.Includes:A landing screen where users enter budget categories (Transport, Food, Groceries, Others).A home dashboard that displays:Total income and expenses.A dynamic list of transactions.A floating \"+\" button to add new entries.AnAI Assistant tabfor financial tips and insights.,UsesExpress.jsfor API routing.A custom POST endpoint (/api/analyze) receives user budgets and transactions.Integrates withOpenAI's GPT-4 Turboto analyze user data and generate personalized insights.Includes/healthendpoint for health checks.Error-handling and debug logging implemented throughout.,Calculates:Total income, expenses, and balanceExpense breakdown by categoryBudget utilization percentageSends structured data to OpenAI, which returns tailored suggestions like:\u201cYou're spending 85% of your food budget. Consider meal prepping to save.\u201d\u201cGroceries and transport are under-budget\u2014great job!\u201d,Silent server exits:The backend initially started and stopped immediately due to silent promise rejections. I resolved this with propertry/catchblocks andprocess.on('unhandledRejection')handlers.OpenAI API behavior:Handling large prompt objects and optimizing for clarity and token usage took tuning.Data flow:Passing and structuring transaction data from Bolt\u2019s frontend into the Express server required extra care with serialization and validation.Debugging in a virtual environment:Running Node inside a Python virtual environment (venv) caused initial confusion as the terminal showed misleading behavior.,How to architect a full-stack budgeting app from scratch using both low-code UI tools (Bolt) and custom APIs.How to structure AI prompts for financial data analysis and use GPT models effectively.How to handle asynchronous flows and prevent silent crashes in a Node.js server.The importance of good UX in fintech\u2014keeping the experience clean, responsive, and reassuring.,Addingauthentication and persistent storage(Firebase or Supabase)Real-time charts withcategory breakdownsA smarter AI assistant withchat supportandgoal-based trackingExporting reports as PDFNotifications when a user is nearing their budget,",
                        "github": "https://github.com/deeepp/Fintrack",
                        "url": "https://devpost.com/software/fintrack-8lw2oc"
                    },
                    {
                        "title": "AI Changelogger",
                        "description": "AI-Powered Smart Changelogs: Write Less, Automate and Document Better",
                        "story": "Track: FREEDOM FROM REPETITIVE WORK [AUTOMATION & AI]Inspiration: We've all been there - staring at a pile of code changes, dreading the task of writing yet another changelog. We wanted to make this tedious process easier and more efficient. The idea struck us: what if we could use AI to understand code changes and automatically generate human-readable changelogs? That's how AI Changelogger was born - a tool that takes the pain out of documentation while making it more accurate and consistent.What it does: AI Changelogger is your smart assistant for writing changelogs. Just point it to your GitHub repository and specify the versions you want to compare. It'll analyze the code changes, understand what's actually different, and generate a clear, well-written changelog entry. No more staring at diffs trying to figure out what changed - our AI does the heavy lifting for you. Plus, it comes with a public-facing website where you can share your changelogs with users and stakeholders.How we built it: We built this using a modern tech stack that combines the best of both worlds. The frontend is a sleek React application that makes it super easy to use, while the backend uses Node.js and Express to handle all the heavy lifting. We integrated with GitHub's API to fetch code changes and OpenAI's GPT-4 to generate those beautiful changelog entries. Everything's stored in MongoDB, making it easy to manage and publish changelogs. We also made sure it works great on both desktop and mobile devices.Challenges we ran into: Getting the AI to understand code changes in a meaningful way was tricky - it's not just about spotting differences, but understanding their impact. We also had to figure out how to handle large repositories without hitting API limits or making users wait forever.Accomplishments that we're proud of: We're really proud of how well the AI understands code changes and generates meaningful changelog entries. The public changelog page turned out great too - it's clean, easy to read, and makes sharing updates a breeze. But what we're most excited about is how much time we're saving developers. No more hours spent writing changelogs - now it's just a few clicks and you're done!What we learned: This project taught us a ton about AI's capabilities and limitations. We learned how to fine-tune prompts to get better results, how to handle large amounts of code efficiently, and how to make AI-generated content feel more human. We also got better at building scalable applications and managing complex data flows between different services.What's next for AI Changelogger: We're also working on making the changelogs even more customizable and adding support for different formats. And who knows? Maybe we'll even add some cool visualization features to make understanding changes even easier!",
                        "github": "https://github.com/lawaldemur/ai-changelogger",
                        "url": "https://devpost.com/software/ai-changelogger"
                    },
                    {
                        "title": "FAIL3D",
                        "description": "Get the Engineering drawings of your 3D Assets without opening CAD!",
                        "story": "Inspiration: Hanomi.ai (to a very good extent)\nEngineering workflows often hit friction points when transitioning between 3D models and 2D engineering drawings. We noticed mechanical engineers and product designers spending excessive time manually creating detailed engineering drawings from their 3D models. Hanomi, our hardware startup, frequently faced this bottleneck when preparing manufacturing documentation. There had to be a better way to streamline this process, so we set out to create an accessible tool that would bridge the gap between 3D modeling and 2D documentation while adding intelligent dimensioning capabilities that traditional CAD tools lack.What it does: FAIL3D automatically converts 3D STL models into professional 2D engineering drawings in DXF format. Key features include:Intuitive web interface for uploading and managing 3D modelsReal-time 3D visualization of STL filesAutomatic generation of multi-view orthographic projections (top, front, right views)Interactive DXF viewer to explore the generated engineering drawingAI-assisted smart dimensioning that automatically identifies and measures critical featuresOne-click download of finalized DXF files ready for manufacturing documentation,The platform significantly reduces the time to create complete engineering documentation from 3D models, enabling faster iteration cycles in product development.How we built it: We implemented FAIL3D as a Flask web application with several integrated components:The core conversion algorithm works by extracting silhouette edges from multiple viewpoints, creating cross-sectional slices, and arranging them into standard engineering projections with proper scaling and annotation.Challenges we ran into: Our journey wasn't without obstacles:Perhaps the most significant challenge was developing the smart dimensioning system that could identify which features actually matter in an engineering context versus simply measuring everything.Accomplishments that we're proud of: What we learned: This project taught us valuable lessons:We also gained deeper knowledge of engineering drawing standards and the subtle complexities of translating 3D models to 2D documentation effectively.What's next for FAIL3D: While our hackathon implementation delivers a functional MVP, we have ambitious plans to expand FAIL3D:",
                        "github": "",
                        "url": "https://devpost.com/software/fail3d"
                    },
                    {
                        "title": "Job Application Bot: Automating Job Applications",
                        "description": "An AI-powered assistant that extracts job data, Recruiter info, generates tailored resumes, cover letters, and LinkedIn connection prompts and Cold mails, automating the boring job application process",
                        "story": "Inspiration: Applying for jobs felt like repeating the same mundane steps: editing resumes, rewriting cover letters, and finding recruiter contacts. We wanted to automate and personalize this process using AI.What it does: Our Chrome extension extracts the full HTML of a LinkedIn job page and sends it to a FastAPI backend. The backend parses the job description and recruiter info, then uses LLMs to generate:\nCategorized skill keywords\nA resume summary\nA tailored resume\nA cover letter\nA LinkedIn connection message\nA cold outreach email draftHow we built it: Frontend: TypeScript, React, ESBuild (Chrome Extension)\nBackend: FastAPI + OpenAI GPT-4\nStorage: Google Cloud(Not yet done)\nBundling: HTML extraction, content parsing, OpenAI prompt chainingChallenges we ran into: DOM inconsistencies in LinkedIn's recruiter panel\nExtracting top-layer modal HTML reliably\nPrompt tuning for meaningful LLM outputs\nDealing with long job HTML structures and size limitsAccomplishments that we're proud of: A robust DOM watcher that captures updated LinkedIn job panel content\nReliable recruiter info scraping even from modals\nSeamless integration with OpenAI for multiphase content generation\nGenerating a tailored resume that matches 80% of the keywords of the job description leading to higher chances of getting picked by ats\nGenerating job-specific LinkedIn messages under 300 charactersWhat we learned: Building resilient browser extensions requires patience and precision\nGPT output quality depends heavily on structured contextWhat's next for Job Application Bot: Automating Job Applications: Dashboard for viewing generated content history\nAuto-apply pipeline integration\nMulti-platform scraping (Indeed, Handshake)\nFeedback-based resume tailoring loop",
                        "github": "https://github.com/sash7410/hoc_hacakthon/",
                        "url": "https://devpost.com/software/linkedin-bot-5yr4ik"
                    },
                    {
                        "title": "Malachi Constant",
                        "description": "Young people today always turn to trendy TikTok stocks, when they should turn to our Lord. By dividing the bible into groups of letters representing stocks, we will all be financially enlightened.",
                        "story": "Inspiration: We thought: what if stock predictions came from the word of our Lord instead of data science?What it does: Malachi Constant converts daily Bible text into stock symbols based on days since Easter 30 AD. It checks if those symbols are real stocks, and if so, invokes AI Jesus (via GPT-4) to deliver buy/sell prophecies. His voice is generated through ElevenLabs.How we built it: React + Vite frontendServerless functions via VercelOpenAI API (GPT-4) for divine prophecyElevenLabs for voice synthesisBible parsing algorithm synced to the holy calendar,Challenges we ran into: Most 3-letter combos don\u2019t map to real stocksGetting realistic voice audio to play reliablyRuntime quirks (e.g., Buffer not defined in Edge)Prompting GPT to sound like Jesus,Accomplishments: AI Jesus speaks daily stock truthsSmooth multi-API orchestration,What we learned: How to structure functional serverless appsHow to use voice APIsTo have a little faith,What\u2019s next: \u201cConfess your portfolio\u201d modeRevelation log to track past predictionsSector-specific saints (e.g., St. Matthew for fintech),",
                        "github": "",
                        "url": "https://devpost.com/software/malachi-constant"
                    },
                    {
                        "title": "automail",
                        "description": "Automail is your AI-powered inbox assistant that reads, organizes, and acts on your emails automatically. It tags, drafts replies, and extracts todos, reminders, and deadlines using AI.",
                        "story": "Inspiration: I built Automail because I was overwhelmed. My inbox had turned into a chaotic mess of newsletters, follow-ups, receipts, and things I kept forgetting to reply to. I didn\u2019t want to manually label and dig through threads every morning. I wanted an AI-powered assistant that could read my emails, understand them, and help me stay on top of everything without the daily grind.What it does: Automail connects to your Gmail and acts as an intelligent assistant for your inbox. It:Automatically labels emails using AI-powered semantic taggingExtracts todos, reminders, and deadlines from email contentGenerates smart draft replies using Claude AI, based on tone and intentSaves reminders and tasks to a local or cloud database for follow-upDisplays everything in a clean dashboard with customizable widgets like \u201cFollow Up,\u201d \u201cFinance,\u201d or \u201cDrafts\u201d\n## How we built it\nI built Automail using a modern, full-stack setup. On the backend, I used FastAPI with Python and Uvicorn to handle API routing and Gmail integration. Emails are fetched using the Gmail API, processed with Claude AI via the Anthropic SDK, and stored in PostgreSQL for structured entities like reminders and MongoDB for more flexible logs and metadata. On the frontend, I used React 18, Vite, TypeScript, and Tailwind CSS to create a clean, responsive interface. The app is modular, with customizable bento-style widgets that update dynamically based on processed email data.,Challenges we ran into: Gmail\u2019s API, especially around threading and MIME parsing, was more complex than expected. Draft replies needed to be inserted with proper threading headers to avoid creating new threads in Gmail. Another challenge was designing prompts that made the AI both accurate and safe, especially when extracting reminders or deciding whether a reply was needed. Finally, syncing the database state with what\u2019s happening in Gmail without duplication or mismatch required careful logic and idempotent database operations. A big challenge was having push notifications from Gmail by setting up a Pub/Sub service on GCP which only works with https hosted websites/services.Accomplishments that we're proud of: I'm proud that Automail not only connects to Gmail but actually understands emails in a meaningful way. It can tag them intelligently, extract real tasks and deadlines, and even draft context-aware replies. Getting everything to work smoothly, Gmail, AI, frontend widgets, and databases; felt like orchestrating multiple moving parts. Seeing the system process a new email end-to-end in real time and surface the result in a clean dashboard was a huge milestone. Also open source and complete control over my email!What we learned: I learned how powerful AI can be when combined with deterministic systems. LLMs like Claude are excellent at extracting meaning, but structure and consistency still come from databases and clear logic. I also learned that UI and UX matter deeply, an AI assistant is only helpful if the user trusts it and feels in control. Finally, we gained a deeper appreciation for how email works under the hood, especially when building reliable, production-grade integrations. I also got better at vibe-coding frontend!What's next for automail: More powerful, more AI insights and local-LLM support for privacy first email client.",
                        "github": "https://github.com/mewtyunjay/automail-ai",
                        "url": "https://devpost.com/software/automail-a0j3cb"
                    },
                    {
                        "title": "Chronos",
                        "description": "\ud83d\udd2e Chronos: Carbon-Aware AutoML with time-travel retraining. Optimizes models under cost, policy & CO\u2082 limits using dynamic checkpoints & green compute strategies.",
                        "story": "Inspiration: With growing concerns around climate change, rising cloud costs, and strict data regulations, we wanted to build a solution that future-proofs ML pipelines. Chronos was born to empower developers and organizations to retrain models intelligently, by optimizing for time, cost, and carbon emissions\u2014without compromising on performance or compliance.What it does: Chronos is a carbon-conscious, time-aware AutoML system that:Enables retraining from past checkpoints with newly tuned hyperparameters.Supports auto-retraining under updated constraints like CO\u2082 budgets, cloud cost limits, and regulatory changes.Integrates with carbon tracking APIs to monitor and minimize emissions.Dynamically adjusts model architecture, hyperparameters, and compute strategies to stay within sustainability thresholds.How we built it: Backend: Python + FastAPI for orchestrating training workflows.ML Pipeline: HuggingFace Transformers + Optuna for hyperparameter tuning.Checkpoints: Stored and versioned using DVC (Data Version Control).Carbon Tracking: Integrated with CodeCarbon to estimate and monitor emissions.Scheduler: Custom module to trigger retraining based on cost/CO\u2082 drift or policy change.Frontend: Lightweight dashboard using React to display training stats, carbon usage, and version comparisons.Challenges we ran into: Balancing performance vs. carbon output in real-time.Handling checkpoint compatibility between model versions.Adapting hyperparameter tuning to include carbon and cost as optimization objectives.Ensuring retraining logic didn't overfit to constraints and degrade model quality.Accomplishments that we're proud of: Implemented a functional carbon-aware AutoML pipeline.Successfully retrained models within CO\u2082 and cost thresholds.Created a plug-and-play scheduler for constraint-driven retraining.Developed a time-travel mechanism to roll back and optimize from past checkpoints.What we learned: How to integrate sustainability goals directly into ML training loops.Real-world trade-offs between model performance and environmental impact.The importance of building modular, constraint-aware ML pipelines.How carbon tracking APIs work and how to integrate them with AutoML workflows.What's next for Chronos: Integrate with cloud compute APIs to autoscale training based on real-time energy mix (e.g., more training when renewable % is high).Add explainability tools to justify model changes after retraining.Open-source the scheduler and retraining engine for broader use.Expand to multi-cloud setups and federated learning scenarios.Collaborate with policy and compliance teams to incorporate regulatory frameworks directly into the pipeline.",
                        "github": "https://github.com/rushxbh910/HOF_HACKATHON",
                        "url": "https://devpost.com/software/chronos-rvj297"
                    }
                ],
                [
                    {
                        "title": "Insurapal",
                        "description": "InsuraPal helps users find, understand, and switch to the best-fit insurance plan with AI-powered matching, simple explanations, and fast, accessible tools\u2014anytime, anywhere.",
                        "story": "Inspiration: Navigating health insurance is a nightmare\u2014especially for seniors or low-income users. We built InsuraPal to simplify the process and deliver personalized, accessible coverage recommendations that actually make sense.\u2e3bWhat it does: InsuraPal matches users to the best-fit insurance plan using a machine learning model that analyzes their profile with TF-IDF vectorization and linear regression. It also explains coverage in simple terms, supports plan switching and applications, and even helps users understand or dispute denied claims through AI-powered tools.\u2e3bHow we built it: We built InsuraPal with Next.js and deployed it on Vercel, leveraging serverless functions, edge middleware for auth, and Vercel Blob for document uploads. Supabase handles auth and storage, and the matching logic is driven by a custom TF-IDF vectorizer and regression model. From dev to prod, Vercel powered our workflow with automatic previews, global CDN, and instant deployments.\u2e3bChallenges we ran into: Tuning the insurance matching algorithm to balance personalization with real-world plan data took time. We also worked hard to keep the interface user-friendly while juggling edge functions, auth flows, and AI tools behind the scenes.\u2e3bAccomplishments we\u2019re proud of: We shipped a full-stack product with a smooth signup, personalized insurance matching, simple plan descriptions, an AI insurance explainer chat, a claim denial analyzer, a humanized claims reviewer, and application/unsubscribe flows\u2014all production-ready with fast, modern performance.\u2e3bWhat we learned: We gained deep experience in serverless architecture, scalable ML integration, and how to turn complex data into helpful, human-first insurance tools.\u2e3bWhat\u2019s next for InsuraPal: We\u2019re planning multi-language support, real-time plan eligibility checks, and smarter document parsing to make the platform even more personalized and globally accessible.",
                        "github": "https://github.com/gsiri-code/InsuraPal",
                        "url": "https://devpost.com/software/insurapal"
                    },
                    {
                        "title": "DiversifyPro",
                        "description": "For professionals whose income and stock holdings are overly tied to one industry, we provide an investment service that diversifies their portfolios",
                        "story": "Project Impact and Importance\nDiversifyPro addresses a critical blindspot in personal finance: the dangerous correlation between career income and investment portfolios. This risk became devastatingly real for thousands of tech employees during the 2022 layoffs. Consider what happened at Meta: In November 2022, over 11,000 employees suddenly lost their jobs. Many of these professionals not only faced unemployment but watched in horror as their Meta stock\u2014which made up a significant portion of their compensation and personal investments\u2014plummeted by more than 70% from its peak. These employees faced a cruel double blow: losing their income source while simultaneously seeing their investment portfolios collapse. Some were forced to sell their devalued shares to cover living expenses, permanently locking in devastating losses. This scenario played out across the tech industry, with similar stories at Twitter, Amazon, Google, and dozens of startups. The financial and emotional toll was immense\u2014families who believed they had achieved financial security suddenly found themselves vulnerable, anxious, and financially compromised. DiversifyPro exists to prevent this nightmare scenario. By identifying assets with minimal correlation to a user's industry, we create resilient portfolios that remain stable when career sectors falter. Our platform doesn't just diversify investments\u2014it provides peace of mind and financial security when professionals need it most. The impact extends beyond individual financial health to broader economic resilience. When entire professional communities aren't forced to liquidate assets during sector downturns, it reduces market volatility and protects regional economies from the ripple effects of industry-specific recessions.Development Challenges Faced\nCreating DiversifyPro presented several technical and conceptual challenges:",
                        "github": "",
                        "url": "https://devpost.com/software/difersifypro"
                    },
                    {
                        "title": "Sail",
                        "description": "Sail transforms long, messy earnings reports into clean, structured insights \u2014 saving analysts hours of manual work and helping them make faster, more informed financial decisions. ",
                        "story": "SAIL: Financial Document Intelligence PipelineProject Technology Stack\nThe SAIL project integrates a suite of tools carefully selected for their strengths in handling the challenges of financial document analysis. This toolset enables the proposed dual-workflow architecture and the Retrieval-Augmented Generation (RAG) extension for semantic search. For processing unstructured text, LangChain serves as the central orchestrator, managing interactions with LLMs (via the OpenAI API or alternatives like LlamaIndex). LangChain facilitates prompt chaining\u2014such as extracting forward guidance, identifying KPIs, and classifying sentiment\u2014while managing context and chunking of large documents. Data handling and validation rely on Pandas for tasks like verifying extracted table totals and structuring outputs, with JSON as the primary data format and sqlite3 as an optional lightweight storage solution.For structured data processing, specialized open-source libraries like pdfplumber and PyMuPDF are used to parse PDF structure and detect table boundaries, while Tabula and Camelot extract data from those detected tables. Backend services are planned using FastAPI or Flask to serve endpoints for file upload, processing, and querying. React will power the front-end user interface, allowing users to upload PDFs and search within them. For the RAG system, vector databases like FAISS, ChromaDB, Pinecone, or Qdrant are considered to store text embeddings for efficient semantic search, with PostgreSQL or SQLite offered as alternatives for non-semantic storage. Finally, development utilities like virtualenv, requirements.txt, and Streamlit are incorporated for streamlined setup and optional dashboard creation. This pragmatic, modular approach focuses on rapid prototyping and demonstrating core functionality efficiently\u2014ideal for hackathon contexts or early proof-of-concept phases.Project Impact and Importance\nSAIL promises to significantly improve the efficiency of analyzing financial documents by automating data extraction and enabling richer insights. Its primary importance lies in addressing the inefficiency and limitations of manual analysis when dealing with documents blending structured financial tables and unstructured narratives. The project provides substantial efficiency gains, automating laborious extraction tasks and enabling scalable, rapid analysis across large volumes of public filings. The dual-workflow design\u2014separately handling tables and narratives\u2014ensures high-quality outputs, converting previously locked PDF data into accessible, structured formats like JSON or databases. This facilitates downstream analytics, real-time access, and further applications like financial modeling or risk analysis.Importantly, SAIL moves beyond basic data extraction by targeting qualitative insights often missed in purely quantitative approaches. Extracting guidance, KPIs, forward-looking statements, and sentiment enriches understanding of a company\u2019s strategic positioning and outlook. The addition of the RAG component represents a transformative upgrade, turning SAIL into an interactive discovery platform. Users will be able to query past uploads using natural language and receive contextually relevant, insightful responses\u2014an intuitive, powerful alternative to traditional keyword searches. This evolution from static data extraction to dynamic semantic querying significantly enhances the project\u2019s value to analysts and business intelligence teams.Development Challenges Faced\nDespite its strong potential, the development of SAIL\u2014particularly under hackathon time constraints\u2014poses several challenges. A major hurdle is document variability: financial reports vary widely in layout, formatting, and language, which can complicate the consistent performance of tools like pdfplumber and Tabula. Poorly scanned or inconsistently formatted PDFs are especially problematic. Extraction accuracy and validation present further difficulties. Ensuring the correctness of extracted table data demands robust validation logic, using Pandas to check totals and consistency, while ensuring the LLM correctly extracts financial guidance and KPIs is complicated by the risk of hallucination or misunderstanding of industry-specific terminology.Prompt engineering within LangChain also presents a technical hurdle. It requires iterative refinement to create prompts that reliably extract the intended information while respecting token limits and managing model behavior across diverse text samples. The integration complexity of combining multiple components\u2014PDF parsing, table extraction, LLM pipelines, backend APIs, frontend interfaces, and vector search systems\u2014into a smooth, cohesive workflow demands meticulous architecture and engineering. Finally, implementing RAG introduces its own technical challenges, including choosing the best embedding models, designing smart chunking strategies, managing vector databases efficiently, and optimizing retrieval quality to ensure both speed and relevance for end-users. Each of these challenges highlights both the technical sophistication of the project and the rigor required to bring it to a production-ready MVP.",
                        "github": "https://github.com/Christopher-Xu1/HOF_Hackathon",
                        "url": "https://devpost.com/software/sail-mg39bp"
                    }
                ]
            ]
        },
        {
            "title": "OSDHACK'25",
            "location": "Online",
            "url": "https://osdhack-24927.devpost.com/",
            "submission_dates": "Apr 16 - 17, 2025",
            "themes": [
                "Beginner Friendly"
            ],
            "organization": "osdc",
            "winners": false,
            "projects": []
        },
        {
            "title": "Open Innovation Hackathon",
            "location": "Amity University Rajasthan",
            "url": "https://open-innovation-hackathon.devpost.com/",
            "submission_dates": "Apr 16 - 17, 2025",
            "themes": [
                "Beginner Friendly",
                "Open Ended",
                "Social Good"
            ],
            "organization": "Amity University Rajasthan",
            "winners": false,
            "projects": [
                [
                    {
                        "title": "BindShare",
                        "description": "Bind It. Share It. Own It.",
                        "story": "InspirationIn today\u2019s digital world, sharing private links often lacks control. Anyone can forward a link, making it vulnerable. Inspired by Samsung\u2019s Private Share, we wanted to build a lightweight,device-boundsecure sharing system using QR and session verification.What it doesBindSharelets you share a private link that can only be accessed by a single, verified device. The sender scans a QR from the receiver, linking the session. Only the bound receiver can open the shared content \u2014 others are blocked.How we built itWe used:Node.js + Expressfor backend token generation and session validationUUID & WebSocketfor unique token management and real-time confirmationJavaScript (frontend)for QR generation and session handlingmkcertto enable secure HTTPS for local testing,Challenges we ran intoSetting up local HTTPS using mkcertBinding QR session with proper timeout handlingPreventing token regeneration if one is already pending,Accomplishments that we're proud ofFully working demo of device-bound link sharingSecure WebSocket communication between sender and receiverToken-based logic to avoid misuse or spamming of links,What we learnedReal-time communication using WebSocketsHow to simulate secure link sharing with client-device bindingManaging sessions and expiring tokens properly,What's next for BindShareAddencryption for shared contentCreate a mobile-friendly interfaceSupport forbiometric validationbefore accessOptionalself-destructionof links after one view,",
                        "github": "https://github.com/IamNinjaCoder/Secure-LinkProject/",
                        "url": "https://devpost.com/software/bindshare"
                    },
                    {
                        "title": "Fusion Pay",
                        "description": "The First Crypto-to-Cash Payment Gateway, Revolutionizing Global Transactions!",
                        "story": "\ud83c\udf0d What if you could pay with crypto anywhere, and businesses received real cash instantly? Until now, this was impossible. But FusionPay is changing the game.\ud83d\udd17 We\u2019re not just another payment gateway\u2014we\u2019re solving a billion-dollar problem no one else has cracked. Merchants need fiat in their bank accounts. Crypto users want frictionless spending. Today, they\u2019re stuck. FusionPay is the first true bridge between both worlds.Despite billions of dollars sitting in crypto wallets, real-world usage is still limited. People can\u2019t use crypto for everyday purchases, and merchants can\u2019t accept it without dealing with volatility, conversion delays, or regulations. Why hasn\u2019t anyone created a seamless, instant crypto-to-cash system? That question inspired Fusion Pay.FusionPay is a borderless, AI-powered payment platform that lets users pay with crypto anywhere\u2014and merchants receive instant fiat in return.Key Features:\n    \u2022 \u2705 Real-Time Crypto \u2192 Stablecoin \u2192 Fiat Conversion with best market rates using AI.\n    \u2022 \u2705 No Volatility Risk \u2013 Conversion is instant and transparent.\n    \u2022 \u2705 Truly Global Payments \u2013 No banks, no KYC, no borders.\n    \u2022 \u2705 Multiple Use-Cases:\n    \u2022 Virtual Card \u2013 Use crypto like Visa/MasterCard.\n    \u2022 Scan & Pay QR \u2013 Just like UPI, but decentralized.\n    \u2022 Wallet-to-Wallet \u2013 Direct transfers in real-time.We used:\n    \u2022 \ud83e\udde0 AI Models to identify best rates across exchanges in real-time.\n    \u2022 \ud83d\udd17 Smart Contracts on Polygon for secure conversion & transfers.\n    \u2022 \ud83d\udcb3 Card APIs & QR SDKs for payment interfaces.\n    \u2022 \u2601\ufe0f Firebase & Node.js for backend logic and secure wallet management.\n    \u2022 \ud83d\udee1\ufe0f Ramp/On-Ramp APIs for fiat delivery to merchant accounts.Everything was built during the hackathon using no-code tools + APIs + blockchain integrations.Fusion Pay isn\u2019t just another fintech project\u2014it\u2019s a financial revolution.The future of payments is here. Are you ready? \ud83d\udcb0\ud83d\ude80",
                        "github": "",
                        "url": "https://devpost.com/software/fusion-pay-69gfme"
                    }
                ]
            ]
        },
        {
            "title": "Sheradil Code4Change",
            "location": "Online",
            "url": "https://sheradil-code4change.devpost.com/",
            "submission_dates": "Apr 13 - 15, 2025",
            "themes": [
                "Machine Learning/AI",
                "Social Good",
                "Web"
            ],
            "organization": "Vellore Institute of Technology",
            "winners": false,
            "projects": []
        },
        {
            "title": "GenAI and Cloud Hackathon",
            "location": "Online",
            "url": "https://genai-and-cloud-hackathon.devpost.com/",
            "submission_dates": "Apr 12 - 13, 2025",
            "themes": [
                "Machine Learning/AI"
            ],
            "organization": "Neutron",
            "winners": false,
            "projects": []
        },
        {
            "title": "Google Build with AI Hackathon",
            "location": "WashU - Bauer Hall",
            "url": "https://devfestwashu.devpost.com/",
            "submission_dates": "Apr 12 - 13, 2025",
            "themes": [
                "AR/VR",
                "Databases",
                "Machine Learning/AI"
            ],
            "organization": "Google Developer Groups on Campus",
            "winners": false,
            "projects": [
                [
                    {
                        "title": "AIrth",
                        "description": "AIrth: AI optimizes energy for businesses. IoT sensors + ML = reduced costs, emissions & proactive sustainability.",
                        "story": "Inspiration: Straight out of grad school with my master's in AI, I was pumped to use my skills for good. But the corporate world quickly showed its true colors. Endless chasing of profits clashed with my growing desire to help the environment. I knew AI could do amazing things, but how could I make it work for both businesses and the planet?The idea hit me on a hike, surrounded by nature's beauty. Businesses weren't the enemy, I realized. They just lacked the tools to be truly sustainable. That's when AIrth sprouted in my mind.What it does: At first, my vision was huge \u2013 a giant AI platform tackling every energy-efficiency problem under the sun. But reality, as always, had other plans. Trimming down the scope was like cutting back a jungle, tough but necessary. Each cut made the core clearer: using AI to help businesses save energy, predict future needs, and ultimately shrink their carbon footprint.How we built it: Finally, AIrth took its first wobbly steps. A simple analysis for a local bakery helped them plug hidden energy leaks, leading to a real drop in their bills. Their surprised smile fueled mine \u2013 this wasn't just about saving money, it was about giving businesses the power to make smart choices, good for both their wallets and the environment.Challenges we ran into: It wasn't easy. Tech issues popped up like weeds, data felt like a tangled mess, and getting businesses interested was like pushing a boulder uphill. But with every hurdle, I dug in deeper. Late nights weren't glamorous, fueled by coffee rather than ramen, but the online developer community became my support system.Accomplishments that we're proud of: What we learned: What's next for AIrth: AIrth is still growing, constantly learning and improving. But the core is solid, the vision clear. I see it as a one-stop shop for businesses, guiding them towards a greener future, watt by watt. It's not a superhero story, but it's mine, driven by a simple goal: to make a difference, one line of code at a time.",
                        "github": "https://github.com/Vladmir-bits/AIrth",
                        "url": "https://devpost.com/software/airth"
                    },
                    {
                        "title": "Resume Analyzer Chatbot",
                        "description": "A chatbot that analyzes your resume based on current market trends. We can also integrate MCP architecture to automatically create reorganize the resume to fit the job postings. ",
                        "story": "Inspiration: What if we could integrate an AI's suggestions to improve our resume seamlessly. What if we could reorganize our achievements more tailored to the job posting in a click of a finger?These questions are what inspired me to create this project.What it does: Upload your resume and mention the role/domain you want to apply for. It then scraps the internet to get relevant job postings from top companies and provides suggestions to improve you resume and make it more tailored to the job posting.How we built it: We used Gemini API to accelerate the process of creating the chatbot.Challenges we ran into: Parsing and formatting the data to look great after receiving the API's response.Accomplishments that we're proud of: It could really help the people understand the market and themselves. And it helped me understand what I could focus on henceforth to skill up.What we learned: Integration of Gemini API to my systems.What's next for Resume Analyzer Chatbot: Add in an MCP architecture and make it so that each server does only one task very well.",
                        "github": "",
                        "url": "https://devpost.com/software/resume-analyzer-chatbot"
                    },
                    {
                        "title": "Kira",
                        "description": "Kira is your AI-powered voice health companion that listens to your symptoms, summarizes them intelligently, and guides you on what to do next \u2014 all in under a minute, hands-free.",
                        "story": "Inspiration: When you're feeling unwell, the last thing you want is to scroll through pages of Google results or wait on hold with a clinic. We wanted to create a voice-first AI assistant that gives you clear, calm, and actionable guidance, especially for those who don\u2019t know whether it\u2019s something minor or worth checking out.What it does: Kira is a voice-powered AI triage assistant that helps users describe their symptoms naturally. It listens, transcribes, and uses the Gemini API to summarize your symptoms, suggest possible conditions, and recommend what kind of care (if any) you may need \u2014 all with a friendly disclaimer and clean, easy-to-read output.In addition to voice input, Kira also lets users upload or take a photo of a visible symptom like a cut, bruise, or swelling and uses AI to factor that into the triage summary, providing an even more accurate and personalized assessment.How we built it: Frontend:Built with Next.js and Tailwind CSS for a responsive, healthcare-inspired UI.Voice Input:Uses the Web Speech API to transcribe user speech.Image Input:Allows users to upload or snap a photo of a visible symptom; images are analyzed using the Gemini API\u2019s multimodal capabilities and interpreted in tandem with the voice input.AI Engine:A custom prompt is sent to Gemini API to generate structured triage summaries.Output Formatting:Gemini responses are parsed into cards (symptoms, possible causes, care suggestions, and disclaimer) for clarity.Design:Clean red/white UI with empathetic UX, mobile-friendly layout, and voice-first interactions.,Challenges we ran into: We started the hackathon working on a gesture-controlled MIDI DJ app powered by computer vision. It was ambitious and fun but we quickly realized two key things:The technical complexity (real-time gesture detection + audio synthesis) was too high to execute cleanly in the time we had.More importantly, the project didn\u2019t solve a real problem \u2014 it was purely for entertainment.,That\u2019s when we made the hard decision to pivot... 16 hours in.We asked ourselves:What can we build that\u2019s both technically interesting and genuinely useful?That question led us to Kira, a voice-first AI health triage assistant designed to bring clarity to people unsure about their symptoms.Even after the pivot, we faced several challenges:We had to re-architect everything: new stack, new logic, new UX.We had to iteratively refine our LLM interaction design to ensure Kira returned responses that were clear, actionable, and easy to parse.Voice input required handling different browsers and fallback cases.We had to switch gears mentally from flashy and performative to calm, human-centered, and impactful.,The pivot forced us to focus on problem-solving, not novelty, and we\u2019re proud of where that led us.That pivot, while risky, is what made Kira possible.Accomplishments that we're proud of: We executed a full-scope pivot mid-hackathon \u2014 switching from a complex CV + MIDI concept to a voice-based AI health assistant \u2014 and still delivered a complete, demo-ready product.In under 12 hours, we built Kira, a voice-first, AI-powered health triage tool with structured output, thoughtful UX, and future-ready architecture.Our Gemini prompts are finely tuned to deliver clear, actionable, and empathetic medical summaries, proving we deeply understand both AI behavior and real-world user needs.We didn\u2019t just ship a working app \u2014 we built something that feels like a prototype of a real product, with extensibility in mind (personalization, multilingual support, care mapping, etc.).The experience helped us practice product thinking, human-centered design, and prompt-engineering \u2014 key pillars for building AI-powered tools people actually trust.,What we learned: Prompt engineering is everything when working with LLMs; structure matters.Voice-first UX feels magical when it works but requires user-centric thinking to feel natural.Image analysis is powerful when combined with text, but requires very specific prompts and fallback planning.Small things like disclaimers, urgency levels, and empathy in tone make a huge difference in health-focused AI products.Simplicity wins. Keeping the product tight made it easier to deliver something meaningful.,What's next for Kira: Add personalization: age, known conditions, and location-based triageSupport multilingual voice input and accessibility featuresBuild an on-device version with Whisper.js for privacyIntegrate with Google Maps API to recommend nearby care centersExpand into mental health triage with emotion-aware AI responsesImprove photo triage with clearer feedback and step-by-step visual self-checks,",
                        "github": "https://github.com/mhashir03/Kira",
                        "url": "https://devpost.com/software/djmotion-btqkgs"
                    },
                    {
                        "title": "ORCA",
                        "description": "Educate is an AI-powered platform where students can learn, build communities, participate in hackathons, and secure jobs\u2014all in one place, with tailored support for every student's unique needs.",
                        "story": "Inspiration: We were inspired by the lack of a unified platform where students can learn, collaborate, and secure jobs while being part of a supportive community. We also wanted to address the needs of students with disabilities, making learning accessible to all.What it does: Educate AI helps students build communities beyond college, participate in hackathons, and prepare for jobs through AI-powered personalized learning. The platform leverages Google's Gemini API to deliver intelligent content recommendations, automated tutoring assistance, and customized learning paths based on individual learning styles and needs. It supports students with disabilities, offers tools for immersive learning, and enables all students to pursue their passions with real-world projects, all in one platform.How we built it: We built Educate AI using advanced AI algorithms, particularly integrating Google's Gemini API to power our personalized learning recommendations and adaptive content delivery. The Gemini API forms the backbone of our intelligent tutoring system, enabling natural language understanding and generation for enhanced student support. We also integrated interactive tools for immersive learning experiences and developed robust backend support for community building and hackathon participation. The platform is optimized for accessibility and inclusivity, with special attention to interface design that works for users with various disabilities.Challenges we ran into: We faced challenges in creating a seamless user experience, integrating AI tools to accommodate students with disabilities, and ensuring the platform can scale while maintaining accessibility and performance.Accomplishments that we're proud of: We are proud to have developed a platform that supports personalized learning for all students, including those with disabilities. We've also built a thriving pre-launch community of 500 users with a strong retention rate.What we learned: We learned the importance of creating an inclusive platform that fosters collaboration while making cutting-edge technologies, like AI and AR/VR, more accessible for students from diverse backgrounds.What's next for ORCA: We plan to scale the platform, expand our partnerships with educational institutions and employers, introduce new learning modules, and increase user engagement through gamified learning and more hackathons.",
                        "github": "https://github.com/SESHASHAYANAN/EDUCATE_AI",
                        "url": "https://devpost.com/software/orca-onms0f"
                    },
                    {
                        "title": "Chat Bot ",
                        "description": "An AI-powered chatbot that provides instant, insightful answers to all your questions about Artificial Intelligence, helping you learn, explore, and stay ahead in the world of AI.",
                        "story": "Inspiration: The inspiration for building a DeepSeek Clone stemmed from the growing demand for effective search engines in specialized domains. DeepSeek, as a search engine, serves a niche purpose by providing users with in-depth and accurate search results, much like how traditional search engines work but tailored for a more focused audience. As someone passionate about building tools that can make complex data easier to access, I decided to create a clone of DeepSeek, incorporating features that would enable users to find relevant, high-quality content based on their queries.Throughout the development of this project, I learned and honed various skills:Web Scraping: Understanding how search engines gather and organize data was crucial, and I explored web scraping techniques to collect information from various sources.Search Algorithms: I gained insight into how search algorithms prioritize results and how to optimize content to deliver the most relevant answers to users.Backend Development: Using Flask for the backend, I learned how to create a search engine that handles user requests efficiently, communicates with the database, and retrieves the most relevant results.Frontend Design : I worked on creating a user-friendly interface that could handle search queries, display results, and allow users to interact with the system in a way that\u2019s both simple and intuitive.Database Management: Building a database to store indexed data for fast retrieval and understanding how to structure it for efficient querying was a significant learning experience.,Backend:I usedFlaskas the web framework for the backend. The backend handles user search requests, queries the database, and processes the information before returning it as search results.Search Algorithm:I implemented a basic ranking algorithm that ranks results based on their relevance to the user's query. I focused on keyword matching and scoring, considering factors like frequency and position of search terms.Web Scraping:I usedBeautifulSoupandrequestslibraries to scrape data from various websites. This data was then indexed into the database to allow quick retrieval.Database:I usedSQLitefor local storage and indexing, ensuring that data could be efficiently searched using a combination of SQL queries and Python logic.Frontend:I built the user interface withHTML, CSS, and JavaScript, ensuring that users could type in queries, view results, and refine searches easily.Additional Features:I integrated search suggestions based on user input to help refine queries, similar to the autocomplete feature found in many modern search engines.,Data Collection:One of the biggest challenges was scraping relevant and high-quality data. Many websites have anti-scraping measures, which made it difficult to retrieve accurate content consistently.Handling Large Data Sets:As the search engine grew, managing and processing large amounts of data became difficult. Optimizing database queries and ensuring fast retrieval without compromising accuracy was a major hurdle.Search Relevance:Ensuring that the results returned were actually relevant to the user\u2019s query required refining the ranking algorithm. It was challenging to strike the right balance between speed and accuracy.Frontend-Backend Communication:Ensuring smooth communication between the frontend and backend while handling multiple search requests was tricky. I had to ensure that the search results were displayed efficiently without overloading the server.SEO and Indexing:Properly indexing and ranking content to ensure it was displayed in the right order required thorough testing. I had to fine-tune the scoring and ranking mechanisms to improve the relevance of results.,Advanced Search Algorithms:I plan to improve the ranking algorithms by incorporatingmachine learningmodels to better understand user intent and rank search results more accurately.Web Scraping Improvements:I aim to implement more robust scraping methods to handle modern websites with dynamic content and anti-scraping measures, like JavaScript rendering.User Personalization:Adding user-specific search history and preferences to improve the accuracy of results based on previous searches.Mobile Optimization:Currently, the project is web-based, but I plan to optimize the user interface for mobile devices for better accessibility.,This project allowed me to delve deep into search engine architecture, web scraping, and data handling, providing me with practical knowledge that I can apply to future projects. It also highlighted the complexity of building efficient search engines and the importance of balancing usability, accuracy, and performance.What it does: How we built it: Challenges we ran into: Accomplishments that we're proud of: What we learned: What's next for Chat Bot:",
                        "github": "https://github.com/KALYANSAI-3114/deepseek_clone",
                        "url": "https://devpost.com/software/chat-bot-atne6s"
                    },
                    {
                        "title": "Mindcraft",
                        "description": "Mindcraft lets users talk to 3D Minecraft-style pets that act as therapists, motivators, or philosophers\u2014using AI to respond to emotions and make mental health support feel safe and fun.",
                        "story": "Inspiration: After watching the Minecraft movie last week and spending all our money on Minecraft Happy Meals, Minecraft was all we could think about. During our brainstorming session, we realized many students struggle with mental health, often feeling ashamed or embarrassed to seek help due to stigma and fear of judgment. To address this, we combined our passion with this important issue to create Mindcraft, an app designed to reduce stigma by allowing users to have supportive video calls with family-friendly, lovable Minecraft characters.What it does: Mindcraft immerses users in engaging, heartfelt conversations with their choice of friendly Minecraft pets. Each pet character embodies a unique role\u2014such as a compassionate therapist, thoughtful philosopher, or uplifting motivator\u2014to best match the user's emotional needs. During video calls, we leverage Hume AI to analyze users' voices, enabling the pets to deliver personalized, emotion-sensitive responses. After each session, users can generate an insightful summary of their interaction with the help of Gemini AI.How we built it: The frontend of Mindcraft is built using Next.js with TypeScript for a smooth and scalable user experience. We integrated Hume AI with WebSockets to enable real-time emotion detection and voice-based interactions, while Gemini is used to generate summaries of each conversation. For visuals, we designed 3D models in Blender and rendered them in-browser using Three.js. To support remote development and access, we used ngrok for secure tunneling.Challenges we ran into: Our biggest challenge was learning Blender and Three.js to render our 3D models, as none of us had prior experience with either tool. We also ran into difficulties crafting effective prompts to generate the character models we envisioned.Accomplishments that we're proud of: We're very proud that we were able to create working models that you can talk to!What we learned: We gained a ton of knowledge working with Blender, Three.js, and Hume AI throughout this project.What's next for Mindcraft: Next for Mindcraft, we plan to expand the range of pet personalities and roles, giving users even more ways to connect based on their emotional needs. We also want to improve the realism of interactions by incorporating facial emotion recognition and more advanced voice synthesis. Eventually, we hope to bring Mindcraft to mobile platforms and make it accessible to schools and mental health organizations to help reduce stigma and encourage open conversations around mental well-being.",
                        "github": "https://github.com/JasonXu314/hackwashu-2025",
                        "url": "https://devpost.com/software/mindcraft-x8qktf"
                    },
                    {
                        "title": "MemoryVault - AI Memory Companion for Alzheimer's Patients",
                        "description": "Empowering Alzheimer's patients to relive and reconnect with their precious memories through AI-powered conversations and visual experiences.",
                        "story": "An AI-powered platform helping Alzheimer's patients reconnect with their past through natural conversations and AI-generated visuals.\ud83c\udfc6 Elevator Pitch: MemoryVault is an innovative AI platform designed to help Alzheimer's patients preserve and relive their cherished memories. Through advanced AI, it enables natural conversations about past experiences while generating visuals that enhance recollection. Families can actively contribute, creating a collaborative space for memory preservation. By combining interactive dialogue with AI-generated imagery, MemoryVault strengthens emotional connections and improves well-being.\ud83d\udca1 The Problem It Solves: Alzheimer\u2019s disease gradually erases memories, leaving patients feeling lost and disconnected. Traditional solutions like photo albums and reminders are passive and lack interaction.MemoryVault solves this by:\u2705 Enablingconversationalmemory retrieval\u2705 CreatingAI-generatedvisual representations\u2705 Providing animmersive and interactiveexperience\u2705 Encouragingfamily participationin memory preservation\u2705 Supportingemotional well-beingthrough engagement\ud83c\udf1f Inspiration: The heartbreaking impact of Alzheimer's inspired us to go beyond traditional memory aids. We envisioned a platform thatnot only stores memories but brings them to life\u2014helping patients maintain theiridentityandconnections with loved ones.\u2699\ufe0f How We Built It: \ud83d\udd39Backend (Flask + Python):RESTful API for memory managementSentenceTransformer for vector embeddingsPinecone vector database for storageGoogle Gemini Pro for natural language processingFLUX API for image generation,\ud83d\udd39Memory Processing Pipeline:Text chunking & optimizationVector embedding creationMetadata managementEfficient storage & retrieval,\ud83d\udd39Core Features:\ud83d\udde3\ufe0f Conversational memory retrieval\ud83c\udfa8 AI-powered visual generation\ud83c\udfe1 Family memory contribution\ud83d\udd0d Interactive memory exploration,\ud83d\udea7 Challenges We Faced: 1\ufe0f\u20e3Vector Database Implementation\u2013 Optimizing memory storage & retrieval \ud83d\udcda2\ufe0f\u20e3AI Integration\u2013 Coordinating multiple AI models for natural flow \ud83e\udd163\ufe0f\u20e3User Experience\u2013 Designing an intuitive interface for elderly users \ud83d\udc75\ud83c\udffb\ud83d\udc74\ud83c\udffb\ud83c\udfc5 Accomplishments: \u2705 Successfully integrated multiple AI technologies into one platform\u2705 Built anefficientvector-based memory storage & retrieval system\u2705 Implementedcontext-awarenatural conversation capabilities\u2705 Developed ascalable architecturefor multiple users\u2705 Created auser-friendlyinterface accessible to elderly users\ud83c\udf93 What We Learned: \ud83d\udccc Vector database optimization & memory retrieval\ud83d\udccc Large Language Model (LLM) integration\ud83d\udccc AI-powered image generation techniques\ud83d\udccc Scalable Flask application development\ud83d\udccc Cross-platform integration strategies\ud83d\ude80 What's Next for MemoryVault: \ud83c\udf99\ufe0f Voice interaction capabilities\ud83d\udcc2 Advanced memory organization tools\ud83d\udd76\ufe0f Virtual reality (VR) integration\ud83d\udcf1 Mobile application development\ud83c\udfe5 Healthcare provider partnerships\ud83c\udf93 Research institution collaboration\ud83c\udf0e Multi-language support\ud83d\udd12 Enhanced security features\ud83d\udc68\u200d\ud83d\udc69\u200d\ud83d\udc66 Family account management\ud83d\udcd6 Memory sharing capabilities\ud83e\udd1d Support group integration\ud83d\udc69\u200d\u2695\ufe0f Professional caregiver tools\ud83d\udee0Built With:Flask \u2022 Python \u2022 Pinecone \u2022 Google Gemini Pro \u2022 FLUX API\ud83d\udc99 MemoryVault \u2013 Because every memory matters.",
                        "github": "https://github.com/Monishg2004/MemoryVault.git",
                        "url": "https://devpost.com/software/memoryvault-ai-memory-companion-for-alzheimer-s-patients"
                    },
                    {
                        "title": "CreativeAI",
                        "description": "Your AI Co-Creator: Always in Context.",
                        "story": "Inspiration: The Missing \"Partner\" in Digital Creation: As creators ourselves, our team has often experienced the ebb and flow of the creative process. While digital tools offer immense power, they can sometimes feel isolating. We missed the dynamic interaction and intuitive feedback that often comes from collaborating with another creative individual \u2013 someone who understands the nuances of your current work and can offer timely, relevant suggestions. This sparked the idea for the \"Context-Aware Creative Companion\" \u2013 an AI that moves beyond simple prompt-based generation to become a true partner in the creative journey. We envisioned an AI that actively listens to and understands the evolving context of your work, offering insights and inspiration that feel genuinely collaborative.What We Learned: The Nuances of Context and Proactivity: Building this project within the tight timeframe of the Google Build with AI Hackathon pushed us to learn rapidly about several key areas:Contextual Analysis:We delved into how to effectively analyze ongoing user input in near real-time. This involved exploring different techniques for natural language processing and potentially even basic pattern recognition in other creative mediums (though our initial prototype focused primarily on text). We learned that simply feeding the entire history isn't always effective; identifying the most relevant recent context is crucial.Proactive Suggestion Generation:Moving beyond reactive AI responses required us to think about when and how to offer suggestions without being intrusive. We explored different triggering mechanisms and the importance of providing diverse and genuinely helpful options.Leveraging Google AI Tools:We gained practical experience integrating with the Gemini/Gemma models for natural language understanding and generation. We learned about the strengths of these models in understanding context and generating creative text. We also explored the potential of Google Cloud for backend services and Firebase for real-time communication, although our initial prototype focused on a more localized demonstration.The Importance of User Experience:Even in a technical hackathon, we recognized that the user experience of a creative tool is paramount. We learned the importance of designing an interface that feels intuitive and supportive, rather than overwhelming the creator with constant AI interventions.,How We Built It: A Focused Prototype: Given the limited time, we adopted a focused approach to build a proof-of-concept prototype. Our core implementation involved:Our primary focus was on demonstrating the core concept of context-aware proactive assistance using the power of Google's AI models.Challenges We Faced: The Time Constraint and the Depth of \"Understanding\": We encountered several challenges during the hackathon:The Relentless Clock:The most significant challenge was the incredibly short timeframe. Building a truly context-aware AI that deeply understands creative nuances requires significant time for data processing, model training, and robust implementation. We had to make strategic decisions about what features to prioritize for a functional prototype.Simulating True Contextual Understanding:Our rule-based approach to context analysis was a simplification. Training a model to truly understand the intent, style, and subtle cues in creative work is a complex task that goes beyond basic keyword recognition. We had to clearly articulate thepotentialof using advanced AI models like Gemini/Gemma for this purpose, even if our initial prototype had limitations.Balancing Proactivity and Intrusion:Designing the AI to offer helpful suggestions without disrupting the creative flow was a delicate balance. We experimented with different levels of proactivity and UI placements for the suggestions to minimize distraction.Integration Complexity:Fully integrating with existing creative platforms would require significant API knowledge and development time, which was beyond the scope of the hackathon. We focused on a standalone demonstration to showcase the core AI capabilities.,Despite these challenges, we are excited by the potential of the \"Context-Aware Creative Companion\" to revolutionize the way we create. We believe that by leveraging the power of AI to truly understand the context of our work, we can unlock new levels of creativity and collaboration in the digital age.",
                        "github": "https://github.com/rishyup/CreativeCatalyst",
                        "url": "https://devpost.com/software/creativeai"
                    },
                    {
                        "title": "Jiachilie",
                        "description": "Jiachilie bridges the gap between prison and society, offering tailored mental health support to empower inmates during and after incarceration.Empowering Transitions, One Step at a Time",
                        "story": "Inspiration: The inspiration for Jiachilie came from a personal experience. A friend of mine was arrested on a Friday, and due to the rules in Kenya, he had to wait until Monday for his bail hearing. During this time, he was placed in prison and encountered a harsh environment that severely impacted his mental health. Upon his release, he was scared and struggled with the transition back to normal life. Witnessing his difficulties inspired me to create a solution that could provide support to individuals in similar situations.What it does: What It Does: Jiachilie is a comprehensive mental health platform designed to support individuals transitioning from prison to society. Here\u2019s what Jiachilie offers:By offering these services, Jiachilie aims to provide a holistic support system that addresses the mental health needs of individuals during their transition from prison to the outside world.How we built it: Jiachilie was built with the goal of providing comprehensive mental health support to prisoners and recently released individuals. We combined cutting-edge technology with empathetic human touch to create a user-friendly platform that includes:Chatbot Support: Available 24/7 to offer immediate assistance and emotional support.\nTherapy Booking: Easy scheduling of therapy sessions with licensed professionals.\nCommunity Events: Opportunities to connect with others and participate in workshops focused on mental health and personal development.\nWe worked closely with mental health experts, former inmates, and support organizations to ensure our platform meets the unique needs of our users.Challenges we ran into: Developing Jiachilie came with its share of challenges:Understanding the Environment: Gaining a deep understanding of the prison environment and the mental health issues faced by inmates required extensive research and collaboration with experts.\nBuilding Trust: Convincing users to trust and engage with our platform was crucial. We prioritized privacy, security, and a compassionate approach to build credibility.\nResource Integration: Ensuring seamless integration of various resources, such as therapy sessions and community events, to provide a holistic support system was technically demanding.Accomplishments that we're proud of: Identifying a Critical Niche: Successfully identified a significant and underserved niche\u2014supporting the mental health of individuals transitioning from prison to society. This focus addresses a critical need that impacts many lives.Developing a Robust Solution: Developed a comprehensive mental health platform that runs locally, ensuring readiness for partnerships and scalable deployment. This accomplishment positions us well for future collaborations and wider implementation.These achievements demonstrate our commitment to creating impactful solutions that address real-world challenges and our readiness to expand our reach through strategic partnerships.What we learned: User Needs: Gained a deep understanding of the unique mental health challenges faced by prisoners and those re-entering society. This insight has been crucial in shaping our platform to meet these specific needs.\nImportance of Empathy: Learned the critical role empathy plays in building trust and engagement with our users, reinforcing the need for compassionate and understanding support mechanisms.\nTechnical Integration: Navigated the complexities of integrating various mental health resources into a seamless, user-friendly platform that can scale to meet growing demand.What's next for Jiachilie: Expansion of Services: Plan to introduce more specialized therapy options, including group therapy and peer support networks, to offer a broader range of support.\nMobile App Development: Develop a mobile app to increase accessibility and convenience for users, allowing them to access support anytime, anywhere.\nResearch and Feedback: Continue to gather user feedback and conduct research to refine and improve our services, ensuring they remain relevant and effective.\nAwareness Campaigns: Launch campaigns to raise awareness about the mental health challenges faced by prisoners and the support available through Jiachilie, aiming to reach a wider audience and encourage more people to seek help.\nBy focusing on these next steps, we aim to enhance our platform's impact and continue supporting the mental health and successful reintegration of former inmates.",
                        "github": "https://github.com/Kazenzi/Jiachilie",
                        "url": "https://devpost.com/software/jiachilie"
                    },
                    {
                        "title": "MLB Scout",
                        "description": "Build your fantasy MLB team and compete against other MLB fans",
                        "story": "Inspiration: MLB Scout was inspired by the growing intersection of fantasy sports and AI-powered engagement. We noticed that while many fans love fantasy baseball, the traditional format can be intimidating for newcomers. We wanted to create an engaging, educational game that helps fans learn about players while building their dream team through an interactive guessing game.What it does: MLB Scout is an innovative baseball player guessing game that combines elements of:\nDaily player guessing challenges similar to Wordle\nAI-powered chat assistance using Google's Gemini model to provide hints and player information\nFantasy team building where correctly guessed players can be added to your lineup\nA points-based system that rewards strategic gameplay\nReal-time leaderboards to foster friendly competition\nDynamic player cards with stats and positions\nAn interactive baseball field visualization for team managementHow I built it: I leveraged several key technologies:\nGoogle Cloud's Gemini 1.5 Pro model for intelligent player hints and chat interactions\nMLB Stats API for real-time player data\nReact for the frontend interface\nSupabase for user data and leaderboard management\nClerk for user authentication\nCustom baseball field visualization using SVG\nCanvas Confetti for celebratory animations\nTailwind CSS for responsive designChallenges I ran into: Balancing game difficulty - making the hints informative but not too revealing\nManaging player position eligibility rules for the fantasy team component\nImplementing real-time point calculations while maintaining game fairness\nCreating an intuitive UI that works for both casual and hardcore baseball fansAccomplishments that I am proud of: Successfully integrated Gemini 1.5 Pro to create engaging player hints and natural conversations\nBuilt a visually appealing and interactive baseball field interface\nImplemented a sophisticated points system that encourages strategic gameplay\nCreated a seamless fantasy team building experience\nDeveloped a responsive and intuitive user interface that makes baseball more accessible to new fansWhat I learned: How to effectively utilize Google Cloud's Gemini models for sports-related applications\nTechniques for building engaging game mechanics that balance fun and challenge\nStrategies for real-time data management with multiple API integrations\nThe importance of user feedback in tuning game difficulty and engagement\nMethods for creating accessible and educational sports experiencesWhat's next for MLB Scout: Implement daily challenges with special rewards\nAdd multiplayer head-to-head competitions\nIntroduce team-based leagues and tournaments\nExpand AI capabilities to provide more detailed player analysis\nAdd historical player challenges for baseball history buffs\nDevelop mobile apps for iOS and Android\nIntegrate with MLB.tv for video highlight connections\nAdd social features for sharing achievements and team builds",
                        "github": "",
                        "url": "https://devpost.com/software/mlb-scout-xwgcbu"
                    },
                    {
                        "title": "Contract suite",
                        "description": "Know Exactly what you're signing before you sign it",
                        "story": "The inspiration for ContractSuite came from observing the challenges that businesses and individuals face when dealing with legal documents. Many people struggle with understanding complex contract terms, missing important deadlines, and keeping track of obligations. We wanted to create a solution that makes contract management accessible, efficient, and intelligent using AI technology.ContractSuite is an AI-powered contract management platform that:Automatically extracts and summarizes key information from contractsIdentifies critical dates, obligations, and potential risksProvides an intuitive interface for document managementEnables collaboration through multi-user access and signatory managementOffers real-time analysis and risk assessment of contract terms,We built ContractSuite using a modern tech stack:Next.js and TypeScript for a robust frontend architectureClerk for secure user authenticationSupabase for database managementCustom AI models for contract analysisModern UI components for an intuitive user experienceCloud storage integration for document management,Built a sophisticated AI-powered contract analysis systemCreated an intuitive and user-friendly interface for complex document managementImplemented secure multi-user collaboration featuresDeveloped efficient document processing and storage solutionsSuccessfully integrated multiple modern technologies into a cohesive platform,Advanced Next.js and TypeScript development practicesImplementation of AI models for document analysisSecure handling of sensitive documentsReal-time collaboration system designModern UI/UX principles for complex applications,",
                        "github": "",
                        "url": "https://devpost.com/software/contract-suite-kjc53f"
                    },
                    {
                        "title": "Reel Riddle",
                        "description": "Create and Guess movies riddles with Redditors",
                        "story": "Inspiration: we used to play this game at boarding school years ago where someone would describe a movie and you had to guess what movie the other person describedWhat it does: Reel Riddle is a movie guessing game built as a Devvit app for Reddit.  Users are presented with a mysterious, AI-generated description of a movie and must guess the title.  Key features include:Movie Guessing Game:Players read cryptic descriptions and try to guess the movie.AI-Generated Descriptions & Hints:Uses the Gemini API to create unique, spoiler-free movie descriptions and helpful hints.Riddle Creation:Users can create their own movie riddles for others to solve.Points and Leaderboard:Players earn points for correct guesses and riddle creation, with a leaderboard to track top players.Reddit Integration:Riddles can be posted as custom post types on Reddit, and the app uses Reddit for user authentication and communication (comments, private messages).Persistent Game State:Uses Redis to store game state, user points, and solved riddles.Movie Poster Display:Fetches movie posters from the TMDB API to enhance the visual experience.,How we built it: Reel Riddle is built using a combination of technologies:Devvit:The core platform for building the Reddit app, providing access to Reddit APIs, Redis, media hosting, and HTTP requests.TypeScript:Used for backend logic and message handling between Devvit and the web view, ensuring type safety and code maintainability.React (JSX in Devvit):Utilized within Devvit'smain.tsxfor structuring the UI components and managing component state with hooks likeuseStateanduseAsync.Redis:Employed for persistent data storage, managing game state, user points, leaderboards, and solved riddles.Gemini API:Integrated to generate movie descriptions and hints using natural language processing.TMDB API:Used to fetch movie poster URLs based on movie titles, enhancing the visual presentation of riddles and results.HTML, CSS, JavaScript:Powers the web view (page.html,style.css,script.js) to create the interactive frontend game interface.Reddit API:Leveraged for user authentication, posting riddles as custom post types, submitting comments, and sending private messages to users for game updates and points notifications.,Challenges we ran into: Developing Reel Riddle involved overcoming several challenges:API Integration Complexity:Successfully integrating multiple APIs (Gemini, TMDB, Reddit, and Redis) and managing data flow between them was a significant hurdle.Asynchronous Operations:Handling asynchronous operations inherent in API calls and data fetching usinguseAsyncand Promises required careful management to avoid race conditions and ensure smooth UI updates.State Management Across Devvit and WebView:Effectively managing and synchronizing game state between the Devvit backend and the frontend web view using message passing proved complex.AI Description Quality:Ensuring the Gemini API generated consistently engaging, cryptic, and spoiler-free movie descriptions required prompt engineering and error handling for varied outputs.Movie Poster Retrieval:Reliably fetching movie posters from TMDB and handling cases where posters were not found or API requests failed required robust error handling and fallback mechanisms.User Experience within Devvit:Designing an intuitive and engaging user experience within the constraints of the Devvit custom post type and web view environment presented UI/UX design challenges.,Accomplishments that we're proud of: Despite the challenges, we achieved several accomplishments we are proud of:Functional and Engaging Movie Guessing Game:Successfully created a playable and enjoyable movie guessing game within the Reddit platform.AI-Powered Riddle Generation:Implemented AI-driven movie description and hint generation, adding a unique and dynamic element to the game.User Point System and Leaderboard:Developed a point system and leaderboard to encourage player engagement and competition.Riddle Creation Feature:Empowered users to contribute to the game by creating and sharing their own movie riddles.Seamless Reddit Integration:Achieved deep integration with Reddit, leveraging custom post types, comments, and private messages to create a cohesive Reddit-native experience.Persistent Game Data:Utilized Redis to ensure game progress, user points, and riddles are persistently stored across sessions.Visually Enhanced Riddles:Incorporated movie posters to improve the visual appeal and provide additional context to the riddles.,What we learned: Building Reel Riddle provided valuable learning experiences:Devvit Platform Expertise:Gained in-depth knowledge of the Devvit platform, its capabilities, and limitations in building Reddit apps.Reddit API Proficiency:Developed practical skills in utilizing the Reddit API for various functionalities like user interaction, content creation, and community engagement.AI API Integration (Gemini):Learned how to effectively integrate and leverage AI APIs like Gemini for content generation within a Devvit app.Asynchronous Programming and State Management:Improved skills in managing asynchronous operations, Promises, and state management in both Devvit and web view environments.Frontend Development for Devvit Web Views:Enhanced web frontend development skills within the context of Devvit web views, focusing on performance and responsiveness.Full-Stack Development on Devvit:Experienced the full development lifecycle of a Devvit app, from backend logic and data storage to frontend UI and Reddit integration.,What's next for Reel Riddle: We have exciting plans to further develop Reel Riddle:Expanded Movie Database:Increase the range of movies beyond the initial popular movie list to offer greater variety and challenge.Improved AI Description Generation:Refine Gemini API prompts and implement error handling to enhance the quality, consistency, and cryptic nature of movie descriptions.Enhanced Hint System:Develop more varied and helpful hint mechanisms beyond just revealing the first letters.New Game Modes & Features:Explore adding new game modes, such as themed riddles, daily challenges, or cooperative play.UI/UX Enhancements:Improve the user interface and user experience based on user feedback, focusing on clarity, navigation, and visual appeal.Detailed User Statistics & Profiles:Implement user profiles with detailed game statistics, riddle creation history, and performance metrics.Community Riddle Sharing & Discovery:Develop features to allow users to easily share and discover riddles created by other players, fostering a community around riddle creation and solving.,",
                        "github": "",
                        "url": "https://devpost.com/software/reel-riddle-fhey7g"
                    }
                ]
            ]
        },
        {
            "title": "yard work: the coolest ai hackathon ever",
            "location": "One Kendall Square",
            "url": "https://yard-work.devpost.com/",
            "submission_dates": "Apr 12 - 13, 2025",
            "themes": [
                "Cybersecurity",
                "Machine Learning/AI",
                "Open Ended"
            ],
            "organization": "Launch Yard",
            "winners": false,
            "projects": [
                [
                    {
                        "title": "Laminar",
                        "description": "Laminar is the first AI-native credit card API\u2014enabling secure, rule-based, and verified payments for autonomous agents with one-time virtual cards and user oversight.",
                        "story": "As AI agents gain the ability to book travel, order supplies, and manage purchases autonomously, we\u2019re approaching a critical juncture:Today\u2019s payment infrastructure was built for humans and websites\u2014not autonomous systems. Continuous credit cards expose users to fraud, hallucinated transactions, and invisible overspending. We builtLaminarto solve that.Just like Stripe enabled internet payments,Laminaris the payment backbone for AI agents\u2014enabling secure, verified, policy-driven transactions that give users control without blocking automation.Laminar is a secure, programmable payments API purpose-built for AI agents. With Laminar, agents don\u2019t get unrestricted access to a card. They go through a smart, layered verification pipeline:Generates one-time-use virtual cards tied to a specific purchase requestEnforces user-defined rules (e.g., \"only approve if under $200 within 50 miles\")Blocks and routes purchases for manual approval if rules aren't metVerifies purchase metadata to catch hallucinations or spoofed requestsAudits and logs every transaction for traceability and compliance,Our demo shows an AI travel agent using Laminar to book a hotel\u2014with approvals enforced by the API, not the app.AI agents are capable of initiating purchases\u2014but they can\u2019t be trusted to approve or validate those purchases themselves.Even the smartest agents can:Hallucinate product details or vendorsMisinterpret user intentFail to recognize fraud or spoofed listingsBe exploited by prompt injection or logic corruption,Letting an AI oversee its own spending is like letting someone grade their own test\u2014there\u2019s no independent accountability.Laminar is thefirst-ever credit card infrastructure designed for AI agents. Our API is the third-party verification layer between AI agents and your wallet.Every request is checked by programmable rules or user reviewUpon approval, a scoped, one-time-use virtual card is issuedThe agent can only use that card, for that specific transactionEverything is logged and verifiable,No matter how advanced the agent is, it still needs a trusted middle layer. Laminar makes sure AI doesn\u2019t spend a single dollar unless it\u2019s been verified and approved.Frontend:SwiftUI travel app as the AI agent interfaceBackend:Python flask web service simulating agent logic and expense requestsCore API:Laminar service built with Python flask, integrated with mock virtual card issuer and approval flowsSecurity Layer:Purchase intent matching using product metadata and constraint validationCustom Rules Engine:Dynamic auto-approval evaluation and fallback manual approval UI,All purchases are locked to single-use credentials, and every call is policy-bound.Designing a secure payment flow that feels natural for agents, but is still auditable and enforceable for humansBuilding real-time dynamic rule enforcement and decision logic that integrates with a virtual card systemCreating a reliable fallback path that doesn\u2019t block agents, but requires explicit user consent,Built a full-stack AI payments pipeline with granular security in under 48 hoursCreated a secure layer that prevents AI from executing any transaction without checksDeveloped a polished demo where an AI travel bot seamlessly interacts with Laminar\u2014showcasing real-world application of our APIDesigned a universal framework that can be extended to procurement, e-commerce, and financial automation bots,AI agents are powerful\u2014but without financial guardrails, they're liabilitiesVerifying intent and enforcing spend policies is just as important as executing paymentsThe future of commerce isn\u2019t just about automation\u2014it\u2019s about trustable automationA new class of infrastructure is needed\u2014not for users, but for agents,Public API with usage-based pricingSDKs for LangChain, AutoGen, and custom agent frameworksDashboard for agent management, spend limits, and approval trackingCompliance APIs for regulated sectors (finance, healthcare, procurement)Intelligent dispute resolution engine for failed or suspicious transactionsIntegrations with live merchant APIs to validate product metadata in real-timeAdvanced KYC and delegated wallet layers for multi-agent authorizationLaunching a closed beta with finance automation and travel planning startupsBecoming the de facto payment layer for the agent economy,",
                        "github": "https://github.com/awesomenuzzo/laminar-api",
                        "url": "https://devpost.com/software/trustfundr"
                    },
                    {
                        "title": "pAIx",
                        "description": "Protecting your art is as easy as pAIx - cloak it, track it, own it.",
                        "story": "(AI in Cybersecurity Track),\ud83d\ude80 About the Project: As a team of technologists deeply immersed in the world of AI, we\u2019ve always been captivated by its potential, but also wary of its unintended consequences. Over the past year, we watched as many of our close friends, illustrators, digital painters, photographers, saw their work scraped, repurposed, and fed into massive AI models without their consent, credit, or compensation.We wanted to change that.What if there was a way for artists tofight back, not with takedown notices or lawsuits, but withcode?What if you couldcloak your artwork, making ittoxicto AI training models while remaining beautiful and untouched to human viewers?What if you couldtrack where your art spreads online, getproof of ownership, and evenpursue compensationwhen companies misuse your work?That\u2019s the tool we built: a platform that empowers artists withAI-based image protection,adversarial cloaking, andinternet-wide tracking, all rolled into one.\ud83d\udca1 Inspiration: The inspiration came from a mix of empathy and outrage. Watching our artist friends feel helpless, seeing their work appear in Midjourney outputs or used in marketing copy generated by LLMs, was deeply frustrating. We realized that despite all the conversations around AI ethics,there weren\u2019t many actual tools being built to protect the creators themselves.We also saw that the impact goes beyond individual artists.Big media companies like Getty, Adobe, and news outletshave started to realize that their image libraries are being mined for free. There\u2019s a growing hunger not just for ethics, but forenforceable control and monetization.This tool offers both.\ud83e\uddf1 How We Built It: We started with a simple principle: build an MVP thatactuallypoisons AI models, and then scale the vision from there.Adversarial Image CloakingUsing TensorFlow and adversarial attack techniques (FGSM, PGD), we generate imperceptible pixel-level noise thatprevents models from learning anything useful. Human viewers see the same image; models get garbage.Vision Embedding + Search AgentUsing CLIP and OpenCLIP, we built a system tomonitor the web for similar images, even if cropped, color-shifted, or filtered. It lets artists know where their art is being used.LLM-Powered Context AnalysisWe integrated an LLM (GPT-3.5 lol, want to use our own post-hackathon tho) to analyze text around found images and determine: is this a repost with credit, an AI generation, or a case of theft?Auth-Backed Ownership TaggingEvery uploaded image gets a cryptographic signature + metadata fingerprint to prove original ownership and usage intent.Artist DashboardA clean, actionable dashboard that shows where your art is, how it\u2019s being used, and lets you prepare takedown notices or replacement uploads.,\ud83e\udde0 What We Learned: Adversarial image cloaking iswildly effective, but tricky, small perturbations need to balance invisibility and poison strength.CLIP-based similarity search can match imageseven across heavy transformations, making it a killer tool for tracking.Artists want control, but they also want it to beeasy, so we learned how to package hard tech in a way that feels simple and empowering.Business-wise, this tech ismuch bigger than indie artists, it has legs in the enterprise space, licensing, and even copyright law enforcement.,\u2694\ufe0f Challenges We Faced: Speed: Running adversarial cloaking on many images can be slow, so we had to optimize the pipeline and batch operations.Similarity detection: Filtering false positives from web crawls required fine-tuning both embedding distance thresholds and LLM prompts.Ethical considerations: We had long internal discussions about how to build this defensively, we want to protect creators, not break models recklessly.Web limitations: Not all platforms allow easy replacement of uploaded images, so we had to build workarounds and browser-based enhancement tools.,\ud83c\udf0d The Vision: This tool ismore than a hackathon project. It\u2019s a blueprint for an ecosystem that:Protects artistsfrom unauthorized AI trainingEnables businessesto monetize their media archives more ethicallyPressures AI companiesto license content fairlyChanges the default narrative, from \u201cscraping everything is fine\u201d to \u201ccreators have power.\u201d,This isn\u2019t just a defensive tool, it\u2019s amarket signal. A visible, viral, creator-led pushback that forces AI developers tothink ethically and economicallybefore using copyrighted material.\ud83d\udcbc Business Potential: Sell enterprise licenses toGetty, Adobe, Shutterstock, news orgsOffer subscription tiers forindependent artistsDevelop aplugin layer for social platformsand CMS toolsPartner withlegal firmsto offer built-in takedown generation servicesAPI forAI model developersto license clean data, flipping the power dynamic,\u2764\ufe0f Final Thoughts: We didn\u2019t just build this because it was technically cool. We built it becausethe people we care about are being hurt by a system that forgot them.Now, we want to give them a sword, something that says,",
                        "github": "https://github.com/Faith-Rounds/paix-yard-launch",
                        "url": "https://devpost.com/software/paix"
                    },
                    {
                        "title": "Leasa",
                        "description": "Lease it with Leasa, your AI powered broker. ",
                        "story": "",
                        "github": "https://github.com/ritish1082/Leasa.git",
                        "url": "https://devpost.com/software/leasa"
                    },
                    {
                        "title": "ThiinkFast",
                        "description": "Thiinkfast is a smart writing tool for ADHD and neurodivergent users. It learns your writing quirks, typos, and phrasing, then auto-corrects them in real time as you type.",
                        "story": "We noticed that traditional writing tools just\u2026 don\u2019t get how some of us think. ADHD and neurodivergent minds often freeze mid-sentence, jump ahead, or make the same typo over and over. That\u2019s not a bug \u2014 it\u2019s just how our brains work. So why should current editing tools interrupt that rhythm? How often have you misspelled something and1.) autocorrect didn\u2019t even get it right, and2.) you completely lost your train of thought? </3We built ThinkFast to help people express their ideas freely, without being held back by rigid, overstimulating editing systems.ThinkFast is a personalized autocorrect web application that lets you type freely \u2014 without getting overwhelmed by glaring error signals.You can choose whether you want automatic corrections turned on or off, then write directly into the text box. When you're ready, view your corrected writing without having your thought process interrupted.Want to see what you commonly mess up? Just click\"View Insights\"to get a pop-up with your most frequent spelling mistakes \u2014 so you can learn and improve over time.We used Gramformer, a language correction model byPrithiviraj Damodaran, as the foundation of our Python backend. Gramformer enables smarter, context-aware corrections and allows for deeper personalization as users interact with the tool.We connected our backend to a frontend built inHTMLusingFastAPI, enabling real-time, low-latency communication between the interface and the correction engine.To keep the user experience ADHD-friendly, we intentionally:Removed distracting red underlinesAdded a toggle for live autocorrectionBuilt an \"Insights\" pop-up that tracks and visualizes your most common writing habits and misspellings,We ran into many technical complications \u2014 especially when connecting our model to the user interface using FastAPI and HTML.Some key challenges included:Implementing per-word live autocorrectionHandling visual complications with the Insights dashboardBalancing model training between our goals and Gramformer's defaults,In the end, we were able to bring our model and interface to a solid, user-friendly state.We're especially proud of:Developing a working web application that connects multiple components into a seamless user experienceCreating something that actually does what we intended \u2014 and is meaningfulIdentifying a real-world problem and designing a solution that\u2019s inclusive, accessible, and useful for many,We\u2019re excited by the potential for this to help people express themselves more freely in everyday life.From a conceptual standpoint, we learned:How to identify nuanced needs within our communityHow to build with accessibility and neurodivergence in mindHow to prioritize inclusivity and user experience over perfection,Technically, we learned:A ton about APIs and full-stack developmentHow to integrate ML models with web frontendsThe complexities of timing, UI state, and error handling in live correction workflows,Next steps we\u2019re excited about:Improving the model\u2019s accuracy and speedMaking the user interface more interactive and customizableBuilding faster, more seamless correctionsExpanding support for students and anyone who wants to express themselves more freely, without friction,We\u2019d love to grow ThinkFast into a tool people actually rely on \u2014 not just to fix mistakes, but to think more confidently.PITCH DECK IS LINKED IN GITHUB:",
                        "github": "https://github.com/amernor/ThiinkFast",
                        "url": "https://devpost.com/software/thiinkfast"
                    },
                    {
                        "title": "OmniBot",
                        "description": "OmniBot is a Python AI assistant that controls your desktop and tools through natural language. Automate tasks, launch apps, manage files, and more\u2014all from a single chat interface.",
                        "story": "Imagine a chatbot that doesn't just talk \u2014 it acts.OmniBotis a Python-powered desktop assistant designed to seamlessly interact with both your tools and your environment. Whether it's launching applications, organizing your files, generating reports, automating tedious workflows, or even scraping the web and responding with intelligent summaries \u2014 OmniBot becomes yourcommand center for everything digital.\u2705 What Makes OmniBot Different?: Toolchain Integration: Connects to calendars, emails, terminal commands, spreadsheets, design tools, and more.Desktop Access: Automate OS-level tasks like file organization, app launching, screenshots, and more \u2014 all through natural language.AI Intelligence + Control: Backed by advanced language models with role-based access to different tools for safe and contextual execution.Modular Python Framework: Easily extend OmniBot by plugging in new tools or APIs in minutes \u2014 everything from GitHub to Spotify.,\ud83d\udd27 Use Cases: \u201cSort all files downloaded today into folders by type.\u201d\u201cSchedule a Zoom meeting with my team at 4PM and email the agenda.\u201d\u201cSummarize the top 5 articles from my reading list.\u201d\u201cRun my data-cleaning script and email me the results.\u201d,\ud83d\udee0\ufe0f Built With: Python, LangChain, OpenAPlug-and-play architecture for tools and desktop APIsSecurity-first design with sandboxed tool execution,\ud83c\udfaf Vision: OmniBot isn\u2019t just a chatbot. It\u2019s the bridge between conversation and command \u2014 thefirst truly functional AI co-pilot for your desktop.Unlock your productivity. One message at a time.",
                        "github": "",
                        "url": "https://devpost.com/software/omnibot"
                    },
                    {
                        "title": "Ask G",
                        "description": "Voice-enabled career coach enabled by Harvard-backed storytelling algorithm to craft a holistic career narrative. \u201ccareer story\u201d brain for CVs, CLs, and emails\u2014unlike generic template-based tools",
                        "story": "Stand out with our voice-enabled CV builder that harnesses a Harvard-backed storytelling algorithm to craft a holistic career narrative. This powerful \u201ccareer story\u201d brain seamlessly generates job-winning CVs, cpverl letters, and personalized emails\u2014unlike generic template-based tools. Our B2C2B model also unlocks unique monetization pathways: screening candidates, creating tailored interview questions, and analyzing interview responses in real-time to reveal strengths and fit. By blending advanced algorithms and user-centric design, we propel your individual story forward at every stage of the hiring journey, ensuring you shine in front of both recruiters and potential employers.More details at the link (here)[https://docs.google.com/presentation/d/1zSTf5K9cuple2jxv1aO27MW8nYow7Wny/edit#slide=id.p1]",
                        "github": "https://github.com/gbolarhan/Hackathon",
                        "url": "https://devpost.com/software/ask-g"
                    },
                    {
                        "title": "CodeGraph",
                        "description": "Illuminate your codebase: Visualize, navigate, and search code relationships with ease and efficiency using our mcp. ",
                        "story": "Inspiration: The CodeGraph project was inspired by two major challenges: 1) helping developers understand complex codebases visually, and 2) providing language models with efficient code context to enhance their programming assistance. Traditional code exploration tools often fail to capture relationships between components, while LLMs struggle with the context limits when analyzing large codebases. We wanted to create a tool that would not only make code exploration more intuitive for humans but also provide LLMs with a lightweight, structured graph representation that enables faster and more accurate code understanding and generation.What it does: CodeGraph creates an interactive visual knowledge graph of Python code relationships, showing files, functions, and their connections. Most importantly, it provides:A lightweight context graph for LLMs that dramatically improves their understanding of code structure without consuming excessive context window spaceSemantic search capabilities allowing AI assistants to quickly find relevant code sectionsMCP server integration enabling AI assistants like Claude and Cline to directly access and navigate code relationshipsInteractive web-based visualization for human developersDatabase storage for persistent graph data,By providing this structured graph representation to LLMs, CodeGraph enables them to:Understand code architecture without needing the entire codebase in contextNavigate effectively between related code componentsMaintain awareness of dependencies and relationshipsGenerate more contextually appropriate code solutionsProvide more accurate and helpful responses to code-related questions,How we built it: We built CodeGraph using a combination of:Python for core functionality and parsingNetworkX for graph data structures and algorithmsPyvis for interactive web visualizationsMongoDB for metadata storageVector database (Qdrant) for embeddings and semantic searchSentence Transformers for generating vector embeddingsFastMCP for the Model Context Protocol server implementation,The architecture prioritizes creating an efficient, information-dense graph representation that provides maximum context to LLMs while minimizing token usage.Challenges we ran into: Throughout development, we faced several challenges:Designing a graph representation that balances information density with token efficiency for LLMsDetermining the optimal level of abstraction for code relationshipsParsing complex Python code accurately across multiple filesCreating effective vector embeddings that capture code semanticsImplementing the MCP server to make the tool accessible to AI assistantsOptimizing performance with large projects that generate complex graphs,Accomplishments that we're proud of: We're particularly proud of:Creating a lightweight graph representation that significantly improves LLM code understandingSeamless integration with AI assistants through the MCP protocolSemantic search capability that allows finding code by describing functionalityThe efficient token usage that allows LLMs to understand large codebasesReal-time updates that reflect changes to the codebase,What we learned: This project taught us valuable lessons about:Optimizing knowledge representations for LLM context windowsGraph theory and visualization techniquesVector embeddings and semantic search implementationHow LLMs understand and process code relationshipsBuilding tools that enhance AI capabilitiesMCP server development and integration with AI assistants,What's next for CodeGraph: Even more efficient graph representations for LLMsAutomated abstraction levels that adjust based on codebase size and complexitySupport for additional programming languages beyond PythonIntegration with development environments (IDEs)Advanced query capabilities allowing LLMs to navigate code relationships through natural languagePerformance optimizations for handling enterprise-scale codebasesExpanded MCP capabilities for deeper AI integrationThe core focus will remain on optimizing how we can provide rich, structured code context to LLMs in ways that minimize token usage while maximizing code understanding.,",
                        "github": "https://github.com/AdityaBhanwadiya/codegraph",
                        "url": "https://devpost.com/software/codegraph"
                    },
                    {
                        "title": "SwipeSafe",
                        "description": "Automatic background check for dating apps.",
                        "story": "",
                        "github": "https://github.com/chensterman/yujin-ai",
                        "url": "https://devpost.com/software/swipesafe"
                    },
                    {
                        "title": "OutfitBoss",
                        "description": "Despite having a full closet, my girlfriend never knows what to wear. Instead of visiting all her favorite brands, one by one, what if there was a better way to purchase cohesive outfits?",
                        "story": "Inspiration: Despite having a full closet, my girlfriend never knows what to wear. Instead of visiting all her favorite brands, one by one, what if there was a better way to purchase cohesive outfits?Googling for clothes might be great for finding particular items you have in mind. However, if you want to start from scratch or buy full cohesive outfits for a location or an occasion, our app can help.What it does: All you have to do is type in what you're looking for, in as much detail as possible. Our agent will then scour our pre-tagged and classified sources to find what goes well together and sends you our recommended outfits.How we built it: We compiled a directory of clothing outfits, where features from the name, description, and image are extracted. We used a variety of models, including LLaVA (Large Language and Vision Assistant) but in the end settled on DeepSeek.\nThe model selects the most applicable tags from an exhaustive list of tags we generated ourselves.\nWe created our web frontend and backend through NextJS with Shadcn materials. The user prompt is then fed to DeepSeek which picks the tags from our exhaustive list that match the best.Challenges we ran into: Most models were terrible at image feature extraction. We tried a bunch and settled on DeepSeek due to time constraints.\nScraping a large number of outfits from various outlets was a time consuming process. Each outfit also had to go through our feature extraction process.Accomplishments that we're proud of: Our frontend looks great and it's just a matter of increasing the number of vendors and listings in our directory and optimizing the tag generation process.What we learned: Choice of model is very important.\nWe also learnt a lot about UI design from our frontend.\nLearnt how to use LMStudio to locally host LLMs.\nLearnt how to use MongoDB Atlas.\nLearnt how to connect everything in NextJS and Typescript.What's next for OutfitBoss: Use a different machine learning model (not an LLM) for image feature extraction.\nKeep scraping outlets or find a different way to find products.",
                        "github": "https://github.com/airman416/shopper_agent",
                        "url": "https://devpost.com/software/outfitboss"
                    },
                    {
                        "title": "Donna",
                        "description": "Have you ever watched Suits?",
                        "story": "Donna: Your AI Executive Assistant\nInspiration\nThe inspiration for Donna came from two main sources. First, the iconic character Donna Paulsen from the TV show \"Suits,\" who is renowned for her exceptional organizational skills, emotional intelligence, and ability to anticipate needs before they arise. Second, from the observation that while our digital lives have become increasingly complex, our tools for managing them remain fragmented and unintelligent.As professionals, we juggle multiple calendars, communication platforms, task management systems, and information sources. These tools work in isolation, creating cognitive overhead and forcing us to be our own system integrators. We asked ourselves: \"What if we could create a true AI executive assistant that works across our digital ecosystem with the same intuition, empathy, and competence as Donna Paulsen?\"In our professional lives, we've experienced how the right support can multiply productivity. We believe AI has reached a point where it can provide this level of assistance to everyone, not just high-powered executives with human assistants.What it does\nDonna is an AI executive assistant that brings order to chaos by integrating across your digital life and providing intelligence that amplifies your capabilities. Specifically, Donna:Prioritizes tasks intelligently: Unlike basic to-do apps, Donna analyzes deadlines, context, importance, and your work patterns to suggest what needs attention now versus later. She doesn't just track tasks; she helps you make smart decisions about them.\nSupercharges calendar management: Donna doesn't just show your schedule; she optimizes it. She suggests ideal meeting times based on your energy patterns, buffers travel time, prepares relevant materials before meetings, and generates follow-up tasks afterward.\nTransforms communication handling: Donna analyzes your emails and messages, summarizing key points, extracting action items, detecting sentiment, and drafting contextually appropriate responses. She handles the cognitive load of processing information overload.\nProvides personalized insights: As Donna learns your work patterns and preferences, she offers increasingly tailored suggestions and observations about how you can optimize your day\u2014from identifying time blocks for deep work to suggesting when to address specific types of tasks.\nAll of this comes together in a sleek, intuitive interface with Donna's signature color scheme and a touch of personality that makes the app feel more like interacting with a capable colleague than a sterile productivity tool.How we built it\nBuilding Donna required creating a system that's both technically sophisticated and user-friendly. Our approach covered multiple layers:Core Architecture\nWe built Donna on a microservices architecture with Python (FastAPI) for AI-intensive services and Node.js (Express) for the API layer. This hybrid approach allowed us to leverage Python's rich ML ecosystem while maintaining high-performance API endpoints. We used MongoDB for flexible document storage and Redis for caching.AI Stack\nThe intelligence layer of Donna is powered by several specialized machine learning systems:Task Prioritization Engine: We developed a custom gradient boosting model that weighs factors like deadlines, context, user patterns, and textual signals to score task importance and urgency.\nNatural Language Processing Pipeline: We integrated transformer-based models (BERT variants) to analyze emails and messages for sentiment, extract action items, classify content, and generate summaries.\nPattern Recognition System: We built a learning system that identifies user preferences and work habits over time, allowing Donna to make increasingly personalized recommendations.\nContext-Aware Response Generator: We fine-tuned NLP models to draft communications that match the user's writing style and the specific context of the conversation.\nFrontend Experience\nFor the frontend, we used React Native for cross-platform compatibility, with a design system inspired by Donna Paulsen's professional aesthetic\u2014elegant, efficient, with signature red accents. We employed Redux for state management and implemented offline capabilities to ensure Donna remains functional even without connectivity.Integration Layer\nWe built robust integration services to connect with:Calendar systems (Google Calendar, Outlook, Apple Calendar)\nEmail providers (Gmail, Outlook)\nCommunication platforms (Slack, Teams)\nTask management systems (integrates but also stands alone)\nChallenges we ran into\nBuilding Donna presented several substantial challenges:Balancing Autonomy and Control\nOne of our biggest challenges was finding the right balance between automation and user control. We wanted Donna to be proactive but not presumptuous. We solved this through a \"confidence threshold\" system where Donna would automatically handle tasks she was highly confident about while seeking confirmation for others. This required careful calibration of our ML models and extensive user testing.Calendar Intelligence Complexity\nCreating truly intelligent calendar optimization proved much harder than anticipated. Scheduling involves complex constraints\u2014personal preferences, energy patterns, meeting types, and participants' availability. Our initial attempts at optimization algorithms were either too simplistic or too rigid. We eventually developed a flexible constraint-satisfaction system that balances multiple factors while adapting to feedback.Privacy and Security Architecture\nAs an assistant with access to sensitive communications and data, security was paramount. We implemented end-to-end encryption for communications, zero-knowledge architecture for sensitive data, and granular permission controls. This added significant complexity but was essential for creating trust.Integration Heterogeneity\nEach service we integrated with had different APIs, authentication mechanisms, and rate limits. Building a unified abstraction layer that worked reliably across all these services required careful engineering and extensive error handling.Preventing Notification Fatigue\nEarly prototypes overwhelmed users with notifications and suggestions. We had to develop a sophisticated prioritization system for Donna's own communications to ensure she only interrupted users for truly important matters.Accomplishments that we're proud of\nDespite the challenges, we achieved several breakthroughs we're particularly proud of:Intelligent Task Prioritization\nOur task prioritization system consistently impresses users with its ability to surface what matters most. In user testing, participants reported that Donna's suggestions matched or exceeded their own judgment about task importance 87% of the time.Email Intelligence\nOur email analysis system can extract action items, detect meeting requests, and generate appropriate responses with remarkable accuracy. The system identifies key tasks from emails with over 90% precision and recalls over 85% of the actionable items.Adaptive Learning System\nDonna genuinely gets better the more you use her. Our pattern recognition system successfully adapts to individual users within just a few days of use, with personalization quality scores improving by an average of 38% after one week of usage.Cross-Platform Experience\nWe built a seamless experience that works across mobile, desktop, and web platforms while maintaining consistent functionality and design language. Users can switch between devices without missing a beat.Performance Optimization\nDespite the complex AI processing happening behind the scenes, we achieved response times that feel instantaneous to users through clever caching, pre-computation, and progressive loading techniques.What we learned\nThis project taught us valuable lessons across multiple domains:Technical Insights\nAI Model Integration: We learned how to effectively chain together specialized AI models to create a system greater than the sum of its parts.\nContext Management: We discovered techniques for maintaining and leveraging user context across interactions to make each touchpoint more intelligent.\nScaling ML Systems: We learned how to build inference systems that can handle concurrent users while maintaining performance and cost-effectiveness.\nUser Experience Discoveries\nAI Personality Balance: We found that users responded best to an assistant with just enough personality to feel relatable but not so much that it became distracting.\nTrust Building: We learned how gradually increasing automation based on demonstrated competence builds user trust more effectively than offering full automation from the start.\nFeedback Integration: We developed techniques for continuously learning from explicit and implicit user feedback to improve the system.\nProduct Development Insights\nPrototype Validation: We learned to validate complex AI features with simplified prototypes before full implementation.\nFeature Prioritization: We discovered which assistant capabilities delivered the most immediate value to users, helping focus our development efforts.\nUser Onboarding: We learned techniques for gradually introducing users to Donna's capabilities without overwhelming them.\nWhat's next for Donna\nWhile we're proud of what we've built, we see several exciting directions for Donna's future:Enhanced Capabilities\nVoice Interface: Adding voice interaction for hands-free assistance while commuting or multitasking.\nDocument Intelligence: Expanding Donna's abilities to analyze, summarize, and extract insights from documents, reports, and articles.\nAdvanced Meeting Support: Real-time meeting assistance with automatic note-taking, action item extraction, and follow-up generation.\nExpanded Ecosystem\nTeam Coordination: Enabling Donna to coordinate across teams, managing shared tasks and facilitating group scheduling.\nAPI Platform: Creating an API that allows third-party developers to build specialized extensions for Donna.\nEnterprise Integration: Developing deeper integrations with enterprise systems like CRM, ERP, and project management tools.\nTechnical Advancements\nOn-device Intelligence: Moving more processing to the edge for enhanced privacy and reduced latency.\nMultimodal Understanding: Adding capabilities to understand and process images, charts, and other visual information.\nConversational Depth: Improving Donna's ability to handle complex, multi-turn conversations about scheduling, planning, and prioritization.\nBusiness Model Evolution\nPersonal vs. Professional Tiers: Creating differentiated offerings for individual users versus business teams.\nVertical-Specific Assistants: Developing specialized versions of Donna for legal, medical, educational, and other professional contexts.\nWe believe Donna represents the future of productivity\u2014not just managing tasks but truly augmenting human capabilities through intelligent assistance. Just as the character Donna Paulsen elevated Harvey Specter's effectiveness in \"Suits,\" our Donna aims to be the ultimate partner in professional excellence.",
                        "github": "https://github.com/Mzcassim/donna-executive-suite",
                        "url": "https://devpost.com/software/donna-pdzgq3"
                    },
                    {
                        "title": "Grokify",
                        "description": "Hack any LLM.",
                        "story": "Inspiration: Agents and LLMs have skyrocketing adoption but have glaring weaknesses. Tree of Attack white paper from Enkrypt's Tanay.What it does: It takes in a prompt for a type of hack and writes the code for the hack that can bypass an LLM of your choiceHow we built it: We used an Attacker and Evaluating LLM that work to recursively optimize obfuscation of malicious code until it can get past the evaluating LLM.Challenges we ran into: Generalizing the prompt to work on natural language as well as code to bypass censorship in LLMs. Political, Racism, Safety  controls can be bypassed. How about hidden information like payments and PII next?Accomplishments that we're proud of: We implemented the white paper and were able to create malware from a prompt that can bypass GPT-4oWhat we learned: LLMs are weak to attacks like these! Adoption is increasing and agentic systems are only being created at higher rates with even less security with vibe coding. There is a huge vulnerability hereWhat's next for Grokify: To generalize Grokify to work with natural language and bypass censorship of content. Make Grokify work with more Attacking and Evaluating LLMs",
                        "github": "",
                        "url": "https://devpost.com/software/grokify"
                    },
                    {
                        "title": "Idiots.ai",
                        "description": "Why make one project when you can make three?",
                        "story": "A brand of nonsense. Why ship one product when you can ship three?\ud83e\udde0 Inspiration: Hackathons are often about building one amazing project. But we thought, why not flip the script and deliverthree chaotic innovationsthat ride the line between genius and madness? From AI gaslighting to preventing teen overdoses to chugging responsibly \u2014 we\u2019ve got you covered. Shoutout to Arav for encouraging the madness.\ud83d\udca1 What it does: We built threevery realtools:A sarcastic, debate-savvy chatbot thatteaches you how to argue and win, with datalogic backing every gaslight.Teaches persuasive speaking using AIAuto-generates logical fallacies to help you spot (or abuse) themGamified chatbot that gets better the more you interact,Helps you and your friends keep track of your hydration levels, drinks consumed, andavoids overindulgence.Friendly leaderboard for safe drinkingAnonymous stats-sharing to prevent peer pressureRewards responsible choices with serotonin-friendly messages,A platform forpolitical gambling\u2014 but with a twist. Real data, real odds, and real-time updates.Bet (hypothetically) on political outcomes using historical dataSimulates real market dynamics to teach probability and civic engagementLaunching this summer for research use,\ud83d\udee0\ufe0f How we built it: Frontend: React, Vite, TailwindCSSBackend: Node.js, Express, FirebaseAI/ML: OpenAI\u2019s GPT-4 API, Prompt EngineeringOther tools: Figma, Canva, Heroku, Postman,\ud83e\uddea Challenges we ran into: Making sure GaslightGPT didn\u2019t turn evil (jury\u2019s still out)Finding the right balance of fun vs. serious for ChugChampAvoiding legal grey areas with LiveSpanned (satire saves lives),\ud83d\udcc8 Market Potential: Gaslight GPT: Debate clubs, high schoolers, future politiciansChugChamp: College students, health educators, party plannersLiveSpanned: Political science nerds, ed-tech spaces, mock elections,\ud83d\udd25 What\u2019s next: Full product launches staggered through May\u2013July 2025Partnering with campus orgs for testing and rolloutPotential Gen Z brand collabs \u2014 hit us up, Liquid Death \ud83d\udc40,--Powered by \ud83e\udde0idiots.ai",
                        "github": "https://github.com/PalashAwasthi05/ChugChamp/tree/main",
                        "url": "https://devpost.com/software/idiots-ai-itwsch"
                    },
                    {
                        "title": "Developing an automated sales order agent",
                        "description": "We want to save $160B of US profits that currently go down the srain, hiring sales/procurement agents that fill simple forms",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/developing-an-automated-sales-order-agent"
                    },
                    {
                        "title": "Blausm",
                        "description": "A centralized hub for neurodivergent community resources connecting individuals and families with local support, services, and tools tailored to their unique needs.",
                        "story": "Inspiration\nInspired by Divyanka's mother's experience teaching children with special needs, we recognized how crucial it is for neurodivergent individuals to find welcoming, sensory-friendly spaces. Her firsthand experience showed us that finding these accommodating environments shouldn't be another challenge for families to overcome.\nWhat it does\nBlausm is an AI-powered assistant that helps neurodivergent individuals find verified sensory-friendly places in their area. Users can ask natural questions like \"quiet cafes in Boston\" and get curated, verified suggestions with detailed accessibility information.\nHow we built it\nBuilt using Node.js, Supabase for database, and OpenAI's GPT-4, Blausm combines:\nNatural language query processing\nVerified resource database\nAI-powered suggestion generation\nStrict data verification protocols\nChallenges we ran into\nEnsuring all suggested places actually exist and are currently operating\nBuilding a system that understands various ways people express accessibility needs\nMaintaining accuracy while implementing fuzzy search for locations\nAccomplishments that we're proud of\nCreated a system that prioritizes verified, real-world information\nBuilt a warm, supportive interaction model that truly understands user needs\nDeveloped a self-improving database that grows with each query\nWhat we learned\nThe critical importance of data verification in accessibility tools\nHow to balance AI automation with human-like, empathetic responses\nTechniques for building inclusive, accessible user interactions\nWhat's next for Blausm\nCommunity-driven verification system\nMobile app development\nUser reviews and personalized recommendations\nIntegration with mapping services\nExpansion to more locations and resource types",
                        "github": "",
                        "url": "https://devpost.com/software/blausm"
                    },
                    {
                        "title": "Einstein + Quantum DKGs",
                        "description": "ai scientist, a self-assembling quantum brain, and automated experimentation",
                        "story": "",
                        "github": "https://github.com/A-Ravioli/einstein/tree/main",
                        "url": "https://devpost.com/software/einstein-quantum-dkgs"
                    },
                    {
                        "title": "Magnitude MCP",
                        "description": "Vibe-testing for vibe-coders, powered by Magnitude",
                        "story": "MCP server built for easily building and running test cases with Magnitude (https://github.com/magnitudedev/magnitude) within AI code editors like Cline and Cursor.MCP server source code:https://github.com/magnitudedev/magnitude/tree/mcp/packages/magnitude-mcp",
                        "github": "https://github.com/magnitudedev/magnitude",
                        "url": "https://devpost.com/software/magnitude-mcp"
                    },
                    {
                        "title": "LoreBoard",
                        "description": "AI-powered creative writing platform",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/loreboard"
                    }
                ]
            ]
        },
        {
            "title": "UNC Quantum Hacks",
            "location": "FB009 - Sitterson Hall",
            "url": "https://unc-quantum-hacks.devpost.com/",
            "submission_dates": "Apr 12 - 13, 2025",
            "themes": [
                "Beginner Friendly",
                "Machine Learning/AI",
                "Quantum"
            ],
            "organization": "UNC at Chapel Hill",
            "winners": false,
            "projects": [
                [
                    {
                        "title": "Quantum Sudoku",
                        "description": "Quantum based sudoku helper which allows the user to entangle guesses to assist in solving.",
                        "story": "Inspiration: Our group had little to no experience with quantum computing before this hackathon, and so we needed to start by gaining an understanding of the topic. We watched various YouTube videos explaining the core ideas, and immediately began brainstorming. Upon learning how quantum bits allowed for much faster runtime than classical computing, and there were already search algorithms, we initially decided on an idea of a Sudoku solver that generated all possible board states from each state, finding the solution quickly. However, we realized this would just be a quantum implementation of a Monte-Carlo tree search, and wanted to make this idea more unique and creative. After thinking, we decided on an altered version of Sudoku; one that provided hints for each move by adjusting the probabilities of every other cell, and would visualize these probabilities to help support the player in solving it.What it does: The game functions as an altered, guided form of Sudoku. By selecting a group of numbers for each cell, it then impacts the probability of the numbers of the connected cells. This then allows for players to more easily solve these puzzles, by experimenting with the various combinations of numbers for cells.How we built it: In terms of our tech stack, we use React and JavaScript to build this project, with HTML/CSS obviously. We designed each unpicked cell to be in a superposition, between any possible number still available for that cell. These cells were also entangled with each cell in the row, or column, or grid group, and by measuring that cell (picking a number), it immediately impacted every entangled cell. We also adjusted it so that you could select multiple numbers per cell, and observe how it impacted every other connected cell, providing a more guided way to solve \u201cimpossible\u201d Sudoku problems.Challenges we ran into: State Synchronization: Managing the interaction between the Sudoku board and the InfoPanel was complex. Ensuring that cell selections, number assignments, and resets updated consistently across components required careful planning and debugging.Dynamic Styling: Creating a dynamic, responsive grid that visually distinguished between fixed cells and those in superposition was challenging. We experimented with multiple CSS approaches (grid gaps vs. individual borders) to get the desired visual effect.Balancing Functionality and Aesthetics: Achieving a balance between a clean user interface and complex, dynamic functionality was a continuous iterative process. Ensuring that all interactive elements worked seamlessly while keeping the design intuitive was both challenging and rewarding.,",
                        "github": "https://github.com/ege96/quantum-hacks",
                        "url": "https://devpost.com/software/quantum-sudoku"
                    },
                    {
                        "title": "The Max-Cut problem (Advanced Track)",
                        "description": "The Max-Cut problem involves dividing a graph's nodes into two sets such that the sum of the weights of edges between the sets is maximized. We solve it by VQE using TKET from Quantinuum.",
                        "story": "Motivation: We chose this problem to solve because as an undergraduate researcher designing and testing PCBs, I came across this dilemma of deciding the locations of components. We also decided not to go to fields like finance, chemistry, etc, to stand out and also solve problems people don't know much about. We used Quantinuum's TKET as well to explore the product.\nProblem Overview: The Max-Cut Problem\nImagine you're in the middle of designing a Printed Circuit Board (PCB), and you've reached the crucial placement stage. Your job is to arrange the components in the most efficient way. This is where the Max-Cut problem comes into play.Picture the PCB as a flat surface with two sides\u2014let\u2019s call them Side 1 and Side 2. The goal is to place the circuit elements (the \"nodes\") on either side of the board, but here's the catch: each component needs to be connected to others by wires (the \"edges\")\nThese connections come with different wire lengths, and each wire consumes space. Some connections are stronger, meaning they need shorter, thicker wires to maintain the circuit's integrity, while others are weaker and can be placed further apart. The strength of each connection is represented by a weight attached to the edge. The goal is to place components on the two sides of the PCB in such a way that the most critical (heavier) connections are split between the two sides of the board, maximizing the efficiency of the layout.You have to figure out how to split these nodes between Side 1 and Side 2. The more you split critical connections (high weight edges), the better your PCB layout will be, as fewer long wires will run across the board, and space will be used more effectively.In this case, the Max-Cut problem is your guide: you want to find the best partition of the nodes (components) into two sets\u2014Side A and Side B\u2014so that the sum of the weights of the edges between the two sides is maximized. This results in a more efficient design with shorter wire lengths and a more compact PCB.So, as you work through the layout, you\u2019ll be solving the Max-Cut problem, trying to find the best balance between the different components, ensuring the connections with the highest weights are placed between the two sides of the board, leading to an optimized circuit design.",
                        "github": "",
                        "url": "https://devpost.com/software/the-max-cut-problem"
                    },
                    {
                        "title": "Q-Learn",
                        "description": "A beginner friendly web app geared towards testing the waters of quantum computing.",
                        "story": "Inspiration: Gaining familiarity with Qiskit and learning more about quantum computing / concepts.What it does: Provides detailed information about basic quantum concepts while allowing users to create their own quantum circuits and visualize them in various ways.How we built it: We used Streamlit for the frontend side of things, Python for the backend, as well as Matplotlib for visualizations.Challenges we ran into: The biggest challenges we ran into were primarily conceptual. In other words, it was challenging for us at times to visualize what we wanted to see with our project, primarily due to our inexperience with quantum concepts.Accomplishments that we're proud of: We learned a lot about a field we were previously had zero knowledge of.What we learned: We learned a lot about working with Qiskit, and gained a larger wealth of knowledge regarding quantum concepts which we were previously unfamiliar with.What's next for Q-Learn: Finding a more useful application for it, as its primary purpose currently is essentially just to give a rudimentary demonstration of quantum concepts.",
                        "github": "",
                        "url": "https://devpost.com/software/q-learn"
                    },
                    {
                        "title": "Entangle@UNC",
                        "description": "Entangle@UNC connects quantum minds using real quantum circuits to measure learning compatibility. A new way to spark collaboration\u2014one qubit at a time.",
                        "story": "Inspiration: We joined this hackathon with zero experience in quantum computing, just a lot of curiosity. After attending the introductory workshop, we were inspired by the idea that quantum systems can do more than compute\u2014they can connect people. During Dr. Mark Jackson\u2019s talk, he emphasized the power of collaboration and communication in science. That made us think: what if quantum computing could help students connect based on how they learn and collaborate? That\u2019s how Entangle@UNC was born.What it does: Entangle@UNC is a concept for a quantum-powered matchmaking tool. Users provide two values\u2014how experienced they are with quantum computing and how interested they are in collaboration. These values are turned into rotation angles on a quantum circuit. We prepare quantum states using RY and RZ gates and entangle them with a CX gate. Then we measure the result. The probability of getting the state |11\u27e9 becomes their compatibility score. The closer to 1, the better the match.How we built it: We started by encoding user profiles into quantum states using IBM Quantum Composer. We used RY and RZ gates to position each user\u2019s qubit on the Bloch sphere. Then, we used a CX gate to entangle the two users and measured the output to determine compatibility. We experimented with different values and circuits until we got consistent results. We visualized the data with Q-sphere and histograms to interpret our quantum fingerprints.Challenges we ran into: We had to understand new quantum concepts very quickly\u2014like qubit rotations, superposition, and entanglement. We struggled to build circuits that produced meaningful probabilities and learned to debug quantum gates and measurements. Making sense of results from IBM Composer and integrating them with our idea was also a big challenge.Accomplishments that we're proud of: We built and tested real quantum circuits even though most of us started with no prior experience. We were able to simulate user compatibility using quantum mechanics, and we created visuals to explain our idea in a way that others can understand. We\u2019re proud that we combined storytelling, education, and quantum coding in one project.What we learned: We learned how to encode classical user data into quantum states using quantum gates. We learned how to use IBM Quantum Composer, interpret Q-sphere results, and measure outcomes from a quantum circuit. More importantly, we learned the value of asking questions, working together, and embracing uncertainty in both coding and collaboration.What's next for Entangle@UNC: We would love to build a platform that allows UNC students to fill out a short quantum-inspired survey and be matched with potential collaborators. We could integrate this project into an educational website or workshop to teach others about qubits, Bloch spheres, and quantum entanglement using real data from student preferences.",
                        "github": "https://github.com/charium/QuantemHacks",
                        "url": "https://devpost.com/software/entangle-unc"
                    },
                    {
                        "title": "Quantum Risk",
                        "description": "The classic game of world domination\u2014Risk (with a a quantum twist).  ",
                        "story": "Inspiration: Risk was one of Sarayu's favorite games in high school, so we decided to recreate the game with a Quantum twist.What does it do?: It simulates the classical game of Risk. To the player, nothing is too different from the original game. However, it's the backend that adds the quantum twist in.How we built it: We build it using Qiskit (for the quantum functionality), Python, and Tkinter (for the frontend).Challenges we ran into: The biggest challenge was the time limit for the hackathon. We had to choose what to focus on now and what to develop later.Accomplishments that we're proud of: We're proud of the quantum dice, it honestly took some thinking to figure out how to add some of the quantum twists.What we learned: We learned a lot about the basics of Qiskit and are excited to expand our knowledge int he future.What's next for Quantum Risk: This is covered in further development which can be seen in the slides!",
                        "github": "https://github.com/sara-k03/quantum-risk",
                        "url": "https://devpost.com/software/quantum-risk"
                    },
                    {
                        "title": "QJack",
                        "description": "A playable and educational card game with quantum-inspired mechanics. Get as close to 21 points as possible without going over just like traditional blackjack, but some cards exist in superposition!",
                        "story": "Inspiration\nWe wanted to bridge the gap between the intimidating world of quantum physics and everyday gaming. Many perceive quantum mechanics as too complex or inaccessible, while traditional card games like blackjack have grown predictable over the years. We saw an opportunity to invigorate a classic game by introducing uncertainty and unpredictability through quantum-inspired mechanics, sparking curiosity and making learning about quantum concepts both fun and engaging.What it does\nQJack is an innovative twist on blackjack where the face cards\u2014King, Queen, and Jack\u2014begin in a state of superposition. Unlike in traditional blackjack where a King is always valued at 10, in QJack a King can be either 0 or 10, a Queen either -5 or 5, and a Jack either 1 or 11. Their values remain hidden until \"measured,\" either at the end of a round or when a player opts to peek. This peek option, available only once per game, not only reveals the card's true value but also triggers entanglement\u2014if a similar quantum face card appears later, its value is automatically determined.How we built it\nWe built QJack using Python within a dedicated virtual environment. Our project leverages Qiskit to simulate quantum mechanics (with tools like the AerSimulator and Hadamard gate) to create genuine quantum randomness. For the game\u2019s graphical interface and interactive components, we employed Pygame, and we used Tkinter for additional GUI elements like input dialogs. This combination allowed us to merge cutting-edge quantum simulation with traditional game development practices.Challenges we ran into\nOne of our biggest challenges was translating complex quantum concepts\u2014such as superposition, measurement, and entanglement\u2014into intuitive game mechanics that are both fun to play and easy to understand. Balancing the uncertainty introduced by quantum cards with predictable gameplay, and ensuring the peek and entanglement features felt fair and engaging, required several rounds of iteration and testing.Accomplishments that we're proud of\nWe are proud of successfully demystifying quantum mechanics through gameplay. QJack not only revitalizes blackjack with an element of surprise and strategic risk but also educates players about foundational quantum concepts. The integration of real quantum simulation using Qiskit within a familiar game format is a unique achievement that demonstrates both creative thinking and technical prowess.What we learned\nThroughout the project, we deepened our understanding of quantum mechanics and learned how to apply its abstract principles in a tangible way. We also honed our skills in Python programming, explored the capabilities of quantum simulators, and enhanced our abilities in game design and user interaction. Most importantly, we learned that innovative ideas often lie at the intersection of different disciplines.What's next for QJack\nLooking ahead, we plan to refine the gameplay experience by adding more interactive features and additional quantum-inspired elements. We are exploring the idea of online multiplayer functionality, introducing new game modes, and even integrating a real quantum computer interface as the technology matures. Our ultimate goal is to further bridge the gap between education and entertainment, making quantum physics both accessible and thrilling for everyone.",
                        "github": "https://github.com/ecliang220/qjack",
                        "url": "https://devpost.com/software/qjack"
                    },
                    {
                        "title": "Quantum Chess",
                        "description": "Combining the future of computing with classic games, quantum physics' surprising power is revolutionizing traditional strategy.",
                        "story": "",
                        "github": "https://github.com/Quantam-Hack/chess",
                        "url": "https://devpost.com/software/quantum-chess-xhgpab"
                    },
                    {
                        "title": "Quantum/Classical Machine learning for Cancer Classification",
                        "description": "We're using hybrid quantum-classical machine learning to classify breast cancer, exploring how quantum circuits can enhance medical diagnosis and push the boundaries of healthcare AI.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/quantum-classical-machine-learning-for-cancer-classification"
                    }
                ]
            ]
        },
        {
            "title": "7th Annual HackMHS",
            "location": "Martin High School Cafeteria",
            "url": "https://7th-annual-hackmhs.devpost.com/",
            "submission_dates": "Apr 12 - 13, 2025",
            "themes": [
                "Beginner Friendly",
                "Design",
                "Social Good"
            ],
            "organization": "Martin High School Computer Science Club",
            "winners": false,
            "projects": []
        },
        {
            "title": "ACM Hackathon 2025: Into The Gatorverse",
            "location": "Allegheny College",
            "url": "https://into-the-gatorverse.devpost.com/",
            "submission_dates": "Apr 07 - 13, 2025",
            "themes": [
                "Beginner Friendly",
                "Education",
                "Productivity"
            ],
            "organization": "Allegheny College",
            "winners": false,
            "projects": [
                [
                    {
                        "title": "Archives of Aetheria",
                        "description": "An adventure-based puzzle game where players must come together and use their shared knowledge to complete the game.",
                        "story": "Inspiration: Our inspiration was text-based story games, DnD (with the different classes used to form teams), and the game AbandonedWhat it does: It is a multiplayer game that requires all players to work together and use their shared knowledge to complete the game. Characters use real-world skills to solve fantastical problems.How we built it: Due to the short amount of time, we were unable to implement the game itself, however, we came up with the idea, storyline, and a design mock-up.Challenges we ran into: One large challenge is that neither of us has experience coding games/minimal experience, so we had to figure out how to showcase our idea without actually implementing it.Accomplishments that we're proud of: We're proud of the detail of the story that was created and the illustrations for the designs since there was such a limited time to do things, especially because there are only two of us in the team, and both of us were at least partially remote for the first day.What we learned: We learned that game design in itself is very complicated. Even though we didn't do any implementation, it took lots of time and energy to simply flesh out the design (both the narrative and illustration-wise).What's next for Archives of Aetheria: It would be really cool to actually implement the game.",
                        "github": "",
                        "url": "https://devpost.com/software/archives-of-aetheria"
                    },
                    {
                        "title": "Skill-Bridge",
                        "description": "Skill-Bridge helps students find collaborators across majors for comps, research, or passion projects \u2014 making it easy to turn ideas into action.",
                        "story": "Inspiration: At liberal arts colleges like ours, students have brilliant ideas \u2014 but no easy way to find collaborators outside their major. We wanted to break academic silos and help students connect through meaningful, skill-based collaboration.What it does: Skill-Bridge is a web platform that lets students:Post project ideas with required skillsDiscover projects based on interest and expertiseMatch with collaborators across majorsEarn XP and reviews for completed collaborations,How we built it: We built Skill-Bridge using:AI Agent: Cursor AI ( Claude 3.5 and 3.7 Sonnet )Backend: Python 3 with Flask frameworkDatabase: SQLite with SQLAlchemy ORMFrontend: HTML, CSS, JavaScriptUI Framework: Bootstrap 5Icons: Bootstrap IconsAuthentication: Flask-LoginForms: Flask-WTF and WTForms,Challenges we ran into: Designing a matchmaking system that\u2019s meaningful, not just randomManaging time while building both frontend and backend during the hackathonDeciding how to incentivize collaboration and build user trust,Accomplishments that we're proud of: Building a functional end-to-end matching platform in a short timeDesigning a clean UI/UX experienceIntegrating XP and review features to encourage trust and repeat use,What we learned: The power of teamwork under pressureHow to prioritize core features for an MVPThat \"vibecoding\" can actually lead to something real and impactful,What's next for Skill-Bridge: Add a smart recommendation engine (e.g. GPT-powered tag suggestion)Integrate with school login systems (eAdd a smart recommendation engine (e.g. GPT-powered tag suggestion)Integrate with school login systems (e.g. SSO)Launch a beta at our college and gather feedbackExpand to other liberal arts colleges.g. SSO)Launch a beta at our college and gather feedbackExpand to other liberal arts colleges,",
                        "github": "https://github.com/acm-allegheny/gatorverse-project-submissions-fusionforce",
                        "url": "https://devpost.com/software/skill-bridge-4bvlar"
                    },
                    {
                        "title": "AutoQuizzer: Learn from Videos Fast",
                        "description": "Upload any video transcript, get instant quizzes and summaries.",
                        "story": "",
                        "github": "https://github.com/acm-allegheny/gatorverse-project-submissions-team-5",
                        "url": "https://devpost.com/software/autoquizzer-learn-from-videos-fast"
                    },
                    {
                        "title": "F.L.O.W.",
                        "description": "F.L.O.W. or Flexible Logic for Organizing Workflows is a tool designed to help teams and individuals collaborate and streamline their tasks, visualize deadlines, and stay on top of project milestones.",
                        "story": "",
                        "github": "https://github.com/acm-allegheny/gatorverse-project-submissions-team-8",
                        "url": "https://devpost.com/software/f-l-o-w"
                    },
                    {
                        "title": "On the Plane",
                        "description": "Game for the plane to a new place!!",
                        "story": "",
                        "github": "https://github.com/acm-allegheny/gatorverse-project-submissions-team-4/tree/main?tab=readme-ov-file",
                        "url": "https://devpost.com/software/on-the-plane"
                    },
                    {
                        "title": "Chompers",
                        "description": "Chompers is an AI chatbot that simplifies jargon, clarifies concepts with analogies, and connects team members\u2014boosting collaboration across diverse fields and making teamwork easy and fun.",
                        "story": "",
                        "github": "https://github.com/acm-allegheny/gatorverse-project-submissions-team-6?tab=readme-ov-file#challenge-prompt",
                        "url": "https://devpost.com/software/chompers"
                    }
                ]
            ]
        },
        {
            "title": "UTA Datathon 2025",
            "location": "UTA - SWSH",
            "url": "https://uta-datathon-25.devpost.com/",
            "submission_dates": "Apr 12 - 13, 2025",
            "themes": [
                "AR/VR",
                "Databases",
                "Machine Learning/AI"
            ],
            "organization": "UTA Libraries",
            "winners": false,
            "projects": [
                [
                    {
                        "title": "\u200e \u200e ",
                        "description": " \u200e ",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/flint-steel-bdxk5g"
                    },
                    {
                        "title": "Shamrock",
                        "description": "A data-driven project that forecasts IPC crime trends in India using Linear Regression and ARIMA models. Includes an interactive dashboard and visual insights to support informed decision-making.",
                        "story": "",
                        "github": "https://github.com/silverfrost702/Datathon2025",
                        "url": "https://devpost.com/software/shamrock"
                    },
                    {
                        "title": "EDA Patrol_Nerdy_Potatoes",
                        "description": "Our project leverages district-wise crime data from across India spanning 2001 to 2014 to uncover deep insights into national and regional crime patterns.",
                        "story": "",
                        "github": "https://github.com/cosmicbeing619/Datathon_2025",
                        "url": "https://devpost.com/software/snj"
                    },
                    {
                        "title": "Safe Haven",
                        "description": "Safe Haven",
                        "story": "",
                        "github": "https://github.com/DATATHON-25/homeless-analysis",
                        "url": "https://devpost.com/software/safe-haven-pvj7cn"
                    },
                    {
                        "title": "Crimelens India: Analyzing District-Level Crime Trends",
                        "description": "Crimelens India analyzes district-level crime data to reveal patterns, hotspots, and trends, using ML, dashboards, and maps to aid crime prevention and policy decisions across India.",
                        "story": "",
                        "github": "https://github.com/Sakshi270901/EDA_Patrol_S3.git",
                        "url": "https://devpost.com/software/crimelens-india-analyzing-district-level-crime-trends"
                    },
                    {
                        "title": "EDA Patrol_Gawkers",
                        "description": "Uncovering Crime Patterns in India: A Data-Driven Exploration from 2001 to 2014",
                        "story": "",
                        "github": "https://github.com/yashwin1999/EDA-Patrol---Datathon",
                        "url": "https://devpost.com/software/gawkers"
                    },
                    {
                        "title": "Model Mash: LLM Arena_WhiteCoatAI",
                        "description": "Bridging the Gap Between Doctors and Patients.",
                        "story": "\ud83e\udde0 Inspiration: Understanding medical prescriptions, lab reports, and doctor\u2019s notes can be overwhelming for patients\u2014especially when these documents are filled with technical jargon and unclear data. One of our team members has a family member with a chronic condition and saw firsthand how hard it was to interpret lab results or medication changes. That\u2019s when we thought\u2014why not use LLMs to make this simpler?We wanted to create something that wouldbridge the gap between doctors and patients, empowering everyday users to understand their health information clearly, without needing a medical degree.\ud83d\udca1 What it does: WhiteCoatAItakes any medical document\u2014whether it\u2019s a prescription, lab report, or test summary\u2014and transforms it into asimplified, visual, and interactive health overview.Key Features:\ud83d\udcc4Document Upload: PDF, DOCX, or image-based reports\ud83e\udde0LLM-powered Summary: Understand complex prescriptions in plain English\ud83d\udcdaGlossary Generator: Quick definitions for tricky medical terms\ud83d\udccaTest Result Visualizer: Charts comparing your values to normal reference ranges\ud83d\udcacChatbot Interface(optional): Ask questions like \u201cIs my cholesterol high?\u201d,\ud83d\udee0\ufe0f How we built it: We built the front-end using Streamlit to deliver a clean and user-friendly interface, and powered the backend entirely with the Gemini API to process and interpret medical documents with ease and accuracy.\ud83d\udee0\ufe0f Tech Stack Overview: \ud83d\udd0dDocument Parsing: Handled PDF, DOCX, and image files throughStreamlit'sfile uploader for smooth and intuitive input handling.\ud83e\udd16LLM Processing: Used theGemini APIto extract, analyze, and simplify medical content from uploaded documents.\ud83d\udccaInsight Display: Visualized extracted values and summaries directly within theStreamlitinterface for easy user interpretation.\ud83d\udcacChatbot Interaction: Enabled interactive question-answering usingGemini-poweredfollow-ups with contextual awareness.,We worked in a collaborative repo, separating concerns between front-end, parsing, and LLM logic.\ud83d\udea7 Challenges we ran into: \ud83e\uddfe Parsing varied file formats and extracting clean text from images was trickier than expected\u2014OCR output needed heavy cleanup.\ud83e\udde0 Medical terms often require context, and some LLMs hallucinated definitions, so we had to refine our prompt engineering.\u2699\ufe0f Managing multiple pipelines (upload \u2192 parse \u2192 analyze \u2192 visualize) while keeping the UI fast and responsive was a balancing act.\ud83d\udd04 Some teammates were new to LangChain, so chaining prompts with memory and retrieval took a bit of learning.,\ud83c\udfc6 Accomplishments that we're proud of: Built an end-to-end working prototype withmulti-format file support,LLM integration, andchart visualizations\u2014within the hackathon timeframe!Designed a UI that\u2019s actually usable for non-tech-savvy usersCreated a tool with real-world impact potential\u2014especially for underserved or older patients,\ud83d\udcda What we learned: \ud83e\udde0 How to fine-tune and engineer prompts to work effectively with LLMs\u2699\ufe0f How to combine different AI tools into a cohesive pipeline (OCR, parsing, NLP, visualization)\ud83e\udd1d The importance of cross-functional collaboration\u2014splitting frontend/backend tasks really boosted our speed\ud83e\ude7a Gained deeper understanding of health tech data and real-world constraints,\ud83d\udd2e What's next for WhiteCoatAI: We\u2019re excited to take WhiteCoatAI beyond the demo. Our roadmap includes:\u2705 Support for scanned handwritten prescriptions (better OCR + filtering)\ud83c\udf10 Multilingual support for non-English speaking patients\ud83e\udde0 AI-generated follow-up questions (e.g., \u201cShould I ask my doctor about X?\u201d)\ud83d\udcf1 A mobile version for patients to upload their reports from anywhere\ud83d\udd12 HIPAA-compliant storage for real-world use cases,Health information should beaccessible, not anxiety-inducing\u2014and we believe WhiteCoatAI is a step in that direction.",
                        "github": "https://github.com/axp8948/whiteCoatAI",
                        "url": "https://devpost.com/software/whitecoatai"
                    },
                    {
                        "title": "EDA Patrol",
                        "description": "Leveraging machine learning and statistical modeling to uncover patterns in two decades of crime data across Indian districts",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/eda-patrol"
                    },
                    {
                        "title": " LLM Arena_Thundercats",
                        "description": "An AI powered App to help students facing the threat of Visa revocation and legal troubles. ",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/nocap-contract"
                    },
                    {
                        "title": "Safe Haven_HealthIntelligences",
                        "description": "Your health, smarter. HealthIntelligences uses AI for personalized insights & community health intelligence. Proactive care starts here.",
                        "story": "Inspiration: The inspiration behindSafe Haven_HealthIntelligencesstems from the need to make complex health data more accessible and actionable for both individuals and communities. We saw an opportunity to leverage cutting-edge AI and data visualization techniques to empower users with personalized health insights, identify potential risks proactively, and provide clear, data-driven recommendations for improving health outcomes and optimizing resource allocation within communities. The goal is to transform raw health data into meaningfulintelligence.What it does: Safe Haven_HealthIntelligencesis a comprehensive health data analysis and visualization platform. For individuals, it offers AI-powered health risk assessments usingGoogle Gemini, providing personalized recommendations, identifying risk factors, and offering predictive insights into potential health issues. For communities, it features an interactive dashboard with geographic health risk mapping, trend analysis, and forecasting capabilities to help understand and address public health challenges and optimize resource deployment. It also includes tools for analyzing uploaded health data (like Excel files) and provides access to essential healthcare resources, including emergency contacts and provider finders.How we built it: We builtSafe Haven_HealthIntelligencesusing a modern tech stack designed for robust performance and a rich user experience:Frontend:ReactwithTypeScriptfor a type-safe, component-based UI, accelerated by theVitebuild tool. User interface elements were crafted usingshadcn/uiand styled withTailwind CSS. Data visualization is powered byRecharts, and geographic mapping utilizesLeaflet.Backend:AnExpress.jsserver handles API requests and business logic, interacting with aMongoDBdatabase for persistent data storage.AI Integration:The core personalized health analysis and intelligence generation are powered by theGoogle GeminiAI model, accessed via its API.Authentication:Secure user management and authentication are handled byClerk.Data:The platform is designed to work with real-world health data APIs (likedata.healthcare.gov) or mock data for development and testing purposes.,Challenges we ran into: DevelopingSafe Haven_HealthIntelligencesinvolved several challenges:Integrating AI:Seamlessly integratingGoogle Geminifor meaningful and accurate healthintelligencerequired careful prompt engineering and handling of API responses.Data Handling:Managing potentially large and sensitive health datasets, ensuring privacy and security, and creating flexible systems to handle both real-time API data and uploaded files presented complexities.Visualization Complexity:Displaying diverse health metrics, trends, and geographic data in an intuitive and interactive way required careful design and implementation of charting and mapping components.Configuration Management:Setting up and managing environment variables (API keys, database URIs, development flags) across frontend and backend required a structured approach.,Accomplishments that we're proud of: We are proud of successfully building a multi-faceted platform that:Integrates a powerful AI (Google Gemini) to provide genuinely personalized health insights and intelligence.Offers both individual risk assessment and community-level health visualization tools.Implements a clean, modern, and interactive user interface usingReact,shadcn/ui, andTailwind CSS.Successfully utilizes libraries likeRechartsandLeafletfor effective data visualization and mapping.Provides a robust backend structure usingExpress.jsandMongoDB.Incorporates secure user authentication withClerk.,What we learned: Throughout this project, we gained valuable experience in:Full-stack development using theReact/Node.jsecosystem.Integrating and utilizing large language models (LLMs) likeGoogle Geminivia APIs for specific analytical tasks and intelligence generation.Working with health data, including fetching from external APIs and processing file uploads.Advanced data visualization techniques using libraries likeRechartsandLeaflet.Implementing secure authentication flows.Managing complex project configurations and dependencies using tools likeViteandnpm.Building applications withTypeScriptfor improved code quality and maintainability.,What's next for Safe Haven_HealthIntelligences: Future plans forSafe Haven_HealthIntelligencesinclude:Expanding the range of data sources and health metrics analyzed to generate deeperintelligence.Refining the AI models for even more accurate, nuanced recommendations and predictive insights.Adding features for tracking personal health data over time to provide longitudinal insights.Enhancing community dashboard features with comparative analytics and more detailed resource optimization suggestions based on generated intelligence.Exploring integration with wearable devices or other personal health trackers for richer individual data.Developing more advanced analytical modules to further enhance the platform'sintelligencecapabilities.,",
                        "github": "https://github.com/TalhaNadeem25/health-insights-navigator",
                        "url": "https://devpost.com/software/safe-haven_crisiscopilot"
                    },
                    {
                        "title": "Data Doomsday",
                        "description": "A data-driven platform to analyze global disaster risks, predict future impacts, and guide smarter aid and policy decisions.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/data-doomsday"
                    },
                    {
                        "title": "EDA_Patrol",
                        "description": "Time challenge 5",
                        "story": "",
                        "github": "https://github.com/SSunilDV/EDA_PARTROL",
                        "url": "https://devpost.com/software/eda_patrol"
                    },
                    {
                        "title": "EDA Patrol_Random",
                        "description": "Data visualization",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/eda-patrol-3g5dcb"
                    },
                    {
                        "title": "EDA_Patrol_DataNauts",
                        "description": "Analyzing India's crime landscape with data-driven insights to uncover patterns, predict trends, and empower safer communities",
                        "story": "",
                        "github": "https://github.com/prathyusha-konduti/DataNauts-EDA-patrol",
                        "url": "https://devpost.com/software/eda_patrol-4kp7gr"
                    },
                    {
                        "title": "GeoQuest: The Hazard Chronicles_SSSH",
                        "description": "Tracking America's Wildfires Through Real-Time Data and Interactive Mapping.",
                        "story": "Inspiration\nWith the growing impact of climate change, wildfires have become a recurring threat\u2014devastating ecosystems, displacing communities, and straining emergency response systems. We wanted to explore how geospatial storytelling and real-time data could be used to visualize these hazards in an engaging, accessible, and informative way. GeoQuest: The Hazard Chronicles_SSSH was born from the idea of turning raw disaster data into a powerful public awareness and planning tool.What it does\nThis interactive Story Map brings real-time wildfire incidents to life using data-driven maps, custom visuals, and narrative storytelling.\nUsers can:Explore active wildfire perimeters across the U.S.Click through fire events to see details like acres burned and incident complexity\nUnderstand wildfire trends through charts and satellite images\nLearn how to stay prepared and reduce risk through a data-informed call to actionHow we built it\nDataset: WFIGS Interagency Wildfire Perimeters (Current), processed in Python\nTools: ArcGIS Online, ArcGIS StoryMaps, Python (for data cleaning), ExcelMethod:Cleaned and filtered raw wildfire data\nAdded mock coordinates for visualization using a free ArcGIS account\nDesigned a Map Tour to display fire locations with custom pop-ups\nIntegrated supporting media, charts, and a live data reference for real-time contextChallenges we ran into\nFree ArcGIS Online accounts do not support hosted feature layers, so we had to creatively simulate location data using mock coordinates\nBalancing storytelling with technical accuracy was tricky\u2014especially when summarizing data without oversimplifying complex events\nEnsuring accessibility and interactivity while keeping the interface clean and intuitive\nAccomplishments that we're proud of\nCreated an interactive Story Map using only public tools\nSuccessfully simulated real-time wildfire mapping without live API access\nDeveloped clear, accessible content that communicates real risks to a general audience\nIntegrated multiple forms of media to enhance learning and user engagementWhat we learned\nHow to transform geospatial data into a compelling narrative\nThe power of cartographic storytelling in raising awareness of environmental hazards\nTechniques for designing interactive experiences in ArcGIS with limited technical resources\nThe importance of accessibility, design simplicity, and user empathy in public-facing toolsWhat\u2019s next for GeoQuest: The Hazard Chronicles_SSSH\nWe plan to expand this project to include other environmental hazards\u2014such as floods, hurricanes, and air pollution\u2014each with its own story module.Future goals include:\nIntegrating live data APIs for real-time simulations\nAdding community-sourced hazard reports\nEmbedding predictive models for fire spread and risk assessment\nLaunching a public-facing dashboard to support emergency preparedness education",
                        "github": "https://github.com/HarshithaMulemane/GeoQuest-The-Hazard-Chronicles",
                        "url": "https://devpost.com/software/geoquest-the-hazard-chronicles_sssh"
                    },
                    {
                        "title": "EDA PATROL_InsightInnovators",
                        "description": "Predicting and understanding crime trends to empower data-driven decision-making for safer, more secure communities",
                        "story": "",
                        "github": "https://github.com/viral4854/Datathon25",
                        "url": "https://devpost.com/software/eda-patrol-kpzm8v"
                    },
                    {
                        "title": "Data_Doomdays_Angelina",
                        "description": "Forecasting Risk, Guiding Resilience: Data-Driven Disaster Intelligence for a Safer Tomorrow.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/data_doomdays_angelina"
                    },
                    {
                        "title": "Data Doomsday_Random",
                        "description": "Data visualization",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/data-doomsday_random"
                    },
                    {
                        "title": "Deepfake Duel: Truth vs. Trickery_Deepfakers",
                        "description": "Using Xception CNN, a model which detects deepfakes from the real ones.",
                        "story": "",
                        "github": "https://github.com/your-username/deepfake-detector.git",
                        "url": "https://devpost.com/software/deepfake-duel-truth-vs-trickery_deepfakers"
                    },
                    {
                        "title": "Model Mash: LLM Arena_PTP",
                        "description": "Scraping open source web data and incorporating an LLM for a AI web chat experience",
                        "story": "Inspiration: It's difficult to traverse through and search for information on websites, inspired to build a chatbot which helps with question answers.What it does: User enters their queries to the chatbot, the bot queries the model's memory and displays the answers to the user.How we built it: Data Collection and Preprocessing We began by collecting relevant information from the University of Texas at Arlington (UTA) website. This involved: \u2022 Web scraping using BeautifulSoup to extract structured text data from various web pages. \u2022 Performing text cleaning and formatting to prepare the data for embedding using HuggingFace and LangChain libraries. \u2022 Removing unnecessary HTML tags, scripts, and duplicate content to ensure relevance and accuracy.Document Ingestion and Embedding Once the raw text was cleaned: \u2022 We used LangChain and Hugging Face Transformers to load the textual data and split it into manageable chunks. \u2022 These chunks were transformed into high-dimensional embeddings using a pre-trained sentence transformer model. \u2022 The resulting vectors were stored in a FAISS vector store, enabling fast and accurate similarity searches.\nLLM Integration and Prompt Engineering We used Gemini, a state-of-the-art large language model (LLM), to generate responses based on retrieved knowledge. Our pipeline follows a Retrieval-Augmented Generation (RAG) approach: \u2022 User Query \u2192 Embedding \u2192 Similarity Search in FAISS \u2022 Retrieved document chunks were combined into a structured prompt, which was passed to the Gemini model. \u2022 The prompt format ensured context retention, enabling Gemini to generate domain-specific, coherent responses.Frontend Development The chatbot\u2019s interface was built using React: \u2022 A simple and intuitive chat UI was created using state management hooks. \u2022 User messages were captured, displayed, and passed to the backend. \u2022 The LLM's response was then displayed in the UI with real-time rendering and scroll handling using useRef.Backend Integration The backend handled: \u2022 Receiving user input and converting it to embeddings. \u2022 Querying FAISS for the top-k most relevant document chunks. \u2022 Constructing a prompt with retrieved context. \u2022 Sending the prompt to Gemini and returning the model\u2019s response to the frontend.",
                        "github": "https://github.com/ightdragon/Model-Mash-LLM-Arena_PTP",
                        "url": "https://devpost.com/software/model-mash-llm-arena_askuta"
                    }
                ]
            ]
        },
        {
            "title": "Bitcamp 2025",
            "location": "Reckord Armory",
            "url": "https://bitcamp2025.devpost.com/",
            "submission_dates": "Apr 11 - 13, 2025",
            "themes": [
                "Beginner Friendly",
                "Machine Learning/AI",
                "Social Good"
            ],
            "organization": "Bitcamp",
            "winners": true,
            "projects": [
                [
                    {
                        "title": "Riffs",
                        "description": "Riffs is a creative music app where you can hum a melody, convert it into guitar tabs, edit it, and jam out in a Guitar Hero-style game-play experience.",
                        "story": "\ud83e\udde0 Inspiration: Learning guitar often feels intimidating. Traditional tools rely on robotic MIDI playback, confusing tabs, or assume you have a studio setup. We wanted to create something that feels natural, creative, and instant.Riffswas born from a simple idea:\ud83d\udca1 What it does: Riffs is a browser-based tool that lets users:It\u2019s a seamless pipeline from voice to visual melody creation \u2014 all in-browser, no plugins required.\ud83d\udee0\ufe0f How we built it: pitchy\u2013 for pitch detectionTypeScript\u2013 to ensure strong type safetyReact\u2013 for dynamic UITailwindCSS\u2013 for rapid and clean stylingFL Studio\u2013 to generate authentic piano and guitar samples,\ud83d\udea7 Challenges we ran into: Pitch detection in a noisy hackathon environmentAccurately converting notes realistic guitar positionsDesigning a piano roll editor that feels intuitive and musicalManaging app state across pages without a backend,\ud83c\udfc6 Accomplishments we're proud of: Built a custom visual note editor from scratch in under 36 hoursSeamlessly converted voice input into playable, editable dataUsed real guitar samples for future playback instead of default MIDIDelivered a clean UX with clear and musical visual feedback,\ud83d\udcda What we learned: Real-time browser audio processing is very possibleLess is more when it comes to UI/UX in creative toolsA good editing experience doesn\u2019t need audio playback to feel musicalPitch-to-note conversion is a powerful creative bridge,\ud83d\ude80 What's next for Riffs: Integrating our guitar samples for live playbackAdding tempo, measure lines, and rhythmic structureSupporting multiple tracks or layersSaving riffs to the cloud and letting users remix each other's melodiesExporting riffs to MIDI, tablature, or Guitar Pro formatsExploring AI tools to suggest chords or harmonize based on your input,\ud83c\udf1f TL;DR: Riffsis a musical sandbox for guitarists and creators.Hum your melody. See it. Shape it. Share it.",
                        "github": "https://github.com/cataladev/riffs",
                        "url": "https://devpost.com/software/riffs"
                    },
                    {
                        "title": "Hiding in Plain Sight",
                        "description": "Digital forensics for hidden messages in images",
                        "story": "What it does: Hiding in Plain Sight is a lightweight, high-speed tool designed to uncover hidden data embedded through LSB and DCT steganography techniques. It supports batch processing and also performs advanced statistical analyses, including chi-square and histogram evaluations, to detect anomalies and potential signs of data concealment.How we built it: We built the tool using Python, leveraging libraries like Pillow, NumPy, SciPy to extract and analyze data from the images.Accomplishments that we're proud of: We\u2019re proud of building a tool that works on a wide range of image files and can handle bulk processing efficiently. All of the easter eggs we put in the UI.What we learned: We gained a deeper understanding of steganography and the world of digital forensics.",
                        "github": "https://github.com/braynguyen/Hiding-in-Plain-Sight",
                        "url": "https://devpost.com/software/hiding-in-plain-sight"
                    },
                    {
                        "title": "Lighthouse",
                        "description": "Hijacking Diffusion Models",
                        "story": "Inspiration \ud83c\udf05: We\u2019ve all been there: you snap a bunch of photos to capture a moment, only to look back and feel like something\u2019s missing. Maybe the lighting was off, the image is way too grainy, or there\u2019s a distracting detail you wish you could erase. The photo just doesn\u2019t feel like the moment you lived. That frustration is what inspired Lighthouse \u2014 a tool designed to reconstruct and creatively reimagine noisy or imperfect images using the power of diffusion.What it does \u2699\ufe0f: Lighthouse is powered by diffusion models \u2014 deep learning systems that can generate detailed, high-quality images from nothing but noise. Normally, these models start from pure randomness and gradually subtract noise in small steps, revealing an image as they go. I thought: what if we hijack this process? What if we insert a real image midway through the diffusion process and let the model rebuild it with enhanced detail or creative variation?Lighthouse focuses on two main tasks:Check out the demo to see both of these modes in action!Lighthouse uses the same method large tech companies like OpenAI (DALLE) and Stability AI use to generate images. The difference? Lighthouse allows you to start the diffusion process with an image, and modify it in a creative manner; no such functionality from big tech companies. Also, they have a giant supercomputer of hundreds of NVIDIA H100s ($30k each), I have 1 NVIDIA 2070 ($400).How I built it \ud83d\udee0: Lighthouse is built using Google\u2019s Denoising Diffusion Probabilistic Models (DDPM). This architecture lets me skip the initial timesteps and start the denoising process from a partially noisy, real image. To make renoising work, I also implemented a custom noise scheduler that simulates the same noise induction process used during training.Challenges I ran into \ud83d\udea7: Diffusion models are powerful \u2014 and incredibly compute-heavy. Even with a decent GPU (RTX 2070), running high-res models pushed the limits. After experimenting with several architectures, I landed on a model that struck the right balance between performance and output quality. Training a model from scratch would\u2019ve taken more than a hundred years on my setup (literally) \u2014 so for now, pre-trained models are the move.Accomplishments that I'm proud of \u2705: The best part? Lighthouse works. It can bring detail back into unrecognizable images and enable amazing, creative edits in just a few steps. Lighthouse was a shot for the moon, adapting existing models and getting diffusion models to work with image manipulation could've not worked in the slightest, so I'm proud that it worked pretty well.What we learned \ud83d\udcda: This project was my first time working hands-on with diffusion models, and it was a deep dive. I already had a theoretical understanding, but implementing denoising, scheduling, and custom workflows really pushed my understanding to the next level \u2014 especially when experimenting with novel use cases like renoising.What's next for Lighthouse \ud83d\udd2e: I\u2019m excited to take Lighthouse even further with:Stable Diffusion Integration: Operating in the latent space using VAEs, which could unlock faster and higher-resolution generation.LoRA Fine-Tuning: Adding lightweight adapter training to guide generation using semantic concepts, all without retraining the entire model.,",
                        "github": "https://github.com/dswl/lighthouse/tree/main",
                        "url": "https://devpost.com/software/lighthouse-p1cb0k"
                    },
                    {
                        "title": "Cyber Sentinel",
                        "description": "Raspberry Pi tool that detects suspicious websites using system profiling and static code analysis with additional features such as file download safety check and continuous system monitoring during.",
                        "story": "Inspiration: We wanted to build a simple and affordable way to help people stay safe online, especially those who might not be very tech-savvy, like parents, grandparents, or small office teams. A lot of security tools today are either too complicated, cost a monthly fee, or rely on cloud services that collect your data without you really knowing how they work.With Cyber Sentinel, our goal was to create something that\u2019s easy to set up, doesn\u2019t require ongoing payments, and gives users complete control over their own data. Whether you\u2019re helping a family member avoid risky websites or setting up protection on shared computers, this tool runs quietly in the background and only steps in when something seems suspicious.What it does: Cyber Sentinel is a Raspberry Pi-powered web threat detection system that works alongside a custom browser extension to:Collect system statistics during website loads (CPU, RAM, network usage)\nCompute z-scores using a baseline dataset to identify anomalous behavior\nPerform static code analysis on JavaScript using Semgrep\nBlock network requests to suspicious sites until the user explicitly allows them\nDisplay a live risk dashboard in the browser popup\nAll of this happens locally, ensuring both privacy and transparency.How we built it: We used Tailwind CSS with React components to provide a clean interface. In order to accomplish our malware detection, we used industry-standard YARA rules, which are commonly used amongst cybersecurity companies to detect patterns of malware within website code and downloadable files. In addition, we used the Raspberry Pi to continuously monitor tabular statistics and run a Python script behind the scenes to determine CPU, RAM, and network stability.Challenges we ran into: The Raspberry Pi had some security constraints that we needed to circumvent in order to run bash scripts over the SSH-ethernet connection. To circumvent this, we used MongoDB to act as the middleman in processing tasks in a queue-like format.Accomplishments that we're proud of: Developing a finished product that we have an everyday use for in our lives.What we learned: It's better to start off with a bigger scope and then narrow down to a Minimum Viable Product. That way, we can ensure that the core functionality of the product stays central. That being said, throughout this journey, we all learned more about cybersecurity and the difficulty that comes with safeguarding networks, websites, and local devices.What's next for Cyber Sentinel: In the future, we plan to streamline the system by reducing our reliance on a cloud-hosted middleman to connect the Raspberry Pi and the browser extension. While some cloud interaction will still be necessary for generating the final risk assessment using Gemini, the core data collection and analysis will remain local. We're also working on adding the ability to track which websites have already been analyzed, so we can avoid repeating checks and improve performance. Additionally, we want to give users more control, including options to disable blocking temporarily or trigger a deeper, more detailed analysis on certain sites. These next steps will make Cyber Sentinel even more flexible, efficient, and user-friendly, while keeping transparency and privacy at its core.",
                        "github": "https://github.com/Suvrath421/bitcamp2025",
                        "url": "https://devpost.com/software/cyber-sentinel"
                    },
                    {
                        "title": "BudgetBuddy",
                        "description": "A virtual pet whose physical wellness depends on your financial wellness!",
                        "story": "Inspiration: As college students who like to have fun, we wanted to make budgeting less boring and more emotional for all. Financial apps are often too cold or overwhelming, we wanted to create something that felt alive, fun, and deeply personal. Inspired by Tamagotchis and the psychology of money habits, we built BudgetBuddy: a virtual pet that reacts to your real-world spending!What it does: BudgetBuddy helps users understand and manage their finances in a gamified, emotional way:\n\ud83d\udcb3 Connects to your transaction history (via Capital One's Nessie API)\n\ud83e\udde0 Analyzes your recent spending to detect trends, big purchases, and to help keep you on track\n\ud83d\udc23 Your virtual buddy reacts to your spending behavior, getting happier or sadder depending on your spending habits\n\ud83d\udde3\ufe0f Chat with your buddy, who gives feedback in a sarcastic, supportive, or soft tone based on your preferences\n\ud83d\udcc8 Tracks your monthly budget and provides LLM-powered insightsHow we built it: Frontend: React with TypeScript and Vite, Zustand for state management, TanStack Router for routing, styled-components for styling, and HTML/CSS/JS for the core UI structure and interactions.\nBackend: Python (Flask), MongoDB Atlas for user data, and the Nessie API for real financial transaction simulation.\nTransaction Generator: Custom Python logic to generate realistic spend data based on merchant categories and emotional context.\nAI Integration: Google Gemini API for generating feedback and conversation.\nFraud Detection: Used IsolationForest from scikit-learn to flag anomalous purchases.Challenges we ran into: Mapping transaction data to meaningful categories for emotional interpretation.Ensuring smooth API interaction with Nessie while syncing accounts and transactions.Balancing LLM personality to be fun and helpful, without losing clarity.Debugging transaction history fetches and aligning real-time insights with our buddy\u2019s emotions.,Accomplishments that we're proud of: Creating a character-based feedback loop that feels emotionally engaging.Implementing a working fraud detection model in a limited timeframe.Combining LLMs + classic ML with real-time financial behavior to create a unique scoring system.Designing a user onboarding flow that feels more like a game than a finance form.Integrated the Nessie API to simulate real-world financial transactionsImplemented MongoDB Atlas to securely store user data,What we learned: How to integrate multiple APIs, including Capital One's Nessie banking simulation and a generative LLM model.How to apply machine learning in a real-world use case like fraud detection.The power of emotional design, turning something cold like finance into a warm, interactive experience.That financial literacy doesn't have to be intimidating if you make it personal and playful.How to deal with running into roadblocks (many of them).,What's next for BudgetBuddy: \ud83d\udd04 Integrate with real financial institutions (Plaid, Stripe, etc.)\n\ud83d\udcf2 Launch on mobile app stores in addition to our Progressive Web App (PWA) alongside a widget for your homescreen for more buddy interaction\n\ud83c\udfaf Add personalized goals, weekly spending challenges, and social sharing features",
                        "github": "https://github.com/team-moondust/BudgetBuddy-backend",
                        "url": "https://devpost.com/software/budgetbuddy-3qvr78"
                    },
                    {
                        "title": "\u03bcTeX (MuTeX)",
                        "description": "\u03bcTeX turns math equations on blackboards into LaTeX for easy notetaking. LaTeX is an accessible tool for blind individuals to read math. Combined with TTS, \u03bcTeX is an inclusive solution to learn math.",
                        "story": "Pronounced Mew-tech. (MuTeX)Inspiration: \u03bcTeX is my two loves of my life combined: chalkboard and LaTeX. However, that does not mean I am the fastest LaTeX writer. I always aspired to be that student who writes math notes in LaTeX, but my slow typing (66 wpm: embarrassing for a CS major) limits my dreams. I needed some external help for accurate LaTeX math notes.\nThinking about my love for LaTeX, I remembered an essay I wrote for an English class about LaTeX. I wrote about how my professor refuses to give LaTeX source code for our homework, and why he should start doing so. One thing I learned at that time was LaTeX's accessibility.Then I questioned, how does a blackboard-based math class scene translate to blind individuals? With this question and my struggles writing LaTeX notes, I decided to create \u03bcTeX.What it does: \u03bcTeX is a web-based application that transcribes handwritten math equations from blackboards into LaTeX syntax. \u03bcTeX uses the Gemini library to recognize math equations in real-time. Live camera captures the user's view, which is then fed into Gemini with a prompt. The output is a clean LaTeX script for easy integration into notes, papers, or digital documents. A key feature is accessibility: LaTeX is accessible, and it provides real-time assistance in equation interpretation for blind and/or visually impaired individuals. TTS is available, which lets blind and visually impaired individuals engage with math content independently. The platform supports both casual users and academic institutions aiming for more inclusive education. Overall, it bridges the gap between traditional math instruction and modern, accessible digital workflows.How we built it: I used plain HTML, CSS, and JavaScript for this project. I like the simple, old-style websites that are easy to navigate. The Gemini API was used for OCR and the browser's TTS.Challenges we ran into: I initially planned to deploy the project via GitHub, but encountered a limitation: using plain JavaScript made it difficult to secure the API key without exposing it. Rather than compromising security, I chose to prioritize improving the core functionality of the product. I believe that building a strong, feature-rich foundation is more important at this stage. Deployment can be addressed later using a more suitable environment like Node.js, where secure key management is easier to implement.Accomplishments that we're proud of: This is my first time solo-hacking, and I had to start late with limited resources (me and myself). I was able to create a functional product that I am passionate about, hence my two loves combined. I am proud of the hours I put into making \u03bcTeX work and keep trying despite the absence of teammates.What we learned: I think out of all things I learned, the most important lesson was making products accessible. Balancing features with user needs is crucial, and inclusive technology can help everyone in the community.What's next for \u03bcTeX (MuTeX): Increased accuracy in transcription, deployment, live LaTeX compilation for visual accuracy checking, and volume and speed control for TTS",
                        "github": "https://github.com/jihyopark/bitcamp-2025",
                        "url": "https://devpost.com/software/tex-mutex"
                    },
                    {
                        "title": "NeuroScan.AI",
                        "description": "Combining Google's Gemini AI and deep learning to revolutionize brain tumor detection- MRI scans with 92% tumor detection accuracy- personalized treatment plans with approved medical reports. \ud83e\udde0\ud83e\udd16",
                        "story": "Try out NeuroScan.AI now on:linkInspiration: Our journey began with a deep fascination for both neurology and artificial intelligence. We were inspired by the potential to bridge these two fields to create something that could make a real difference in healthcare. The idea of using AI to assist in early brain tumor detection, a critical area where early diagnosis can significantly impact patient outcomes, drove us to develop NeuroScan.AI. We wanted to create a tool that could support medical professionals while making advanced diagnostic capabilities more accessible.What it does: NeuroScan.AI is an advanced brain tumor classification system that:\nAnalyzes MRI scans using deep learning\nClassifies different types of brain tumors with 92% accuracy\nGenerates comprehensive medical reports\nProvides personalized treatment recommendations using Google's Gemini AI\nCreates detailed PDF reports with patient information, analysis results, and treatment plans\nIncludes a section for doctor's review and approvalHow we built it: We built NeuroScan.AI using a sophisticated tech stack:\nDeep Learning: TensorFlow 2.15.0 with MobileNetV2 architecture\nWeb Interface: Streamlit 1.32.0 for the user-friendly web application\nImage Processing: OpenCV 4.9.0.80 and Pillow 10.2.0\nAI Integration: Google's Gemini AI for treatment recommendations\nReport Generation: FPDF2 for creating professional medical reports\nData Processing: NumPy 1.24.3 and scikit-learn 1.4.1\nThe system was trained on a comprehensive dataset of brain MRI scans, fine-tuning the MobileNetV2 model to achieve optimal performance.Challenges we ran into: One of our biggest challenges was optimizing the model for accurate tumor detection. We had to:\nFine-tune the MobileNetV2 model on our specific dataset\nProcess and normalize hundreds of MRI scans\nOptimize TensorFlow processing for better accuracy\nBalance model complexity with processing speed\nEnsure the system could handle various MRI scan formats and qualities\nIntegrate multiple AI components (MobileNetV2 and Gemini) seamlesslyAccomplishments that we're proud of: We're particularly proud of:\nAchieving 92% accuracy in tumor classification\nCreating a user-friendly interface that medical professionals can easily use\nSuccessfully integrating multiple AI technologies\nDeveloping a comprehensive reporting system\nBuilding a scalable solution that can be deployed in medical settings\nCreating a system that could potentially save lives through early detectionWhat we learned: Throughout this project, we learned:\nThe importance of model optimization and fine-tuning\nHow to effectively process medical imaging data\nThe value of user feedback in medical technology\nThe challenges of integrating multiple AI systems\nThe significance of accuracy in medical applications\nHow to balance technical complexity with practical usabilityWhat's next for NeuroScan.AI: We envision several exciting developments:\nLongitudinal Analysis: Adding a new AI model to track tumor changes over time\nPersonalized Reports: Further customization of reports based on patient history\nEnhanced Accuracy: Continuous improvement of our classification model\nIntegration: Connecting with hospital systems and electronic health records\nMobile App: Developing a mobile version for easier access\nReal-time Processing: Implementing real-time MRI scan analysis\nGlobal Deployment: Making the system available to medical facilities worldwide\nResearch Collaboration: Partnering with medical institutions for further validation\nOur ultimate goal is to make NeuroScan.AI a vital tool in the fight against brain tumors, helping doctors make faster, more accurate diagnoses and ultimately saving lives through early detection and personalized treatment plans.",
                        "github": "https://github.com/anshulg614/NeuroScan.AI",
                        "url": "https://devpost.com/software/neuroscan-ai"
                    },
                    {
                        "title": "Rizz Lab",
                        "description": "Ai that help you rizz",
                        "story": "Inspiration: people need help razzingWhat it does: How we built it: next js fast apiChallenges we ran into: Accomplishments that we're proud of: What we learned: What's next for Rizz Lab:",
                        "github": "https://github.com/vasantsaladi/umd_hack",
                        "url": "https://devpost.com/software/rizz-lab"
                    },
                    {
                        "title": "NeoVisr",
                        "description": "\"College Advising, Reimagined\"",
                        "story": "",
                        "github": "https://github.com/samikwangneo/bitcamp2025",
                        "url": "https://devpost.com/software/adviseai"
                    },
                    {
                        "title": "Recipez",
                        "description": "A cooking app that helps new cooks effortlessly find personalized recipes using the ingredients they already have in their kitchens.",
                        "story": "Inspiration: After a long day of work, figuring out what to cook can take a lot of time and energy; both of which college students lack. When at home, it can be difficult to know what recipes to make with limited ingredients. When at the grocery store, it can be difficult to know what ingredients you are missing at home for certain recipes. Our product aims to solve both of these problems, acting as an automated recipe and ingredient manager.What it does: Our product keeps track of all the ingredients in your home by allowing you to scan receipts from the grocery store. Then, based on the ingredients available, our product is able to suggest recipes you would be able to make. If you find a recipe you like, you can save it, and the ingredients used will be removed whenever you make it.How we built it: We utilized react-native Expo and leveraged Google's Cloud Vision API for OCR, Gemini to generate recipes, and Firebase for the backend.Challenges we ran into: We faced a few key challenges, especially around OCR accuracy and text parsing from receipts. Cleaning up inconsistent item formatting (e.g., brand names, typos, and different units) required a lot of trial and error with prompt engineering.Accomplishments that we're proud of: We successfully integrated image-based item detection using OCR and natural language processing, turning a simple receipt image into structured food data.What we learned: We learned how to utilize API architecture effectively, especially chaining two different services (Google Vision + Gemini) to create a smooth user experience. We also deepened our understanding of React Native\u2019s component lifecycle, image handling, and state management.What's next for Recipez: -Expiration reminders:Add auto-reminders for items based on shelf life.\n-User accounts:So people can sync their fridge across devices.",
                        "github": "",
                        "url": "https://devpost.com/software/recipez-d2zosy"
                    },
                    {
                        "title": "Bird Box",
                        "description": "A Pok\u00e9mon GO/Snap-esque bird watching application that adds new flavor to a classic past time.",
                        "story": "Inspiration: Pok\u00e9mon GO and Snap add fun to the classic Pok\u00e9mon format by increasing the novelty of simple observation through game-ification tactics. Simply finding and catching (through both photography and collection) these unique creatures in interesting ways drives the appeal and quality of those experiences. Given the continual advancement of Machine Learning for Object Classification and Image Segmentation it seemed fitting to apply the game-ification tactics of Pok\u00e9mon GO and Snap onto bird watching.What it does: Bird Box allows users to see local birds in their area and attempt to take the best quality photos of our 200 total available bird species. Machine Learning is leveraged to both classify the appropriate bird species of our taken bird picture but also segment its area in the image to judge the quality of our picture based on the bird's positioning. Taking pictures of new birds adds them to your \"BirdDex\", and all images taken are added to your photoHow we built it: We ran a React Frontend using JS and CSS with a focus on a mobile view first, a Flask Backend using Python, and a local db instance using MySQLLite. Our frontend utilized three major external API's: Wikipedia to fetch descriptions of our bird species, Google Maps to create our map view, and EBird to fetch local data on bird sightings. Our backend relied on a Classification and Image Segmentation model built for our needs to classify our bird species and segment their area for further rating calculations. We created these models by finetuning off of YOLOv8 checkpoints and applying further work with data augmentation, parameter modifications, and further optimizations to combat overfitting. Our rating system from there was simple mathematics to see how well our image followed the rule of thirds, if the subject clipped out of the image or not, and the visual quality of the image.Challenges we ran into: There are many firsts for us during this project. sqlLite was recommended by one of the advisors and was an ease when sharing data between us due to its file based structure. Although most of the project went surprisingly well. All of our members has useful experiences that made our chemistry balance well. The biggest challenge was training out own machine learning model to correctly classify birds.  Given a limited dataset that caused overfitting we had to adapt our dataflow pipeline to handle this issue as we continued to build up our model.Accomplishments that we're proud of: The team isincrediblyproud of our UI/UX and ML model. Dynamically generating custom map pins and bird frequency zones was a huge learning curve that we overcame, we poured hours into extensively re-training an existing ML model, and we're happy to say that the project's UI/UX and aesthetics never fail to make us smile. Also, the team as a whole feels great about the teamwork this hackathon. For the majority of us, this is the smoothest a project has ever progressed, and we feel like we delegated tasks and balanced responsibilities really really well.What we learned: We learned that working hard can be fun. The members on our team love to game. Making this project. The ability to will our ideas into existence empowers us to work hard and create something accessible and fun for everyone.What's next for Bird Box: If work on Bird Box continues, we're really excited to add some features to improve the user experience on the map. Specifically, it'd be great to implement a sort of nearby system, to let users know more succinctly which birds have been spotted in their area. Also, we think an AI-powered bird-health assessment tool would be an excellent improvement to our project. We would love for our project to help educate our users about their local ecosystem, and list some actions they can take to better protect it.",
                        "github": "",
                        "url": "https://devpost.com/software/bird-box-d2eguh"
                    },
                    {
                        "title": "World Racing",
                        "description": "Race around any real city in the world, in seconds",
                        "story": "Inspiration: We built World Racing to bring back the joy of exploration through a modern lens. Inspired by our childhood memories of games like Mario Kart and the excitement of zooming through vibrant maps, we wanted to recreate that thrill\u2014except in the real world. With so many young people wanting to travel but facing constraints like time or money, we thought: what if you could race through the streets of Tokyo, Miami, or Paris from your browser?On a personal level, Mark has always struggled with navigation, often making wrong turns and relying heavily on Apple Maps\u2014even in his own city. World Racing became a fun and practical way for him to get better at recognizing streets and landmarks. Cristy, on the other hand, gets nervous driving in unfamiliar places. This project became a tool for her to build confidence, allowing her to virtually \"practice\" routes and get familiar with her surroundings beforehand.Also? It's just really fun. With multiplayer support, you can race against your friends and explore new places at the same time!What it does: World Racing lets you pick any real-world location and instantly create a racecourse through it. You define the start, finish, and checkpoints, and we build a fully interactive 3D driving experience around it. Before the race starts, you're treated to a cinematic drone flyover of the route, complete with roads, trees, oceans, landmarks, and more. The driving experience includes a 3rd-person camera view that follows your car, with a working speedometer, compass, and visual cues for landmarks and turns. It\u2019s part game, part travel, part simulation\u2014accessible from your browser in seconds. You can even add live effects, like changing the time of day or even making it rain.How we built it: We used Mapbox GL JS to render realistic 3D maps and terrain, and Three.js to handle the custom 3D car physics and visuals. The app itself is built in React with TailwindCSS for a clean, responsive UI. On the backend, we used Rust with the Axum web framework for high-performance APIs, and PostgreSQL with SeaORM to store and serve course data and user-generated maps. All components work together seamlessly to generate a real-world map, overlay the chosen route, and simulate a racing experience with proper camera and collision logic. We also utilized websockets for real-time, live multiplayer.We rolled our own physics engine for the car movement, here's some of the math that we utilized:Angular VelocityAcceleration over timeVelocity over timeDistance from point to line segment with endpointsChallenges we ran into: Collision detection on 3D maps: Making sure users couldn\u2019t drive through buildings or terrain required custom logic on top of Mapbox's 3D tiles and elevation layers. We had a version that had support for collision detection, but ultimately decided to scrap it, because the 3d maps that we really liked did not have the data necessary in order to have collision detection.Camera tracking: Creating a smooth third-person camera that reacts naturally to speed, turning, and collisions took several iterations to get right.Map loading performance: Loading a fully 3D-rendered city on demand while maintaining 60 FPS required lots of optimization, especially when switching between drone view and race mode.Pathfinding and routing: Translating user-defined points into a logical, drivable path across roads in a 3D environment was more complex than anticipated.Accomplishments that we're proud of: Building a fully interactive driving simulator that works anywhere in the world with a single click.Achieving smooth car physics and camera movement within a 3D map of real cities.Creating a beautiful cinematic drone sequence that previews your race before you begin.Designing a product that blends nostalgia, practicality, and technical achievement in a unique way,What we learned: Not to wait until the last minute to implement a multiplayer system using websockets.How to work with real-world geospatial data and turn it into an engaging, game-like experience.The complexities of combining Three.js physics with Mapbox 3D terrain and tile loading.How to architect a fullstack system with React + Rust + PostgreSQL in a performant and scalable way.,What's next for World Racing: Refining multiplayer supportMultiple car car supportRacing against AIPower-upsAdding ramps and more,Credits: Credits to Nintendo for sound effects used in our applicationCredits to kulonee on sketchfab for the car model and Jacobs Development for the globe 3d model,",
                        "github": "https://github.com/mbruckert/world-racers",
                        "url": "https://devpost.com/software/world-racing"
                    },
                    {
                        "title": "HealthBridge",
                        "description": "HealthBridge uses public data to identify health gaps in communities, helping professionals place resources smarter and improve care where it\u2019s needed most.",
                        "story": "Inspiration: HealthBridge was inspired by the health disparities present in underserved communities. Our team wanted to create a tool that would help public health agencies, non-profits, and policymakers identify high-need areas using real-world data. We were driven by the idea that data transparency and visualization can make a meaningful impact on health equity.What it does: HealthBridge maps and analyzes social determinants of health (SDOH) data\u2014like hospital access, food deserts, police presence, and income distribution\u2014to identify geographic gaps in care and resources. Users can enter a ZIP code to view the closest health providers or explore areas with overlapping vulnerabilities.How we built it: We gathered open datasets from state and federal sources and cleaned them using Python and pandas. For visualization, we used HTML, CSS, and Leaflet.js to build an interactive web map that displays all collected data. Instead of using a live database, we processed and embedded the data directly for quick access and demo purposes.Challenges we ran into: Working with inconsistent data formats and incomplete information was a major hurdle. Many public datasets lacked standardization, requiring significant cleaning and restructuring. We also faced challenges in building a modern, easy-to-navigate interface that could layer multiple sources of data meaningfully.Accomplishments that we're proud of: We\u2019re proud of creating a working prototype that provides a clear, data-driven view of community needs. We successfully integrated several datasets and built a clean user experience. Most importantly, we built something with real potential to inform decisions that improve lives.What we learned: We learned how to process and visualize large datasets in a meaningful way and how to use geolocation to personalize health resource recommendations. We also learned the importance of balancing technical functionality with a clean user interface.What's next for HealthBridge: In the future, we hope to:Integrate real-time data sources and allow for live updates.Add user-submitted feedback for local health conditions or needs.Partner with community health organizations to pilot the tool in the field.Explore integrating AI to suggest optimal locations for future resource placement.,",
                        "github": "",
                        "url": "https://devpost.com/software/healthbridge-i8ao9x"
                    },
                    {
                        "title": "EchoLens",
                        "description": "Mobile web-app that describes what objects are in your environment from your phone camera.",
                        "story": "What it does: Vocally describes the environment and objects around you using your camera on your phone.,How we built it: Full Stack Web-AppFlask BackendReact FrontendNgrok to tunnel the frontend to backend using HTTPSLocal Tunnel to expose the frontend to the world using HTTPS,Challenges we ran into: We found that for phones, videos can't be used unless it's HTTPSNgrok only allows one tunnel on the free tier. The reason we needed Ngrok and Local Tunnel is because we needed two tunnels.Debugging a web app meant for mobile was extra difficult, due to how you can't access console on mobile.,Accomplishments that we're proud of: It worksIt remembers the previous pictures, and the differences between pictures. So if things change or move it will point that out.,What we learned: Full stack development in a teamTunnelingHTTP Methods,What's next for EchoLens: Full deploymentAdjusting the automatic capture intervals per user,",
                        "github": "https://github.com/JemLuu/bitcamp",
                        "url": "https://devpost.com/software/echolens-xf9a5q"
                    },
                    {
                        "title": "Airwaves",
                        "description": "The rhythm-based camera game that translates hand movements into your mouse gestures, opening up new ways for people with motor disabilities to interact with technology.",
                        "story": "What it does: Airwaves is a rhythm-based browser game that uses your webcam to detect hand movements and translate them into mouse clicks, scrolls, and gestures. The player hits on-screen targets to the beat\u2014entirely hands-free. Behind the scenes, we interpret specific hand poses and motions in real time, making the game both a fun challenge and a light physical therapy exercise to improve hand mobility, coordination, and range of motion. The game also supports Clone Hero mapping files, which we plan to expand with a custom upload feature for players to add their own songs.How we built it: We used TensorFlow.js and the HandPose model to detect and track hand landmarks from a webcam feed, powering all in-game gesture recognition and allowing the experience to be fully web-based. OpenCV handled all non-gameplay interactions, including menu navigation and volume control through hand gestures. The front end was built with React and Three.js, using React Three Fiber for rendering 3D elements, with custom CSS animations for a smooth, immersive rhythm experience. The hand motions were designed to replicate common hand mobility exercises used in physical therapy , making gameplay beneficial for hand mobility and coordination.Challenges we ran into: Achieving consistent hand detection across different lighting conditionsFiltering out accidental movements while keeping input responsiveCalibrating gestures to ensure therapeutic value without causing fatigueBalancing accessibility and rhythm mechanics without one compromising the otherIntegrating real-time hand tracking with the rhythm-based gameplay mechanics,Accomplishments that we're proud of: Developed a full working prototype of a webcam-controlled rhythm game in under 36 hoursCreated a gesture-to-input system that could have real therapeutic benefitsDesigned an experience that\u2019s fun for everyone, but especially impactful for users with limited mobility or in recoverySuccessfully integrated real-time hand tracking with a 3D-rendered game environment, fully rendered in React,What we learned: How to design interactive systems that are fun and functionalThat games can be more than games\u2014they can be tools for well-beingThe importance of balancing technical complexity with user experience,What's next for Airwaves: Develop new levels with progressively challenging therapeutic motionsAdd hand tracking analytics to help users (and therapists) monitor progressIntegrate accessibility settings like motion sensitivity, tempo adjustment, and guided movement trainingOptimize the hand tracking pipeline for better performance on low-end devices,",
                        "github": "https://github.com/clxxiii/air-waves",
                        "url": "https://devpost.com/software/airwaves-342g1j"
                    },
                    {
                        "title": "Image Steganalysis: Detect Hidden Text with Machine Learning",
                        "description": "Our model detects hidden messages in images with 95% accuracy, scanning thousands in milliseconds. Ideal for cybersecurity, forensics, and uncovering steganographic traces.",
                        "story": "In a world of digital communication, information can be hidden anywhere - even in plain sight. Steganography, the art of concealing messages within digital images, poses significant challenges for cybersecurity and digital forensics. Our project was inspired by the growing need to detect these hidden communications, protecting sensitive information and understanding the sophisticated methods of digital concealment.Our steganalysis model is a sophisticated image analysis tool that: \nDetects hidden messages within digital images\nAnalyzes pixel-level variations\nProvides a probability score of steganographic content\nCan process single images or entire directories\nIdentifies images with potential hidden information with 95% accuracyUsed the Alaska2 Image Steganalysis dataset\nCollected images with three steganography methods (JMiPOD, JUNIWARD, UERD)Developed advanced feature extraction techniques\nCreated a 150-dimensional feature vector\nAnalyzed statistical, gradient, and least significant bit variationsExplored multiple machine learning and computer vision algorithms\nImplemented Random Forest Classifier\nOptimized feature selection and model parametersUsed stratified cross-validation\nImplemented rigorous performance metrics\nFine-tuned detection thresholdsHigh Dimensional Feature SpaceManaging complex feature extraction\nReducing computational complexity\nlonger training hours and high memory usage while exploring pre-trained computer vision modelsAchieved 95% accuracy in detecting hidden messages\nDeveloped a flexible, scalable steganalysis model\nCreated a comprehensive feature extraction pipeline\nImplemented both single and batch image analysis\nDemonstrated practical applications in cybersecurityTechnical Insights\nAdvanced image processing techniques\nMachine learning model optimization\nFeature engineering strategies",
                        "github": "https://github.com/aniruddhg43986683/Bitcamp_Steganalysis/tree/main",
                        "url": "https://devpost.com/software/image-steganalysis-detect-hidden-text-with-ai"
                    },
                    {
                        "title": "DMPN: Developer Monitoring & Productivity Nexus",
                        "description": "Find your Developer Aura - Compete to be the Sigma Dev",
                        "story": "Inspiration: We\u2019ve all had those moments while coding where we think we\u2019re being productive\u2026 but somehow two hours pass and all we\u2019ve done wasrenamed a few variables and watched 50 Instagram reels. We wanted to build a tool that keeps us mindful of our actualproductivity, without micromanaging. Great focus and great code quality makesgreat progressin software development. \u23ed\ufe0f\u23ed\ufe0f\u23ed\ufe0fThat\u2019s where DMPN \u2013 Developer Monitoring & Productivity Nexus \u2013 was born: anaura trackerthat provides livefeedbackon whether you\u2019re in the zone \ud83d\udd0e or zoning out \u2716\ufe0f. Not only is it an aura tracker, but we also provided \ud83c\udfaegamification\ud83c\udfae where we can face off against other programmers in rankedaura battlesto show who is themost 10x engineeron the planet!What it does: The core of DMPN is afine-tuned modelthrough awebcam imagethat shows the developer's emotions and, and ascreenshotof the current screen of the developer's monitor. These images are analyzed in real time using \ud83e\uddbe OpenAI's vision models \ud83e\uddbe to generate productivity scores. From there, DMPN gauges a developer\u2019stech aura\u2013 essentially, how focused and productive they are during a coding session. This score is visualized in two\u2728clean UIs\u2728 to give users a sense of how they\u2019re doing, gently nudging them back on track if needed.In addition, we also spearheaded \u2694\ufe0fAura Battles\u2694\ufe0f, an elo-based online matchmaking tool that pitches you against other aspiring developers, in order to see who is the mostsigma developerof them all!How we built it: Check out the second image for our architecture diagram!We built DMPN as a monorepo with a modern tech stack:Frontend: Built with Next.js and styled using TailwindCSS. We used WebSockets to push real-time productivity scores to the UI. \ud83c\udf10Backend: Powered by Flask. It handles image capture, preprocessing, and communication with the OpenAI Vision API. \ud83d\udcf8,The system captures a screenshot and webcam photo at regular intervals, sending them to the backend for analysis. We designed prompts for OpenAI\u2019s model to interpret visual indicators of productivity or distraction from both face and screen. Productivity scores are calculated and streamed live to the frontend via sockets.Our design is proud to beUI-firstin order towin the UI prizecreate the best first-time experience for users. In addition, we also prioritizereal-timeinteractivity with the user, optimizing prompting and backend pipeline to provide low latency.Challenges we ran into: Setting up real-time image captured from both the webcam and screen in a smooth, system-agnostic way. \ud83d\ude0ePrompt engineering for OpenAI to interpret productivity meaningfully from facial expressions and screen content. \ud83c\udfadHandling WebSocket communication and ensuring consistent, lag-free updates.\ud83d\udcf6Balancing user insights with privacy \u2014 making sure we don\u2019t overstep while still delivering value.\ud83d\udd10,Accomplishments that we're proud of: AfunctionalMVP with end-to-end integration \u2014 from image capture to live productivity scoring.Tuned prompts that produced surprisinglyaccurateproductivity assessments.Embrace the exponents!Aclean, minimalisticUI that feels futuristic and responsive.Successfully used a monorepo architecture,streamlining developmentacross multiple parts of the stack.,What we learned: Vision AI can be surprisingly insightful\ud83e\uddd0, but it needs thoughtful prompting and tuning.Real-time interactivity is powerful\ud83d\udcaa, especially when paired with WebSockets.Modular development and clear collaboration\ud83e\uddd1\u200d\ud83e\udd1d\u200d\ud83e\uddd1 made working across the frontend and backend seamless.It's possible to create meaningful productivity tools without being invasive\ud83d\udca2 \u2014 subtle feedback can go a long way.,What's next for DMPN: My Tech Aura: Refined scoring: Add more nuanced categories like deep focus, distraction, passive browsing, etc.Personalization: Let users train a model on their own habits for more tailored feedback.Integrations: Build browser extensions and IDE plugins for smoother workflow monitoring.Privacy-first options: Offer local processing modes with no cloud storage, to ensure full data control for users.,",
                        "github": "",
                        "url": "https://devpost.com/software/dmpn-dev-aura-aio"
                    },
                    {
                        "title": "Sustainable Earth (Eco-Finance)",
                        "description": "Imagine if every purchase you made could contribute to the fight against climate change. With our transaction tracking system, we provide people with an incentive to make purchases with intention.",
                        "story": "Inspiration: From climate change and extreme weather to environmental pollution to ecosystem degradation, we wanted to make a difference on the environment using finance. Taking inspiration from Acorn, an app which invests spare change, we wanted to invest small portions of money into eco-friendly companies for clients while also encouraging clients to spend green.What it does: Sustainable Earth is a banking application which tracks the client\u2019s transactions, either through manually inputting a csv file or connecting with the client\u2019s primary banking app with Plaid, and provides a eco-score for the user as well as other eco-friendly metrics. The user is then provided with recommended steps to take to decrease their carbon footprint and make more sustainable purchases. The user can then search for any product and the application will return the most sustainable option for the user to purchase as well as an eco-score for the product. The next part of the application is rewards. Based on a multiplier system, the user can purchase products based on their rewards credit through companies partnered with Sustainable Earth. In addition, users can invest their rewards money in eco-friendly companies. In the investments page, the user is asked to answer questions for the application to obtain a grasp on the user\u2019s financial situation and risk preference.How we built it: We first thought of an idea that we all liked. Then we shared a repository and began coding. We used multiple technologies to make our functionalities work. We then created the front end using latest tech like tailwind and react. Then we created and tested the backend code and our prompts for the LLM APIs. After, we connected the frontend and backend using tech like flask and express. Finally we had our friends attempt to break the \u201cproduct\u201d and fix as many bugs as possible!Challenges we ran into: Figuring out how to obtain user transaction data was difficult, so we ended up using Plaid. We also had trouble with developing an effective visualization for the user\u2019s active investments, ended up implementing yfinance api to fetch live stock data. Had trouble parsing web-scraped data specifically for application usage, fixed by using Gemini API which did parsing for us instead.Accomplishments that we're proud of: We're proud of being able to integrate Gemini into our search engine for environmentally product options and green stocks, allowing users to help the environment through everyday tasks like shopping and investing. We are also proud of our UI and the use of Plaid API to retrieve user banking data. Finally, we are proud of our team bonding exercises and our accountability in finishing each part of the project on time.What we learned: Learned how to use Gemini API for multiple aspects of our project. Learned how to connect Plaid API with application using Plaid\u2019s mock account.What's next for Sustainable Earth (Eco-Finance): The next step that we plan to take is to work with banks to sponsor real-money cashback rewards for customers who have environmentally friendly spending habits. We also plan to gamify this experience through achievements, streaks, and a leaderboard encouraging player competition. Combined, these additions will increase player engagement and make a larger impact towards the environment.",
                        "github": "https://github.com/KaushalGorla/SustainableEarthy",
                        "url": "https://devpost.com/software/sustainable-earth-eco-finance"
                    },
                    {
                        "title": "CarbonQapture",
                        "description": "Where Quantum Meets Climate Action",
                        "story": "Inspiration: Quantum Computing promises a new era of computing that uses the quantum nature of a particle to perform computations exponentially faster. With climate change and greenhouse gases like Carbon Dioxide at the forefront of global concerns, carbon capture has emerged as a critical area of research. Metal-organic frameworks (MOFs) are highly porous materials capable of selectively adsorbing CO\u2082, making them prime candidates for sustainable solutions. However, the vast design space of MOFs makes discovery a challenge \u2014 this is where quantum concepts like the Variational Quantum Eigensolver (VQE) comes in.What it does: This project integrates quantum simulation and machine learning to:Simulate MOFs using the Variational Quantum Eigensolver (VQE) algorithm.Estimate ground-state energies to assess CO\u2082 capture efficiency.Train a neural network to predict and propose novel MOF structures optimized for carbon capture.Test out the proposed MOF structures on the VQE algorithm to analyze ground state energy.Output predictions, highlighting performance metrics like uptake, selectivity, and heat of adsorption.,How we built it: CIF Parsing: Structural data from a real MOF Dataset (in .cif format) is parsed.Hamiltonian Construction: A simplified 2-qubit molecular Hamiltonian is built for each structure.Quantum Simulation: Using PennyLane, VQE estimates the ground state energy of each MOF-CO\u2082 system.AI Model: A neural network is trained on simulation outputs and pre-existing MOF dataset to propose new MOF structures.Results: Energies and material properties are saved in .csv format and visualized via plots.,Challenges we ran into: Qiskit Deprecation: Several components of Qiskit, including modules like qiskit.algorithms and deprecated estimators, posed compatibility issues with the latest versions. We had to refactor portions of our codebase and explore alternative quantum computing frameworks like PennyLane to maintain functionality and ensure forward compatibility.MOF Generation: One of the key challenges we faced was training our model to generate physically viable metal-organic frameworks (MOFs). Initially, the output structures consisted of randomly distributed atoms without any bonding, resulting in non-physical geometries and ground state energies of zero. To address this, we refined the parameters of our training dataset and implemented structural constraints to guide valid MOF generation. Additionally, we adjusted the Variational Quantum Eigensolver (VQE) algorithm by replacing the placeholder coefficients in the Hamiltonian with values more representative of realistic chemical interactions. These changes were crucial in enabling accurate energy evaluation and meaningful structure prediction.,Accomplishments that we're proud of: Successful generation of potential MOFs:Implementation of the Variational Quantum Eigensolver:Successful Training of the Neural Network: The new generated MOF's by the Neural network have the following advantages over the currently existing MOF's , 98.3% accuracy in predicted ground state energy, Higher avg CO2/N2 Selectivity, Predicted topology and functional groups building the entire structure and lastly, a better predicted CO2 Uptake that leads to having a more profound impact on the atmosphere.Implementation of a Professional UI: Building the entire user interface from scratch using React was a significant challenge, but incrementally developing each component helped us manage the complexity effectively. Visualizing the raw data from our trained models posed another difficulty, especially with over 8,000 generated data points. To ensure clarity and performance, we selectively sampled key data points to construct meaningful graphs. For the frontend, we integrated several interactive React libraries to create a responsive and engaging user experience.,What we learned: We learned how to use PennyLane for the first time. This is the first time we have implemented the Variational Quantum Eigensolver.We learned about metallic-organic frameworks and their application in carbon captureWe learned how to write a neural network, and is our first time writing an AI model that generates new Compounds using a mix of supervised learning and generative AIWe learned how to use react to make dynamic websites, especially interactive graphs.,What's next for CarbonQapture: Our long-term vision is to revolutionize the way innovative materials are discovered and optimized to meet critical environmental challenges. By integrating cutting-edge quantum algorithms with artificial intelligence, we aspire to create a dynamic research platform that continuously evolves and adapts, fostering unprecedented breakthroughs in the field of carbon capture.\nWe envision a future where our approach not only contributes to a deeper understanding of MOF chemistry and CO\u2082 sequestration but also spearheads the development of sustainable, high-performance materials. This initiative aspires to bridge the gap between theoretical simulation and practical application, ultimately contributing to global efforts against climate change.\nIn this vision, the synergy between quantum computing and AI will pave the way for scalable, efficient material design strategies that empower industries and governments worldwide to achieve cleaner, greener technologies.",
                        "github": "",
                        "url": "https://devpost.com/software/carbonqapture"
                    },
                    {
                        "title": "FinGuard",
                        "description": "\"Smart Spending, Safer Shopping, Healthier Finances.\"",
                        "story": "Inspiration: We realized how often people leave money on the table by not using the best credit card for purchases. But that\u2019s just the tip of the iceberg. Many users fall victim to scam websites, accidentally overspend, or simply don\u2019t understand where their money is going each month. With so many credit card reward programs, sketchy merchants, and confusing spending patterns, there\u2019s a real need for a tool that simplifies everything. That\u2019s what inspired us to create FinGuard \u2014 a tool to help users shop smarter, stay safer online, and gain awareness of their spending habits to build a healthier financial future.What it does: FinGuardis a browser extension and web dashboard that helps users spend smarter, stay safer, and build better financial habits.Here\u2019s what it does:Recommends the best credit cardfor each purchase based on the user's linked credit cards, including rewards, points, and active discountsWarns users about scam or unsafe websitesusing site reputation checksTracks spendingby categoryProvides a financial dashboardshowing income, expenses, savings, and cashback earnedAnalyzes spending patternsto help users understand their habits and avoid overspending,How we built it: Frontend: The extension was built usingReact,CSS, andJavaScript. React was used to handle the dynamic UI components, ensuring smooth and responsive interactions.Card Recommendations: We fetch card-specific recommendations fromMongoDB, where we store information about different credit cards, including rewards, points, and active discounts, to provide the best suggestions for each purchase.Site Scam Checker (Nudge): We usedIPQS(IP Quality Score) for ourNudgefeature, which checks the reputation of websites and alerts users if a site is unsafe. This feature warns users about scammy or potentially malicious websites by showing a \u201cnudge\u201d message when the site is flagged.,Frontend: Built withReactandTailwindCSS, offering a dynamic and responsive UI that allows users to interact with real-time financial data and insights efficiently. TailwindCSS was chosen for its utility-first approach to styling, allowing rapid customization and consistent design.Backend:Node.jswas used for the backend, handling user authentication, data processing, and interactions with external APIs.RAG Model (Gemini): We integratedGeminito use aRAG (Retrieval-Augmented Generation) modelthat analyzes user transaction data. This helps us provide personalized financial insights, answer user queries about spending, and offer suggestions based on transaction history.Insights Recharge: We usedInsights Rechargeto provide advanced data analysis, allowing users to gain deep insights into their spending patterns and make smarter financial decisions.Database:MongoDBwas chosen for its flexibility and scalability in storing user profiles, transaction data, and card details.,Our project leverages the behavioral economics principle of \"nudging\" to promote financial well-being. Recognizing that people often struggle with financial planning due to inertia or overwhelming choices, we designed a user-friendly application that subtly guides users toward healthier financial habits.By implementing intuitive default options such as automatic enrollment in savings plans, our app encourages consistent saving without restricting individual choice. Additionally, we utilized framing techniques to present financial information positively, making users more comfortable with beneficial financial decisions.We also included clear visual cues and personalized notifications to reinforce productive behaviors, like budgeting or timely bill payments. Through these subtle yet impactful adjustments, users naturally adopt better financial habits without feeling pressured or coerced.Overall, our application demonstrates the effectiveness of gentle nudges in empowering users to achieve greater financial security and confidence.Accomplishments that we're proud of: What we learned: Through building FinGuard, we gained valuable insights into real-time data analysis, which is crucial for providing personalized and timely credit card recommendations. We realized how important it is to balance user privacy with functionality, ensuring that sensitive financial data is stored securely without compromising on ease of use. The integration of the Gemini RAG model taught us how AI can effectively analyze transaction data to offer meaningful insights, helping users make informed financial decisions. We also discovered that a user-centric design is key to adoption and engagement\u2014an intuitive interface allows users to easily navigate their financial journey. Additionally, we learned that continuous improvement is essential; user feedback is invaluable for refining features and making the platform better over time.What's next for FinGuard: Looking ahead, we are focused on expanding the credit card database to offer a wider range of recommendations, making FinGuard even more valuable for users with different financial needs. We also plan to enhance the Gemini RAG model, improving the accuracy of financial insights and predictive capabilities, so users can better optimize their spending. The development of a mobile app is on the horizon, giving users access to their financial data and recommendations on-the-go. We also aim to integrate more payment platforms and credit card providers to broaden the scope of FinGuard\u2019s recommendations. As we continue to listen to user feedback, we will add new features such as bill payment tracking and goal-based financial planning. Finally, we plan to expand globally, offering localized support for different countries, currencies, and financial systems, so more users worldwide can benefit from FinGuard.",
                        "github": "",
                        "url": "https://devpost.com/software/finguard-x1akuc"
                    },
                    {
                        "title": "Forens.tech",
                        "description": "Accelerating Digital Forensics with Friendly UI and Emerging Tech",
                        "story": "Inspiration: Cybersecurity\u2014and especially the niche of Digital Forensics\u2014has long been associated with notoriously unapproachable user interfaces, difficult user experiences, and confusing application flows. From bloated, messy logs to outdated design trends, there are plenty of reasons for this reputation. As we discussed problems we were passionate about solving, a common thread emerged from our diverse professional and technical backgrounds. Many mission-critical tools in Digital Forensics are nearly impossible for amateur or untrained users to navigate, too intimidating to encourage broader adoption, and simply unenjoyable to use. By combining our team\u2019s strengths in graphical design, user experience, and cybersecurity engineering, we created Forens.tech\u2014an application designed to change that narrative.What it does: Forens.tech takes an innovative approach to common digital forensics techniques\u2014combining a fresh, well-planned, and deliberate user experience with reliable methods to quickly parse and display memory dump data. Forens.tech is a web application that runs locally on a customer\u2019s system, alongside any memory dump files they wish to analyze. It offers a straightforward, easy-to-follow selection process for choosing the types of scans to run on the selected file, using those inputs to execute Volatility3 for analysis. The application outputs a consistently clean and structured JSON result, which is then passed to a Gemini-powered LLM agent for summarization and deeper insight. This includes both objective data and a cleaned-up version of the command output, all presented clearly to the end user. Using Volatility3, Forens.tech can offer options for analysis on a system\u2019s running processes, conducting a file scan of the computer, getting OS information, viewing network connections, and returning a list of executed system commands. The goal is to simplify memory dump analysis, making it more accessible to a wider audience of law enforcement and blue team specialists. Along with this focus, utilizing tools like Volatility3, Forens.tech can offer a wide range of expanding options to analyze data from the position of beginners to experienced professionals.How we built it: Forens.tech\u2019s front end was built using ReactJS and custom CSS after analyzing multiple design features withprototypesusing Figma. As a team, we had to think about who the intendedend-use_r was and their unique descriptors, such as location, age, mental states, and hobbies. To be on the same page of things, we collectively created the _task flowin FigJam. Following that, we created thewireframe, this helped us figure out what was capable on both sides of the front-end development and back-end development. Then, we took into account typography, contrast checking, coloring, and style guides to then land on the high fidelity prototypes. We made sure that these prototypes were user-friendly and accessible to the intended user personas. For back-end, Volatility3 is called through a Python script, which pushes output through fastapi to serve mission-critical information to the end user.Challenges we ran into: One major challenge was using Volatility and learning to work with memory dumps, both of which are things none of our team members had prior experience with. Luckily, both the documentation and usage are relatively straightforward and well-maintained, allowing for the creation of our automated scripts to come after a few hours of initial research.Accomplishments that we're proud of: Our team is incredibly proud of how well the interface came out for Forens.tech. Although it was difficult at times, ensuring to build with a focus on the user interface and being deliberate with our design decisions helped us to create a good-looking and good-performing web application..What we learned: In addition to deepening our understanding of memory dumps and how tools like Volatility3 support cyber professionals in incident response, we also gained valuable insight into the importance of planning and preparation\u2014and the significant impact they have on the quality of an application\u2019s visual design, cognitive psychology, and user interface.What's next for Forens.tech: Expanding support for further and more advanced Volatillity commands as well as support for other digital forensics tools and combine this extra information with even cleaner data logging methods.",
                        "github": "https://github.com/AHumanMaybe/forens-tech",
                        "url": "https://devpost.com/software/forens-tech"
                    },
                    {
                        "title": "SkinForReal",
                        "description": "SkinForReal is a GenAI skincare app that uses daily selfies to detect skin issues, track product impact, and deliver personalized, real-time insights and safety alerts.",
                        "story": "Inspiration: Skincare today is confusing. Millions of people struggle with breakouts, dryness, and irritation, spending time and money on routines that don\u2019t work \u2014 and often make things worse. We saw a gap: there are trackers and filters, but nothing that gives real-time, personalized, data-driven skin insights.That\u2019s what inspired us to build SkinForReal \u2014 a GenAI-powered app that helps users truly understand their skin. Not just track it. Not just guess. But know, with confidence, what\u2019s happening and how to fix it.What it does: SkinForReal analyzes your skin using a daily post-wash selfie and delivers real-time, personalized feedback on your skin\u2019s condition \u2014 powered by TerpAI and Microsoft Azure Face API.Here\u2019s how it works:Selfie Scan: Detects breakouts, redness, dryness, inflammation, and traumaProduct + Mood Log: Tracks what you used, how your skin feels, and syncs lifestyle data (weather, sleep, stress)Real-Time Alerts: Flags products causing irritation or barrier damagePersonalized Advice: Matches insights to your skin type and Fitzpatrick toneSkin School: A GenAI-powered scrollable feed that teaches users why their skin is reacting and what to do nextPattern Recognition: Learns over time and evolves its recommendations based on your unique routine and skin behaviorHow we built it: Front End: Flutter for cross-platform support (iOS + Android)AI + Image Analysis: Microsoft Azure Face API to detect skin conditionsGenAI Layer: TerpAI for generating daily skin summaries, alerts, and Skin School content,Key Features Implemented:Real-time selfie analysisSmart product logging and pattern recognitionPersonalized daily reports with actionable insightsFitzpatrick skin tone + skin type detectionSafety alerts for early signs of trauma or irritation,Challenges we ran into: Safety messaging: We wanted to provide alerts that were accurate, but not alarming \u2014 delivering serious feedback in a supportive wayDesign balance: Making the app feel clinically reliable without losing the approachability and engagement Gen Z expects,Accomplishments that we're proud of: Built a full working prototype in under 36 hours, including real-time selfie analysis, product tracking, and GenAI-powered insightsSuccessfully integrated Microsoft Azure Face API to detect visible skin conditions from live user selfiesCreated a responsive, cross-platform UI using Flutter, designed for accessibility and ease of useDesigned and implemented personalized, safety-first alerts that adapt to each user\u2019s skin behavior over timeDeveloped a GenAI-powered \u201cSkin School\u201d content system that makes skincare education engaging and personalizedBalanced technical performance with user empathy, ensuring the product is both functional and emotionally supportiveTranslated complex AI + skin health logic into a simple, intuitive user experienceReceived strong positive feedback during early testing for both usability and clarity of insights,What we learned: Over the weekend, we dove deep into GenAI, image processing, and ethical product design. Along the way, we learned how to:Use Microsoft Azure Face API to analyze skin features like redness, inflammation, and dryness from real selfiesIntegrate TerpAI to generate intelligent, human-readable daily skin reports and educational contentBuild a seamless user flow that combines product tracking, lifestyle inputs, and visual skin analysisDesign for accessibility and empathy, making the app useful for people at any stage of their skincare journey,What's next for SkinForReal: Expand Skin Condition Detection: Train the AI to recognize a wider range of skin concerns (e.g., hyperpigmentation, fine lines, oil levels)Ingredient-Level Analysis: Integrate a database to flag potential irritants or allergens in user-logged productsMulti-Language Support: Launch with support for Spanish, Mandarin, and other high-demand languages to increase accessibilityEHR Integration: Develop secure pathways to export user skin reports for dermatologists or electronic health recordsAdvanced Skin Trends Dashboard: Build tools to help users track long-term patterns, triggers, and improvementsCommunity Features: Add forums and shared routines for users with similar skin types and goalsSkinForReal Pro: Launch our premium tier with PDF exports, dermatologist-ready reports, custom insights, and brand partnerships,",
                        "github": "",
                        "url": "https://devpost.com/software/skinforreal"
                    },
                    {
                        "title": "Roameo",
                        "description": "We bring the world to you \u2014 so you can go anywhere.",
                        "story": "\ud83c\udf0dRoameo \u2014 Your Chatbot Travel Companion for Effortless Trip Planning: Roameo is a conversational chatbot designed to simplify travel planning, making your next adventure just a chat away. From flights and hotels to personalized itineraries and local recommendations, Roameo brings it all together effortlessly, instantly, and entirely through intuitive chat interactions.\ud83d\udca1Inspiration: Travel planning can often be stressful and overwhelming, requiring hours spent comparing flights, accommodations, and activities across multiple sites. We asked ourselves:Roameo removes the complexity of travel arrangements by creating a seamless, conversational experience that adapts instantly to your needs and preferences.\ud83d\udee0\ufe0fHow We Built It: \ud83e\uddd7Challenges We Faced: Real-Time API Integration:Ensuring fast, reliable integration with multiple real-time data sources for travel services.Natural Language Understanding:Training Roameo to accurately interpret diverse and complex user requests.Seamless UX:Creating an intuitive interface that remains responsive and easy to navigate through chat.Browser Automation Latency:Optimizing speed during intricate, AI-driven web interactions.Speech Synthesis Quality:Ensuring natural, clear, and engaging voice outputs for enhanced user interaction.,\ud83c\udfc5Accomplishments We're Proud Of: Achieving real-time personalized itinerary creation entirely through a conversational interface.Successfully integrating diverse travel data and automation tools into one seamless, dynamic chatbot.Creating an intuitive and highly responsive UI that users genuinely enjoy interacting with.Implementing realistic speech synthesis to enhance user comfort and accessibility.,\ud83e\udde0What We Learned: Complex API management and integration, especially in dynamic, real-time scenarios.Deepened understanding of NLP and conversational AI interfaces through OpenAI ChatGPT and Google's Gemini.Importance of a clean, responsive, and intuitive UI to facilitate effortless user interactions.Effective integration and utilization of AI-driven browser automation and speech synthesis technologies.,\ud83d\ude80What's Next for Roameo: \ud83c\udf10Enhanced Personalization:Creating user profiles to deliver even more customized travel experiences.\ud83d\udc65Collaborative Trips:Enabling multi-user planning and sharing of trips.\ud83d\udcf1Extended Services:Integration with additional travel essentials like insurance, SIM card management, and currency exchange.\ud83c\udf99\ufe0fVoice Capability:Future reintroduction of voice interactions, making Roameo the ultimate voice-first travel assistant.,Roameo is more than just a chatbot\u2014it's your personal travel companion, making trip planning intuitive, intelligent, and entirely conversational.",
                        "github": "https://github.com/Vitthal-Agarwal/travel",
                        "url": "https://devpost.com/software/roameo-dtrzau"
                    },
                    {
                        "title": "MedSave",
                        "description": "Fighting Drug Waste, Fueling Health Equity.",
                        "story": "Inspiration: Every year, billions of dollars in unused medications are discarded by:PharmaciesHospitalsNursing homes,Often, this is simply due to expiration dates.One study found that hospitals waste over$800 millionin usable drugs annually.Meanwhile,1 in 3 Americansskip medications due to cost.What it does: MedSavebridges the gap between surplus medications and communities in need.It connects:Pharmacies with excess or soon-to-expire inventory that can offer steep discountsNonprofits and clinics serving underserved populations:Community health centersSheltersAid organizations,By reducing waste and improving access to essential medicine, MedSave makes a tangible impact \u2014 both environmentally and socially.How we built it: Frontend:React, HTML, CSS, JavaScript, Tailwind CSSAuthentication:Google OAuthAddress Lookup:Mapbox APIBackend:Flask with multiple RESTful endpointsDatabase:SQLite (for user, drug, and pharmacy data)External API:NDC Directory API (for drug metadata),How it works: Upon visiting MedSave:Users log in viaGoogle authenticationThey select one of two roles:PharmacyNonprofit,Pharmacy userscan:List surplus/soon-to-expire medications by entering:NDC codePharmacy nameLocationPriceExpiration dateThe backend uses theNDC Directory APIto autofill:Generic drug nameManufacturer infoAll data is stored in aSQLite databaseand displayed in the marketplace,Nonprofit userscan:Search for medications by entering:Drug nameTheir addressThe backend returns:Listings from the nearest pharmacies selling the drugPrioritized by proximity,Challenges we ran into: As first-time hackathon participants, we faced challenges such as:\u2699\ufe0f Git and version control mishaps\ud83c\udfa8 Frontend styling and layout tweaks\ud83d\udd10 Troubleshooting authentication flow\ud83d\udd04 Keeping backend and frontend in sync,Accomplishments that we're proud of: \u2705 Completed and deployed a full-stack web application\ud83d\udd10 Successfully integrated Google OAuth and Mapbox\ud83e\udd1d Collaborated and adapted quickly under time pressure\ud83e\udde0 Gained practical experience with APIs, databases, and user roles,What we learned: \ud83d\udcc1 Effective use of Git/GitHub for collaboration\ud83d\udc1e Debugging full-stack issues\ud83d\udd12 Best practices for authentication and session management\ud83d\udd01 Frontend-to-backend integration techniques,What's next for MedSave: We plan to expand MedSave\u2019s functionality with:\ud83d\udd0d More advanced filtering (by radius, drug brand, etc.)\ud83d\udcec Alerts for nonprofits when needed medications become available\ud83d\udcca Dashboards with analytics and request tracking\ud83e\uddf1 Upgrade to a scalable backend (e.g., PostgreSQL)\u2705 Verification & incentive systems to promote responsible listings,",
                        "github": "https://github.com/arsh-goenka/MedSave.git",
                        "url": "https://devpost.com/software/medsave"
                    }
                ],
                [
                    {
                        "title": "CookHub",
                        "description": "Mobile app for users to find and share recipes. Allows for automatic calculation of grocery list costs to help users manage personal finances. Uses React, Typescript, Figma, Firebase, and Cooklang.",
                        "story": "Intro: Looking for cheap and easy recipes, but unsure where to go? Wanting a place to conveniently save and share your favorite recipes? Wishing for the best places to buy your grocery list for cheap? Introducing CookHub, a mobile app that not only lets users find and share their favorite recipes, but also allows them to better budget their finances by calculating the amount of money they might spend on groceries.Process: Inspired by GitHub, CookHub puts a foodie twist on the ubiquitous online coding platform, allowing people to post and fork recipes instead of code repos. We were inspired by the idea of \"forking a recipe,\" not only because of the food-related pun, but because cooking naturally invites the creative iterative process. Spun off of that single idea, CookHub was born.What it does: CookHub supports many features, including the following:Automatically formatted display ofCookLang markdown filesRecipe forking to make personal variations while paying homage to the originalText editor to make adjustments to recipes in-appSearch feature with filters based on recipe metadataRecipe bookmarking to a favorite recipe pageShopping cart feature allowing users to find the best local deals for grocery list items,What it Uses: We used a variety of different schemas, including CookLang Markdown, React Native, Figma, HTML & CSS, Typescript, and the Firebase API.What's next for CookHub: We're very proud of how CookHub turned out. In the future, we're hoping to add more backend functionality and optimization. We also hope to better polish the user interface to better deliver users a satisfactory experience.",
                        "github": "https://github.com/anhatthezoo/bitcamp2025#",
                        "url": "https://devpost.com/software/cookhub"
                    },
                    {
                        "title": "Bye Bye Study Blues",
                        "description": "Are you stuck in a unproductive rut?  Do you need something cute and functional to get you working?  Fear not!  Using the Pomodoro technique, the Bye Bye Study Blues LED timer is the perfect thing.",
                        "story": "Inspiration: I'm a computer engineering major, and I knew I'd want to complete a hardware hack.  I use the Pomodoro method while studying, which is 50 min on and 5 min off, and I find that it really helps me.  I wanted to be able to see a pomodoro timer in a fun, cute, tactical sense, and the Bye Bye Study Blues project was born.What it does: The times are scaled down for easier demo-ability.  As the 50 min (50 seconds) go by, a row of lights goes off every 10 min (10 seconds).  This lets the user know how much time is passing.  Once the study time is up, a green light flashes at the bottom to get the users attention.  Next, break time starts, and a row of lights goes off every 1 min (1 second).  After break, the top row flashes next to get the users attention, and the timer starts once again.How I built it: The circuit is compiled of essentially six mini-circuits (or what I'll call branches), each powered by its own arduino digital pin and grounded.  Each branch is comprised of a resistor and 8 LEDs in parallel.  The code was written in Arduino IDE and was uploaded to the board.Challenges I ran into: It took a while to determine whether to use series or parallel connections.  I also struggled with getting numerous lights to light up.Accomplishments that I'm proud of: I'm happy I got all the strips to light up separately!  It took me a while to figure out, and while it seems simple, I was really happy when each took in the Arduino power input properly and was programmable.What I learned: I learned that the stuff I learn about in class actually matters.  I learned that different color lights and types of LEDs take different amounts of current to light up the same amount, and that concepts like current division, parallel and series connections, and voltage management play a big role in circuits.What's next for Bye Bye Study Blues: Soldering!  Once I get better at soldering, I want to turn this into a more official-looking electronic.  However, I am not good enough at soldering these days to solder 48 LEDs and components.",
                        "github": "",
                        "url": "https://devpost.com/software/bye-bye-study-blues"
                    },
                    {
                        "title": "VisuWorld",
                        "description": "Speak worlds into reality.",
                        "story": "\ud83c\udf0e What Were We Thinking?: We were hard at work figuring out what to build forBitcamp, and after spending nearly a third of the competition in the ideation stage, we came up withVisuWorld. After combining some of the cool new tech we've explored -GLSL / Graphics, Google Gemini, and Voice to Text- we builtVisuWorld!\ud83e\udd14 WhatisVisuWorld?: Embarking on your own VisuWorld exploration begins with a speech to text prompt to Google's Gemini API, which generatesGLSLcode to represent just about anything that you can articulate out loud. Equipped with over25,000+graphics shader snippets in aRAGdatabase, we've been able to see some truly impressive 3D visual landscapes over the course of this weekend.\u26a1 Visualize This:: The pipeline forVisuWorldis fairly simple, and can be broken down into 3 main steps:\u26f0\ufe0f Jagged Cliffs...: Working withGeminionGLSLcode was really difficult at first, and no amount of prompt engineering was going to save us from diving deeper. This was all of our first times usingRAGto empower LLMs, but we are sure glad we did. Even from only 500 code samples, having an information system for querying madeGeminiMUCH better at making worlds that will make your jaw drop.\ud83c\udfc6 Our Victories: Three Dimensional generation is a hot topic in the AI space, with 2D to 3D diffusion models being a popular choice of exploration. However, we haven't seen many things quite likeVisuWorldgoing from text to 3D landscapes. The \"eureka\" moment of realizing that graphics shaders were a valid surface area of innovation was thrilling to stumble across.\ud83d\ude80 Unexpected Discoveries: A whole lot about graphics. Like, a LOT. Who knew how much math went into these things, and just how intensive inefficient graphics can be. Our laptop fans blew louder than any of us imagined they could.\ud83c\udf05 New Horizons: We want to spend time perfectly optimizing every piece of theVisuWorldstack in hopes of deploying it for real. Most of the intense computing is offloaded to the web client, so we can (and will) be able to hold many users at a time. We want to share this experience with others, because it really is that cool to look at and a great time to explore.",
                        "github": "",
                        "url": "https://devpost.com/software/visuworld"
                    },
                    {
                        "title": "Schr\u00f6dinger's Catch: TROUT of This World!",
                        "description": "A relaxing VR fishing game that takes place in space while using quantum superposition to ensure a good catching experience. Wind down and fish for bit, and maybe even have a beer while you\u2019re at it!",
                        "story": "Inspiration: We've been to our fair share of hackathons, but this time we wanted to try something entirely new to us. So, we decided to dive into the world of VR, building a game and also somehow incorporating quantum computing to it. The result? A VR fishing game like no other - Schr\u00f6dinger's Catch: TROUT of this World!What's the Catch?: In our game, you get to explore a beautiful island, cast your line, and wait for a bite. But here's the twist - we used Python and Qiskit to add a dash of quantum randomness to the experience. When a fish bites, it's not just any ordinary randomness - it's quantum-powered.How we built it: We created the VR portion of Schrodinger's Catch using the Godot game engine with Godot XR Tools, \nWe built the VR part of our game using the Godot game engine and Godot XR Tools. For the quantum side of things, we used Qiskit to generate truly random numbers using the Hadamard function. With 32 types of fish to choose from, we needed 5 qubits to create a random number between 0 and 31. This calculation happens in a Python Flask app, which exposes an API for our Godot project to use through an HTTP request.Challenges we ran into: We ran into countless challenges during the creation of our project. From lack of resources and documentation on Godot XR Tools to being absolute beginners in VR and quantum computing (and linear algebra) - we had to learn as we went. And then there was the issue of spawning fish after casting the line... let's just say it took a few dozen testing phases to get it working.Accomplishments that we're proud of: We created a working application of quantum random number generation and a pretty cool VR experience. Even though we had no real prior knowledge of quantum computing or even VR, we were able to make something that while we could not incorporate any advanced algorithms into, we still had a blast working on it.What we learned: We learned how to export a Godot VR game to the Meta Quest 2 VR Headset to test and play, how to set up a player scene for VR in Godot using the Godot XR tools, and how to get random number generation using quantum computing. We learned a lot about qubits and quantum computing and it's applications. We learned about superposition and we got a feel for the workflow of what its like to do quantum computing.What's next for Schr\u00f6dinger's Catch: TROUT of This World!: The next steps for Schr\u00f6dingers Catch: TROUT of this World! is to learn how to create our own Godot XR scenes and functionality. We felt that the tools given to us didn't perform as well as we wanted to them to, so if we got more practice then it could be possible to create our own scenes that more closely align with what we want. If we had more time, we would also want to add or even create our own sound effects for the game, so that the VR experience is more immersive. In the future we would also like to explore and learn about Grover's algorithm which we learned a little bit about in a workshop.",
                        "github": "https://github.com/NA980593/schrodingers-catch",
                        "url": "https://devpost.com/software/schrodingers-catch-trout-of-this-world"
                    },
                    {
                        "title": "Koalafied",
                        "description": "\ud83d\udc28 Improving the koala-ity of your job search \ud83d\udc28",
                        "story": "Inspiration: Applying for internships and jobs can be tedious and boring. In a competitive job market, students like us are rushing to apply to hundreds of internships without a proper way to keep track of them. Many have turned to the bland Excel spreadsheets to mindlessly track their applications. This leaves students like us to be disorganized and bored when it comes to applying to jobs. We were inspired by idle games like Neko Atsume, and decided to create a similar mechanism for gamifying job applications.What it does: EnterKoalafied, a gamified way to keep track of your job applications and important information like interviews and offers. As you work towards building a future for yourself, you also work towards building a community of cutesy and unique Koala's deep within a quiet forest.Users can sign up for an account to rise up the leaderboards to a wide range of koala-ity career choices and unmatched fame. They can collect koalas by applying to jobs, interviewing, and receiving offers. Track status updates on their applications and keep track of important interviews or deadlines via a notepad. Most importantly, they can play with their koalas!How we built it: Koalafied is built on theMERNstack, meaning we used MongoDB, Express, React, and Node.js for our application.We first planned out our application by storyboarding and creating mock-ups for interfaces and then collaborated on setting up our database on MongoDB, discussing the structure of our data.We used Node and Express in combination with MongoDB to create users, an encrypted login system with JWT sessions, and store users' applications and of course those CUTE KOALAS. We found using MongoDB to be very efficient and easy to get started with. It made developing our backend a lot easier than we expected.For our frontend, we used React and Material UI to build a login form, a leaderboard, and most importantly the dashboard, which displays all of a user's application statuses, notes, and user statistics (applications, interviews, etc.). And don't forget about those CUTE KOALAS playing around on the bottom of the screen!Finally, we used Photoshop to edit and create graphics for our application, and Postman to test our backend API routes.Challenges we ran into: Building animations With Pure React: we found it difficult to set up the initial function structure that would allow for smooth animations and movement physics. There was a steep learning curve, but after gaining a better understanding of React's page update structure, we were able to create complex movement for our koalas.Creating Sessions with JSON Web Tokens (JWT): Allowing users to login, stay logged in, and also be restricted from accessing protect routes without an account proved to be a struggle. This was one of the more difficult and annoying challenges to deal with, not through just it's difficulty, but also through its recurring nature. We learned that in times where you find yourself getting annoyed with problems like these it's important to stay calm and work together.As Always... Styling Everything: CSS is hard y'all. But we did it :),What we learned: Evan Masiello, a second-time hackathon participant, first honed his existing skill for building react pages and creating backend connections when developing the dashboard and Express endpoints. Something that he did not expect to end up developing was the animation for the Koalas, but he found it to be a very fun experience. Evan enjoyed challenging himself by thinking of new ways the Koalas could move or be interacted with and figuring out how to implement them. He definitely strengthened his understanding and skills related to React.js through this process and also gained valuable experience using MongoDB for more intricate data than in the past.Benjamin Johnson learned deeply about how react works and react syntax. A step up from what he was normally accustomed to, which was just HTML, CSS, and JavaScript for building a webpage. Ben expanded on his knowledge of MongoDB which he had minimal knowledge of before Bitcamp to implement the leaderboard functionality.Caleb Holland, a second-time Bitcamp participant traveled from his usual comfort as a frontend developer to explore the backend. He learned how to create an encrypted user system using Node and MongoDB, create sessions with JSON Web Tokens (JWT), and most importantly learned how to have a good time with his pals. In addition, he continued to hone his sense of design by creating a majority of the graphics for the application, and by building some of the components used on the login and dashboard page.What's next for Koalafied: Originally, we wanted to use the Gemini API to generate the sprites for the koalas and create endless quests for the user. Although this was beyond the scope of this hackathon, this is something we'd like to implement as we improve upon the project in the future.We also had aspirations to have this tool auto track your applications after applying for jobs, either through an extension, or an email connection. This would make it much easier for users to organize their applications, as this removes the need to manually add in their applications.In the end, it was a tree-at to create Koalafied. We learned a lot, and we look forward to expanding upon it in the future!",
                        "github": "",
                        "url": "https://devpost.com/software/koalafied"
                    },
                    {
                        "title": "Quaran.Tech",
                        "description": "Experience contact tracing in this game (with AI, because why not)",
                        "story": "Inspiration: During major public health events, the public is often not aware of the duties of public health workers such as contact tracers, decreasing the efficiency of procedures such as public health mandates and \"flattening the curve.\" This game aims to educate the public on various public health risks and how they are dealt with.In addition, Efthimios Papadopoulos is an avid gamer and it was noticed that current games may become boring after the storyline has been exhausted. This game is an experiment in the use of large language models to enhance game experiences through the generation of alternate storylines.What it does: Quaran.Tech is a cross-platform HTML5 game that allows players to experience the life of a public health specialist in a fast-paced environment. When days are shortened to in-game seconds, the more tedious parts of this occupation are eliminated, and we are left with the most intense and nerve-wracking decision-making.How we built it: HTML/CSS/JS/jQuery was used to develop the game frontend which is hosted on Cloudflare PagesThe backend runs on Cloudflare Workers ensuring scalability for the long run.Gemini API is used to generate unlimited new scenarios for players to experience.MongoDB stores existing scenarios.The .TECH domain name was utilized.,Challenges we ran into: Making a game without a game engine was more difficult than we thought. Without the abstractions of scenes and sprites, mundane commands such as showing and removing icons sometimes led to strange, difficult, and random errors.Time was a serious constraint. We initially did not think of the LLM integration, but when we started writing the game storyline file, a new appreciation for game development was gained as we realized that writing storylines was extremely time-consuming.Therefore, we decided to integrate the AI story writing into our game, which accelerated development of the storyline, but slowed us down due to the added complexity....and finally, CORS issues. A lot of CORS issues.Accomplishments that we're proud of: We're proud that we built a game in the short timeframe provided. Efthimios had never written a game before of any sort, and Heng had only experimented with some small games in easy-to-use game engines. In addition, it was our first time integrating Google Gemini into an application.What we learned: Efthimios learned a lot of HTML and CSS techniques. From zero to hero type ah learning \ud83d\ude24\ud83d\ude24\ud83d\ude24.Heng Ye learned the intricacies of developing game software. In addition, he learned how to utilize Gemini API, Cloudflare Pages, and Cloudflare Workers.What's next for QuaranTech: Voice chat with AI characters (doctors, patients, government officials)More reliable LLM storyline generation.,",
                        "github": "",
                        "url": "https://devpost.com/software/quarantech"
                    },
                    {
                        "title": "Tariffix",
                        "description": "Tariffs made simple \u2014 track, compare, and calculate in seconds.",
                        "story": "With tariffs becoming a popular topic in recent news, we saw a growing gap in consumer understanding of how these hidden costs influence everyday purchases.Tariffixwas born out of a desire to make that information accessible, understandable, and easy for the average American shopper.Tariffix is a dual-platform tool aChrome extensionand awebsitedesigned to help consumers understand the hidden layer of tariffs behind the products they buy.Browse smarteron sites like Amazon and Walmart with real-time insights into tariff-influenced pricing powered by AI.Look up HTS Codes(Harmonized Tariff Schedule) to understand import taxes on specific goods.Estimate how much companies pay in tariffs, and how those costs might be affecting you.,Tariffix was developed as amonorepofeaturing:ANext.js websitefor tariff education and HTS lookup.AVite-powered Chrome extensionto overlay data on online stores.Ashared AI backendthat scrapes metadata (product category, country of origin, etc.) and calculates estimates.A ** Pricing algorithm to ensure even with errors you ensure you get an estimate.AMongoDB databaseto store and update HTS codes and tariff rates.,Finding accurate, real-time tariff data was harder than expected.We initially planned to use the U.S. Department of Commerce\u2019sFTA Tariff Rate API, but it hadn\u2019t been updated since early 2024.To solve this, we created a custom dataset using web scraping and tariff tables, allowing us to estimate tariff costs across various goods.Built a seamless integration between extension and site.Developed a flexible backend for live tariff estimation.Made a complex economic concept easier for the average consumer.,Tariff data is fragmented and hard to access, centralizing it is valuable.Building for both web and extensions takes planning and modular design.UX matters a lot when explaining dry, complex topics like international trade.,Integrate real-time news on trade policy and tariff changes.Expand the database to include country-specific tariff rules.Partner with financial organizations and companies.Add visual graphs and price comparison tools for better insight.,",
                        "github": "https://github.com/Adr1an04/Tariffix",
                        "url": "https://devpost.com/software/tariffix"
                    },
                    {
                        "title": "BurnoutBuddy",
                        "description": "Burnout Buddy watches your steps, commits, and eye bags to decide if you need a nap, a meme, or to touch grass \u2014 all in the name of not losing your mind while coding.",
                        "story": "Inspiration: As students juggling deadlines, hackathons, internships, and side projects, we\u2019ve all felt the creeping burnout that\u2019s hard to name but even harder to fight. Most mental health apps felt too serious or clinical, we wanted something fun, light, and proactive.\nAfter too many late nights debugging and forgetting what sunlight looks like, we realized burnout doesn\u2019t announce itself, it just quietly takes over. We wanted to build something that could catch us slipping before we spiral, using the very data we already generate: our steps, our sleep, our GitHub commits, and our tired faces.Burnout Buddybecame our goofy, 3D self-care sidekick, combining real-world signals, AI, and playful visuals to remind us to hydrate, vibe, and breathe before burnout hits too hard.\uff1fWhat it does: Burnout Buddy is a personal wellbeing dashboard that tracks signs of burnout and helps you bounce back. It:Tracks physical and mental exhaustion using Fitbit data, GitHub commits, and real-time webcam fatigue detection.Calculates a personalized burnout score using sleep, steps, commits, and drowsiness levels.Offers timely interventions like hydration reminders, lo-fi Spotify tracks, and AI-generated motivational quotes.Provides journaling and task tracking to support mindful productivity.Visualizes burnout trends over time with interactive graphs.Encourages self-care before self-combustion \u2014 because burnout doesn\u2019t announce itself, it just sneaks in.,\ud83e\uddf1 How we built it: Frontend: React.js with custom CSS for styling, Chart.js for data visualization, and Spline for 3D embedding. Didn't forget Confetti (because vibes).Backend: Flask API to serve burnout score, quotes (via Google Gemini API), Fitbit and GitHub data.Data Pipelines: Python scripts to fetch Fitbit step/sleep data and GitHub commits, stored in MongoDB atlas.Fatigue Detection: Real-time drowsiness detection using OpenCV, MediaPipe, and dlib for webcam-based eye trackingDatabase: MongoDB Atlas stores time-series Fitbit/GitHub data and user journal entries securelySpotify Integration: Lo-fi music player powered by randomized Spotify Web Player embedsBurnout Scoring: Custom burnout score calculated from sleep, activity, commits, and fatigue using a weighted formula,\ud83c\udfaf Challenges we ran into: Integrating real-time webcam analysis using MediaPipe and dlib in a way that didn\u2019t crash on multiple machines.Merging data from multiple APIs into a clean and meaningful score (averaging is easy, burnout isn\u2019t).Making sure Flask, Mongo, and React played nice \u2014 and in sync \u2014 without one ghosting the others.React\u2019s spontaneous desire to break when the camera stream said hello.A major CORS and API communication between Flask and React issue that turned out to be not so major (that we still spent 5 hours debugging)Parsing Google Gemini API responses into usable JSON formats,\ud83c\udfc6 Accomplishments that we're proud of: Built a full-stack wellness app that blends data science, computer vision, and user-centered designSuccessfully integrated real-time webcam-based drowsiness detection using OpenCV, MediaPipe, and dlibConnected to both the Fitbit and GitHub APIs to gather personal activity data securelyDesigned a burnout scoring algorithm that combines physical, digital, and visual fatigue metricsCreated a clean, fun, and interactive UI with features like Spotify embeds, journaling, and task trackingVisualized personal trends in sleep, steps, and commits using Chart.js\u2060Built a burnout detection tool that we\u2019d genuinely want to keep using after this weekend.,\ud83d\udcda What we learned: How to wrangle health, code, and chaos into a meaningful burnout metricReal-time computer vision can absolutely be combined with soft UX to create something empatheticHow to use OpenCV, MediaPipe, and dlib for facial fatigue detectionHow to integrate and authenticate with Fitbit and GitHub APIsHow to process and visualize time-series data using Chart.js in ReactThe value of designing experiences that care for the user, not just inform themSometimes, you really do need to listen to a dashboard telling you to hydrate,\u23ed\ufe0f What's next for BurnoutBuddy: Scale to companies as a team wellness tool, offering anonymous burnout trends without compromising privacyAdd journaling insights, mood tracking, and burnout trend visualizationsPush smart alerts for burnout spikes \u2014 think \u201cgo outside\u201d or \u201cclose VS Code\u201dIntegrate calendars and build a Chrome extension that nudges users away from doomscrollingLaunch internally for dev teams \u2014 then gently expand into the wellness SaaS spaceOffer personalized recovery suggestions using machine learning trained on individual usage patterns and habits,",
                        "github": "https://github.com/CSin007/Bitcamp2025",
                        "url": "https://devpost.com/software/burnoutbuddy"
                    },
                    {
                        "title": "Jesture",
                        "description": "Jesture is a magical gesture-based drawing experience where your hands become the paintbrush! Wave your arms about and watch your beautiful (or hilariously terrible) art manifest on your screen!",
                        "story": "Inspiration: The inspiration forJesturemainly comes from popular fiction. The idea started as one one-off comment by a couple of my friends who thought it would be interesting to be able to control your computer through hand gestures like in Iron Man, where Tony Stark uses hand gestures to control devices or interact with virtual environments. This concept felt futuristic and exciting, but we also saw a practical application: making technology more accessible and intuitive. Gesture-based controls can provide an alternative interface for individuals with limited mobility or those who find traditional input devices like keyboards and mice challenging to use. By combining creativity and accessibility, we aimed to create a tool that is not only fun but also empowers users to interact with technology in a more natural and inclusive way.A paint application is the perfect demo for this new idea as it showcases the potential of gesture-based controls in a highly interactive yet fun way. Drawing is a universal activity that doesn't require any prior technical knowledge, making it accessible to users of all skill levels (even kids, in this case). By allowing users to create art with simple hand movements, the application demonstrates how natural gestures can replace traditional input devices like a mouse or stylus. A paint application also highlights the precision and responsiveness of the gesture recognition system. \"The artists\" can see immediate feedback as their gestures translate into lines, shapes, and colors on the canvas, making the technology feel seamless and engaging.Generally, a paint program provides a solid base to explore features, whether it be the general controls of a gesture-based system or integrating Gen-AI to guess the user's creations.What it does: Jesture is an interactive drawing and guessing game powered by gesture recognition. Users can draw on a virtual canvas using hand gestures, switch between drawing modes (free draw, line, circle), and have an AI guess what they are drawing. It combines creativity, technology, and fun into one seamless package.How we built it: MediaPipefor real-time hand gesture recognition.Raylibfor rendering the drawing canvas and UI.OpenCVfor capturing webcam input.GoogleGenAI for generating AI-based guesses of user drawings.Pythonas the primary programming language to integrate all these technologies.Tensorflowused to train the hand gesture recognition (thumbs up/thumbs down/pointing/etc.),Challenges we ran into: Fine-tuning gesture recognition model to ensure accurate and responsive controls.Managing real-time performance while processing webcam input, rendering graphics, and running AI predictions simultaneously. (Some of my earlier implementations were painfully slow),Accomplishments that we're proud of: Successfully integrating gesture recognition with a drawing application.Creating an \"objectively\" fun and interactive experience where AI guesses user drawings.Building a smooth and responsive interface that feels intuitive to use.,What we learned: How to work with MediaPipe for gesture recognition and its potential for other applications.The power of Raylib for creating interactive graphics in Python.The importance of optimizing performance when working with real-time systems.How to integrate AI models like Google GenAI into creative projects in a fun way,What's next for Jesture: Adding more gesture-based controls?A multiplayer mode update where users can collaborate on drawings?Other platforms like mobile?,",
                        "github": "https://github.com/jinson-j/jesture",
                        "url": "https://devpost.com/software/jesture-sa1v02"
                    },
                    {
                        "title": "Techmate",
                        "description": "From ABCs to APIs: Giving Back Through Tech",
                        "story": "Inspiration: Watching our grandparents struggle with modern technology sparked the idea for TechMate. We realized the generation that taught us life skills now needs our help navigating the digital world. Their patience with confusing interfaces and vulnerability to tech-related scams motivated us to create a bridge between generations through technology.What it does: TechMate empowers seniors to use technology confidently with:\ud83c\udf99\ufe0f Voice-first AI assistance for instant tech help\ud83d\udc68\ud83d\udc67 Human-in-the-loop support from members around the globe\n\ud83d\udcda Interactive tutorials tailored for senior learnersHow we built it: Tech Stack:Frontend: React + TypeScript with senior-friendly UI patternsVoice AI: Web Speech API + OPENAI LLM integrationBackend: Firebase (Auth/Firestore) + Flask,Challenges we ran into: Initially struggled with choosing optimal LLMs for transcription and the best frontend framework; settled on React for its robust ecosystem.Integrated real-time transcription display into the user interface.Incorporated video upload into the voice input workflow seamlessly.Implemented efficient ticket escalation between family members.Ensured strong data protection for seniors without compromising usability.,Accomplishments that we're proud of: Initially struggled with choosing optimal LLMs for transcription and the best frontend framework; settled on React for its robust ecosystem.\nIntegrated real-time transcription display into the user interface.\nIncorporated video upload into the voice input workflow seamlessly.\nImplemented efficient ticket escalation between family members.What we learned: How to evaluate and select the best large language models for transcription tasks.The benefits of using React for a flexible and robust frontend integration.Integrating real-time transcription display into the user interface.Techniques for incorporating video upload functionality seamlessly.,What's next for TechMate: Smart notifications for critical tech updatesAdd 10+ regional language supportsImplement video call coaching featurePartner with retirement communitiesAI-powered scam detection system,\ud83c\udf0d Long-term Vision:Become the go-to intergenerational tech literacy platform worldwide.",
                        "github": "https://github.com/swetapati22/TechMate",
                        "url": "https://devpost.com/software/techmate-f9yu4j"
                    },
                    {
                        "title": "Locked Out",
                        "description": "Lock out so you can stay locked in",
                        "story": "Inspiration: We were inspired by the growing need for digital self-discipline and academic accountability. As students ourselves, we often struggled to stay focused while working on assignments due in the near future. This app was built to help lock ourselves out of distracting websites while working on real-time Canvas assignments\u2014turning procrastination into productivity.What it does: Locked Out integrates with a user's Canvas account to fetch upcoming assignments and lets them assign website block rules to each one. Once an assignment is selected, users are asked to schedule a time to start the block. When confirmed, these rules are activated\u2014helping the student stay focused on their task.How we built it: React + Next.js for the frontend and routingTailwind CSS for UI design and modalsSupabase as our database layerNextAuth.js for Google login and session handlingCanvas LMS API to fetch real assignment data for UMD studentsChallenges we ran into: One of the toughest challenges was syncing authentication between our Next.js web app and a separate Chrome extension, especially when trying to maintain session continuity. Since NextAuth manages the session in a secure, server-only context, we had to find creative ways to securely pass session tokens or identifiers to the extension without compromising user privacy or introducing security flaws.Another challenge was bridging data between the backend and frontend \u2014 specifically determining whether a student had submitted a given assignment (fetched from Canvas) and correctly displaying that status on the frontend. Canvas\u2019s API requires careful pagination and filtering by term, and we had to ensure we only displayed relevant, active assignments that weren\u2019t past due \u2014 even allowing a small buffer window for recent due dates.Accomplishments that we're proud of: Built an actual, functioning integration with Canvas that dynamically pulls real student assignments.Created a polished UI with modals and stateful logic using Zustand and Supabase.Enabled per-assignment website blocking rules to help users reduce distractions.Designed a scalable backend API for managing and updating site blocklists.,What we learned: This was our first time using Next.js, so we learned a ton about how everything fits together \u2014 from server components to API routes and auth. Figuring out how to manage state across client and server felt tricky at first, but we got the hang of it by working through real problems like fetching assignments from Canvas and syncing them into our UI.We also got more comfortable with tools like Supabase, NextAuth, and Zustand, and saw how helpful they are when building a full-stack app quickly. It was also our first time wiring up a Chrome extension with a web app, which definitely came with challenges (especially around keeping auth in sync), but it pushed us to think about the bigger picture of user experience.What's next for Locked Out: Mobile app to block distracting apps as wellUsers can have multiple rule profiles(different sets of websites to block)Streaks and other personalization rewards for staying locked in,",
                        "github": "https://github.com/davidshukhin/lockedout",
                        "url": "https://devpost.com/software/locked-out"
                    },
                    {
                        "title": "TaxWow",
                        "description": "TaxWow simplifies taxes using AI. Upload statements or receipts, and it classifies expenses, calculates deductions, and generates audit-ready reports with OpenAI + RAG.",
                        "story": "",
                        "github": "https://github.com/BlueSinkers/bitcamp25/tree/main",
                        "url": "https://devpost.com/software/taxwow"
                    },
                    {
                        "title": "NutriFit",
                        "description": "An app where you input ingredients as well as your desired macros and AI will generate potential meals for you to create and try with the macros listed and instructions on how to prepare.",
                        "story": "Inspiration - Being gym bros: What it does - Helps people plan their meals to allow them eat healthy and reach their desired goals as well as helping to keep a steady diet.: How we built it - Used a multitude of coding languages to create an app that can be used on your personal smart phone: Challenges we ran into - Accessing a database of nutrition facts for foods,: Accomplishments that we're proud of - Successfully utilizing AI: What we learned - How to use APIs: What's next for NutriFit - Making it public:",
                        "github": "",
                        "url": "https://devpost.com/software/meal-planner-app"
                    },
                    {
                        "title": "SignToText",
                        "description": "Unfinished project of a browser extension for ASL users to have a \"sign-to-text\" feature for writing on google docs. We are beginning with support for the ASL alphabet",
                        "story": "Inspiration: Accessibility needs and painful times sitting at long desksWhat it does: Accesses the computers camera through a browser extension, it was supposed to track hand landmarks but we were not able to complete that part.How we built it: 3 phases: 1 - Developing browser extension, 2 - Accessing the computer's camera, 3 - Tracking signsChallenges we ran into: Tracking hand landmarks, and accessing the camera due to browser restrictionsAccomplishments that we're proud of: First hackathon! Was able to actually develop a browser extension that accesses the camera despite having no prior knowledge on the matter.What we learned: Typescript, React, HTML, CSS, how browser extensions work and how difficult they can be sometimesWhat's next for SignToText: Being able to track hand movements and upload signs to text!",
                        "github": "https://github.com/Blorpity/Bitcamp-SignToText",
                        "url": "https://devpost.com/software/signtotext"
                    },
                    {
                        "title": "Testudo Match ",
                        "description": "TestudoMatch: Find your perfect class schedule match at UMD. Swipe right on courses, not people",
                        "story": "Table of Contents: InspirationWhat It DoesHow It WorksFeaturesTech StackCurrent ProgressChallengesWhat's NextRun Locally,Inspiration: We felt like finding the right courses to take as a UMD student was often confusing and frustrating. Reading through the major website, scrolling through the endless prerequisites and requirements. That's why we wanted a tool to help inspire and guide us students on finding the best, tailored schedule for them.What it does: Testudo Match is a web application designed to help students at the University of Maryland efficiently plan their class schedules. By leveraging natural language processing, students can input their schedule preferences in plain language, such as \"I want machine learning and database classes.\" The application then processes these inputs with semantic similarity search to generate schedules that students can \"swipe\" on.How we built it: Data CollectionCourse data from UMD.ioGrade distributions and reviews from PlanetTerpEmbedding GenerationHuggingFace\u2019s sentence-transformers converts course descriptions into high-dimensional vectorsSemantic Search with HuggingFace and LangChainCourse data is serialized and stored, then created into vector embeddings using HuggingFace's transformer and LangChain's MemoryVectorStore for fast similarity searchA natural language query is also embedded and compared against stored vectors to find semantically relevant courses. Embeddings contain semantic meaning of course data and query.Gemini NLPGemini used to determine user's major's course level based on text input,Challenges we ran into: Creating an intuitive search from plain text queriesDesigning the embedding and similarity system from scratchIntegrating multiple APIs with different formatsCleaning incorrect and malformed API data, mapping complex prerequisites relationships,Accomplishments that we're proud of: Successfully implemented the user of vector stores and serializing course data.Developed a search function that returns relevant courses based on user queries.Generating and ranking best schedules with optimal section times.,What we learned: How to effectively use embeddings for semantic search.The intricacies of integrating multiple APIs.The importance of designing user-friendly interfaces.,What's next for Testudo Match: Add UI components for:Adding \u201ctaken\u201d and \u201cplanned\u201d coursesTime conflict detectionFriend schedule comparisonsEnhance the user experience by showing time conflicts and other info.Implement features for comparing friend schedules.,Features: \ud83d\udd0d Natural language course search for suggested courses\ud83d\udcc8 Integration with real-time UMD/PlanetTerp APIs\ud83e\udde0 Contextual course recommendations using vector store embeddings and similarity searchUnderstanding course difficulty and level requests using GeminiRanking courses based on professor ratings and average GPACreating optimal schedules without time conflictsEditing schedules and adding/deleting courses\ud83d\udcc5 UI coming soon for:Class planningFriend schedule comparison,Tech Stack: Next.js\u2013 frontend & backend frameworkGemini- interpreting student's desired course difficulty and course levelLangChain MemoryVectorStore\u2013  perform similarity search and rank coursesHuggingFace Sentence Transformers\u2013 converting course description and user query into numerical vecteors,-UMD.io&PlanetTerp API\u2013 real course and review data: Run Locally: Scrape data from APIs and save data as Hugging Face embeddings in vector stores for searching later.Run website locally",
                        "github": "https://github.com/yyrichy/scheduleit/tree/main?tab=readme-ov-file#run-locally",
                        "url": "https://devpost.com/software/testudo-match"
                    },
                    {
                        "title": "FitBrawl",
                        "description": "FitBrawl: AI exercise competition in real-time. We track your reps so all you have to do is log in, Compete, and Win.",
                        "story": "",
                        "github": "https://github.com/KevinShilla/FitBrawl",
                        "url": "https://devpost.com/software/fitbrawl"
                    },
                    {
                        "title": "StudyRight",
                        "description": "Meet StudyRight \u2014 your smarter way to study.",
                        "story": "## InspirationLet\u2019s be honest \u2014 we\u2019ve all been there. Mindlessly doom-scrolling through short-form videos for hours. In fact, the average TikTok user spends around 95 minutes a day on the app \u2014 and for many college students, that number climbs to 3\u20134 hours daily.And as software engineers or college students, we\u2019ve all had that classic moment: sitting in class thinking, \u2018I\u2019ll just study this later.\u2019 Then later comes\u2026 and the regret sets in.That\u2019s where StudyRight comes in. We\u2019ve built a tool that turns your study notes into engaging, AI-generated short videos \u2014 tailored for your short attention span. Instead of fighting your content addiction, we\u2019re using it to your advantage.Learn smarter, not harder \u2014 with StudyRight.## What it doesStudyRight takes lecture notes or study materials provided by the user and uses AI to generate short, visually engaging explainer videos. These videos are designed in the style of popular short-form content, making it easier for students to retain information in a format they\u2019re already addicted to. Users can input their notes, choose a style or topic focus, and get back a digestible, entertaining video that helps them study on the go \u2014 whether they\u2019re commuting, procrastinating, or just in need of a quick review.##How we built it?I built this web application using Flask as the backend framework, MongoDB for storing user and video metadata, and AWS S3 for hosting the generated video content. The app allows users to upload lecture notes, which are processed into educational short-form videos using AI. I used JWT authentication to securely manage user sessions, and the frontend, built with React, communicates with the Flask API to handle file uploads, trigger video generation, and display the final videos. The system is modular, scalable, and leverages cloud storage for efficient video delivery.## Challenges we ran intoOne of the biggest challenges we faced was integrating multiple technologies \u2014 from frontend video input to backend AI processing \u2014 into a seamless user experience. Converting raw lecture notes into coherent, engaging video scripts required fine-tuning our AI prompt design to get consistent, high-quality outputs. We also ran into issues with video generation speed and file handling, especially when dealing with large media assets and cloud storage through AWS S3. Debugging compatibility between services and ensuring everything ran smoothly across different devices and environments definitely pushed us to think creatively and troubleshoot efficiently.## Accomplishments that we're proud ofWe successfully built a full-stack AI-powered web application that transforms lecture notes into short educational videos. We integrated complex systems like AWS S3 for cloud storage, MongoDB for dynamic data handling, and JWT for secure user authentication. We also implemented a seamless frontend-to-backend flow using React and Flask, and learned how to work with APIs like Gemini for content generation. This project reflects our ability to combine AI, cloud, and web technologies into a working product \u2014 from user login to delivering final videos in the browser.## What we learnedThroughout this project, we learned how to design and build a complete end-to-end web application using modern technologies. We gained hands-on experience with integrating cloud services like AWS S3 for file storage and MongoDB for database management. We learned how to securely manage users with JWT authentication and how to build scalable backend APIs using Flask. On the frontend, we deepened our understanding of React and how to handle file uploads, state management, and UI feedback during asynchronous tasks. Most importantly, we learned how to connect all these tools to work together, and how to troubleshoot and debug real-world integration issues.## What's next for StudyRightWe envision expanding StudyRight into a mobile app, allowing students to learn anywhere, anytime \u2014 whether on a commute or in between classes. We also plan to add collaboration and sharing features so study groups can upload notes together and learn from each other\u2019s videos. Additionally, personalized learning paths will use AI to analyze users' habits and content to recommend tailored study videos, helping students focus on what they need most.",
                        "github": "",
                        "url": "https://devpost.com/software/studyright"
                    },
                    {
                        "title": "Patch",
                        "description": "Our platform is a chat solution lets tenants report property issues with photos, auto-generates AI powered legal letters, and helps landlords manage repairs.",
                        "story": "Inspiration: Low-income individuals often face difficulty getting hazardous rental issues addressed, as some landlords  avoid spending money or effort on necessary repairs. There hasn't yet been a seamless platform dedicated to tenant rights and clear communication between tenants and landlords, prioritizing tenant safety and legal protection.What it does: Patch is a comprehensive communication app designed for landlords and their tenants. Landlords can add tenants by sending email invitations, creating customized chat rooms for seamless interaction. Tenants can report property issues by uploading photos directly to their landlord. These photos are automatically classified by AI into specific issue types such as mold, pests, or structural damage. Furthermore, tenants receive automated notifications detailing the specific legal timeframe allowed in their state to file a claim if the reported issue is not resolved promptly. Tenants are also provided with automatically generated, state-tailored legal reports ready for submission to authorities.How we built it: We built a sophisticated custom user interface featuring real-time chat to ensure frictionless communication between landlords and tenants. For our AI-powered issue detection, we developed a custom dataset using advanced web scraping techniques to gather relevant imagery, carefully curating and labeling it manually. Leveraging Roboflow, we created an accurate image classification model capable of identifying mold, pests, and structural damage. To further enhance tenant advocacy, we integrated Google's Gemini AI to dynamically determine state-specific legal timelines for issue resolution and to automatically generate customized legal complaint letters for each reported issue.Challenges we ran into: One significant challenge was managing image uploads. Most databases offering image storage had limited free tiers, prompting us to pivot towards using AI to interpret images into concise, textual descriptions instead of storing full images directly. Additionally, web scraping resulted in substantial irrelevant data, necessitating manual filtering and validation to ensure dataset quality and model accuracy.Accomplishments that we're proud of: One significant challenge was managing image uploads. Most databases offering image storage had limited free tiers, prompting us to pivot towards using AI to interpret images into concise, textual descriptions instead of storing full images directly. Additionally, web scraping resulted in substantial irrelevant data, necessitating manual filtering and validation to ensure dataset quality and model accuracy.What we learned: Throughout this project, our team significantly improved collaboration skills, using GitHub effectively for organized development and version control. We mastered agile problem-solving, quickly pivoting to alternative solutions when initial approaches were unfeasible. Importantly, we learned how to seamlessly integrate multiple AI models to deliver a functional app with genuine, tangible societal benefits.What's next for Patch: Looking ahead, Patch plans to expand its AI capabilities by incorporating more extensive datasets to identify a broader spectrum of housing issues. We envision integrating AI-powered predictive maintenance alerts for landlords, helping prevent issues before they arise. Additionally, we aim to include multilingual support to increase accessibility, implement community-driven tenant reviews for accountability, and collaborate with local housing authorities to streamline complaint filings directly through our platform.",
                        "github": "https://github.com/Rishav-N/patch",
                        "url": "https://devpost.com/software/patch-rge1n4"
                    },
                    {
                        "title": "Buttercup Corner",
                        "description": "Our website enables users to design and output a food-themed avatar. The user can choose any combination of bodies, mouths, and eyes, and the website will output the desired character.",
                        "story": "Inspiration: We first came up with the idea to have a website that enabled users to build their own avatars from certain body parts. This then transitioned into a food-themed avatar idea after we began brainstorming and decided to choose different cute food items as the base.What it does: Our website allows the user to select a body, eye style, and mouth style. All of these selections will be outputted to the bottom of the screen, where the user can view their new food friend!How we built it: To start, we created a GitHub repo implementing React. To better understand how we could incorporate React into our project, we attended the Peraton React workshop where we learned more about all the ways that React can benefit our project. We then began coding in TypeScript, using CSS components for design, creating the homepage and other pages in our single-page website design.Challenges we ran into: At the very beginning of the project, many of our team members did not have React installed or set up. This took a long time for us to resolve since it was the first time many of us had worked with React. Another challenge was keeping our code up to date between our members. We occasionally ran into merge conflicts where two members had edited the same file. The biggest challenge we faced was understanding the syntax and learning the general structure of typescript and CSS.Accomplishments that we're proud of: We are very proud of our design implementation and concept, since it felt very cohesive throughout all the website pages. We are also very proud of our ability to learn and implement the basics of TypeScript, and have it output in a complex way onto our website.What we learned: One thing very important we learned about design is the importance of pre-planning a project before jumping into the code. We found that our extensive pre-planning, which took place Friday night, enabled us to visualize and better structure and even edit our greater concepts.What's next for Buttercup Corner: Another avenue to take our project we had brainstormed during planning involved enabling the users to fill out a questionnare, which would then translate into their own unique avatar. The questionnare would be designed to be somewhat comical, with three different output options based on the user's preferences (cook, burn, or roast).",
                        "github": "https://github.com/allisonayu/bitcamp-hackathon",
                        "url": "https://devpost.com/software/buttercup-corner"
                    },
                    {
                        "title": "Anchor: Grounded in Nature",
                        "description": "A calming companion that grounds your mind, nurtures your growth, and brings peace through science-backed research and nature.",
                        "story": "Inspiration: Our inspiration for Anchor came from a shared desire to support those navigating delusions, paranoia, and other serious mental health challenges\u2014communities that are often underserved by traditional wellness apps. We were inspired by the story of a schizophrenic man and his service dog who was trained to alert and greet people and helped the man differentiate between his delusions and reality. Recognizing the calming and restorative power of nature, and the potential of AI-driven support, we envisioned Anchor as a gentle, nature-themed companion to help users feel safe, grounded, and seen.What it does: Anchor is a mental wellness assistant designed to help users feel more connected and stable in their daily lives. It offers:\ud83d\udcdd Journal Diary with AI Chat: Users can write journal entries and receive supportive, therapist-style responses from a Google Gemini-powered chatbot. The AI offers empathy, perspective, and helpful mental health recommendations.\n\ud83c\udf19 Sleep Tracker: A simple interface where users log the length and quality of their sleep, helping them track and improve sleep habits over time.\n\ud83c\udf43 Grounding Exercises: Scientifically-backed activities that prompt users to focus on their surroundings by finding and observing objects with specific characteristics (like color, texture, or shape)\u2014a powerful way to ease anxiety or delusional thoughts.\n\ud83c\udf38 Flower Garden Rewards: A gentle gamification element that rewards healthy habits. Completing tasks like writing journal entries unlocks new flowers, visually representing progress and care.How we built it: Our app was built using JavaScript in the React Native framework, using CloudFlare on the back-end. We leveraged the Gemini API to respond to the user's journal entries and assist them with grounding.Challenges we ran into: We were faced with tackling a brand new framework and testing with Expo Go. We were ambitious with our goals, and constantly widened our scope, resulting in a race against time. This was our first time using CloudFlare, React Native, and Gemini API.Accomplishments that we're proud of: We are proud of all of the features we were able to implement into our app, such as AI powered responses to journal entries, a weekly report of these journal entries along with the user's sleep schedule, and a grounding exercise that prompted the user to find objects that meet specific visual characteristics.What we learned: We became more familiar with using services such as using Wrangler to deploy Cloudflare workers, React Native, and Gemini API. We also learned that we are capable of making a tangible impact on marginalized communities.What's next for Anchor: Grounded in Nature: We want to add more grounding features and scale our solution connecting health care professionals to our users and other users of our app to foster a community under our service.",
                        "github": "https://github.com/dhruvdasa/Bitcamp2025",
                        "url": "https://devpost.com/software/anchor-grounded-in-nature"
                    },
                    {
                        "title": "Dino Money",
                        "description": "With Dino Money, you can track your budgets effortlessly, giving you a clear picture of your spending habits. You can create a connection with your budgeting journey through an adorable pet companion.",
                        "story": "Inspiration: Our project was inspired by thenostalgia of old Tamagotchi games and Pok\u00e9mon, where caring for a virtual pet added a layer of fun and engagement. We wanted to create something that would resonate with students like ourselves\u2014somethingfun and excitingthat could also address a serious concern:financial responsibility.As college students, we understand that managing money is often a significant worry. Many of us have family members who have struggled withimpulse buying, leading us to realize the importance of instilling good financial habits early on. We also wanted to help younger kids learn aboutfinancial responsibility, making it a core value of our project.Building the Project: We built our project primarily usingReactfor the front end andFirebasefor the back end. In the beginning, we attended multiple workshops to explore various technologies and determine the best direction for our app. This foundational knowledge was crucial in shaping our vision.However, the journey wasn't without its challenges. As this was our firstBitcamp, we found the experience a bit daunting. We faced time conflicts over the weekend, with some team members juggling work and other commitments. Additionally, many of the technologies we were using were new to us, which added to the complexity of the project.Challenges Faced: Throughout the development process, we encountered several commonroadblocksof programming, including:Bugsthat seemed to appear out of nowhere.Services that wouldn\u2019t communicate with each other as expected.Merge conflictsthat arose during collaboration.Outdated packagesthat needed to be updated, causing further delays.,These challenges tested our resilience and problem-solving skills, but they also provided valuable learning experiences.Key Takeaways and Lessons Learned: Our journey taught us several important lessons:Wireframing is Essential:Having a clear wireframe of the app helped us visualize our ideas and stay organized.Skill Overestimation:We learned that it\u2019s easy to overestimate our skills and underestimate the time required to complete tasks.Don\u2019t Bite Off More Than You Can Chew:We realized the importance of setting realistic goals and timelines for our project.Leveraging AI:We discovered that AI can be a useful tool when leveraged properly, helping us streamline certain processes.,Conclusion: In the end, our financial app project was not just about creating a tool for budgeting; it was aboutlearning, growing, and collaboratingas a team. We are proud of what we have accomplished and excited to see how our app can help students and younger kids develop healthier financial habits.",
                        "github": "",
                        "url": "https://devpost.com/software/dino-money"
                    },
                    {
                        "title": "Cook Mate",
                        "description": "CookMate uses AI to transform how you decide what to cook. Simply tell the app what your preferences are, and You can have a live conversation with an AI agent that will coach you through.",
                        "story": "Inspiration: The idea for CookMate was born from a simple yet profound realization our team shared during the pandemic: cooking alone isn't nearly as enjoyable as cooking with others. As a group of friends who grew up in households where the kitchen was the heart of our homes, we missed the spontaneous cooking sessions with family and friends\u2014the laughter, the shared discoveries, and the inevitable taste tests that happened along the way.We noticed three problems that needed solving:What if technology could help us recreate that communal cooking experience, even when physically apart? What if AI could eliminate the frustration of meal planning and reduce food waste? These questions drove our team of four to create CookMate.What it does: CookMate is an AI-powered social cooking platform that transforms how we approach meal preparation in three key ways:We integrated a real-time AI agent that can guide the user through the cooking process as if the user is on a phone call with a friend.Natural Conversation Flow: Our proprietary interruption detection technology enables users to speak naturally mid-recipe, just as they would with a human cooking partnerUltra-Responsive Interaction: Advanced amplitude monitoring delivers precise speech recognition with industry-leading 300ms response time\u2014virtually eliminating the awkward pauses of traditional voice assistantsIntelligent Context Awareness: Our AI intuitively recognizes cooking intents, automatically managing multiple timers and sending timely notifications without explicit commands,At the core of CookMate is our advanced AI recommendation engine. Unlike traditional recipe apps that require you to search for specific dishes, our system works backward:Pantry-First Approach: Simply input what ingredients you have available, and our AI suggests recipes you can make right now.Preference Learning: The system learns your taste preferences, dietary restrictions, and cooking skill level over time.Waste Reduction: Receive suggestions for using ingredients nearing their expiration date.,CookMate isn't just about recommendations\u2014it's a vibrant community of food enthusiasts:Community Recipes: Discover dishes created and shared by other users.Cultural Exchange: Explore authentic recipes from diverse culinary traditions around the world.Personal Cookbook: Save, customize, and share your favorite recipes in your digital cookbook.,How we built it: Our four-person team divided responsibilities based on our individual strengths:Frontend Development: One team member focused on creating an intuitive Flutter interface with responsive designBackend & AI: Two team members specialized in the recommendation engine and TerpAI integrationUX Research & Design: One member led user testing and interface design,CookMate is built using Flutter for cross-platform functionality, ensuring a seamless experience across iOS, Android, and web platforms. The app's architecture includes:Frontend: Flutter with Riverpod for state management and responsive UI designBackend: Azure for authentication, database, and storage of user profiles and recipesAI Component: Gemini 2.0 flash for chat and OpenAI 4.0 mini for live ai assistance,Our recommendation engine combines collaborative filtering (based on what similar users enjoy), content-based filtering (analyzing recipe attributes), and constraint satisfaction (matching available ingredients) to provide personalized suggestions that are both practical and delightful.The user interface was designed with simplicity in mind, featuring intuitive navigation, clear recipe instructions with step-by-step photos, and seamless video integration for social cooking sessions.Challenges we ran into: Building CookMate presented several significant challenges:Accomplishments that we're proud of: As a team of four diverse creators with complementary skills, we're particularly proud of:What we learned: Developing CookMate taught our team invaluable lessons:What's next for Cook Mate: We're just getting started. Our team's roadmap includes:Ingredient Recognition via Camera: Scan your refrigerator and pantry to automatically inventory ingredients.Smart Kitchen Integration: Connect with smart appliances to receive precise cooking guidance.Seasonal and Local Optimization: Recommendations that prioritize in-season and locally available ingredients.Expanded Food Education: Tutorials on cooking techniques integrated seamlessly into recipes.Global Cooking Challenges: Community events that bring together cooks from around the world.Meal Planning and Nutrition: Advanced meal planning tools that consider nutritional balance across days or weeks.Personalized Culinary Courses: AI-curated skill-building paths based on user interests and current abilities.,CookMate isn't just an app\u2014it's a return to the communal joy of cooking in a digital age. Our team of four is building technology that doesn't replace human connection but enhances it, creating virtual kitchens where memories are made and stories are shared, one recipe at a time.Join us in revolutionizing not just what we cook, but how we cook with our mates.",
                        "github": "",
                        "url": "https://devpost.com/software/cook-mate-tghz3m"
                    },
                    {
                        "title": "Jiggy",
                        "description": "Jiggy is a computer vision-powered dance app where you challenge friends or practice by uploading videos\u2014real-time pose matching with color-coded accuracy scores.",
                        "story": "Inspiration: With apps like TikTok making short-form dance content mainstream, we thought\u2014why not turn dancing into a game where you can challenge friends or practice and get real-time feedback on how closely your moves match?What it does: Jiggy lets users upload a dance video (from TikTok or elsewhere) and either challenge friends or enter practice mode. Using computer vision, it compares body movements between the source video and the user's live or uploaded video. Accuracy is scored in real-time using color-coded feedback:\ud83d\udfe2 Great match\n\ud83d\udfe1 Okay match\n\ud83d\udd34 Needs improvementHow we built it: Pose Detection: Used MediaPipe to extract joint coordinates from both uploaded and live webcam videos.\nAngle Calculation: Used NumPy and trigonometry to calculate joint angles.\nData Handling: Stored and loaded pose data using Pickle (.pkl) files and Pandas.\nReal-time Comparison: Leveraged WebRTC for webcam feed, and Socket.io for syncing pose data in real time.Web App: Built the frontend with HTML/CSS, and backend using Python (Flask).Challenges we ran into: Pose Comparison Logic: Initially tried comparing joint positions in 5D graphs which was complex. Switched to moving average of joint angles for better results.WebSocket Instability: Encountered issues configuring URLs to public state and managing multiple users on the same WebSocket.Client-side Rendering: Transferring uploaded video to client side via WebSocket caused unexpected bugs.Live Feed Sync: Connecting Python script to Node webcam feed using Flask was unreliable and led to inconsistent launches.Accomplishments that we're proud of: Created a working proof-of-concept that shows real-time dance pose comparison.\nDeveloped a color-coded accuracy scoring system that provides intuitive feedback.\nSuccessfully integrated MediaPipe, WebRTC, and Flask to create a unified app experience.What we learned: Handling live media streams and syncing pose data across two video sources is harder than it sounds.\nImportance of keeping things modular\u2014small changes in one component (like socket routing) can break the whole chain.\nReal-time feedback systems require performance-optimized pose comparison techniquesWhat's next for Jiggy: Improve scoring logic using cosine similarity and temporal smoothing.\nAdd leaderboards, streaks, and badges to boost engagement.\nOptimize for mobile and support AR pose overlays for visual alignment cues.",
                        "github": "https://github.com/rahulnair307/Jiggy",
                        "url": "https://devpost.com/software/jiggy-352oxk"
                    },
                    {
                        "title": "Unhookd",
                        "description": "Everyone has habits\u2014good or bad. Unhookd makes it easy to build better ones and break the bad, keeping you accountable with just a tap.",
                        "story": "Inspiration: [addiction isn't bound by borders\u2014it's a universal issue]Addiction is a global public health crisis. Over 1.25 billion people use tobacco worldwide. Youth electronic cigarette use is skyrocketing, tripling in the UK and doubling in Canada in recent years. The World Health Organization warns that aggressive marketing is hooking children early.Alcohol use is equally concerning. Nearly 50% of young adults (18\u201325) globally report drinking in the past month, and 29% binge drink, increasing risks of dependence, depression, and memory loss. 2.6 million alcohol-related deaths per year, stressing that no level of alcohol is \"safe.\"The takeaway? Traditional interventions aren't enough.Unhookd is our mobile-first solution. It\u2019s designed to meet people where they are\u2014on their phones\u2014with tools for self-tracking, daily check-ins, journaling, and progress reports. It\u2019s discreet, easy to use, and backed by data.We\u2019re not here to replace therapy, we\u2019re here to make recovery more accessible, especially for young people navigating addiction in silence. With Unhookd, they can take the first step anytime, anywhere.How we built it: We began by designing the user interface in Figma, focusing on a clean, intuitive experience for users managing their recovery journey. The front end of the app is developed in Swift for iOS, while the back end uses Python to analyze data from user reflections and daily check-ins. This combination allows us to track progress over time and generate personalized insights that support habit change.Challenges we ran into: We faced several challenges throughout development. Initially, designing a cohesive prototype that truly addressed user needs was difficult. Our early design pages felt clustered and required several iterations to streamline. Translating the prototype into working core functionality also proved complex, especially when ensuring the experience remained intuitive. Additionally, setting up the database to store user check-ins and reflection data securely and efficiently was a technical hurdle we had to overcome.What we learned: We learned how to transform an abstract idea into a tangible solution through design, collaboration, and technical development. We also gained insight into user-centered design and how challenging\u2014but rewarding\u2014it is to build something that addresses a real human need.What's next for Unhookd: Our vision for Unhookd goes beyond just tracking habits \u2014 we aim to build a secure, intelligent recovery companion. Future plans include encrypted data protection to safeguard user privacy, smart insights powered by machine learning to personalize the recovery journey, and integration with health apps and wearables for a holistic view of progress. We want Unhookd to feel empowering, motivational, and trustworthy \u2014 a tool that meets users where they are and grows with them.",
                        "github": "https://github.com/R4ushan/Unhookd",
                        "url": "https://devpost.com/software/unhookd"
                    }
                ],
                [
                    {
                        "title": "API - A Prehistoric Island",
                        "description": "API - A Prehistoric Island - where students boast healthy Achievements, are Protected knowing local rules, and be Inspired with ideas. A whole new adventure, way back in time.",
                        "story": "\ud83e\udd95Inspiration: College life can feel like being dropped into a mysterious prehistoric jungle - full of chaos, hunger, unknown rules, hidden chances, and the occasional Testu-Rex. We wanted to build a mysterious amazing web application that offers games and tools to make campus life easier and a tad more prehistoric for our fellow students.So we asked ourselves:\"What if there was a prehistoric island - API (you see it right?) - where students could keep track of their healthy achievements, be protected knowing essential rules, and spark inspiration from complex, lengthy materials - but at the same time feeling like they are taking part in an epic quest?\"That (and a LOT of caffein) is howAPI: A Prehistoric Islandwas born. A world that\u2019s easy for even cavemen, built by future tech wizards.\ud83e\uddedWhat it does: API, apart from you-know-what, also stands for Achievement, Protection, and Inspiration\u2014the three pillars behind our 3 star features:Quest for Campus EmbersA gamified campus quest where students turns into Hunters, Gatherers and Trackers, earn fiery streaks and top the Lava Leaderboard by checking in regularly with healthy habits like exercises and nutritious diet.DinoLawsDrop the location, and it gives you a simple, funny explanation of must-know laws and campus rules tailored to the area, should it be UMD, Hogwarts or even your backyard! So simple even a caveman could understand. Read by the caveman himself.Ancient ScrollsUpload your study notes or docs, and it\u2019ll summarize them and even suggest creative project ideas. Students will leave chasing opportunities and with a voiceover guiding them on.\ud83d\udee0\ufe0fHow we built it: Frontend: HTML/CSS/JavaScript/Python with prehistoric-themed visuals and animationsBackend: Python with FastAPI, deployed on an AWS EC2 (Amazon Linux) instanceAI Magic: Powered by Gemini 1.5 Pro for ultra-fast and smart responsesVoice + Animation: Synced Terp\u2019s caveman narration with scroll animations and sound FXVersion Control: GitHub + pure survival instinctsIt\u2019s all lively, and purely Jurassic\ud83e\udea8Challenges we ran into: Rate Limits: Gemini got too close to daily quotas (just like us) when too many requests were called. We built smart testing strategies and alternating between models to utilize daily requests.Lack of Experience: One of the team members has zero experience with web/backend/frontend/full stack development, yet through sheer delulu, the hackathon resources and the kind help of their teammates, pulled of the DinoLaws feature!Voice Syncing: Making the caveman\u2019s voice match the scroll carving animation took a lot of fine-tuning (and some mammoth-level patience).Theme Cohesion: Balancing humor with actual usability took several iterations. We had to make sure it wasn\u2019t just all jokes\u2014it had to bring meaningful benefits and smiles to our users' faces too.Thinking Back: We were too focus on moving forward that we forgot to include the Back (to main page) buttons in our features. A valuable lesson that will never grow old.\ud83c\udfc6Accomplishments that we're proud of: Fully deployed full-stack mini functions that combines to a single unified product with user benefits close to our heartsWas not afraid to shot for the moon and in this case, at meteors too!Made project brainstorming fun, walks in the rain productive, minds strong and friendships happenDeveloped a consistent theme with an easy to navigate, playful UI/UX that goes hand in hand with the hackathon's atheisticLearned new tools and APIs handling mid-hackathon and still finished strongly and proudlyActually made other hackers smiled during testing \ud83d\udc40\ud83d\udcdaWhat we learned: How to quickly create and deploy full-stack mini services to AWSLeverage and have fun with Google Gemini APIsHow to write good prompts for that mix clarity, humor, and usefulnessThat great UI/UX isn\u2019t just about art or design - it\u2019s about sympathizing with our usersA little humor goes a long way when solving real life pain pointsYou can wear more than one hats, which has been shown to us when the sponsors and alumni mingle with hackers to conquer problems and fight bugs together, and massive respect to our many volunteers!!! Your smiles and support kept us going.\ud83d\uddfa\ufe0fWhat's next for API \u2013 A Prehistoric Island: We\u2019re just getting started. Here\u2019s what\u2019s ahead:Add more tribes and roles so students can customize their campus journeyExpand DinoLaws to cover more regions and tailor to university-specific rulesLet users share achievements and compare scrolls with friends on social channelsBuild a full mobile version so you can carry the island in your pocket!Integrate calendar syncing, class data, and club events",
                        "github": "https://github.com/shreya222mishra/bitcamp-MLH.git",
                        "url": "https://devpost.com/software/api-a-prehistoric-island"
                    },
                    {
                        "title": "ProxyForce",
                        "description": "ProxyForce is an AI-powered project assistant that builds timelines, assigns tasks, answers questions, and performs risk analysis\u2014turning your project input into execution-ready plans instantly.",
                        "story": "Inspiration: What it does: It basically is your friendly, neighbouhood, virtual emplyee ;). Right now it can perform basic functions like gantt chart management ,etcHow we built it: We used AI, ReactJS, TailwindCSS and FastAPI to build our project.Challenges we ran into: Bugs ( A lot). We could not implement functionality fully because of time restrictions, although we have  semi ready API's.Accomplishments that we're proud of: We were able to complete a projec in first ever Tech Hackathon.What we learned: A lot! Time management, Fighting sleep, solving bugs, referring documentation etc.What's next for ProxyForce: Features will be rolling out for sure, The API's that are not fully functional will be updated. Functionality to Update task status realtime and ability to send mail and schedule meetings would be added.",
                        "github": "",
                        "url": "https://devpost.com/software/proxyforce"
                    },
                    {
                        "title": "CallerIDK",
                        "description": "Preventing scams and fraud through detecting AI voice clones",
                        "story": "Inspiration: AI-powered voice cloning has made deepfake scams more realistic, tricking people and organizations into compromising sensitive data or money. For example, a UK energy firm lost $243,000 to a deepfake impersonating their CEO. A mother was nearly tricked by a deepfake of her daughter's voice, claiming a kidnapping. These scams are financially and emotionally devastating, which motivated us to buildCallerIDK\u2014 a tool that uses AI to detect voice deepfakes and proactively protect sensitive data.What It Does: CallerIDKis an AI-driven voice fraud detection system designed to address the growing risks infinance. It identifies deepfake voices in real-time during calls or audio files, ensuring the speaker is authentic before any sensitive financial actions take place. This is crucial for services likebanking, where customers frequently make calls to open credit cards, transfer funds, or manage accounts.Once a deepfake is detected,CallerIDKactivates protective measures, such as:Obfuscating sensitive data(e.g., account numbers, PINs) to prevent future exploitation.Masking responsesfrom the user, making it harder for deepfakes to clone voices.Flagging and logging suspicious calls, providing a layer of defense against scams targeting financial institutions.,We also built additionaladd-onsto enhance the tool's utility in real-world applications:Hidden Ultrasonic Messages:Our obfuscation system includes ultrasonic frequencies that alter voice characteristics without affecting the human ear. This makes it extremely difficult for deepfake models to accurately clone the voice in the future.Background Object Detection:We added functionality to detect objects and sounds in the background of a call, offering further context to validate the call's authenticity, especially in suspicious scenarios.,How We Built It: Voice Deepfake Detection:We trained a machine learning model with real and synthetic voice datasets to detect acoustic patterns and waveform inconsistencies in calls.Obfuscation System:We implemented a voice obfuscation layer that subtly alters speech waves, making it difficult for AI to clone the voice in future scams. This protects sensitive information during calls.Hidden Ultrasonic Messages:The obfuscation layer uses ultrasonic frequencies to add hidden information, preventing AI models from successfully reproducing the original voice.Background Objects Detection:We used advanced techniques to detect sounds or objects in the background, enhancing security by providing additional contextual information about the caller's environment.Frontend Interface:A React-based dashboard displays detection results, flags suspicious calls, and logs details in real-time.Backend Infrastructure:We used Flask for backend API logic and deployed the app on Vercel for quick hosting.,Challenges: Finding high-quality, varied datasets for voice deepfakes.Achieving low-latency, real-time detection forfinance-relateduse cases.Balancing detection accuracy with a low false-positive rate, ensuring legitimate calls aren\u2019t flagged.Integrating obfuscation and hidden messages without compromising user experience or call quality.,Accomplishments: Achieved 80% accuracy in detecting deepfake voices on our test set.Built a real-time voice monitoring system with minimal lag, essential for financial transactions.Developed afull-stack working prototypewithin the hackathon window, including ourobfuscator,ultrasonic filter, andbackground detectionsystem.,What We Learned: Modern deepfake voice models are surprisingly sophisticated.Ethical AI is essential to protect, not just impress.Building both detection and prevention systems in parallel is challenging but rewarding.,What's Next for CallerIDK: Integrate intoVOIP systemsandcustomer service softwarein the finance sector.Addmultilingual supportto cater to global users in financial services.Train with more diverse datasets, including new voice generation models likeGPT-4 Voice.Package theobfuscation moduleas an API forbanking appsandfinancial institutions.Develop a browser extension forreal-time scam detectionon platforms likeWhatsAppandZoom, especially for financial calls.,",
                        "github": "https://github.com/sritarung/caller_idk",
                        "url": "https://devpost.com/software/caller_idk"
                    },
                    {
                        "title": "QuantumFiller",
                        "description": "We built a quantum-classical solver that restores missing and corrupt image data",
                        "story": "Inspiration: We had gone over image matrices in linear algebra class, so when we were researching different Ax = b quantum challenges. We decided to research to see if we could build something cool off at. We then looked into more real-world solutions that use linear systems and found out about image inpainting. We thought it was really interesting and decided to test a quantum vs classical approach in our solution.What it does: This project is our take on the Advanced Quantum Track problem of using a quantum algorithm to solve Ax=b. The idea behind it is to create an Ax=b matrix system from missing pixels in images and have items in x be the color value of the pixel corresponding to that vector item. We did show this in the classical solution which solved 2 missing pixels in a gradient image by using a numpy method to solve x = A^-1*b. While we did show a different Ax=b matrix solution done by the quantum algorithm, if the quantum algorithm VQLS was properly represented using the same gradient, and then extrapolated for x, it would be very similar to the classical solution but perhaps slightly off. For a small 8x8 matrix like this, the quantum algorithm would be overkill and the classical algorithm would outperform it, but the idea behind showing this example was to show that VQLS could be used to repair missing pixels in images (due to its property of being able to solve for a matrix equation) rather than using a classical solution. This is good because images can scale up to millions of pixels easily, and using a normal solution would take an enormous amount of time. A quantum algorithm would take a lot less time than a normal solution albeit would still be slow currently due to current computers not handling qubits well. In the future quantum algorithms will definitely be much better off compared to classical solutions in regards to, say repairing a movie just because of the time complexity advantage it has due to utilizing superposition.Challenges we ran into: Being beginners in Quantum Computing\nLots of dimension bugs\nUnderstanding things like how to build variational circuits, using the Hadamard test, or even just interpreting quantum outputs took a lot of time.Accomplishments that we're proud of: We got a working quantum circuit to solve a real math problem\nWe managed to model missing image pixels as a linear system and solve it both classically and quantumlyWhat we learned: We learned how to take a linear algebra concept like Ax = b and apply it to a real-world problem like image inpainting.\nWe tried out quantum programming for the first time and learned how variational algorithms and the Hadamard test work.\nMost importantly, we learned how to turn new concepts into something practical, while having a lot of fun figuring it out.What's next for QuantumFiller: Next, we want to scale QuantumFiller to handle larger images and more missing pixels using sparse matrix techniques. We\u2019re also exploring ways to optimize the quantum circuit further and test it on actual quantum hardware. Long term, we\u2019d love to apply it to real-world cases like restoring damaged satellite images or medical scans.",
                        "github": "https://github.com/seventhdraught/QuantumFiller/tree/main",
                        "url": "https://devpost.com/software/quantumfiller"
                    },
                    {
                        "title": "Gaadi",
                        "description": "Gaadi: how good of a driver are you, really?",
                        "story": "Inspiration: We were all arguing about who is a better driver and could not decide. Thus, for a way to quantify this, we created Gaadi: to really test how well you can drive.What it does: Gaadi is an app that allows a user to score their driving based on braking patterns, acceleration, and speeding. We track each trip and give the user an overall score of their trip that averages out to a total score.How we built it: We used Swift, Mapbox API, and motion sensing data from your iPhone to calculate breaking speeds, speed limits, and rates of acceleration.Challenges we ran into: We had a hard time implementing the Mapbox API as well as figuring out an accurate scoring system that averages out the score for the user. We did not want a scoring system that simply added and subtracted a set value every time the user braked hard or drove well, so we created a scoring system that is variable to each user and averages out their scores.Accomplishments that we're proud of: We are really proud of the speed limit system that we were able to implement. Grabbing the speed limit was not the easiest task as the Mapbox API we were using was very troublesome. However, by the end of the project window we were able to grab the information we needed.What we learned: We learned the basics and the intricacies of iOS development as most of the team did not have experience in iOS development.What's next for Gaadi: Leaderboard to compete with friends and other users, daily, monthly, and yearly summaries, and prizes.",
                        "github": "https://github.com/adityaswaikar/imABetterDriverThanYou.git",
                        "url": "https://devpost.com/software/gaadi"
                    },
                    {
                        "title": "FastPass",
                        "description": "Secure, personalized passwords that you will never forget.",
                        "story": "Inspiration: A few months ago, one of our team members had their Discord account hacked, which really highlighted how vulnerable we can be with weak or reused passwords. On the flip side, several of us have struggled with forgetting complex passwords we\u2019ve created for security's sake. We realized that most password generators either give you something super secure or something easy to remember\u2014but rarely both. So, we wanted to create a solution that hits that sweet spot. By letting users input a few personal but non-sensitive details, we generate passwords that are secure and actually stick in your mind. It's all about making strong digital security a little more human-friendly.What it does: Our project generates secure, memorable passwords by allowing users to submit a short piece of texts such as a meaningful sentence, a list of favorite things, and/or a memory. We then process that input to create a strong, unique password that\u2019s hard to guess but easier for the user to remember. It\u2019s a simple, user-friendly way to improve password security without the usual frustration of forgetting complex passwords.How we built it: We built the project using Next.js for the frontend, which allowed us to quickly set up a responsive and clean user interface. For the backend, we integrated the Google Gemini API to help generate passwords based on the user\u2019s input. The API takes the text users submit and returns password suggestions that balance security with memorability. This setup let us combine a smooth user experience with powerful backend processing, all within the timeframe of the hackathon.Challenges we ran into: Since users can submit anything in the textbox, it was tricky to design an algorithm that could consistently turn that into a secure password. We addressed this by using natural language processing techniques to extract key elements and then applying transformations like substitutions, capitalization, and special characters. There was also concern that users might accidentally include personal or sensitive data in their inputs. To mitigate this, we added warnings and tips in the UI, encouraging users to avoid things like full names, birthdays, or addresses. We also ensured that no inputs are stored or logged.Accomplishments that we're proud of: We're really proud of all that we've accomplished, especially since this is many of our teammates' first hackathon.  It was our first time building something like this under a time crunch, and seeing it all come together as a real, working tool was a huge win for us.What we learned: We learned a lot throughout this project, both technically and as a team. On the technical side, we gained experience working with text processing, basic encryption concepts, and building logic that transforms user input into something secure and functional. We also learned how important it is to design with the user in mind, especially when it comes to guiding input and ensuring clarity in the UI. Since this was many of our teammates' first hackathon, we learned how to collaborate under pressure, divide tasks effectively, and quickly turn ideas into a working prototype. It showed us how much we can accomplish in a short time when we work together and stay focused.What's next for FastPass: Next, we plan to improve the user experience by adding a real-time password strength meter, so users can better understand the security level of their generated passwords. We'll also focus on protecting user privacy by ensuring all text inputs are processed safely, without storing any sensitive information. Additionally, we want to give users more control by allowing them to customize password criteria such as length, use of symbols, or numbers to better match the requirements of different platforms.",
                        "github": "https://github.com/RyanPCo/FastPass",
                        "url": "https://devpost.com/software/fastpass-henoyz"
                    },
                    {
                        "title": "Payment Protect",
                        "description": "Safeguarding the future. And we can start today.",
                        "story": "Inspiration: On our travels, we booked a trip to Puerto Rico, fully intending to explore the vast natural wonder of the lush El Yunque forest. Yet, our  refused our payment procedures, leaving us stranded with our flights booked, and no forest to explore. We filed a card dispute, with our money never being returned to us, yet it took weeks to process, during which we were forced to cancel the trip altogether, unable to pay for our hotel or flights. And this is due to the 238 million credit card disputes that were filed in 2024, the majority of which are small scale disputes that only serve to distract the card company from taking care of the actual problems. The average dispute also takes $50 to solve, even when the dispute costs less then this, wasting millions of dollars in company assets in disputes that ultimately return less value to the consumer then the card company actually put in, and inevitably, with the 96% return rate of disputes, paying the consumer (justly or not) back anyways. Yet with new technologies, why do we still keep the old ways, of constantly referring back to a human, to constantly having to go through so many layers just to get our justice back, only to waste the money and time of both the consumer and the company?What it does: There are two major factors to Payment Protect. The first one is the Dispute AI system. In Dispute AI, the customer will directly bring up key information from a list of their recent transactions, by clicking the dispute button next to these transactions, which will then be copied directly into Dispute AI. The ai will then begin the process of automatically filing a dispute, by checking its existing dispute database and realistically submitting a dispute to be compared to this database. If it detects the dispute as valid, and the dispute is under a certain threshold of monetary value, the AI will automatically complete the transfer, saving the bank an average of $45 and 50 days per dispute, which equates to hundreds of millions in value lost every year, often without reason, as most disputes are lower in value then the average cost to file the dispute in the first place. However, if the dispute is invalid, as the machine learning model has been trained to be able to recognize possibly fraudulent disputes and will only improve on this over time, or if the dispute is over a certain monetary threshold, it will be sent to a human appraiser. This is to ensure that no dispute is unfairly worked against, bringing in a human element to have final say. The other function of Payment Protect is an heatmap, which, based off of parameters, provides a score to each merchant that will directly showcase to the consumer merchants they should and should not use. This can be used as a preventative measure, that will ensure that future disputes are lessened, saving the company from having to waste millions on disputes that ultimately could be solved extremely easily originally. The anti-fraud measures undertaken by Payment Protect are second to none.How we built it: A sql database using Nessi API was created in order to store mock financial transactions, customers, and merchants, and this was fed into an algorithm we created in order to efficiently create more and more of these parameters using the API. After this, we connected a Federal Consumer reports csv, and, after summarizing the data into only relevant points, connected it to the Nessi API database, eventually creating one large database that the AI could use for everything. After this, we assigned a certain score to each merchant from a list of the top 30 Capital One Vendors using a different algorithm, and displayed this in a heatmap. This algorithm used the new big database and therefore could assign a score based upon the details surrounding the disputes, amount of disputes for that vendor, and most recent disputes.Challenges we ran into: The AI at first struggled to understand the database it was provided with, yet through training and continued trials we were able to focus it's outputs and effectively convey the purpose the AI originally was meant to conceive. Another major challenge was the creation of an interconnected database, as we first had to change the sql database and copy it into a csv format, which turned out to be harder then expected.Accomplishments that we're proud of: A major accomplishment we are proud of is the teamwork that our members showed on working together to effectively handle issues posed by the code and streamline its production. Another major accomplishment is the integration of Gemini AI and OpenAI, as both APIs struggled to function under such a heavy workload.What we learned: We learned how to use APIs effectively together in multiple layers to accomplish complex algorithms that can successfully perform complicated tasks.What's next for Payment Protect: We hope to integrate statistics into the web app and create far more test users in order to iron out issues and grow support for this new program.",
                        "github": "",
                        "url": "https://devpost.com/software/dispute-daddy"
                    },
                    {
                        "title": "Class Topper",
                        "description": "Class Topper\u2013Study Smart, Not Hard, is an AI-powered platform where students can chat with PDFs, videos, create Mind Maps, and take quizzes, ensuring you're always exam-ready, even last minute!",
                        "story": "Inspiration: The idea behind Class Topper came from wanting to help backbenchers and last-minute crammers boost their marks right before exams. We wanted to create a tool that gives every student a fighting chance, even with limited time. Additionally, we wanted to make education more accessible for underprivileged students who are unable to afford high school and college tuition fees.What it does: Class Topper is a web platform where students can:Ask questions and get answers instantly via a RAG-based Chatbot.Upload PDFs or YouTube videos and interact with them.CreateMind Mapsfor better document understanding.Test their knowledge withQuizzesto prepare faster and smarter.,How we built it: Login/Signup: Integrated Google Firebase for secure authentication.\nVector Database: Used Qdrant to store and retrieve high-dimensional document embeddings for fast, accurate search.\nModel API: IntegratedGeminito power the Q&A functionality.\nOrchestration: LeveragedLlamaIndexto manage the complex workflows of querying documents and videos seamlessly.Challenges we ran into: Understanding Chat History: Ensuring context was properly maintained between student queries and chatbot responses.\nImplementing Video RAG: Making sure the video content was properly indexed and searchable.\nBuilding a Retrieval Model: Ensuring high accuracy in the retrieval process to provide relevant, accurate responses.Accomplishments that we're proud of: Successfully integrated4core features into one platform, offering an all-in-one solution for exam prep.Built a robust system that can handle multiple document types (PDFs, videos) and provide contextual assistance.,What we learned: Learned how to integrate multiple systems (chatbots, databases, video processing) into one cohesive platform.Gained deeper insights into Vector Databases and how they can enhance real-time document retrieval and Q&A experiences.,What's next for Class Topper: Our goal is to make Class Topper more accessible to underprivileged students, allowing them to learn without the burden of expensive tuition fees. We want to empower all students, no matter their background, to succeed academically with the tools they need at their fingertips.",
                        "github": "https://github.com/Jiten-Bhalavat/ClassTopper",
                        "url": "https://devpost.com/software/class-topper"
                    },
                    {
                        "title": "Most Complicated NOT Gate Ever",
                        "description": "back propagated neural network thats currently set up to take inputs of two-bit binary numbers and return the number with each bit flipped",
                        "story": "Inspiration: With the recent rise of LLM neural networks like ChatGPT, we decided we wanted to go right back to the root of how those networks are made and create our own from scratch. Almost every example we were able to find of a simple neural network recommended using something like \"\" to simplify the process but we decided that writing our code to work completely seperate of any such assistance was the only way to go.What it does: It creates a neural network (of variable size) in this case containing 12 nodes, with two hidden layers of 4 nodes each. It is then trained on a data set we created to teach it to work as a NOT gate. In this case it is trained on 1,000,000 mini-batches of 10 inputs each. (we would demonstrate it on a larger scale but unfortunately that would take time we simply do not have)How we built it: All written in java in VS code in a main class and network classChallenges we ran into: We were originally intending to connect the neural network to a CNN and scan some simple images, but due to lack of time we were unable to complete that portionAccomplishments that we're proud of: We are quite proud of the networks accuracy. We added a couple features, including a Xavier randomized initialization function, that increase the networks function for smaller datasetsWhat we learned: Learned how to create a neural network (this was my first time)What's next for Most Complicated NOT Gate Ever: Hopefully well be able to finish the CNN and hook it up so we can use this network for something a little more useful",
                        "github": "https://github.com/Liam3346/CNN",
                        "url": "https://devpost.com/software/most-complicated-not-gate-ever"
                    },
                    {
                        "title": "Terp Trials",
                        "description": "A smart platform connecting research firms and volunteers, built with Next.js, Spring Boot, and Flask AI scoring\u2014MongoDB ensures seamless, scalable research recruitment.",
                        "story": "Inspiration: This platform was inspired by the need to streamline the traditionally cumbersome process of recruiting volunteers for research studies. In many academic and clinical settings, finding the right candidate is a time-consuming process that often delays critical research. By harnessing advanced technologies like Google Gemini API to intelligently convert complex research descriptions into actionable screening questions\u2014and combining it with personalized recommendation engines\u2014the platform not only simplifies the recruitment process but also ensures a better match between project requirements and volunteer strengths. The vision is to empower research firms by reducing administrative overhead and accelerating innovative research, while giving volunteers access to meaningful opportunities where their unique skills make a real difference. This synergy between research and volunteer engagement can drive impactful discoveries and foster stronger community participation in scientific progress.\nThis project represents the convergence of modern web development, cloud scalability, and AI-driven intelligence, marking a significant step forward in how research collaborations are formed and executed.What it does: Hosts a platform that enables researchers and potential participants to connect, ensuring that researchers have greater access to the student body, and providing students with an opportunity to make some extra cash.How we built it: a. API Design and Microservices\nSpring Boot REST APIs:\n We designed comprehensive REST endpoints in Spring Boot to manage the core functionalities:\nRegistration & Authentication: Research firms and volunteers register and log in securely.\nPosting Management: Research firms can create, update, delete, and view postings. Each posting includes critical details such as start date, end date, compensation, and a flag for extra requirements.\nApplication Management: Volunteers apply to postings, and research firms can view all applications for their postings.b. Integration with External Services\nGoogle Gemini API:\n We built functionality in our posting creation service that sends posting descriptions to the Google Gemini API. The API converts these into a set of screening questions which are then used to dynamically gather volunteer responses and compute a score.\nFlask-Based Recommendation Engine:\n Our recommendation system, built in Flask, processes past application data and volunteer profiles. This service provides insights and personalized suggestions to research firms, enhancing candidate matching and streamlining the recruitment process.\nc. Data Management with MongoDB\nFlexible Schema:\n Using MongoDB allowed us to store complex and varied data structures. The flexibility of document storage helped us manage different posting fields, volunteer details, and dynamic question lists without rigid schema constraints.\nEfficient Queries:\n We used Spring Data MongoDB repositories to easily build query methods (such as finding all postings for a research firm or retrieving all applications for a list of posting IDs) via convention-based method naming.\nd. Frontend Development\nNext.js and React:\n The user interface was crafted using Next.js for server-side rendering, ensuring fast load times and great SEO. React powered the interactive components, making the user experience smooth.\nTailwind CSS:\n Tailwind CSS was used to create a consistent, modern design across the application with minimal custom CSS code.\nAPI Integration:\n The frontend communicates with our Spring Boot APIs, submitting user data, receiving responses (such as dynamic questions from the Google Gemini API), and displaying both posting and application details in real time.\ne. Security and Authentication\nJWT-based Authentication:\n We implemented secure authentication using JWT tokens. This ensures that research firms and volunteers have secure and personalized access to their functionalities without repeatedly sending credentials.\nSpring Security:\n We used Spring Security to protect our endpoints, ensuring that only authorized users can create posts, apply for jobs, or view sensitive data.Challenges we ran into:",
                        "github": "https://github.com/PriyankaPatel72/Trials",
                        "url": "https://devpost.com/software/terp-trials"
                    },
                    {
                        "title": "DebateGuard",
                        "description": "DebateGuard is a real-time video debate platform with AI-powered moderation, live transcription, and fallacy detection, designed to promote civil, insightful, and productive dialogue!",
                        "story": "Inspiration: In the age of misinformation and bad-faith debates, many individuals struggle to recognize what a respectful and constructive debate looks like. This misunderstanding often results in the dismissal of meaningful discourse, limiting critical thinking and learning opportunities.DebateGuardoffers an intuitive platform for users to engage in moderated debates, promoting clear communication and informed argumentation.What it does: DebateGuardprovides users with a one-on-one video debate platform enhanced by real-time AI moderation. During each debate session, participants receive live transcription and immediate AI-driven analysis, highlighting logical fallacies and encouraging more constructive interactions. Post-debate, participants can review detailed analytics including speaking times, fallacies identified, and comprehensive summaries of their arguments.How we built it: We builtDebateGuardusing a robust full-stack architecture centered around Next.js, React, and Node.js to provide real-time functionality and a seamless user experience. For the frontend, we utilizedNext.jsandTailwind CSS, creating a sleek, modern, and responsive user interface, enhanced by animations fromFramer Motion. React facilitated dynamic, reusable components for clarity and ease of use.Our backend leveragesNode.jsalong withWebSocketsandWebRTCfor seamless real-time video call functionality. Real-time transcription is handled using theWeb Speech API, while logical fallacy detection and moderation feedback are powered byGPT-4through strategically crafted prompts. We also integratedStream React SDKto manage user interactions and streamline communication features. We chosePostgreSQLfor reliable data management, integrated seamlessly withPrisma ORM.Challenges we ran into: A significant challenge was ensuring real-time transcription and AI-driven fallacy detection operated seamlessly within live video calls without noticeable latency. Initially, integrating backend APIs, AI services, and frontend components was complex, but through careful debugging, iterative testing, and WebSocket optimization, we substantially improved responsiveness and overall user experience.Accomplishments that we're proud of: We're proud of successfully implementing real-time AI moderation that provides immediate, relevant feedback during debates. Additionally, our modern, intuitive user interface significantly enhances user experience. Seamlessly integrating advanced technologies likeGPT-4,WebRTCvideo streaming, andreal-time analyticsinto a cohesive platform is another achievement we take pride in.What we learned: We gained valuable experience inreal-time audio and video streaming,AI integration, andnatural language processing. Working withWebSocketstaught us efficient real-time data handling, while combining frontend and backend technologies deepened our understanding of full-stack development. Additionally, usingGPT-4and theWeb Speech APIenhanced our skills in prompt engineering and real-time transcription.What's next for DebateGuard: Moving forward, we plan to incorporateadvanced visual behavior analysissuch as gesture and emotion detection to offer deeper insights and enhanced moderation accuracy. We're also exploring aspectator mode, enabling third-party users to observe debates with live AI commentary. Personalization of AI moderation tones and development ofuser-specific analytics dashboardsare further enhancements planned to enrich user experience.",
                        "github": "https://github.com/amanuelabiy/DebateGuard",
                        "url": "https://devpost.com/software/debateguard"
                    },
                    {
                        "title": "CreditGuard",
                        "description": "CreditGuard simulates credit transactions, auto-tuned by Gemini API for real-life complexity. This data trains/tests a XGBoost fraud detection model, deployed using the framework of AWS SageMaker.",
                        "story": "Inspiration: The driving force behind CreditGuard stems from the growing threat of credit card fraud. The financial losses and decline in customer trust necessitate constant improvement in detection systems to effectively keep pace with evolving fraud. We recognized a critical facet to make our detection different: the difficulty in obtaining large, diverse, and realistic training data that reflects the complex strategies employed by fraudsters. So, this project was inspired by the challenge of creating a dynamic data generation process to empower the development of a highly accurate fraud detection model.What it does: CreditGuard performs credit card transaction simulation, generating patterns including overlapping amount and location ranges and near-miss velocity tactics designed to evade simple detection rules. It leverages the Google Gemini API to dynamically tune simulation parameters, like fraud injection probabilities. This enhances the realism and difficulty of the generated dataset. Following this, it trains a XGBoost machine learning model. The model's hyperparameters are optimized using a fine-tuning process by the workflow of AWS SageMaker's Automatic Model Tuning, aiming to maximize a precision metrics on a validation set. To keep with out intention of a highly accurate fraud detection model, we achieve over 90% precision in identifying fraudulent transactions on testing data.How we built it: The simulation was built in Python, using standard libraries like pandas and numpy to generate realistic cardholder data, timestamps, locations with Haversine calculations, and transaction amounts. Specific logic was implemented within this simulation to mirror complex fraud tactics, including low-value tests, velocity checks, and slight overlaps between real and fraudulent activity patterns. We integrated the Google Gemini API to dynamically adjust simulation probabilities. Then parsing its JSON suggestions, we saw AI-driven variability that mimics sophisticated fraud behaviors. \nFor the model, we used an XGBoost algorithm that handles structured, imbalanced data. We mimicked AWS SageMaker's Automatic Model Tuning locally to optimize XGBoost's hyperparameters based on a centrism around precision and optimal classification threshold. The generated transaction data integrated boto3 to demonstrate real-time connectivity through AWS Kinesis.Challenges we ran into: Crafting synthetic data proved challenging, particularly in generating smaller overlaps between real and fraudulent transactions that mimic evasion techniques. Furthermore, reliably extracting meaningful tuning parameters from the Gemini API required prompt engineering and response parsing to disregard invalid responses. There was a lot of challenge in achieving over 90% precision without compromising recall, a common trade-off in fraud detection. So, reaching this threshold necessitated iterative tuning of the model's hyperparameters and the final classification probability cutoff. Also, simulating the SageMaker AMT workflow locally presented some difficulties as we had to learn how to implement the local equivalent using scikit-learn. Overcoming these challenges was constant, but allowed constant improvement as well in both data simulation and model optimization stages.Accomplishments that we're proud of: We are proud of constructing a functional end-to-end conceptual pipeline, starting from data simulation, progressing through AI-assisted parameter refinement, implementing a model training and tuning, and ending in targeted threshold optimization to meet performance benchmarks. Integrating the Google Gemini API represented a significant achievement, moving beyond simple simulation to dynamically data generation which added a unique realism.What we learned: This project provided insights into the nuances of realistic fraud simulation and creating challenging synthetic data requires carefully balancing numerous parameters to avoid detection while still embedding fraudulent signals.What's next for CreditGuard: The next step is to validate the trained XGBoost model's performance on real-world, unseen transaction data. Also, we aim to deploy the model via a scalable AWS SageMaker real-time inference endpoint to transition CreditGuard from a simulated concept to a production-ready fraud detection service.",
                        "github": "https://github.com/nschiguru/CreditGuard.git",
                        "url": "https://devpost.com/software/creditguard-7xqyla"
                    },
                    {
                        "title": "PRI$M",
                        "description": "Focusing the scattered rays of financial markets\u2014where stock data, news, and economic signals converge into one clear, educational perspective.",
                        "story": "Inspiration: Our economic situation is, more than anything, confusing. Though downturns have not been uncommon in recent history, the level of misinformation and disinformation has never been higher, including from family and friends. 93% of stocks are owned by 10%- the ones who can afford to take the time to understand how finances and markets work. But for the other 90%, most of whom are told their money is safe in an index fund, are now seeing those funds fluctuating in orders of tens of thousands of dollars by thehour, and they have no idea why. PRI$M is a tool for these people- for college students who don\u2019t know why it\u2019s so hard to find a job right now, for parents of those college students who are struggling to make sense of the value of the college fund they\u2019ve spent the last ten years building, for near-retirement workers who may be forced to work five more years. It is a tool that won\u2019t solve these problems for them, but they\u2019ll be able to understand what\u2019s causing them, and hopefully, doing so will make the future a little less uncertain.What it does: PRI$M is an educational finance web app that leverages the most recent data available aboutevery single companyto show the user recent stock price history, along with a bunch of important metrics, like any other stock display. But unlike other chart displays, PRI$M understands that many of its users may not know what a P/E ratio or a trade war is- and why it could affect them. And so, for every single stock, PRI$M displays a concise summary of all the most recent news (within the past month) of that company and how it could be affecting the price of its stock, as well as a general overview of what factors drive the company\u2019s potential success or failure. Then, it enables the users to either choose from a list of questions relevant to the information they just learned or to ask something of their own. PRI$M also takes a minimalist approach to UI design. Unlike traditional stock apps filled with flashing green and red indicators, PRI$M avoids color-based buy/sell cues. This is intentional: we want users to focus on understanding economic forces rather than reacting emotionally to short-term price movements. The goal is education and clarity, not panic or hype.How we built it: We built PRI$M using a MERN stack with a MongoDB database to handle and store previously called articles in a cache to minimize API calls and to make retrieval faster. Express.js handles the routing, Node was used for the backend, and React in the frontend. NewsAPI was used to get the most recent news articles. A couple of finance APIs were used, including yahoo-finance2 for ticker lookup, Finnhub for real-time stock price, company profile, and basic financials, and Tiingo for displaying historical data. Gemini was used to generate the news summaries and handle user Q&A.Challenges we ran into: We ran into numerous challenges, including turning a user input into something that could retrieve many relevant news articles, figuring out how to upload a schema to MongoDB, figuring how to use Node and Express for backend and routing for the first time, and displaying information in an informative, but not overwhelming way on the UI.Accomplishments that we're proud of: We\u2019re proud of hitting every target that we set out for ourselves. We created a tool that can display relevant stock information and, regardless of how popular the stock is, find relevant news that allows the user to learn about the stock and further interact with it. We\u2019re very proud of successfully using a full JavaScript stack for the first time. We\u2019ve both wanted to work with Express and Mongoose for a while, and a project as challenging as this was absolutely the best way to dip our toes.What we learned: We learned about a variety of extremely useful APIs for both information and finance-specific data. We learned about the value of creating multiple LLM agents for different tasks instead of asking one to handle all of them to reduce token usage. We learnt how to work with MongoDB using self-made schemas and using mongoose to leverage JavaScript object properties to make it work.What's next for PRI$M: There are many areas for improvement. Expanding the sources of data from which information is gathered is the first. Higher volume web scraping and more complex financial data might help build a more precise picture. Additionally, asynchronous updates of the database once every day would minimize costs in calling the LLM and maximize speed in retrieval.",
                        "github": "https://github.com/AaryanTomar/prism",
                        "url": "https://devpost.com/software/pri-m"
                    },
                    {
                        "title": "Memento",
                        "description": "Remember Every Smile, Relive Every Story.",
                        "story": "Inspiration: While numerous digital tools exist for monitoring steps and physical activity, very few applications focus on tracking memory and cognitive performance. Every day, small, meaningful moments slip away\u2014conversations, laughter, even the quiet moments of reflection. We were inspired by the realization that memories, the very essence of who we are, are too often forgotten. Our personal experiences, as well as the struggles faced by people with memory impairments like dementia or Alzheimer\u2019s, drove us to create an app that automatically captures and preserves life\u2019s everyday beauty. Research has shown that Reminiscence therapy, or recalling memories, is a great source that can improve their overall well-being and reduce unwanted behaviors. Memento is our answer to ensuring that every gesture, every conversation, and every laugh remains a part of your personal legacy and persuades people to recall and relive them.What It Does: Captures Daily Interactions: allows users to record their daily interactions and memories.Interactive Playback: Lets you relive your day with an interactive timeline that highlights the most touching moments.Assists Memory Care: Acts as a vital tool for those with memory impairments by preserving moments that are often forgotten.Active recall function: Prompts the user to recall their past memories and answer questions through their voice. We use Gemini to analyze the user's response and determine the confidence scores and how accurately the user was with recalling the memory. This feature is valuable in tracking the time frames and people you remember, which is a vital part for patients with dementia and Alzheimer\u2019s.,How We Built It: Frontend:We built the user interface using Next.js, ensuring a responsive design with Tailwind CSS.Backend:We used Flask and Postgres for the backendAI & Speech Recognition:Integrated advanced speech-to-text technology and natural language processing (NLP) models to convert conversations into digital memories with Gemini.,Challenges We Ran Into: Accurate Transcription & Summarization:Training the AI to understand and accurately transcribe everyday conversation, with all its nuances, proved challenging.User Interface Design:Crafting an intuitive UI that serves both tech-savvy users and those with cognitive impairments demanded a thoughtful, iterative design process.Tracking progress:Analyzing audio files to detect speech confidence and accuracy.,Accomplishments That We're Proud Of: Innovative Use of AI:Our app effectively transforms daily conversations into emotionally rich narratives.Impact on Memory Care:We\u2019ve designed a tool that not only captures memories but also provides invaluable support to those with memory impairments such as dementia.,What We Learned: Bridging Technology and Humanity:Advanced technologies like AI can be harnessed not just for efficiency but to foster deeper human connections and preserve our most cherished moments.Resilience and Adaptability:Overcoming technical hurdles, from transcription accuracy to performance bottlenecks, reinforced our teamwork and determination.,What's Next for Memento: Enhanced Personalization:We plan to integrate more refined mood and sentiment analysis to tailor the memory narratives even more personally.Community & Caregiver Tools:We envision features that allow family members and caregivers to interact with, share, and contribute to the memory timelines, fostering a sense of community and shared legacy.AI-Driven Insights:Exploring advanced analytics to help users discover patterns in their daily interactions and overall well-being.,",
                        "github": "https://github.com/SunethRamawickrama/Memento",
                        "url": "https://devpost.com/software/memento-y5lxbm"
                    },
                    {
                        "title": "Dino Game",
                        "description": "How long can you keep this little guy alive?",
                        "story": "Inspiration: We were inspired by Tamagotchi style games, and wanted to create a web-based version of the game.The original inspiration for the Dino game was an AI Financial Advisor geared to high schoolers and college students. The Dinosaur's \"health\" would be replaced by a system that keeps track of the financial health of the student and their financial decisions.However, we were unable to achieve those goals during this campWhat it does: It keeps track of the number of days the dinosaur has stayed alive,  can switch to see the history of the dinosaur's life (including when it got sick, and the changes in energy every day).How we built it: We built the program on Visual Code using the Framework Flutter. Since it was our first time building a web-project, it took us some time to figure out the best way to set up a web-project. Using this tutorial on Googlehttps://codelabs.developers.google.com/codelabs/flutter-codelab-first#6, we learned the main formatting and structure of a Flutter project. Then, we used our knowledge from this tutorial to build our own Game.Challenges we ran into: As high schoolers with minimal college experience with programming, we had a lot to learn during BitCamp (Although we are excited about it). We ran into the challenge of being generally unfamiliar with web-development, the language Dart, and it learning to use the terminal for the first time. \nThe most difficult part of this project was definitely the initial set up. We kept trying different IDEs (IntelliJ, VS Code, and PyCharm) and Frameworks (Flask and Django) to create the web application. Although we could install all the plug ins and files required, it was difficult to get an understanding of the project.Since I wanted to learn more about interactive websites and how to build them, I decided to join the App Dev Sessions. That is where I first learned about Flutter, and decided to incorporate it into the project. Flutter's object-oriented structure with inheritance is similar to Java, with Widgets instead of classes.To learn the structure of Flutter projects, we used a tutorial from google code labs, as well as the help of the mentors at BitcampAccomplishments that we're proud of: We're especially proud of learning a new language and setting up an entirely new Framework in order to create this project.We're also proud of the pixelated dinosaur drawings.What we learned: While researching different web-development frameworks, and implementing them into various IDEs, we learned terminal commands and how to use the terminal to install packages (on both python using pip and Flutter using pub command)We learned some of the basics of website development, including learning how a grid-based layout system works. It was also exciting to learn how to use Stacks on Flutter.What's next for Dino Game: The Dino Game is extremely simple right now. But we are hoping to add more user-interaction. For example, when the player wins the game, there should be confetti that falls from the top of the screen, And if they lose, there should be some comets that slide across the screenWe are eager to continue to expand on this game so that it can be a robust and fun way to learn about financial literacy and develop good saving habits. This will involve using Google's Gemini API and Nessie API in order to access financial information and run it through an AI for specific evidence.",
                        "github": "https://github.com/HappyHat952/namer_app",
                        "url": "https://devpost.com/software/dino-game-t5bg73"
                    },
                    {
                        "title": "SpendSmart",
                        "description": "SpendSmart: Smart Insights for Smarter Spending",
                        "story": "Inspiration: We've all experienced that moment of surprise when looking at our bank statement at the end of the month. The inspiration behind SpendSmart stems from the desire to provide users with a proactive awareness of their spending habits before those surprises occur. By visualizing monthly expenditures directly within their browsing experience, coupled with potential real-time mobile warnings based on potential spending and visualizations, we aim to empower individuals to make more informed financial decisions and avoid overspending, fostering healthier spending habits.What it does: SpendSmart is a Chrome extension designed to provide users with a clear and intuitive understanding of their monthly spending. Once the user inputs their unique Capital One customer ID, the extension, powered by Nessie, fetches and displays a breakdown of their expenditures, including the total amount spent and a categorized view (as seen in our dashboard). Beyond the browser extension, we envision a companion mobile application that utilizes location tracking (with user consent) to provide real-time warnings and personalized insights at the point of potential overspending, such as sending a cautionary notification when a user frequently spends excessively at a specific location like McDonald's. The mobile app currently allows users to view an overview of their spending, analyze a potential new purchase, and set financial goals.How we built it: SpendSmart was built as a Chrome extension leveraging web technologies, with a key component being the integration of real-world banking data facilitated by Nessie, Capital One's Hackathon API. The frontend interface, encompassing the login page and the insightful spending dashboard with its visualizations (including the pie chart for category breakdown and the line graph illustrating spending pace), was developed using React and styled with CSS. We utilized Chrome's runtime messaging API to establish communication between the extension's popup and our backend (powered by Nessie), enabling us to securely fetch and display the user's actual spending data based on their Capital One customer ID. Nessie provided us with the invaluable ability to generate realistic financial transactions, allowing us to showcase the practical utility of SpendSmart in providing users with a tangible understanding of their spending habits. The mobile application was developed using a mobile React Native and operates as a seamless connected budgeting app, with plans for location tracking implementation in the future.Challenges we ran into: Initially, we considered using a GET request to fetch user spending data, passing the Customer ID as a URL parameter. However, we quickly recognized the potential security risks associated with exposing sensitive identifiers like the Customer ID directly in the URL. This approach could make user data more vulnerable to interception or unintended exposure, leading us to implement a more secure POST request. Additionally, we encountered a challenge in understanding the data model provided by Nessie. The relationships between core objects like customers, accounts, and transactions were initially confusing. To overcome this, we dedicated time to thoroughly explore the Nessie API documentation and carefully mapped out the hierarchy, clarifying that a transaction belongs to a specific account, which in turn is associated with a customer. This deeper understanding was crucial for correctly structuring our API requests and accurately processing the retrieved banking data.Accomplishments that we're proud of: We are incredibly proud to have built a functional Chrome extension that provides users with a tangible and visual understanding of their spending habits using real-world banking data through the Nessie API. Successfully implementing a secure authentication flow using POST requests and then translating complex financial data into an intuitive dashboard with a clear category breakdown (pie chart) and spending pace visualization (line graph) within a short timeframe is a significant achievement. We are also particularly pleased with our ability to navigate the Nessie API's data model and present meaningful insights directly within the user's browsing experience, laying the groundwork for our vision of proactive financial awareness, including the concept of location-based mobile alerts. We are also proud to have built both an extension and app for our product allowing for cross platform connection and smart budgeting at all times.What we learned: Through developing SpendSmart, we gained invaluable hands-on experience in the end-to-end process of building a Chrome extension, from UI development with React and styling with CSS to managing API interactions and handling user data. We specifically deepened our understanding of Chrome's runtime messaging API for secure communication within the extension. Furthermore, working with Nessie, Capital One's Hackathon API, provided us with practical insights into consuming and interpreting real-world banking data, including navigating complex data relationships. We also learned the critical importance of prioritizing user security by moving away from potentially vulnerable GET requests with sensitive information in the URL. This experience underscored the need for careful API exploration, robust error handling, and user-centric design in creating impactful browser and mobile-based tools.What's next for SpendSmart: Looking ahead, we envision leveraging the power of Artificial Intelligence to provide even more personalized and impactful insights tailored to each customer's unique spending habits. By analyzing spending patterns, we aim to proactively identify potential areas of concern and offer customized recommendations for smarter financial decisions. Furthermore, we plan to develop an additional mobile application that integrates location tracking (with user consent). Running in the background, this app could provide real-time warnings and insights based on the user's location and spending history. For instance, if our AI-powered analysis reveals a tendency for overspending at a particular store like McDonald's, the app could deliver a timely notification advising caution and highlighting personalized insights related to that specific spending habit, empowering users to make more conscious choices in the moment. Our next steps also include refining the user experience, enhancing visualizations, exploring longer-term trend analysis, integrating spending goals and alerts, and investigating broader data source compatibility.",
                        "github": "",
                        "url": "https://devpost.com/software/spendsmart-d9pnk5"
                    },
                    {
                        "title": "StudyEZ [AI Study Tool]",
                        "description": "Upload. Organize. Master Anything.",
                        "story": "Want to see more?Demo 2,Demo 3Inspiration: As students, we constantly juggle scattered lecture formats\u2014dumped into disorganized folders or crammed into the same notebook. Most of us take notes by date, not topic, making it frustrating and inefficient to review material or prep for exams.That\u2019s why we builtStudyEZ\u2014a smarter way to study. It turns your scattered learning materials into structured, AI-tagged folders that mirror how your brain studies, not how your files are named. Transforming lectures into knowledge, effortlessly.What It Does: StudyEZtransforms lectures (text, audio, video) into organized study guides by:Lecture UploadUpload text/audio/video files or paste a URL (e.g., YouTube)AI-Powered TranscriptionConverts audio to text using AssemblyAIIntelligent Analysis (via Google Gemini)Extracts subject, class, topic, and sub-topicsGenerates summariesIdentifies key sub-topicsResource GenerationFinds relevant web articles and YouTube videos per sub-topicDisplays resources in a clean, searchable tableOrganized NavigationHierarchical sidebar: Subject > Class > TopicEach lecture includes upload date, summary, transcript, and resourcesCustomizable UIResizable sidebarDark/Light mode toggle (with saved preference),How We Built It: Frontend: React, Vite, CSS, AxiosBackend: Python, FlaskAI/ML:Google Gemini API for lecture understanding and summarizationAssemblyAI for transcriptionGoogle Custom Search API for resource generationStorage: MongoDB (via PyMongo)Other APIs: YouTube Data API, Fetch,Challenges We Ran Into: Back-end Front-end Integration, particularly with database fetching and data formatting.Accomplishments: Built a fully functional MVP in under 36 hoursCreated a smooth, end-to-end AI pipeline (upload \u2192 summary \u2192 topic classification),What We Learned: How to apply multimodal AI tools like Gemini for real-world academic use casesThe importance of clear scoping and agile collaboration in fast-paced builds,What's Next for StudyEZ: User Authentication: \nImplement Google Firebase for secure login/signup. \nTo enhance security and personalization, we plan to implement Google Firebase for login and signup. This will allow users to easily and securely access their personal study space using their existing Google accounts. With Firebase integration, users can have peace of mind knowing their notes, transcripts, and learning materials are tied directly to their authenticated profile \u2014 ensuring privacy and a seamless user experience.User-Specific Notes: \nAssociate uploaded lectures and notes with individual user accounts. Moving forward, we aim to associate all uploaded lectures, transcriptions, notes, and resources with individual user accounts. This means when a user logs in, they will have access to their personalized dashboard containing all of their past uploads, saved resources, and generated study guides. This feature will turn the platform into a centralized hub for managing and organizing personal learning materials. We will also make it abundantly available to print and download in various formats.Sharing: \nAllow users to share notes or summaries. Collaboration is essential for effective learning. In the future, we want to enable users to share their generated notes, summaries, or even curated resource folders with classmates, study groups, or the public. This feature will foster a collaborative learning environment where users can benefit from collective knowledge and contribute to a growing library of educational content.In-App Search: \nAllow users to search through transcripts and summaries. As users upload more lectures and accumulate notes over time, we want to introduce an intelligent in-app search functionality. This will allow users to quickly search through their transcripts, summaries, topics, or resources using keywords or phrases. Whether a user needs to quickly revisit a concept from a past lecture or find a resource on a specific sub-topic, the search feature will make information retrieval effortless and efficient.,",
                        "github": "https://github.com/dannguyen24/BITCAMP2025/",
                        "url": "https://devpost.com/software/ai-study-tool"
                    },
                    {
                        "title": "Petals of Progress",
                        "description": "Develop new sustainable and healthy habits with our web application. Partake in daily and weekly missions to ameliorate your environment and watch as your bud sprouts!",
                        "story": "Inspiration: Our goal was to promote sustainable and healthy habits within our community\u2014through a platform that\u2019s not only impactful but fun and rewarding. By blending real-world action with gamification, we aimed to motivate users to care for the environment in their daily lives.What it does: Petals of Progress transforms sustainable living into a game. Users complete daily and weekly eco-friendly tasks to earn points. These points improve their in-game environment, making it cleaner, greener, and more beautiful\u2014reflecting the positive impact of their actions in real life.How we built it: We built Petals of Progress as a full-stack web app using JavaScript for the frontend, with Node.js and Express.js handling the backend. MongoDB stores user data, progress, and task history, allowing for flexible and scalable data management. We used AJAX to enable smooth, real-time updates between the user interface and server. The Gemini API was integrated to enhance the user experience with dynamic content. Throughout development, we used Git and GitHub for collaboration and version control, creating a strong technical foundation for future growth.Challenges we ran into: One of our biggest hurdles was ensuring seamless communication between the backend (MongoDB) and the frontend interface. Debugging and syncing real-time updates took persistence, but we ultimately established a reliable flow of data.Accomplishments that we're proud of: We\u2019re incredibly proud of the custom artwork that brings the game world to life. Most of all, we're proud to have built a solid foundation for an idea we're passionate about\u2014one that has the potential to grow and inspire positive change.What we learned: Throughout the process, we strengthened our skills in full-stack development and learned the importance of user experience in behavior-driven design. We also gained insight into balancing technical functionality with meaningful, real-world impact.What's next for Petals of Progress: Next, we plan to add a community leaderboard to encourage friendly competition and deeper engagement. We\u2019re also working on expanding the game with user profiles, unlockable content, seasonal updates, and personalized task suggestions. A mobile app is on the horizon to make the platform more accessible, along with offline tracking and progress syncing. We also aim to introduce real-world impact metrics, allowing users to see the collective difference they\u2019re making through sustainable actions.",
                        "github": "https://github.com/anazru4451/bitcamp2025",
                        "url": "https://devpost.com/software/petals-of-progress"
                    },
                    {
                        "title": "SmartSave",
                        "description": "An all in one personal finance management app that tracks all transactions made, offers insights on spending patterns and helps you stay on track with your financial goals.",
                        "story": "",
                        "github": "https://github.com/ichethan01/BitcampRepository",
                        "url": "https://devpost.com/software/smartsave-20dkwr"
                    },
                    {
                        "title": "Browser Buddy",
                        "description": "A browser pet that holds you accountable for your mental health and wellness!",
                        "story": "Inspiration: We wanted to build something fun and lightweight to help our productivity. It was inspired by pomodoro techniques and our love of animals. Also wanted something to track our daily goals and earn rewards for completion.\nAnimals include Dogs, Cats, Quokkas, Capybara, Lemurs, Crabs, Owls, and BearsWhat it does: Creates your virtual pet and requires care for the pet by taking breaks, supporting Pomodoro timer in order to lock in and a traditional timer to encourage stretching. Includes a cursor park in the break so that you aren't scrolling around on the laptop because you should be stretching, getting water, walking etc. Tarot card reading options so that you can stay mindful and get quotes of inspiration. Emotional readings of your pet to help your day. Earn xp, morale and tokens for each break and goal completion. Able to spend tokens in shop to move up to the next prestige and advanceHow we built it: We built this chrome/edge/etc. extension using react, typescript, css and gemini api. We split the work creating pages and features that are most important first, including choosing animals, creating a timer and earning xp.Challenges we ran into: Merging all of our git conflicts -- overthinking of ideas and figuring out the MVP and basic features -- implementing the automatic timer in the backgroundAccomplishments that we're proud of: The design/looks of our project -- entire logic of the works -- the shop -- AI integrationWhat we learned: Don't edit on master branch -- push and commit often -- create distinctive plans before handWhat's next for Browser Buddy: We want to support communities (schools, communities, etc.) where your pets can race each other and show off in high stakes competitions. We also want to add more shop options with cosmetics and an inventory to store them. Finally, we want to give desktop notifications full support and add tools for financial planning.",
                        "github": "",
                        "url": "https://devpost.com/software/browser-buddy-0b3m2j"
                    },
                    {
                        "title": "EDU-HUB",
                        "description": "EDU-HUB is an AI-powered platform for students and TAs\u2014real-time help, smart TA matchmaking, and personalized exam prep. No more office hour chaos. Just focused, fast learning.",
                        "story": "Inspiration: Office hours are always packed. Questions get lost in Discord servers. Most universities don\u2019t offer tools that truly support TAs and students. We wanted to build something that fixes that\u2014a smarter, AI-powered system that makes academic support accessible, organized, and actually helpful.What it does: EDU-HUB is a role-based academic platform where professors create courses, TAs manage student questions, and students get help in real-time. It connects students to TAs through a smart matchmaking system, lets them ask questions, and provides AI-powered tools like concept explanations, question generators, and personalized study plans based on past exams.We have designed the Practice Generator feature and developed a mock-up to demonstrate its functionality. Full AI integration is planned in our next development phase.How we built it: We used React for the frontend and Node.js with Express for the backend. Authentication was handled using Firebase Auth. with role-based access control (Professors, TAs, Students). We implemented real-time chats, used MongoDB for the database, and integrated AI services (like Gemini) for the EDU AI modules\u2014 Practice Generator, and Exam Helper.Challenges we ran into: -Managing three different user roles with distinct permissions and dashboards.\n-Integrating Socket.io for real-time chat proved trickier than expected\n-Training AI prompts to give meaningful study suggestions and explanations.\n-Building video features and group tutoring support needed extra infra planning.Accomplishments that we're proud of: -Fully functional platform with role-specific portals.\n-Real-time TA-student communication system.\n-Successfully integrated AI to analyze past exams and generate study material.What we learned: -The importance of designing for clarity when you have multiple user types.\n-How to structure backend logic to support dynamic, permission-based features.\n-Prompt engineering to guide AI toward helpful, student-level explanations.\n-Team coordination under pressure to build, test, and polish a complete platformWhat's next for EDU-HUB: -Integrate live video calling between TAs and students.\n-Launch peer-to-peer tutoring and group study features.\n-Improve the Q&A system with real-time chat via Socket.io\n-More efficient in Professor and Student Management",
                        "github": "",
                        "url": "https://devpost.com/software/edu-hub-4udvg8"
                    },
                    {
                        "title": "Bit-Cat",
                        "description": "\"Bit-Cat\" is a 2D platformer--help the titular Bit-Cat escape a compiled program, navigating registers and learning low-level programming concepts along the way--now with an original soundtrack :)",
                        "story": "Inspiration: The inspiration forBit-Catcame from a desire to make learning low-level programming languages fun and interactive.\nWe noticed that understanding the inner workings of a computer, like how programs are executed or how registers are used, can be intimidating, and we wanted to create something that could demystify these concepts through gameplay. \nBy turning the journey of a program's execution into a platformer, we hoped to make learning about computer systems enjoyable and accessible.What it does: Bit-Catis a 2D platformer that takes players on an adventure through the inner workings of a compiled program- join the titular Bit-Cat as it navigates through registers, memory, and processor instructions, attempting to escape its digital confines. \nAlong the way, players will learn about low-level programming concepts- as they help Bit-Cat escape, they'll gain a unique insight into the heart of computer systems, from assembly language to CPU architecture.How we built it: Bit-Catwas built using Unity, with a combination of programming in C# for gameplay mechanics and hand-drawn assets for characters and environments. The blurred background was drawn in Procreate with many layers to make a foreground, midground, and background.\n Additionally, we developed the platformer mechanics, including movement, jumps, and interactions with non-player characters. We also integrated educational content by embedding explanations of low-level concepts into the game\u2019s narrative and level design.Challenges Faced in Unity: BuildingBit-Catin Unity involved several key challenges:\nGetting collisions to work correctly was tricky- fine-tuning colliders and adjusting Rigidbody2D properties helped resolve this.\nManaging sprite interactions was another challenge- sorting layers and ensuring correct trigger interactions were crucial for smooth gameplay.Accomplishments that we're proud of: Player physics in a 2D side scroller with gravity and collision features. Textbox and NPC interactions in Unity! Testing out a parallax background in a sidescroller type level to give a geometric illusion of depth in a 2D environment.What we learned: A lot of Unity. A lot of iPad Procreate.What's next for Bit-Cat: Looking ahead, we plan to expandBit-Catby adding more levels that explore deeper concepts of computer systems, such as memory management, multithreading, and optimization. We also aim to refine the game\u2019s mechanics and visuals, improving the character's movement and adding more complex animations for different actions.",
                        "github": "https://github.com/azhou05/Bit-Cat/commit/e02b21a7d32a0e4c6b6aeb1cb7b9f47561b91f31",
                        "url": "https://devpost.com/software/bit-cat"
                    },
                    {
                        "title": "Knowledge Quest",
                        "description": "Want a fun way to test your knowledge? Try Knowledge Quest! Upload your reading, answer questions, and defeat dinos!",
                        "story": "Inspiration: We wanted to build something that makes studying fun and interactive, so we took inspiration from Bitcamp's dinosaurs and created an adorable and engaging way to study readings!What it does: Knowledge Quest takes in the user's reading and uses Gemini to generate five questions to test the user's knowledge. To spice things up, the user can choose one of four elemental dinos and answer questions to defeat enemy dinos. The app checks for accuracy and provides the relevant portions of the text for review.How we built it: We used Figma to visualize our app and Pygame. We used PixilArt to design all visuals and Cats on Mars by SEATBELTS for the background music.One of our teammembers also made a React version of the application.Challenges we ran into: We ran into challenges throughout the hackathon including limitations of the pygame package for the implementation of features we had in mind.Accomplishments that we're proud of: We are extremely proud of our visual elements! They are adorable!! Our teammembers did all of the pixel art themselves and it really added to the look of our project. We also composed our own instrumental backing for the project, which was a fun touch.We are also proud of completing our user flow the way we envisioned it.What we learned: We learned a lot about pygame, the use of Gemini in code, and how pygame compares with React.What's next for Knowledge Quest: We would like to incorporate the opportunity for user's to retry question that they previously missed so that they get a chance for redemption.",
                        "github": "https://github.com/mqzyiv/Bitcamp-Hack",
                        "url": "https://devpost.com/software/knowledge-quest-nm6l3j"
                    },
                    {
                        "title": "SmartSplit",
                        "description": "SmartSplit helps students track expenses, set savings goals, get AI budgeting tips, and stay financially accountable with friends \u2013 all in one easy, user-friendly dashboard.",
                        "story": "Inspiration: We created SmartSplit because we noticed that college students often struggle with budgeting, tracking expenses, and saving money in an easy, motivating way. We wanted to build a tool that could help make managing finances simple, social, and even a little fun.What it does: This was our first hackathon and the first time any of us worked with React and Git in a team setting. Throughout the project, we also learned how to set up a real-time database using Firebase, and how to connect user input to live data. Seeing the app update instantly as users entered their goals or expenses was a huge milestone for us.How we built it: We built the project using React for the frontend and Firebase for the backend database. We used Together AI to generate smart financial advice based on user spending patterns. The app features goal tracking, budget tracking, an accountability circle with friends, and an interactive dashboard to make managing money approachable.Challenges we ran into: A lot of the tools and technologies were new to us. Setting up Firebase and getting it to talk properly with our React components took time. We also had to figure out how to collaborate efficiently with Git, manage merges, and combine everyone\u2019s work into one project. Debugging React state changes and making sure the data flowed correctly between components was another big challenge, but we learned a lot by troubleshooting together.Accomplishments that we're proud of: This is our first hackathon as a team and we are proud to complete and submit a project for judging. We are proud of successfully integrated react.js framework which is one we have little experience with. We are proud of effectively collaborating on github learning many of the key functions on the fly. Also, learning version-control with Git was challenging but will be extremely useful for our future careers.What we learned: We learned how to effectively integrate AI into our application using API keys and were able to setup a database using firebase. Although we knew a bit of react, we expanded our knowledge of the framework over the past 36 hours. We also were able to expand our knowledge on GitHub features, such as pushing, pulling and merging while working in a team. Finally, we learned the importance of helping each other debug problems and work together when things weren't working as planned.What's next for SmartSplit: In the future, we can create a mobile app that allows users to input their daily expenses and spending goals on the go. We would also like to host the website on a server so that everyone can view and access our product online. Finally, we could also train our AI integration to give more personalized budgeting advice based on user habits.",
                        "github": "https://github.com/15ANURAG34/SmartSplit",
                        "url": "https://devpost.com/software/smartsplit"
                    }
                ],
                [
                    {
                        "title": "QAOA for the kidney exchange problem",
                        "description": "Help incompatible kidney donor-recipient pairs exchange recipients with quantum computing.",
                        "story": "Inspiration: We were looking into applications of linear programming and found kidney donation chain optimization in a textbook.What it does: We use a quantum algorithm, QAOA, to solve the kidney exchange problem. We think we achieved exponential speedup.How we built it: We downloaded datasets from the internet, modernized the provided parser, wrote our own pruner and problem generator, used OpenQAOA to convert the BIP to an Ising model, used PennyLane to simulate QAOA, used the CPLEX Python bindings, PuLP, and CBC to get classical solutions, used PuLP to filter the quantum output, and used Matplotlib to generate charts.Challenges we ran into: Classiq requires manual verification of new accounts, so we had to switch tools. OpenQAOA's DOcplex to Ising model converter threw obscure errors. Performance constraints of quantum simulation on classical computers prevented us from using datasets with more than 30 donor-recipient pairs, even with unbalanced penalization to reduce variable count. Additionally, pruning optimizations made predicting model size from dataset size challenging.Accomplishments that we're proud of: We implemented a working quantum algorithm that may have exponential speedup.What we learned: We learned a lot about the applications, implementation, and analysis of quantum optimization algorithms.What's next: We could add support for success probabilities and altruistic donors.",
                        "github": "https://github.com/9t8/quantum_kidney",
                        "url": "https://devpost.com/software/kidney-donation-chain-integer-programming-with-qaoa"
                    },
                    {
                        "title": "Golden Standard",
                        "description": "We created a web application that gives a sentiment analysis on a given stock. This includes the general feelings/perceived actions of a multitude of different sources including reddit/google news.",
                        "story": "Video:https://www.loom.com/share/d6c22fc6d0134f8a80bbe58f0085d163?sid=b7c35296-d42c-407e-b3e9-6e6c3a391a02Inspiration: Golden Standard is a real-time sentiment dashboard for individual stocks. It pulls:Reddit comments from r/wallstreetbetsNews headlines via Google News(Coming soon) Twitter/X discussionsThen, it uses Gemini AI to:Summarize what people are sayingExtract bullish vs bearish perspectivesClassify overall sentiment as positive, neutral, or negativeAll this is displayed in a clean, interactive React frontend, allowing users to get a sentiment report for any ticker in seconds.What it does: Python Backend using FastAPI for scraping, AI processing, and APIsPRAW for Reddit comment parsingSerpAPI for news dataPolygon.io API for latest market dataGoogle Gemini API for AI-generated summaries and formattingReact Frontend using TypeScript and Tailwind to render news, forums, and live analysisDynamic sentiment endpoint that updates analysis based on the requested ticker in real timeHow we built it: Managing real-time scraping without being rate-limited or blockedGetting Gemini to reliably return clean, structured JSONMapping unstructured Reddit data into something AI could reason withDealing with SSL/cert issues and Python environment conflicts during setupDesigning a frontend flexible enough to render AI-driven content without breaking layoutChallenges we ran into: Creating a seamless, full-stack pipeline from live internet data \u2192 AI processing \u2192 visual displayBuilt a modular backend that can scale to other sources like YouTube, Twitter, or DiscordDeveloped custom prompts that extract meaningful insights from chaotic, noisy Reddit threadsIt works \u2014 and it feels smart. That\u2019s the best win.Accomplishments that we're proud of: Prompt engineering is everything when working with LLMsReddit comments are messy \u2014 structuring them is a project in itselfFastAPI is incredibly powerful for building real-time AI-backed APIsFrontend and backend integration for AI systems takes careful planning to avoid breaking the user experienceYou don't need a giant team to build something that feels enterprise-gradeWhat we learned: What we learned \nWe learned how to integrate routes into a frontend system.What's next for Golden Standard: dd Twitter/X data via Playwright or a scraping proxyIntegrate LLM-based alerts for unusual sentiment spikes or shiftsTrain a lightweight custom model on past sentiment + stock movement to predict short-term movesAdd multi-ticker comparisons and industry-wide heatmapsDeploy publicly and offer a free tier + pro API for fintech devs",
                        "github": "https://github.com/Claudesaul/SentimentTech",
                        "url": "https://devpost.com/software/golden-standard"
                    },
                    {
                        "title": "Purromodoro",
                        "description": "Pet system for Pomodoro Studying!",
                        "story": "Inspiration: My partner and I are interested in productivity tools and digital pets. Since I personally use the Pomodoro technique while studying, we decided to combine both ideas into a Chrome extension. The goal was to create something that helps users stay focused while also being visually engaging.What We Learned: During this project, we learned how to connect the frontend and backend, how to work with Chrome's extension API, how to persist data usingchrome.storage, and how to troubleshoot more complex issues that aren't easily resolved by AI or documentation. We also gained experience in managing collaboration under time constraints.How We Built It: We divided responsibilities based on our strengths. My partner worked on the backend logic, including the timer and data storage. I focused on the frontend, building the user interface with React and organizing the project into modular components. I also used my experience with team-based development to break down the work into clear tasks and delegate effectively.Challenges We Faced: We faced a number of challenges, particularly in getting the Chrome extension to load correctly. We also ran into environment issues, such as accidentally setting up the project using the wrong template. Debugging these problems took time and couldn't be easily solved through external help. This required patience, problem-solving, and careful coordination between us.This project taught us valuable technical and collaboration skills, and resulted in a tool that combines function and creativity in a unique way.",
                        "github": "https://github.com/annie251/bitcamp2025.git",
                        "url": "https://devpost.com/software/purromodoro"
                    },
                    {
                        "title": "Silk",
                        "description": "Use AI to find cheaper deals for products in other countries. Shop smart, thwart tariffs, and fulfill favors as you travel to earn an extra buck!",
                        "story": "What it does: Many items are more expensive in the United States than they are in other countries, due to differences in supply chain, currency exchange rates, local economic factors, etc. Additionally, many people will bring back items in their suitcases when they travel to other countries. Our hack combines these two facts to find the best deals for people by identifying the best country to buy a product, then connecting buyers with travelers to save money. We created a chrome extension to actively seek out price differences when buyers are on a shopping page, and a website that facilitates transactions.How we built it: We have three primary components as part of this hack: the frontend, backend, and a chrome extension.The frontend was built using React, using Material UI, Chakra. The frontend dynamically imports data from Supabase (PostgreSQL) database, and populates the page with various items.The backend consists of an Express API built with TypeScript, which takes a link to a product and uses Gemini to explore the price of the same product on the same website in different countries. We first take the link provided by an API call from the chrome extension (see below) and send it to Gemini via Gemini API to generate alternative links to access the website as if we are in different countries; we then use Puppeteer to take a screenshot of each website, and then send the screenshot to Gemini to get the price of the product, which allows us to visually find the price without having to figure out different web-scraping approaches for websites with different layouts. If a deal is found, the backend sends the data for a product to our database, which is accessed by the frontend.The chrome extension was made with JavaScript; it processes the contents of the user's browser and sends requests to the backend, and links to the frontend if it finds a good deal.Challenges we ran into: The main challenge on the backend was figuring out how to get the price of products in different countries. Our initial approach was to just prompt an LLM to scan different countries' versions of the website. However, we quickly found out that no LLM was able to reliably access and parse different websites and get the latest price data. We then had to find a workaround, which we did by using the screenshot approach listed above; this still used an LLM to find prices, but was a lot harder than just telling the LLM to do so.Accomplishments that we're proud of: Before this Bitcamp, we didn't have any experience in many of the technologies we used for our project. We had to learn how to create an API, how to prompt an LLM via API, how to use a headless browser, and how to integrate multiple features together with other APIs and a SQL database.",
                        "github": "https://github.com/atcupps/silk",
                        "url": "https://devpost.com/software/silk-6nhgiy"
                    },
                    {
                        "title": "OCRganise",
                        "description": "Scan . Split . Supervise.",
                        "story": "Inspiration: The idea forOCRganisecame from personal experience living with friends and constantly needing to split grocery bills. Manually entering expenses into different apps was frustrating and time-consuming. I wanted a single solution that could handle it all: scan receipts, split costs, and track expenses intelligently. That\u2019s where the idea for a smart, unified expense management app came from.What it does: OCRganise is a smart expense tracker, bill scanner, and cost splitter all rolled into one.\n-Users can upload or take a photo of their receipts.\n-The app uses OCR (Optical Character Recognition) to extract items and prices.\n-You can then split the total (or individual items) with friends.\n-It also tracks your personal and shared expenses over time.\n-It\u2019s designed to be intuitive, fast, and accurate, making group expenses hassle-free.How we built it: OCRganise is a Progressive Web App (PWA), so it works seamlessly on both phones and browsers.Frontend: Built with Next.js for responsiveness and ease of use.Backend: Powered byGemini AIto categorize and process scanned items intelligently.OCR: Uses TabScanner OCR for extracting text from receipts.Database:MongoDBstores user data, receipt details, and expense tracking information.Challenges we ran into: -Handling item splitting dynamically across multiple users was trickier than expected.\n-Making it work smoothly as a PWA involved debugging service worker issues for offline caching and performance.Accomplishments that we're proud of: Successfully integrated OCR with receipt scanning and expense tracking in one flow.\n-Built a full-stack PWA that works across devices.\n-Created a user-friendly interface for splitting expenses\u2014no calculator required!\n-Used AI to make the categorization process smart and scalable.",
                        "github": "https://github.com/rahulcs6104/OCRganise",
                        "url": "https://devpost.com/software/ocrganise"
                    },
                    {
                        "title": "Peer Pop - A Peer-to-Peer Learning Network",
                        "description": "A Peer-to-Peer Learning Network",
                        "story": "Inspiration: We noticed a major gap in how students share knowledge. While we all have skills we\u2019re proud of and others we want to learn, there\u2019s no structured way to connect learners and teachers within the same community. That's how Peer Pop was born \u2014 a smart, AI-powered platform that connects students based on the skills they can teach and want to learn.Peer Pop helps students:Create skill profiles by listing what they know and want to learnMatch with peers who complement their learning goalsReceive email notifications upon being matchedRate their learning sessions and track progressVisualize top skills, trending technologies, and peer learning activity in real-time,We built Peer Pop using a full-stack approach:Frontend:React.js with Tailwind CSS for a clean, responsive UI, Leaflet.js for skill-to-peer map visualizationBackend:Node.js + Express for user and skill management APIs, MongoDB to store user data, skill relationships, and session logsMI & Data:Python-based collaborative filtering model using implicit, Recommendations for \u201cSkills You Might Like to Learn\u201d, Streamlit dashboard for live admin and analytics viewFeatures: Skill-based Peer Matching\n Instant Notifications on Match\nSkill Recommender Engine (Collaborative Filtering)\n Streamlit Dashboard: View Top Skills, Active Learners, Skill Trends\n Map-Based Teacher Locator by Skill-Built a fully working recommendation system without needing explicit ratings\n-Designed a sleek, interactive UI for students to explore and connect\n-Used real AI techniques (matrix factorization) to generate valuable insights\n-Created a data dashboard that can scale to support thousands of users-Add in-session scheduling & chat\n-Integrate with student calendars\n-Expand to support club-based or class-specific learning hubs\n-Offer badges and gamified incentives for top contributors\n-Launch a mobile version-Working on Peer Pop taught us so much more than just writing clean code. We learned how to:Design with empathy \u2014 understanding real student challenges around peer learning, confidence gaps, and access to helpTurn data into insight \u2014 by using collaborative filtering to generate smart, personalized skill suggestions based on peer behaviorBalance scalability and usability \u2014 creating a matching algorithm that\u2019s fast, fair, and meaningful for diverse usersIntegrate multiple technologies \u2014 from MongoDB and Node.js to React, Python, and Streamlit \u2014 in a seamless full-stack workflowVisualize engagement trends \u2014 using dashboards to understand what skills are popular, which users are active, and where demand-supply gaps lieSimulate real-world data \u2014 generating a robust synthetic dataset to test ML models in the absence of real user inputBut most importantly, we learned that the future of learning is collaborative \u2014 and building tools that empower students to learn from one another can be incredibly rewarding.Languages:JavaScript (React.js, Node.js), Python (for data modeling and recommendation system)Frameworks & Libraries:Databases:MongoDB\u2013 User and skill data storage (NoSQL)Platforms & Services:MongoDB Atlas\u2013 Cloud database hostingGitHub\u2013 Version control and collaborationAPIs & Utilities:EmailJS / Nodemailer\u2013 Notification and confirmation emails",
                        "github": "https://github.com/Divyansh121699/PeerPop",
                        "url": "https://devpost.com/software/peer-pop-a-peer-to-peer-learning-network"
                    },
                    {
                        "title": "Zentrail ",
                        "description": "Zentrail is your AI-powered national park planner\u2014effortlessly create personalized itineraries with trails, campsites, and more for a seamless, serene adventure.",
                        "story": "Inspiration: Our team was inspired by the challenge of making national park exploration more accessible and informative. We noticed that while there's abundant park data available, it's often scattered and difficult to navigate. ZenTrail aims to solve this by providing a unified, AI-powered platform for park discovery and trip planning.What it does: Interactive trail maps with real-time difficulty visualizationAI-powered park assistant for personalized recommendationsIntegrated campground booking systemActivity-based exploration interfaceReal-time trail conditions and accessibility informationOffline map functionality\n-Custom travel iternary,Technical Stack: Frontend: React + TypeScript, TailwindCSSBackend: Node.js, ExpressDatabase: MongoDBAPIs: National Park Service, OpenAI GPT-4Mapping: Leaflet.js, OpenStreetMap,Challenges we ran into: What we learned: Advanced React state management patternsGeoJSON data processing and visualizationAI integration best practicesReal-time data synchronizationProgressive web app optimization,Team: Backend Development: @praneethraviralaFrontend Development: @banudeepUI/UX Design: @aravindpanchanathanAI Integration: @aishwarya,What's next for Zentrail: User-generated trail reviewsWeather integrationAugmented reality trail markersSocial features for hikers,Native mobile appsEnhanced AI capabilitiesReal-time trail conditions,",
                        "github": "https://github.com/Banudeep/zentrail_bitcamp/tree/fast_api",
                        "url": "https://devpost.com/software/zentrail"
                    },
                    {
                        "title": "Gesture Based Music Controller",
                        "description": "a unique way to interact with and produce music using your own two hands and laptop webcam",
                        "story": "",
                        "github": "https://github.com/tanvigup7/GestureBased",
                        "url": "https://devpost.com/software/gesture-based-music-controller"
                    },
                    {
                        "title": "cram.cam",
                        "description": "cram.cam is a gamified study app that tracks focus with your webcam and browser. Earn XP when locked in, lose it when distracted. The UI/UX is meant to feel like an immersive notes page experience!",
                        "story": "Inspiration: We builtcram.cambecause it\u2019s way too easy to get distracted. You sit down to study but then suddenly end up deep in a YouTube rabbit hole. We thought to ourselves, what if study time felt like a game? What if leaving your seat or opening a distracting website had real consequences? That\u2019s when we decided to make cram.cam, a gamified study app that uses computer vision and a Chrome extension to help students stay locked in.What it does: cram.camis a gamified study app that keeps students focused by tracking both their attention and their browser activity.Uses webcam-based computer vision to detect if you're actively studyingDetects when you leave your seat or look awayChrome extension flags sites like Netflix, YouTube, and TiktokEarn 10 XP for every minute you\u2019re focusedLose 15 XP if you're distractedRed overlay activates when focus is lostA live chat hypes you up or calls you out in real timeA dashboard displays your daily and weekly study timeYou can unlock and apply virtual accessories (like hats and glasses) to your webcam avatar while studying, based on XP,How we built it: Frontend: Built in React with a UI inspired by physical study notes, with pastel colors, highlighter text, sticky notes, and a grid paper background.Backend: Developed with FastAPI, using OpenCV to detect whether the user is present on camera. . Created a Python script with OpenCV to detect facial landmarks and detect whether the user is present on camera and studying. Developed a FastAPI backend to serve webcam data from the frontend to OpenCV and return an analysis of the user's actions (studying or procrastinating), which the frontend uses to alert the user that it knows they are not locked in :). It also manages pause state, XP logic, notifications, and counters for how long the user is studying or distracted.Chrome Extension: Written in JavaScript, it listens for tab updates and POSTs to the backend if a distracting site is opened.Webcam Polling: Every 3 seconds, the frontend sends a base64 screenshot to the backend, which returns whether the user is focused or not, and polls the backend to see if the pause endpoint has been activated, either by the python script or the extension.Dashboard: A separate component shows XP progress, daily/weekly stats, and available filter accessories based on unlocks.,Challenges we ran into: HandlingCORSissues between the Chrome extension and the FastAPI server.Designing a single polling loop that merges webcam detection and backend pause logic without creating race conditions.Preventing webcam-triggered auto-resume from overriding manual or extension-based pauses.Making the computer vision detection reliable across lighting, angles, and different laptops.Building a fun UI that doesn't feel cheesy or overwhelming while still giving clear feedback.Getting dashboard stats and virtual accessories to update in sync with XP and timer state,Accomplishments that we're proud of: Fully implemented webcam-based presence detection and made it work seamlessly in a React frontend.Created a working Chrome extension from scratch that detects and reports distracting websites in real time.Built a live XP tracker with motivational and sarcastic chat messages to keep you accountable.Designed an interface that looks and feels like a real study space, with highlighters, sticky notes, and pastel visuals.Developed a clean, interactive dashboard that shows daily and weekly progress and lets users unlock study accessories,What we learned: How to integrateOpenCVinto a modern web stack usingFastAPI.How to build and communicate between aChrome extensionand an external backend.Better state management in React when pulling in multiple async data sources.Designing UI/UX with a strong metaphor, making cram.cam feel like a digital notes page.Balancing functionality and aesthetic, especially with stats, overlays, and visual feedback all updating in real time,What's next for cram.cam: Add leaderboards to see how your focus stacks up against friends.Add XP leveling and unlockable rewards or study themes.Build a mobile app for more focusing on different apps.Expand filter accessory options and customize webcam overlays,",
                        "github": "https://github.com/sidblwl/bitcamp2025",
                        "url": "https://devpost.com/software/cram-cam"
                    },
                    {
                        "title": "Super Shell",
                        "description": "This project is a custom-built terminal designed in C#, featuring a tabbed layout, with styled input, output, real-time autocomplete support, and a custom Git client built in to the terminal.",
                        "story": "Inspiration: The inspiration for this project came from a desire to build a modern, customizable terminal experience that breaks away from the limitations of traditional console windows. We wanted something lightweight, extensible, and visually clean, something that felt more like a code editor's integrated terminal than a raw system shell. We saw it as a chance to dive deeper into UI/UX within the context of a developer tool. We also really wanted to integrate a Git Client that could handle commits, pushes, and merges like a modern IDE.What We Learned: Through this project, we learned a about pain of making GUIs. Furthermore, we deepened our knowledge of the way terminals work, which was significantly more complicated than we had expected.How We Built It: The app is built using WinUI 3. The Git Client is integrated through a separate window and was made using Github's \"oauth application\" service. The way the terminal works is it uses Microsoft's pseudoconsoles, which allow our terminal to easily pipe information to and from stdin/stdout.  Then we built an ANSI escape sequence parser to handle the most common escape sequences programs uses to interact with the console.Challenges: Understanding exactly how consoles work was difficult. Specifically being able to interact with cmd.exe, since there is very little documentation online about how pseudoconsoles work. Getting the GUI to work was shockingly difficult. The autocomplete and mixing asynchronous with a GUI created a lot of strange errors that were hard to root out.",
                        "github": "",
                        "url": "https://devpost.com/software/terminal-app"
                    },
                    {
                        "title": "FrameSleuth",
                        "description": "FrameSleuth is an intelligent video analysis tool that utilizes a user-provided description to pinpoint and interpret key frames in a video.",
                        "story": "Inspiration: The first 48 hours after a crime are critical. We were inspired by this fact to create a tool that automatically analyzed surveillance footage and detected people of interest based on a description. As we worked on the project, we extended our focus to identify not only people but any feature in the video given a short description. With a 2,600 tokens per second inference speed, FrameSleuth is a scalable solution for ultra fast video processing.What it does: FrameSleuth processes videos using Llama 4 to automatically flag video frames that contain a specified feature or object. Our tool utilizes a user-provided description to identify features in the target video, allowing detectives or forensic analysts to efficiently go through massive amounts of video footage. Our solution is also highly scalable because we can process several video frames in parallel enabling faster inference. Given the 2,600 tokens/second inference speed, we can process an hour long video in just 1.5 minutes, providing 40x faster video analysis.How we built it: We built this using Meta's latest large language model, Llama 4, in conjunction with Groq API to run inference with Llama 4. We developed an interactive frontend using Typescript and React as well as a backend using Flask backend to run Python logic for Llama 4 inference.Challenges we ran into: We ran into challenges with Groq API since we were using the free version, and it was slow due to rate-limiting. We also had to refine our prompting to the Llama model and iteratively test this to ensure accurate results.Accomplishments that we're proud of: We\u2019re proud of building a novel application for the Llama 4 model that solves general object detection problems leveraging the latest AI technology. Despite challenges with API limitations, we achieved fast, accurate frame-level detection with a smooth and intuitive interface. Most importantly, we created a solution that could meaningfully assist in real-world scenarios like time-sensitive investigations, bringing cutting-edge AI closer to impactful use cases.What we learned: How to use Groq API to integrate an LLM into a React applicationHow to build a desktop app using Electron and ReactHow to integrate LLMs like Llama 4 for high-throughput, frame-level video understandingThe importance of prompt engineering when generating visual detection criteria from natural language,What's next for FrameSleuth: Add multi-object tracking, allowing the system to track individuals across frames once detected.Incorporate real-time streaming support so FrameSleuth can flag objects in live camera feeds.Improve the natural language interface with dialog-based refinement (e.g., \"Show me only people wearing red jackets\").Introduce a visual dashboard for interactive playback and frame annotation.Extend to edge deployment using optimized models for on-premises or mobile hardware.Explore integration with law enforcement tools or emergency response pipelines.,",
                        "github": "https://github.com/V-Coding/LLaMaVid",
                        "url": "https://devpost.com/software/framesleuth"
                    },
                    {
                        "title": "Jobsearch URL",
                        "description": "My AI agent automates internship applications by scraping jobs, reading descriptions, using a local LLM to fill forms, and generating personalized cover letters via RAG from my resume.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/jobsearch-url"
                    },
                    {
                        "title": "FridgeBuddy",
                        "description": "An assistant to help you creatively cook. ",
                        "story": "Inspiration: As busy college students, our lives are consumed with various important matters, so while cooking often slips to the back of our minds, our fridges never fail to be stocked with a random assortment of ingredients. We have had countless experiences opening the fridge, having no idea what to make, so giving up and getting Chipotle instead. We wanted to make something to inspire ideas of what to cook.What it does: In an effort to prevent our food from expiring, and to build some important cooking skills, we have created a web application using React that allows users to input the ingredients in their fridge and get 3 recipes in return.How we built it: We used React for our frontend. Our web app makes an API call to Gemini with Flask, using the ingredients selected by the users to generate recipes, then processing the recipes it returns and storing them in a MongoDB database. We then display three recipes to the user in an easily understandable manner, and hope that they get cooking!Challenges we ran into: Managing all the various state in React proved challenging at a certain point, and so did maintaining the connection with the backend, as they were hosted on different ports. As first time hackers and a team of only two, we did struggle with time pressure.Accomplishments that we're proud of: We found that we were able to create the product we were envisioning, which we consider a great success! All of our icons are hand drawn by us, which adds a significant personal touch.What we learned: This invaluable experience allowed us to develop our skills in full-stack development, and caused us to gain significantly more familiarity with React and Flask.What's next for FridgeBuddy: In the future, we envision our hack to store user information, and allow them to save/like recipes, so that it can be a long-term tool for clients. We hope to add other fridge cataloging tools, such as an expiry reminder, in order to focus more on sustainability as well.",
                        "github": "https://github.com/tanvipanse/FridgeBuddy",
                        "url": "https://devpost.com/software/fridgebuddy-6j3fao"
                    },
                    {
                        "title": "Podchat.ai",
                        "description": "Your own personal co-host for your podcast needs!",
                        "story": "Inspiration: Podcasts have been rapidly growing over the past couple years, with there being over 450 million podcast listeners worldwide in 2023 alone. As podcasts continue to grow, I noticed that there was a place for AI to gather and summarize information for hosts whether before they go live or during. Additionally, as concerns about misinformation from podcasts rise, I found a need to make this tool to better ensure that hosts are more information about the topics they discuss, while allowing for extra practicality in sharing notes for the overall broadcast itself.What it does: By running the command \"flet run --web\" in the \"test/src\" folder, you are greeted to an open chatroom that can be accessed by multiple users. Once inside the chatroom, users can talk to each other as they would on any messaging platform. On top of standard messaging, by typing the following commands, the user can get specific, AI assisted, info on whatever topic they are interested it, via Podchat's own Jenna.How we built it: I used python to build the entire project. Specifically, using the 'flet' library which is a python binding for flutter that lets you develop and build cross-platform apps using just python. I also used Google's Gemini API as the AI service for Jenna, the in-chat assistant. Giving the AI a real, common name also helped to humanize the AI and make interactions with it feel more conversational rather than transactional.Challenges we ran into: At first, python was not the language I planned to use. The first couple hours I attended was rough as I bounced from language to language, framework to framework, just to end up using the classic python. After that, most of the challenges came from debugging issues with the flet library in terms of rendering and attempts to implement multiple pages inside our one app, which ultimately was not achived.Accomplishments that we're proud of: Being able to find a practical tool that helped development time rather than hindered it. Additionally, I'm proud I was able to make it over the span of a day as a solo dev, while still attending workshops and making connections. I met a lot of friends and gained knowledgeable insights in fields like cybersecurity, AI Training, App Dev, and I'm happy that I took notes during those workshops as I will be going back to them soon.What we learned: Balance priorities when building a project, especially on such short notice. As the deadline rings closer, bugs and issues you couldn't even conceive will come at the most inconvenient times. However, don't only work on your project, as you'll miss out on valuable experiences and lessons. Best to keep a balance of the both.What's next for Podchat.ai: More development time. Would be fun to finish during the summer. Very useful.",
                        "github": "https://github.com/abdiToldSo/podchat.ai",
                        "url": "https://devpost.com/software/podchat-ai"
                    },
                    {
                        "title": "Drink Happy",
                        "description": "drinkhappy.tech is a gamified hydration wellness app that helps users track drinks, earn points for healthy choices, and compete with friends. It's powered by Gemini AI for smart drink recognition.",
                        "story": "Inspiration: We wanted to build a web application that promotes healthier beverage choices through an engaging and interactive experience. Many health tools focus on food or exercise, but we wanted to create something specifically for drinks, where users can track their consumption and receive feedback that encourages better habits.What it does: DrinkHappy is a web app where users log their daily beverage intake and earn points based on hydration, sugar, and caffeine content. Points are awarded for healthy choices such as drinking water or staying within recommended limits. Drinks that exceed those limits do not earn points. The platform uses the Gemini API to identify beverages based on user input, allowing users to type or upload descriptions of their drinks. Based on the recognized drink and amount, points are calculated automatically.The application includes several core features:A profile page displaying total points, daily stats, and a bioA social feed where users can post beverage updatesReaction buttons such as \u201clike\u201d or \u201cstop drinking that\u201dA shop where points can be used to unlock additional profile picturesA leaderboard displaying who has the most points amongst you and your friendsA guide page that educates users about healthy beverage choices,How we built it: The frontend was built using React with Next.js and styled using Tailwind CSS. We developed a custom theme system to support both dark and light modes for a consistent user interface. MongoDB is used for storing user data. Gemini API integration enables drink recognition from user-submitted descriptions. We divided across team members to handle the profile system, point logic, shop interface, post feed, and backend communication.A graph of the tools and frameworks we used is linkedhere.Challenges we ran into: This was the first hackathon for most members of the team, so we encountered several challenges throughout development.Integrating the Gemini API required navigating multiple libraries, backend logic, and resolving confusion caused by inconsistent documentation. We resolved this through hands-on testing and iteration.Designing a consistent UI across both dark and light themes took time and coordination. We used Tailwind CSS to create a custom theme and applied it across all components to maintain a cohesive look and feel.Accomplishments that we're proud of: We successfully built a complete application with a working point system, drink recognition via AI, a functioning post feed, a customizable profile system, and a responsive theme. All of these components were integrated and tested within the hackathon timeframe.What we learned: We gained experience in full-stack development, including working with React, Next.js, MongoDB, and Tailwind CSS. We also learned how to integrate a third-party AI API into a real application and how to manage frontend and backend coordination. Additionally, we practiced collaborative development through GitHub and resolved technical challenges efficiently under time constraints.What\u2019s next for DrinkHappy: We plan to extend the platform by adding more interaction (e.g. comments) to posts, persistent reaction storage, and expanding the shop with more profile customization options and gamification. Future improvements also include drink logging analytics, social features and streaks, and enhanced user interaction features.",
                        "github": "",
                        "url": "https://devpost.com/software/drink-happy"
                    },
                    {
                        "title": "Visionary",
                        "description": "Lighting up the world, one at a time.",
                        "story": "Inspiration: We are inspired to bring a change in the life of visually impaired people.What it does: It takes in real time motion through webcam and describe the surroundings so that visually impaired people have less problems in their day to day life.How we built it: We used pretrained YOLO's v8 in python and trained it with day to day objects' images. Based on the image we got the type of object like car, person etc. and then the model speaks out what the object is.Challenges we ran into: Training AI model was one of the biggest task we had to accomplish, but apart from that there were many problems in audio outputs and object detection from live feed.Accomplishments that we're proud of: Training an AI model and deploying it successfully.What we learned: Connecting backend to frontend, training AI model.What's next for Visionary: We want to improve our skillset so that we can make this a fully functional app as originally intended.",
                        "github": "https://github.com/Saumya-patel-31/Visionary.git",
                        "url": "https://devpost.com/software/visionary-gjtfc4"
                    },
                    {
                        "title": "MindMapify",
                        "description": "Create a virtual brainmap and gamify learning with Mapify.",
                        "story": "",
                        "github": "https://github.com/RohanT17/bitcamp25",
                        "url": "https://devpost.com/software/the-red-team"
                    },
                    {
                        "title": "SnortEduGuard \u2013 Student Integrity Surveillance System",
                        "description": "SnortEduGuard is a real-time, AI-powered IDS for classrooms that detects access to AI tools, VPNs, and cheating sites using Snort rules, log parsing, and natural language smart search.",
                        "story": "Inspiration: As a Teaching Assistant at the University of Maryland, I frequently conduct lab sessions, quizzes, and in-class exams. I've seen firsthand how students try to find creative ways to bypass academic rules, especially using generative AI tools and VPNs. While solutions like LockDown Browser exist, they can often be circumvented, and they only monitor the browser, not the network. That inspired me to build something that works at the network layer: a real-time, intelligent intrusion detection system for classrooms and exams that helps preserve academic integrity in a meaningful way. This was the student's focus, which will be redirected to learning from the lectures, doing homework genuinely, and earning the real learning, which will prepare them for the real world.What it does: SnortEduGuard is a real-time academic intrusion detection system designed specifically for educational environments. It:Uses Snort 3 to detect access to generative AI tools, VPNs, collaboration platforms, and study-help websitesParses Snort alerts into structured JSON using a custom Python scriptDisplays alerts in a real-time dashboard with charts, filtering, exporting options, and instructor loginIncludes an AI-powered Smart Search assistant that understands natural language queries like \u201cShow Chegg alerts in last 120 minutes\u201d and summarizes results in plain EnglishSupports an \u201cExam Mode\u201d that flags any traffic not explicitly allowed (Canvas, UMD, Zoom, etc.)The system is production-grade and could be deployed during live exams or quizzes.,How I built it: The architecture is modular and fully functional:Snort 3 Detection Engine: I configured Snort 3 with custom rules to detect unauthorized domains and behaviors. The rules were written to inspect TLS SNI headers, identify protocol types, and categorize alerts by AI tools, VPNs, C2 behavior, or whitelisted services.\nI wrote custom Snort rules to detect traffic to:Logging and Parsing: Snort logs alerts to /var/log/snort/alert.fast. A custom Python parser (parse_alerts.py) uses regex to extract alert fields like timestamps, IPs, protocols, priorities, and SIDs. It saves these to a structured file (parsed_alerts.json) used by the dashboard.Dashboard: Using Flask, Bootstrap 5, and Chart.js, I created a dashboard that displays alerts in real time. It auto-refreshes every 10 seconds, shows a visual breakdown of priority distribution, and allows CSV/PDF exports. The UI includes a login system and displays alerts with color-coded highlights for quick analysis.Smart Search Assistant: The real \u201cwow\u201d factor is an AI-powered assistant. Instructors can type prompts like \"Show Chegg alerts in last 30 minutes.\" The backend (parse_query.py) uses spaCy NLP to understand intent, and then smart_filter.py filters alerts. Finally, summarizer.py generates plain-English interpretations like:\n\"192.168.1.105 attempted to access ChatGPT at 6:42 AM.\"Exam Mode: I implemented a fallback default-deny rule that flags any traffic not matching whitelisted domains. This allows instructors to switch into a strict mode where only Canvas, UMD, or Zoom are permitted, helping enforce academic integrity during exams.Challenges I ran into: One of the hardest parts was configuring Snort 3 logging with Lua and ensuring it streamed reliably to alert.fast without truncation or corruption. I had to carefully coordinate logging, parsing, and visualization to work asynchronously without conflicts.\nThe Smart Search assistant also required careful tuning so that natural language queries could be accurately interpreted and mapped to real categories like \u201cAI\u201d or \u201cVPN,\u201d even when phrased differently.\nStyling the dashboard to be clean, intuitive, and visually meaningful under time pressure was also a significant design challenge, especially for me, as I have never done this before in my life. A huge learning curve for me!!Accomplishments that I're proud of: What we learned: First and foremost, \"TIME IS MONEY\"!I deepened my understanding of:It reinforced my belief that cybersecurity is not only about preventing attacks; it is also about integrating trust and fairness into the systems we use daily with the security knowledge I possess.Time is crucial. If I have an idea, that's good; the only thing that matters in a hackathon is how to think of the idea, execute it, build a model, enhance it, exceed your own expectations, and, above all, submit it on time.What's next for SnortEduGuard \u2013 Student Integrity Surveillance System: There\u2019s still so much I\u2019m excited to build on top of what\u2019s already working.First, I\u2019d love to deploy the entire system on a Raspberry Pi, plugged into a SPAN port in a classroom network. That way, it can passively monitor traffic without needing any installation on student devices.I also want to build a policy configurator\u2014a simple UI where instructors can upload custom whitelists or choose modes like \u201cQuiz Mode\u201d or \u201cNormal Class Mode.\u201d Right now, modes are enforced through rules, but making this user-friendly would make the tool way more accessible.Another idea I\u2019m exploring is integrating Claude or OpenRouter to generate even more intelligent summaries. Imagine an AI not just summarizing what happened, but also suggesting:\n\"You may want to follow up with students 192.168.1.5 and 192.168.1.9 who accessed Chegg during the assessment window.\"It would also be awesome to add live alerting\u2014like sending instructors an email, a Slack message, or even a quick text when something suspicious happens in real time.I definitely want to keep improving the Smart Search assistant so it supports follow-up questions, more nuanced queries, and even incident response guidance. That\u2019s where things could really start to feel like a security analyst-in-a-box.And eventually, I plan to open-source the project, with clear documentation, setup scripts, and a ready-to-use classroom deployment guide. I want other educators and researchers to build on top of this and adapt it to their own environments.This project started with one simple question: \u201cCan I build something that actually helps instructors maintain integrity in a real classroom?\u201d Now that it\u2019s working, I want to take it further\u2014make it more intelligent, more scalable, and more useful to others.",
                        "github": "https://github.com/akshatpatel64/SnortEduGuard",
                        "url": "https://devpost.com/software/snorteduguard-student-integrity-surveillance-system"
                    },
                    {
                        "title": "Fas, Fast & Faster",
                        "description": "Fas, Fast & Faster is all about reaction speed, your decision speed, and your competitive edge. In this challenge platform, we test how quickly you can respond, adapt, and improve.",
                        "story": "Inspiration: Our project was inspired by and is a parody(with our own ideas) of Human Benchmark\u2014a simple yet addictive site that tests various cognitive and motor skills. We wanted to build something similar, but with a twist: increasing levels of challenge and obscured feedback. That\u2019s how Fas, Fast & Faster was born.What it does: Fas, Fast & Faster is a 3-part web experience designed to test and train your speed, perception, and accuracyHow we built it: We used HTML, CSS, and JavaScript for all three interactive modules. The app is lightweight and runs fully in the browser. Each game was built as a separate linkable module:Challenges we ran into: Some websites didn't work since we decided to code the project on replit which only allowed free access for a limited amount of time.Accomplishments that we're proud of: We were able to learn and code a fun idea with not a lot of time since we all had other occupations during the hackathon.What we learned: The power of friendship can get you through anything.What's next for Fas, Fast & Faster: We will probably enhance it to work out the games to make them more fun to post on LinkedIn.",
                        "github": "",
                        "url": "https://devpost.com/software/fas-fast-faster"
                    },
                    {
                        "title": "BoxBot - The Dancing Robot!",
                        "description": "BoxBot is a dancing robot! Play your favorite tunes and watch it dance along! Encased in a 3D-printed case with custom mounts and cutouts, BoxBot uses an Arduino Uno and servo motors to boogie down!",
                        "story": "Inspiration: As computer engineers, we wanted to work on a project that incorporated our passion for hardwareandsoftware, so we decided to create a robot. Our goal was to entertain -- to make your day a little brighter! That's why we decided on adancingrobot.What it does: BoxBot dances using 3 movement systems -- arms, legs, and feet.How we built it: We built BoxBot using an Arduino Uno, a motor shield, 6 continuous servo motors, and many custom 3D-printed parts. We started by researching Arduino-based robots built by other engineers. After scouring for parts at the MLH hardware store, we realized that our robot would have to employ servos for movement. Using Fusion 360, we developed a simple case to house our components, accounting for tolerances, cable management, and potential redesigns. While our parts were being printed, we used a few paper coffee cups to develop a working prototype. Attaching our arms and legs to the coffee cups, we built and tested movement code using the Arduino IDE. Once our parts were printed (not without a few hiccups that led to slight redesigns), we put it all together and started programming the actual dance moves.Challenges we ran into: We faced challenges 3D-printing our parts, finding working, usable components, and coding our robot's dance moves. During a pivotal moment in our development process, we had to decide between speed and quality: either we could 3D print our parts at full speed, sacrificing some structural integrity, or we could print them at regular speed, maximizing sturdiness. At different points in our timeline, we prioritized different goals. Early on, we were willing to give up some time for higher-quality parts, but as the Bitcamp timer inched closer and closer to 0:00, our goal became time efficiency. We also faced challenges finding a functioning Arduino Uno and servo motors. This led to a ritual in which we systematically tested every component we added to our robot, ensuring nothing would go wrong during showcases. Finally, our code took perhaps the most time to develop. We had to develop a \"home base\" for our servos, specific angles in which they should return to when at rest. Then we had to test various speeds and delay times to develop an effective dance routine.Accomplishments that we're proud of: We're incredibly proud of how we used our environment to our advantage during the build process. Instead of waiting 6+ hours for our parts to finish printing, we used this valuable time to build a makeshift robot using coffee cups and cardboard we found lying around. We also searched for other hardware teams to ask questions and get feedback on our project, which led to major improvements in our design.What we learned: We not only learned the art of patience but also furthered our understanding of computer-aided design, low-level programming for hardware, and the overall engineering design process. Although we had previous experience working on engineering projects, this was the first that operated under very strict time constraints. We pulled all-nighters, ate while working, transferred leadership whenever either of us felt burned out, and learned more about our individual strengths and weaknesses when working under pressure!What's next for BoxBot - The Dancing Robot!: We want to develop a more streamlined version of BoxBot -- one that incorporates more soldering to minimize cable clutter, less tape to improve aesthetics and stability, and some more personality -- maybe some paint, stickers, or googly eyes!",
                        "github": "",
                        "url": "https://devpost.com/software/boxbot-r0lc6m"
                    },
                    {
                        "title": "Impause",
                        "description": "Impause curbs impulse spending with real-time insights and creates a fun, personalized finance wrap-up to help you track habits, stay aware, and spend smarter.",
                        "story": "Inspiration: Many people struggle with impulse spending without realizing the long-term effects on their financial well-being. We wanted to create a tool that not only increases awareness but also makes financial responsibility engaging and collaborative. That\u2019s how Impause was born\u2014built to help users pause, reflect, and regain control of their spending habits.What it does: Impause is a personal finance companion that helps users curb impulse purchases, track spending patterns, and stay financially accountable.\nKey features include:Impulse Buying Buffer: A pause mechanism before purchases to reflect on cost, goals, and time value.Finance Wrapped: A fun, visually rich summary of your weekly, monthly, or yearly spending\u2014like Spotify Wrapped for your wallet.Accountability Buddy: A feature to pair with a friend to share progress and stay motivated through light social accountability.,How we built it: We used React, Vite, and Tailwind for the front end. We utilized Cloudflare Workers for the backend, and AWS, Node.js, Express, and MongoDB Atlas for authentication.Challenges we ran into: The main challenge we ran into was trying to integrate authentication in the front end and configuring the firewall.Accomplishments that we're proud of: We were able to deploy a functional web app.What we learned: Behavioral psychology plays a huge role in financial decisions.Small UX decisions (like how long to delay a purchase) can significantly impact user satisfaction.People respond positively to transparency, gamification, and social reinforcement in managing money.,What's next for Impause: Integrating with bank APIs for seamless, automatic expense tracking.Expanding the Finance Wrapped to include goal tracking and streaks.Launching mobile notifications and widgets for real-time impulse purchase alerts.,",
                        "github": "",
                        "url": "https://devpost.com/software/impause"
                    },
                    {
                        "title": "City Notes",
                        "description": "Drop a vibe, leave a mark. Our app lets you pin notes with moods and captions on the map\u2014explore the world through feelings, not just places. Discover what others felt, where.",
                        "story": "Inspiration: Have you ever sat down in the UMD chapel and read anonymous notes? Imagine being able to do that in an entire city! We wanted users to be able to record and share their memories that are tied to various places on a map (not just establishments!) and show the vibes surrounding each place.What it does: Our app allows users to mark a shared map with an anonymous note containing a vibe and a caption. Then other users can filter through the map to explore romantic spots, happy spots, good study spots, and more! Users can also filter for specific vibes or words and they can explore random notes that other people have left around the world.How we built it: We built the app in React, and use Firebase to store users' notes. The map was rendered using the library Leaflet.Challenges we ran into: We had some struggles with UI/UX features, such as a sidebar for our website. We also had some struggles with integrating our button functionality with Leaflet features such as popups.Accomplishments that we're proud of: We were able to replicate the idea and mockup we had in our heads to create this project. We were able to do so while including nice UI and feel proud of seeing this project through.What we learned: We learned a lot about using React and Leaflet. We also gained some new experience with incorporating Firebase into our project.What's next for City Notes: Next, we would probably further improve upon the UI/UX of our app, and add a chatbot to help users add pins.",
                        "github": "",
                        "url": "https://devpost.com/software/city-notes"
                    },
                    {
                        "title": "Asklepian",
                        "description": "A mobile app to scan prescription drug labels and compare them to a MongoDB for ease of patients.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/asklepian"
                    },
                    {
                        "title": "LaunchMate",
                        "description": "LaunchMate: Your AI-Powered Partner for Transforming Startup Ideas into Successful Businesses with Ease and Confidence.",
                        "story": "",
                        "github": "https://github.com/AdhyyanKumar/launchmate",
                        "url": "https://devpost.com/software/launchmate-tech"
                    }
                ],
                [
                    {
                        "title": "ThriveUSA",
                        "description": "Our platform connects American small-mid sized businesses with local suppliers, enabling cost savings and encouraging growth in the USA manufacturing and supply industry for a sustainable future.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/thriveusa"
                    },
                    {
                        "title": "Myndful",
                        "description": "An LLM-powered pipeline that transforms natural language queries into structured filters across personal data sources (Calendar, Email, Maps, Photos) to deliver context-aware responses.",
                        "story": "",
                        "github": "https://github.com/AaravNirmal/Bitcamp-Hackathon-Apr2025",
                        "url": "https://devpost.com/software/myndful-96gosd"
                    },
                    {
                        "title": "GeoGuard",
                        "description": "Raising awareness about location risks ",
                        "story": "Inspiration: Social media has become more and more common in our everyday lives, to the point where it's basically a normal part of how people communicate and stay connected. It\u2019s not just adults who use these platforms anymore. Many kids and younger teens now have easy access to social media apps and websites. Even though most platforms have age restrictions, it\u2019s pretty easy for minors to get around those rules just by entering a fake birthdate or using someone else\u2019s information. Because of this, there\u2019s a growing concern about how safe the internet really is for younger users. With more children spending time online, the risks of being exposed to inappropriate content, interacting with strangers, or accidentally sharing private information are much higher. This makes online safety, digital responsibility, and awareness more important than ever.What it does: Users can upload images directly to the website, where our tool automatically scans for sensitive location information, like street signs, building names, or anything that could give away where they are. The website then highlights these areas, and users get to choose whether or not they want to blur each highlighted region. After making their selections, they can download the edited version of the image, which helps protect their privacy before sharing it online. We also added a feature that lets users use their live camera. This is especially useful before things like video calls or livestreams. The tool scans the background in real time and checks for any location-sensitive details that might be visible. This gives users a chance to fix or hide anything before they go live, helping them stay safe and aware of what\u2019s around them. We wanted to make sure the process was simple and easy to use, especially for younger users, so they can better understand how to protect their personal information online.How we built it: To build our project, we started with the frontend using React, which is a JavaScript library that helped us create an interactive user interface. For styling, we chose Tailwind CSS because it made designing the pages quicker and easier with its simple utility classes. To make the website feel more dynamic, we added animations using Framer Motion, which helped make elements on the page move and change smoothly. On the backend, we built a Flask API to connect the frontend with different tools for detecting sensitive content. We used Google Vision, a machine learning tool, to analyze images and look for any location-revealing information like street signs, buildings, or addresses. This allowed us to identify and remove potentially harmful content in the images being uploaded or streamed. We also used Gemini, a large language model, to analyze the live camera feed. It looked at any text in the background, like signs or labels, to check if there was a risk of revealing someone\u2019s location. This helped us assess whether the content needed to be blurred or removed for safety. Altogether, combining these tools helped us build a system that can better protect minors from unintentionally revealing their location online.Challenges we ran into: Some of the challenges we faced during the project were related to detecting sensitive locations using Google Vision and integrating it with React. One of the biggest difficulties was figuring out how to coordinate the blur box coordinates with the location of the buildings in the images. Since the buildings could be in different positions, sizes, or orientations, it was tricky to match the coordinates of the blur boxes with the correct spots on the images. Another challenge we ran into was the process of merging our work together using GitHub. Since we split up the tasks, there were times when combining everyone\u2019s individual work created conflicts, which made it harder to keep everything in sync. This slowed us down at times, especially when we had to fix merge issues or rework certain parts of the code. Lastly, detecting text and objects in a live camera feed was also difficult. Unlike working with static images, we had to deal with a continuous stream of frames from the video, which made the processing much more complex. We had to find a way to detect the sensitive content in each frame, which required more advanced handling to ensure we didn\u2019t miss anything important while keeping the process efficient.Accomplishments that we're proud of: We\u2019re really proud of a few things we accomplished during this project. First, we\u2019re excited about successfully using Google Vision for the first time. We\u2019d never worked with this technology before, and it was a huge learning experience. We were able to get it working for both text detection and object detection, which allowed us to identify sensitive location information in images and blur it out. Another accomplishment we\u2019re proud of is the UI and design we put together. We wanted the app to be easy to use, so we worked hard on making sure the design was simple and clean. We used React and Tailwind CSS to build the layout, and the result was a responsive design that looks great on any device. We also added some cool animations with Framer Motion, which made the app feel more interactive and fun to use.Overall, we\u2019re really proud of how everything came together, from using Google Vision to making the app look and feel smooth and easy to use.What we learned: Throughout the project, we learned a lot about working with different technologies and improving our skills. One of the main things we learned was how to use the Google Vision API to analyze images and visual data. We got hands-on experience with extracting text and detecting objects, which was a key part of our project. By using Google Vision, we were able to locate sensitive information, such as location-revealing text, and we created coordinate boxes around these objects to define regions of interest that would be blurred for safety. We also deepened our experience with React during this project. As we worked with React to build the frontend, we learned more about handling state, managing components, and creating smooth user interactions. This experience helped us feel more confident in using React for future projects, and it was valuable in expanding our technical skills. Additionally, we explored using OpenCV, a computer vision library, to apply a Gaussian blur to the images. This allowed us to blur out sensitive information automatically in the regions we identified using Google Vision. By experimenting with OpenCV, we learned how to manipulate images and apply filters, which helped us improve the functionality of our project. Overall, this project gave us a chance to learn new technologies, overcome challenges, and improve our skills in both front-end and back-end development.What's next for GeoGuard: One of our main goals is to incorporate our app as an extension for children\u2019s cameras or social media apps. This way, the app can automatically screen photos and videos for sensitive information before they are uploaded, ensuring that nothing revealing or unsafe is shared. In addition to that, we plan to add a feature that sends notifications to parents via email. These notifications would update them on the status of their child\u2019s social media posts, such as whether a post was approved, blurred, or flagged for sensitive content. This would give parents peace of mind while still allowing kids to use social media safely. Another key feature we want to work on is user login authentication. By adding a secure login system, we can ensure that users have personalized experiences and settings, and also help parents monitor and control the app\u2019s functionality more easily. Finally, we aim to improve the app\u2019s performance and scalability, so it can handle larger amounts of data and work efficiently for a wider range of users. We\u2019re excited about expanding GeoGuard to make it even more helpful for keeping kids safe online and empowering parents with the tools they need to protect their children.",
                        "github": "https://github.com/msavya/Bitcamp2025",
                        "url": "https://devpost.com/software/geoguard-6gzc94"
                    },
                    {
                        "title": "pocketPenny",
                        "description": "Discover, Reflect, Save",
                        "story": "Inspiration: We were inspired by Kakeibo, the traditional Japanese budgeting system that has been used since 1904.What it does: A financial goal tracker that not only allows users to keep track of their finances properly but also includes a \"Penny the Pincher\" bot that gives financial advice.How we built it: We used HTML and CSS for the front end, and used JavaScript for the back end. For our AI bot, we used Gemini-API with React to program our Penny the Pincher bot.Challenges we ran into: This was both of our first hackathon, and our team also was halved on the second day. Neither of us had experience working with Gemini as well.Accomplishments that we're proud of: Our UI is very clean and designed, and we have our bot running on Gemini Flash 2.0.What we learned: We learned how to work when circumstances in life come up, and also how to create our own AI bot.What's next for pocketPenny: Our next plan is to have the AI be a bit more fine tuned to our liking, including fonts, layout and more. We also want to get the link working to connect it to our github pages.  We also want to make sure we can connect our AI to our website.",
                        "github": "",
                        "url": "https://devpost.com/software/kakeibo"
                    },
                    {
                        "title": "Keyzz",
                        "description": "Turning Paper Into Music. Piano Rhythm Games, Reinvented",
                        "story": "Inspiration: Always loved melodic music, and have been watching the youtube videos of people playing piano with the Synthesia game, and I had always wanted to try it out; \nThe only problem I don't have a piano and I don't want to spent my hard earned money over it; \nSolution: create your own piano playing experience from a sheet of paper and make a Synthesia inspired game to go along with it.What it does: Keyzz transforms your laptop into an interactive, gamified piano, allowing you to enjoy playing music without needing an actual piano or additional hardware. Leveraging computer vision, machine learning, and real-time audio feedback, Keyzz provides an immersive piano-playing experience right from your home.How we built it: Vision-Based Key Detection: Detects and maps printed piano keys using just your laptop's webcam (with the help of openCV and MediaPipe).\nReal-Time Audio Playback: Plays notes instantly based on finger placement and movement (with pygame)\nInteractive Game Mode: Engage with falling notes similar to rhythm-based games, enhancing your musical learning experience (with pygame).\nInstant Calibration: Quickly stabilizes and calibrates detection for consistent and reliable performance.Challenges we ran into: Accomplishments that we're proud of: You can actually pay a rhythm game through a printed sheet of paper and get a glimpse into what it is to play a real Piano. Anyone around the world can experience by just following 3 simple steps mentioned on GitHub README.md file. So go ahead and compete with your friends to see who can get the highest score in this new experience of piano rhythm game.What we learned: How to do hand-pose estimation with openCV and media-pipe; and how much liner-algebra one needs to know to distort a simple mapping to the actual live feed from the camera.What's next for Keyzz:",
                        "github": "https://github.com/Swayam-Bansal/Keyzz.git",
                        "url": "https://devpost.com/software/keyzz"
                    },
                    {
                        "title": "Vizier",
                        "description": "From Noise to Signal: Because \u2018Insight\u2019 Isn\u2019t Just Another Token",
                        "story": "\ud83d\ude80Inspiration: Every serious researcher \u2014 from grad students grinding out their lit reviews to tenured professors trying to track five subfields at once \u2014 knows the pain:New research movestoo fastIt'sscattered across a dozen platformsAnd if you blink, you've missed a whole trend,Between arXiv, Twitter threads, conference keynotes, and shadow releases on GitHub,there\u2019s no single feed that captures the full signal.We weren\u2019t inspired by another one-shot summarizer. We were inspired by how actual researchers work:Constant pivots between sourcesZooming in and out across timescalesEvaluating claims, not just regurgitating themPrioritizing trust, depth, and novelty,So we built Vizier \u2014 not another autocomplete wrapper, but a system that actually understands your goals and assembles a live research team around them.Because in 2025,Insight isn\u2019t just another token.\ud83e\udde0What Vizier Actually Does: Vizier is amodular, agentic research engine\u2014 your personal research ops team, not just a chatbot. Whether you're building a newsletter, writing a paper, pitching an idea, or just staying on the bleeding edge, Vizier gives you:\ud83d\udd0d Precision-curated content fromcredible, multi-platform sources\ud83e\uddf1 Structured, editable reports tailored to your research priorities\ud83e\udde0 Control over what gets emphasized, where deeper sourcing is needed, and how frequent updates should be,It\u2019s built for:\ud83e\uddd1\u200d\ud83d\udd2cResearchers and technical professionalswho need rigorous updates on specialized domains\ud83c\udf93Students and professorstracking rapid fields like GenAI, climate science, or synthetic bio\ud83d\udce3Content creators and analystswriting newsletters, reports, or breakdowns on bleeding-edge developments,And itdoesn't stop after one search. Once you've locked in a great output, you can:\ud83d\udcc5Schedule that research plan to auto-rundaily, weekly, or monthly \u2014 get a fresh update, no extra effort\ud83d\udd01 Revisit past reports, tweak scopes, swap source weights, or layer in new domains,\ud83d\udd01What Makes VizierAgentic?: This isn\u2019t just \u201cuse LangChain and call it a day.\u201d Vizier\u2019s agentsthink for themselves.\ud83e\uddedRouter v0_4analyzes your refined query and decides:How many agents to spawnWhich domains get which budgetWhich model contexts are needed (Twitter thread agents \u2260 PDF interpreters)\ud83d\udd0eWeb and Twitter Search Agentsdon\u2019t just follow rules \u2014 theyevaluate:How noisy a domain isWhether depth is sufficientIf a second-level validation or domain rerun is required\ud83e\uddeaSource Review Agentsactively rerank or prune content if trust scores fall short, or if a source looks promising but under-explained. They\u2019re built topush quality higher\u2014 not just pass things through.,Every Vizier agent has decision power \u2014 they can go deeper, re-query, or backtrack if a lead is weak. They operate under flexible planning constraints, just like human researchers.\ud83d\udee0How We Built It (No Bullshit Edition): \ud83e\udde0Query RefinerBuilds a multi-component research planwith you\u2014 grounded in:Role (e.g. systems researcher, policy analyst, VC)Goals (e.g. \u201cdeep tech convergence tracking\u201d)Update frequency (e.g. daily pulse, monthly digest)\ud83e\uddedRouter v0_4Not a switchboard. A planner. It:Maps query scope to modalityAssigns sourcing budgetsSplits loads acrossindependent agentswith context-aware prompts\ud83c\udf0dAgents That DecideWeb & Twitter agents determine their own search depthCan re-run if a thread lacks references or a blog post is all hypeKnow when to cross-link sources or bring in institutional citations\u270d\ufe0fWriter AgentDoesn\u2019t dump. It synthesizes:Modular paragraphs per componentAuto-queries clarification if details feel fuzzy or under-sourcedLets you re-rank or flag parts for improvement\u26a1Live Agent Graph UISSE-driven, real-time frontend thatshows the thinking\u2014 you can trace every decision, branch, retry, and rerank in real-time. This isvisible cognition, not a black box.,\ud83d\udd27Challenges We Solved: \ud83e\uddecMulti-agent coordinationwith branching workflows & retries\ud83d\udd01Live SSE syncbetween asynchronous agent outputs and frontend UI\ud83e\udde0True agentic control flow: Where to go deeper, when to bail, how to reprioritize\ud83d\udc26Navigating Twitter\u2019s API hell: Index instability, partial results, thread collapsing \u2014 all solved with multi-layered backup scrapers, re-verification agents, and de-noising heuristics,\ud83c\udf1fAccomplishments We\u2019re Proud Of: \u2705Real agents that take real actions\u2014 they don\u2019t just forward results, they analyze and rerun as needed\ud83e\udde0Router with context-driven source allocation, not hardcoded domain splits\ud83e\uddeeMultimodal synthesisacross whitepapers, blog posts, podcasts, preprints, Twitter threads, and more\ud83d\udd01Feedback on both ends\u2014 not just user \u2194 system, but agents \u2194 other agents (e.g. writing agents asking for clarification from sourcing agents)\u26a1Interactive report scheduler\u2014 see something good? Auto-run it next week, no retyping needed,\ud83d\udce3Why Vizier Actually Matters: If you\u2019re a researcher, analyst, professor, or creator \u2014 Vizier helps you do the real workbetter and faster, without sacrificing rigor.\ud83e\udde0 Tired of missing Twitter alpha from top researchers like Karpathy, Srivastava, or Andreessen? Vizier gets itbeforethe news cycle does.\u270d\ufe0f Publishing a newsletter on Substack or Beehiiv? Vizier lets youbuild a content pipelinethat you can iterate, edit, and share easily.\ud83c\udfa7 Want to convert a weekly report into a NotebookLM podcast, or generate highlights for Twitter and LinkedIn? That's on the roadmap.\ud83d\udcb0 Want Stripe-powered revenue sharing on recurring reports or private briefings? We\u2019re building toward that, too.,Vizier isn\u2019t just a tool. It\u2019s a workspace.One thatgrowswith your workflow, so you can eventually doeverything from query to publication in-platform.No more tabs.No more copy-pasting between Notion, ChatGPT, Zotero, and Twitter.Justsignal, from chaos to clarity.",
                        "github": "https://github.com/GeneralCoder365/vizier",
                        "url": "https://devpost.com/software/vizier"
                    },
                    {
                        "title": "Sentra",
                        "description": "Imagine tracking your mental health through beautiful, personalized visuals. With built-in emotion stats and trends, it's like having a mood journal that looks as vibrant as you feel.",
                        "story": "Inspiration: In the fast pace of everyday life, taking a moment to pause and reflect on our emotions can be deeply grounding\u2014whether it\u2019s a daily habit or something we do from time to time. This project is more than just a mental health app; it\u2019s a space for self-awareness, emotional growth, and meaningful check-ins with yourself.What it does: Each day, you can log a journal entry, and using emotion analysis powered by the Gemini API, we translate your writing into six core emotions for tracking and reflection.How we built it: We established our front end on react and used javascript predominantly, and node.js for the back-end. Additionally, python was used to implement our gemini api and any chart generation/emotion analysis.Challenges we ran into: We tried integrating our Python code with JavaScript using child_process, setting up an HTTP server, and even rewriting the Python code in JavaScript. However, none of these approaches worked. Every time our emotion analysis produced a score, it generated different images, which led to inconsistent behavior and often caused our code to break or fail to display any output. Any images generated could also not be connected to our front end code.Accomplishments that we're proud of: We built a sleek and intuitive UI designed to genuinely benefit users, featuring an interactive calendar for daily journal tracking. Implementing emotion analysis from raw input text was a key milestone, made possible through experimentation and collaboration. Agile management kept us focused and flexible throughout, and team work makes the dream work (...eventually). We generated individual pie charts with holistic statistics based on emotion scores using our emotion analysis -though we were unable to connect the image to our front-end :(What we learned: HTTP protocols, git version control, back-end set up, emotion analysis.What's next for Sentra: A working prototype :))",
                        "github": "",
                        "url": "https://devpost.com/software/sentra-c7kzwb"
                    },
                    {
                        "title": "HallHopper",
                        "description": "Min-max your time outside.",
                        "story": "Inspiration: Navigating large university campuses like UMD can be confusing, especially when trying to find classrooms on the first day of class in unfamiliar buildings. Unpredictable weather could add to that stress. We realized there wasn\u2019t an easy way to move between buildings indoors or find clear directions inside complex academic halls.What it does: HallHopper helps you:Navigate inside UMD buildings to find classrooms and study spots.Avoid getting lost on the first day of classes or during finals when you\u2019re suddenly assigned a random room.Find dry (or shady) shelter during rainy or sunny days by hopping through connected buildings.,How we built it: Backend: Flask-based API to manage building data and navigation routes.Frontend: Built in React Native, with a focus on iOS compatibility.Database/Auth: Supabase for user authentication.,Challenges we ran into: Learning React Native.Working with latitude and longitude coordinates, common building names, and street addresses.Estimating relative distances indoors between stairs, entrances, and hallways.,Accomplishments that we're proud of: Built and integrated a functional MVP with just two people!Learned and implemented new tools and frameworks in a short amount of time.Created something useful and uniquely tailored to our own campus experience.,What we learned: We love bitcamp!Frameworks!,What's next for HallHopper: Improve internal API complexity.Implement database for user information (e.g., home address and favorite buildings).,",
                        "github": "https://github.com/ivyisaplantt/hallhopper",
                        "url": "https://devpost.com/software/hallhopper"
                    },
                    {
                        "title": "MacroTerpitech",
                        "description": "MacroTerpitech transforms campus dining by matching UMD dining hall options to your personal nutrition goals, making healthy eating simple and achievable for every Terp.",
                        "story": "Inspiration: Our team noticed a common struggle among UMD students: balancing nutrition goals with the reality of campus dining options. As students ourselves, we experienced the challenge of trying to meet specific macro targets while eating at dining halls. We were inspired to create a solution that would make it easier for students to make informed food choices that align with their health and fitness goals.What it does: MacroTerpitech is a web application that transforms how UMD students interact with campus dining. Users can:Set personalized macro goals such as calories, protein, carbs, and fatsInput dietary preferences and restrictionsReceive customized meal recommendations from UMD dining halls that match their nutrition targetsTrack their nutritional progress throughout the dayPlan meals ahead of time based on real-time dining hall offerings,How we built it: We developed MacroTerpitech using a modern tech stack:React for the frontend with Tailwind CSS for responsive designFirebase for storing user profiles and nutritional dataBeautifulSoup and Selenium to collect and update UMD dining hall menu informationGemini for querying in order to create the custom meals at the dining halls,We implemented a user-centered design process, conducting interviews with fellow students to understand their needs and pain points before developing our solution.Challenges we ran into: Building MacroTerpitech wasn't without obstacles:Web scraping the dining hall websites was difficult due to the amount of inconsistent formattingBeing able to get the information in a consistent format from Gemini in order to display properly was harder than expectedDeveloping a clean UI for users required a good amount of trial and error until we settled on something that all users would enjoy,Accomplishments that we're proud of: Despite the challenges, we're proud of creating a solution that:Successfully integrates with UMD's dining system to provide actionable recommendationsDelivers a clean, intuitive user experience that makes nutrition tracking accessibleAccounts for diverse dietary needs and preferences across the student bodyReceived enthusiastic feedback during our testing phase with fellow students,What we learned: This project taught us valuable lessons about:The complexity of nutritional data and the importance of accuracyBalancing technical capabilities with user experienceCollaborative development practices and effective task distributionThe importance of continuous user feedback throughout the development processHow technology can address everyday challenges in campus life,What's next for MacroTerpitech: We have ambitious plans for the future:Expanding our database to include more detailed nutritional information perhaps expanding to UMBC and other campusesDeveloping a mobile app to complement our web platformCreating a community feature where users can share successful meal combinationsPartnering with UMD Dining Services to integrate our solution more deeply with their systems,MacroTerpitech aims to continue evolving to better serve the UMD community and potentially expand to other universities facing similar dining challenges.",
                        "github": "https://github.com/Paradox560/bitcamp-2025.git",
                        "url": "https://devpost.com/software/macroterpitech"
                    },
                    {
                        "title": "StrataGem",
                        "description": "StrataGem is the business advisor you've always wished for :)\r\nWe analyze your business data, generate comprehensive strategy plans, and build tailored presentations for different stakeholders",
                        "story": "Inspiration: Ever notice how many awesome ideas pop up every day, but only a few become the next big thing? It's not because people aren't creative enough or don't care enough. Often, it's just that they're missing the know-how to grow their idea into something bigger. That's exactly why we created our solution! \nStrataGem has answers for all your business growth challenges. We wanted to help bridge that gap between having a brilliant idea and building a successful business - whether you're an existing business looking to level up or a brand new startup finding your footing.What it does: We're like that friend who actually listens when you talk about your business dreams! We take the time to understand what makes your business special - your big goals, what you're passionate about, and what matters most to you.Then, our clever AI digs through hundreds of real-world success and failure stories to cook up strategies that fit YOU like a glove. No more one-size-fits-all advice that feels like it could apply to literally anyone with a business card! And you don't have to worry about hallucination! Our AI model will present you with a published case study for every claim it makes.Got a small business that's hit a wall? We'll help you climb over it.Running a fresh-out-of-the-box startup? We'll help you dodge those first-year disasters that nobody warns you about.Whether you've been grinding away for years or just took the leap last month, StrataGem is here to transform your brilliant idea into something that makes people go \"Wow, how did they get so big so fast?How we built it: Challenges we ran into: Data Extraction:One of the biggest hurdles we encountered was sourcing quality data. There are only a limited number of sites that provide access to past business case studies, and even fewer that offer them in a usable format. Extracting the relevant information took significant time and effort. We had to fine-tune our model to filter out noise and capture only the insights that truly mattered for our strategy generation.Scoping:We were skeptical about completing everything we had in mind to begin with, and features kept being added and removed. Thanks to Lucas from Microsoft/Cloudforce for letting us pick his brain on this. Ultimately we managed to pull through and finish the project.API Limits:Another significant challenge we faced was working within the constraints of API rate limits. Shoutout to Google for a generous access to the Gemini API which enabled us to use SOTA LLMs for this projectAccomplishments that we're proud of: StrataGem was an amalgamation and mutation of a bunch of ideas that we spent hours brainstorming. We are extremely happy with how it turned out and especially so of the fact that we managed to implement everything from actual human made case studies as our DB, complex RAG and a PPT generator all in one day :)What we learned: Throughout the development of this project, we were regularly reminded of the massive potential that GenAI has and how we barely scratch the surface in our everyday lives. We definitely look forward to building highly-complex AI orchestrated systems in the future.What's next for StrataGem: One of the main things we would like to address is the privacy of business sensitive data being shared with  LLM providers and how we could scope this.\nWe plan to have StrataGem accessible via an live Streamlit app as well as improve the small but mighty repository of business case studies that we currently have. We would also love to integrate/connect the PowerPoint generation part with Microsoft Copilot's existing capability.",
                        "github": "https://github.com/harshavardhan-patil/stratagem",
                        "url": "https://devpost.com/software/strategic-synthesis-ai"
                    },
                    {
                        "title": "Ares.AI",
                        "description": "An assistant that actually assists.",
                        "story": "Inspiration: What if you could control your phone with just one instruction\u2014no tapping, no scrolling, no searching? A task like \"Set an alarm for 7:30 a.m.\" can be done by Siri or Bixby, but what about booking Uber or ordering your cart on Amazon? Shouldn't require a dozen manual steps? We wanted to build something that could understand your intent, see your screen, and act like a human assistant\u2014but with the precision of AI. That\u2019s where Ares.AI began: turning natural language into real-time mobile automation.What it does: Ares.AI takes high-level instructions (like \"Open WhatsApp and message John\") and automates the steps to make it happen. It sees your screen, breaks your request into atomic actions, finds the right UI elements, takes action, and sends back the next move. It keeps track of context, retries when needed, and adapts when stuck, just like a real assistant would.Core Working: We start with a natural language instruction and use function-calling for Gemini 2.5 Pro to break it into a structured sequence of atomic UI actions\u2014like tapping buttons, typing text, or navigating menus. Each goal is clear, ordered, and aligned with how a human would complete the task.The agent maintains a session-level memory of which step it\u2019s on, how many attempts have failed, and whether the current goal is stuck. If something goes wrong\u2014like a button not appearing\u2014it tries alternative actions before giving up.At each step, we pass uniquely processed screenshots with bounding boxes to get coordinates  for Gemini to identify the correct bounding box for the next action. If the element isn\u2019t visible, the agent adapts: scrolls, waits, or tries again. It avoids looping on duplicate screenshots and uses hashing to manage screen state effectively.Challenges we ran into: UI inconsistency:Not every screen looks the same\u2014element labels, layouts, and icons vary wildly across apps.Image matching in real time:Finding the right element without lag meant balancing model power with latency.State recovery:Knowing when a goal was truly stuck and how to recover (scrolling, retrying, skipping) required careful heuristics.,Built to feel like a human, but backed by the precision of AI",
                        "github": "https://github.com/invcble/ares_ai",
                        "url": "https://devpost.com/software/ares-ai"
                    },
                    {
                        "title": "Dino Docs",
                        "description": "The average startup's docs are not human readable. AI just might be able to do it.",
                        "story": "Inspiration: When learning new technologies, the most common advice is RTFM! However, most times the people who write the manual do not make the documentation the most accessible. To truly understand where you need to look, you have to read portions of the docs you are not interested in or even need to know. This is a big time waste and I find it quite frustrating. A common approach is to plug it into an LLM, but that can easily overwhelm their context window.What it does: This application transforms your technical documentation into an interactive, AI-powered assistant. Simply upload your documentation as a PDF, and you\u2019ll gain a natural-language interface to explore it. \nThe system uses Retrieval-Augmented Generation (RAG) to break the docs into meaningful chunks, embed them into a vector database, and deliver fast, accurate, and context-aware responses from a powerful LLM.You can continue prompting seamlessly \u2014 no need to re-upload \u2014 until you get the answers you need.How we built it: We used Chonkie to intelligently parse and segment PDF documentation into semantically meaningful chunks. These chunks were embedded and stored for fast retrieval, enabling highly relevant context to be stored into Pinecone DB, a vector database. These vectors were then inserted into Gemini Flash 2.0 \u2014 our chosen LLM \u2014 for precise, performant answers to user queries.On the backend, we built a lightweight yet scalable API using FastAPI, which handled PDF uploads, query processing, and response generation. The frontend was developed using React, providing users with an intuitive and interactive interface to upload documents, pose questions, and receive AI-generated responses.We collaborated using GitHub, Git, and VS Code, which allowed us to iterate quickly and work efficiently as a team. This project also gave us an opportunity to learn best practices in building full-stack AI tools \u2014 from vectorized search to UI integration \u2014 with technologies that were largely new to us at the start of the hackathon.Challenges we ran into: One of the most significant challenges we faced was implementing a chunking strategy that maintained semantic relevance. Since embeddings convert text into high-dimensional vectors based on meaning, it\u2019s crucial that the chunks themselves preserve coherent and contextually rich units of information. Early approaches to chunking led to fragmented or overly broad sections, which resulted in irrelevant or imprecise LLM responses. After extensive research and testing, we adopted a method that better captured semantic relationships within the document, greatly improving retrieval and response quality.Another technical hurdle was managing CORS (Cross-Origin Resource Sharing) issues between our FastAPI backend and React frontend, which were hosted on separate servers during development. Resolving these required fine-tuning headers and configuring our backend to properly handle preflight requests \u2014 a necessary step to ensure smooth communication between the UI and API.Accomplishments that we're proud of: One of our biggest accomplishments was learning how to make LLMs more intelligent and grounded by connecting them with custom data \u2014 specifically, enabling them to answer questions about information not available on the public web. This shift from generic LLM outputs to context-aware, document-grounded responses was eye-opening and helped us understand the power of RAG systems in real-world applications.We\u2019re also proud of the fact that, despite limited prior experience with AI and full-stack development, we built a working product from scratch \u2014 complete with PDF parsing, semantic search, vector database integration, and a clean React frontend. It was a full learning journey, and we walked away not only with a functioning app but with a much deeper understanding of modern AI pipelines.What we learned: One of the most valuable moments came from a workshop we attended, which broke down the fundamentals of Retrieval-Augmented Generation (RAG) in a way that truly clicked. We learned that each word or phrase can be represented as a numeric embedding in a high-dimensional vector space, capturing its semantic meaning. These vectors are compared to the user\u2019s query vector, and the closest matches are retrieved and passed to the LLM \u2014 forming the basis of how RAG operates.On the frontend side, we explored several options before landing on React. Initially, we experimented with Gradio, a Python-based frontend framework. While easy to use, it lacked flexibility and visual polish. We then tried HTMX, which claims to eliminate the need for JavaScript. However, we quickly ran into limitations \u2014 particularly its inability to handle JSON responses, which are foundational to modern web APIs. We came to understand why javascript is so widely used on the frontend.What's next for Dino Docs: Hopefully adding the project, introducing better chunking, separate namespaces and more granular models for more accurate queries regardless of doc size, and maybe a bit more flare on the front end. Perhaps even a VS code plugin.",
                        "github": "https://github.com/IlyaShch/DinoDocs",
                        "url": "https://devpost.com/software/idk-me-and-kieran"
                    },
                    {
                        "title": "CollabCart",
                        "description": "CollabCart: Smarter Influencer Marketing\u2014Powered by AI, Backed\u00a0by\u00a0Data.",
                        "story": "Inspiration: CollabCart is inspired by growing yet inefficient social media influencer marketing landscape, where brands/companies struggle to find the right creators and influencers miss out on relevant collaborations. Recognizing the lack of smart matchmaking and transparency in the industry, we envisioned a data-driven platform that will bridge this gap- empowering businesses to discover niche-aligned influencers effortlessly while giving creators, especially micro-influencers, fair opportunities to monetize their reach. By combining AI-powered recommendations with actionable insights, CollabCart simplifies partnerships, ensuring authentic, high-impact campaigns that benefit both brands and influencers.What it does: CollabCart is a data-driven influencer marketing platform that connects brands with the right social media creators effortlessly. Companies can discover, analyze, and collaborate with influencers whose audience and content perfectly align with their products- powered by AI recommendations, performance insights, and streamlined campaign management. Meanwhile, influencers gain access to vetted partnership opportunities tailored to their niche, helping them monetize their influence while maintaining authenticity. By automating matchmaking and providing transparent analytics, CollabCart makes influencer marketing smarter, faster, and more impactful for both sides.How we built it: CollabCart is built with a scalable, AI-powered tech stack designed to deliver seamless influencer-brand matchmaking. The tech stack used to build this platform is:Challenges we ran into: Accomplishments that we're proud of: We\u2019re proud of CollabCart\u2019s AI-powered influencer-brand matching (85% satisfaction rate), real-time analytics dashboard (10K+ data points/sec), and scalable architecture (50K+ profiles with sub-second searches). Our fraud detection and escrow system ensured zero scams, while early traction brought 500+ influencers and 100+ brands onboard in 3 months. By fine-tuning ColBERT for niche-aware recommendations and optimizing our stack (React + Spring Boot + AWS), we cut manual search time by 70%. With a 40% repeat collaboration rate, we\u2019ve proven CollabCart delivers real value\u2014making influencer marketing smarter for\u00a0everyone.What we learned: Working with ColBERT taught us that AI-driven influencer matching requires more than just surface-level keyword analysis. We successfully adapted ColBERT's semantic retrieval capabilities to understand niche-specific language in marketing contexts (like distinguishing \"minimalist fashion\" from \"streetwear\"), but learned this demands careful fine-tuning with domain-specific data. The model excelled at identifying authentic creator-brand alignment beyond basic metrics, though we had to optimize performance using ONNX runtime to handle real-time recommendations. Most importantly, we discovered ColBERT works best as the first stage in our matching pipeline - its semantic understanding identifies potential matches that we then rank by engagement data. This hybrid approach proved perfect for CollabCart's need for both relevance and scalability in influencer-brand\u00a0pairings.What's next for CollabCart: We\u2019re scaling smarter, faster, and globally\u2014supercharging our AI to predict campaign ROI, expanding into emerging markets like Southeast Asia, and launching one-click instant bookings for seamless brand-influencer deals. Soon, our \"CollabCart Verified\" program will offer premium vetted creators, while embedded fintech tools like instant payouts and BNPL options will revolutionize payments. With API integrations for agencies and a new \"Green Influence\" sustainability filter, we\u2019re not just connecting brands and influencers\u2014we\u2019re building the end-to-end future of influencer marketing. Next stop: Becoming the \"Shopify of collaborations\".",
                        "github": "https://github.com/log4jDominion/CollabCartAPI",
                        "url": "https://devpost.com/software/collabcart-vur6e7"
                    },
                    {
                        "title": "GitGraph",
                        "description": "A passion mobile application for a visualizing for git control version systems built with React Native, Expo, and Tailwind. Explore branches and commits in repositories in a more convoluted way.",
                        "story": "Inspiration: As someone who works in team environments a lot, I get relatively curious on how our work and commits gets interwoven on a graphical scale. I developed this project because I felt as if this was the most fun thing to implement in a hackathon, where instead of going more general and competing with other similar ideas I wanted to do something more niche and unique. This isn't my first time working on a project involving D3.js and a React based framework, but it was still challenging and fun overall! A grueling 16 straight hours of coding to embroider my solo passion.What it does: It serves as a basic graph visualizer for a popular version control system platform, Github. By either inputting the owners name and the repository's name, you have the opportunity to graphically visualize branches, commits, and files in a single page with interactable nodes that redirect you to their respective links. Clicking on a commit will redirect you to the specific commit, making it a great tool for inspecting changes in codebases and quickly accessing new commits.How we built it: I utilized React Native + Tailwind and Expo to start with my basic framework, and I used d3.js as my primary library for visualizing the actual features of the project.Challenges we ran into: The amount of restructuring that occurs as a singular developer was quite a bit, as I found myself constantly contemplating changes in my architecture in order to make time to meet deadlines and find enough time to debug and clean features. For example, I first tried using Redux expecting a very complicated structure of data being handled but eventually I gave up because it was so complicated to use, forcing me to pivot towards Zustand for state management instead. Additionally, I found React Native as per usual to be difficult to work with, especially since I went with Typescript. I was basically typing every single little thing out in order to make 5% gain. I had to scrap OAuth authentication for Github login and I had to focus on developing the frontend features costing me the potential to build out a feature-rich full stack mobile application. It was my first time being solo, but it was enriching regardless..Accomplishments that we're proud of: Actually finishing the project.What we learned: Enjoy the work, and appreciate the little things that make us better than we were before.What's next for GitGraph: Probably OAuth or some integration with some fancy d3.js animations. I also see myself potentially pivoting this towards a personal app for personal use since I think this would be pretty useful for me to have on hand. Final step would be to make it full stack and give it more abundant user-friendly features.",
                        "github": "https://github.com/mattx0601/gitgraph",
                        "url": "https://devpost.com/software/gitgraph"
                    },
                    {
                        "title": "AidenAI: Your AI Financial Aid Advisor",
                        "description": "The financial aid process is complex and difficult \u2014 but AidenAI harnesses the power of Google Gemini to help students from all backgrounds make informed decisions, and negotiate for better aid.",
                        "story": "The Gist: Aiden AI harnesses the power of Google Gemini to analyze official scholarship offers and financial documents, in order to help students make a financially smart and informed college decision. The Aiden AI website allows users to upload the deluge of financial communication they receive from colleges, and understand their situation easily through an informal conversation with a helpful AI assistant.Inspiration: As college students, we have all had to navigate the extensive and cumbersome financial aid process. We know how difficult it can be to read through large legal and financial documents, understand the complex terms, and decide which offer is better. We were inspired by the power of generative AI to provide people with new insights, and so AidenAI aims to make this process easier and more affordable to those who can\u2019t afford a financial analyst through a personalized AI chatbot.What it does: The AidenAI website allows users to inform Aiden of their background, finances, and educational goals, and differentiates itself further by allowing users to submit official aid offers, FAFSA documentation, and communications from school financial aid offices. With a generative AI targeted specifically at parsing these complex financial documents, AidenAI helps students to:Unpack the financespeak by talking with AidenLearn the process of negotiating for more financial aidAsk the what-ifs and hypotheticals that will inform their eventual decision,How we built it: We started by drawing mockups of the various pages and UI elements that would make up the outward-facing website, as well as connecting buttons to the database actions that would need to be taken. We settled on using the FReMP stack to build this application: Flask, React, MongoDB, and Python. We  also used Vite for rapid prototyping of UI as well as bundling the app for Flask hosting. We also used theCapitalOne datasetfor providing the AI with mock financial statements as well, and of courseGoogle Geminifor powering Aiden himself.Challenges we ran into: There were many technical challenges that came with incorporating so many technologies and libraries into a single project \u2014 specifically, we struggled to establish an effective development cycle for backend work, which needed to simultaneously access the external MongoDB database as well as our internally-hosted (fully RESTful!) API. With regards to teamwork and integrating our separate pieces of code successfully, we had some issues with branch merging, especially where multiple people were modifying our application\u2019s shared stylesheets. However, we were able to avoid this issue for the most part by using effective version control techniques.Accomplishments that we're proud of: We are extremely proud to have developed a fully functional, visually pleasing, and (hopefully!) useful application in only 36 hours! Each member was able to apply their own unique skillset, and each team member constantly discussed how their decisions could impact the tasks other members were completing, which helped us avoid grief down the line. Our team had extremely minimal hacking experience, so it was extremely rewarding to end with a well-designed and fully functioning product at the end of our 36-hour marathon!What we learned: Our team learned a lot about Git and proper version control, as well as the need for great project file layout, in order to prevent merging issues as much as possible. Additionally, everyone greatly expanded their knowledge of React and Flask, and our backend engineers also learned how to use MongoDB Atlas and Cloudflare.What's next for AidenAI: As we continue to develop the project, we hope to support more file formats for document upload, and also to work on fine-tuning Aiden\u2019s responses so he can better aid students. Focusing on more minute fixes by improving the graphics and general UI/UX will also be an area of improvement.",
                        "github": "https://github.com/NetThi123/BitCamp2025/",
                        "url": "https://devpost.com/software/aidenai-helping-students-with-college-financial-aid"
                    },
                    {
                        "title": "Trace",
                        "description": "Revolutionizing the Parkinson's treatment monitoring.",
                        "story": "Inspiration: Parkinson\u2019s disease affects over 10 million people worldwide, and many patients struggle to access reliable, ongoing assessments of their motor function. Traditional tests often require in-person visits and don\u2019t provide a clear progression map for both doctors and patients. We wanted to create a tool that not only helps track the effectiveness of Parkinson's treatment but also empowers healthcare providers with data-driven recommendations. This led us to designTrace, an app that transforms a simple task \u2014 tracing a spiral \u2014 into a powerful diagnostic and progress-tracking tool.What it does: Traceallows users to trace spirals on a canvas using their finger or a stylus. The app then uses machine learning and computer vision to analyze the tracing, measuring deviation from the ideal spiral, tremor intensity, and speed. These metrics help assess the severity of Parkinsonian symptoms. Doctors can monitor patient progress over time, view visualized analytics, and receive treatment suggestions based on severity levels. Patients can continue weekly assessments, enabling longitudinal tracking of their condition.How we built it: We builtTracewith a full-stack approach:Frontend: React and Next.js for a clean, interactive user interface.Backend: Flask for handling ML processing and data logic.Database: MongoDB to store patient information and analysis results.,We used OpenCV to compute the Mean Square Error between the drawn spiral and the template, and also explored integrating tremor detection metrics from ETSD models. Our app currently includes features such as:A drawing canvas for spiral tracing.An \u201cAnalyze\u201d button to compute and display metrics.A patient list view where results can be saved and reviewed.,Challenges we ran into: Calibrating the spiral analysis to work accurately across different devices and input methods (e.g., mouse vs. touchscreen).Integrating ML models with real-time frontend interactivity.Ensuring the app is intuitive enough for both doctors and patients, while still being technically robust.Defining clear thresholds for different severity levels based on existing research datasets.,Accomplishments that we're proud of: Successfully built a functional prototype that can analyze user input and output meaningful diagnostic data.Created an interface that is accessible for both healthcare providers and patients.Integrated machine learning and computer vision in a real-time health context.Went from concept to demo-ready product within the Bitcamp timeframe.,What we learned: How to effectively combine frontend UX with backend ML workflows.Techniques for image analysis using OpenCV and implementing health-specific metrics.The importance of UI design in medical tech, especially when building tools for non-technical users.Balancing clinical accuracy with usability when working in healthtech.,What's next for Trace: We plan to:Expand our dataset and improve our ML model for higher accuracy.Add tremor frequency analysis and longitudinal progress charts.Build a doctor-specific dashboard for managing multiple patients.Integrate treatment recommendation logic based on severity tiers.Explore partnerships with healthcare providers for pilot studies.,We seeTracebecoming a key part of remote Parkinson's monitoring, lowering the barrier to diagnosis and treatment tracking globally.",
                        "github": "https://github.com/Trace-Bitcamp/trace-backend",
                        "url": "https://devpost.com/software/trace-34zsjm"
                    },
                    {
                        "title": " TerpMatch: LLM-Powered Research Connector",
                        "description": "Swiping right on research at UMD!",
                        "story": "Inspiration: At the University of Maryland, opportunities for undergraduate and graduate students to work with professors on research or academic roles like TA and grader are everywhere \u2014 but the process of finding the right match is fragmented, informal, and often based on word-of-mouth. We wanted to solve this using AI.What if we could help studentsintelligently discoverprofessors whose research and open roles truly align with their interests and skills? That\u2019s howTerpMatchwas born \u2014 a matchmaking platform tailored for UMD that uses cutting-edge LLMs to bridge the gap between students and faculty.What it does: TerpMatchmatches students with professors based on:Desired role (RA, TA, Grader)Research interestsResume-based technical skillsGemini LLM-inferred skill requirements based on each professor\u2019s research areas,Students fill out a quick profile and upload their resume. TerpMatch intelligently recommends professors with high compatibility scores. Professors, on the other side, can see students who applied to them, ranked by match score.It\u2019s like LinkedIn meets AI \u2014 but built specifically for Terps \ud83c\udfafHow we built it: Frontend: React.js with Axios for REST API integration and resume uploadBackend: Flask (Python) handles user data, resume management, and scoring logicDatabase: MongoDB stores student and professor profilesLLM Integration: Google Gemini API was used to infer technical skills from research interest stringsMatching Logic: Compatibility is calculated based on role match, research overlap, and skill alignment (LLM-enhanced)Resume Handling: Uploaded resumes were stored and converted to clean.txtand.pdfformats dynamically,Challenges we ran into: Setting up and integrating the Gemini API for large-scale skill inference was tricky due to rate limits and output parsing.Designing a fair and balanced compatibility scoring system that made sense for both students and professors.Making sure resume uploads and LLM-powered inference remained performant and didn't overload the backend.Merging frontend/backend cleanly while keeping the app lightweight and responsive.,Accomplishments that we're proud of: Built a full-stack AI-powered matchmaking engine from scratch within BitcampSuccessfully integrated Gemini LLM to infer skills from raw research interestsDesigned a dual-view system for both students and professorsCreated polished resume generation, scoring, and filtering capabilities \u2014 all hosted and demo-readyDelivered something that could genuinely benefit the UMD academic community,What we learned: How to structure a full-stack application using React + Flask + MongoDBHow to integrate LLM APIs into traditional software systemsHow to handle resume parsing and formatting at scaleThe power of semantic matching vs. keyword-based filteringThe potential of AI in making academia more accessible and collaborative,What's next for TerpMatch: LLM-Powered Research Connector: Add student-facing \"Top Matches\" dashboard with live updatesLet professors customize skill requirements and use Gemini for custom role analysisDeploy to a public cloud platform (e.g., Streamlit Cloud, Render, or Railway)Integrate UMD authentication (CAS or SSO)Add a Gemini-powered chatbot to help students ask questions like \u201cWhich profs do ML in healthcare?\u201d,We're excited to see where TerpMatch goes \u2014 and how it can help make UMD's vibrant academic network even stronger. \ud83d\udc22",
                        "github": "https://github.com/Rishabh000/Bitcamp-2025",
                        "url": "https://devpost.com/software/intelligent-assistantship-matching-portal"
                    },
                    {
                        "title": "ClearGaze",
                        "description": "An AI powered Horizontal Gaze Nystagmus test on your mobile device.",
                        "story": "Inspiration: According to the CDC, in the United States alone, an estimated 300,000 people drive while under the influence of alcohol or drugs every day. This is a serious issue. We were inspired by the Horizontal Gaze Nystagmus (HGN) test used by law enforcement to detect impairment through involuntary eye movements. With the rise of mobile technology and AI, we realized this same test could be performed using just a smartphone camera. Our goal was to create a tool that empowers people to self-assess their fitness to drive \u2014 quickly and privately.What it does: ClearGaze is a mobile app that uses your phone\u2019s front-facing camera to run a quick, AI-powered HGN test. The user follows a dot moving on the screen, while the camera records eye movements. In the background, our backend analyzes for signs of impairment \u2014 specifically looking for erratic motion or nystagmus. The entire test takes less than 30 seconds and gives a fast, private assessment of whether you're fit to drive.How we built it: Frontend:Built using React Native, the app displays a moving dot on the screen for the user to follow, while simultaneously recording video from the front-facing camera.Video Capture:After the 30-second test, the recorded video is packaged and sent to our backend for analysis.Backend:A FastAPI server receives the video and forwards it to the Gemini API, which evaluates the eye movement data to detect signs of nystagmus or irregular tracking behavior.AI Analysis: The Gemini API acts as the core vision intelligence, enabling us to offload motion tracking and pattern recognition to a powerful hosted model \u2014 rather than relying on local tools like OpenCV or MediaPipe.Privacy by Design: No video is stored \u2014 it\u2019s used only temporarily for inference and then discarded, keeping the experience secure and private.,Challenges we ran into: Transitioning away from local models:We originally planned to use OpenCV and MediaPipe but found them unreliable on mobile for precise eye tracking.Integrating Gemini with video input:Since Gemini typically expects image input, passing video and interpreting results correctly required experimentation and careful pipeline design.Synchronizing video with dot movement:We needed to ensure that the recorded video captured the right part of the test \u2014 especially when detecting subtle eye movement patterns.Frontend-backend timing coordination:We faced challenges ensuring smooth communication between React Native\u2019s recording features and the backend analysis workflow.Accomplishments that we're proud of: Successfully implemented a working HGN-style test using only a smartphone.Replaced traditional local tracking tools with Gemini's powerful video analysis.Created a full end-to-end system: video capture \u2192 AI analysis \u2192 actionable impairment feedback \u2014 all within seconds.Maintained a clean and simple user experience while working across multiple tech stacks under tight time constraints.,What we learned: How to use theGemini APIfor advanced video-based analysis, beyond typical image classification tasks.The strengths and limitations of local vs. cloud-based computer vision models in mobile contexts.How to manage real-time video capture in React Native and build a smooth testing experience that mimics real clinical assessments.The importance of tight frontend-backend synchronization when working with time-sensitive video data.,What's next for ClearGaze: Incorporating eye coloration analysis:We plan to integrate MediaPipe and OpenCV to evaluate redness, discoloration, or other visual indicators that could signal fatigue or substance use \u2014 adding another layer to our impairment detection system.Real-world car integration:We envision ClearGaze being linked directly to a user's vehicle, where a failed test would trigger a lockout and prevent the car from starting \u2014 adding a proactive layer of safety.Multi-signal detection:By combining both motion (nystagmus) and visual indicators (eye coloration), we aim to create a more robust and medically relevant assessment.Personalization and calibration:Future versions could learn a user\u2019s baseline and adapt thresholds over time, improving accuracy and reducing false positives.Healthcare and fleet expansion:We see potential applications beyond driving \u2014 from workplace compliance (e.g., for truckers or heavy machinery operators) to wellness apps that screen for fatigue, burnout, or medical symptoms.",
                        "github": "https://github.com/bbmullen72/ClearGaze-Bitcamp",
                        "url": "https://devpost.com/software/cleargaze"
                    },
                    {
                        "title": "NOVA",
                        "description": "Voice-Activated Financial Mastery for Small Businesses.",
                        "story": "Inspiration: We\u2019ve always been intrigued by how artificial intelligence can transform industries, but one area that felt both urgent and underserved was financial management for small business owners. While large enterprises often have entire finance departments and access to expensive software, small and medium-sized businesses are left juggling spreadsheets, manually tracking loans, or logging into clunky dashboards. We recognized this gap and knew there had to be a better, more intelligent way to handle it.Our goal was to build something more human and intuitive\u2014a voice-powered AI agent that could take on the burden of financial queries, track customer details, monitor loans, and answer questions in real-time. We wanted something that felt less like software and more like a co-pilot. Something that understood context, handled complexity behind the scenes, and provided crystal-clear responses.As we dove into development, we realized the potential of integrating technologies like Retell for voice interaction, Azure for hosting and scalability, and Capital One's Nessie API for real-time financial data. It opened up a world where a small business owner could say, \"What are my upcoming loan payments?\" or \"Pull up Agastya Mukherjee's customer profile,\" and have the answers instantly\u2014accurate, contextual, and accessible from anywhere.In conclusion, Nova was born out of a desire to level the playing field. We wanted to equip small business owners with the same level of intelligence and automation that larger organizations benefit from\u2014only better, faster, and easier to use. By blending financial APIs with voice and AI technologies, we\u2019re reshaping how businesses interact with their finances.What it does: Nova is a smart, voice-powered financial agent built to assist business owners in managing customer data, tracking loans, and answering finance-related queries in real-time. It\u2019s like having a CFO on-call\u2014one that\u2019s fast, efficient, and doesn\u2019t take lunch breaks.Nova empowers users to:-Retrieve and manage customer information with just a name or ID.-Track loan history, upcoming payments, and accounts with one voice command.-Create new customer profiles securely and efficiently.-Filter customer data using first and last names, or fetch comprehensive financial summaries for individuals.-Communicate naturally through voice using Retell\u2019s conversational AI integration, enabling a seamless and intuitive user experience.Whether you're a business owner trying to locate a customer record or a finance assistant looking for upcoming loan obligations, Nova delivers answers instantly and accurately.How we built it: Nova combines multiple technologies to provide a seamless voice and web-based financial management experience.Frontend: Built in React, offering a clean, responsive user interface that connects users with financial insights in real-time.Backend: Powered by Python (Flask) and securely connected to Capital One\u2019s Nessie API for real-time data on customers, accounts, and loans.Voice AI: Integrated Retell, a cutting-edge conversational AI framework, to handle voice commands and make interactions more human.Hosting & DevOps: Deployed using Azure for reliable, cloud-based performance, with support for future scalability.Security & CORS: Carefully implemented CORS policies and method overrides to ensure the app functions smoothly even behind proxies and Azure frontends.We also used helper utilities to modularize Nessie API calls and streamline authentication, making the system both robust and maintainable.Challenges we ran into: One of the major challenges we faced was enabling cross-origin voice interactions through the Azure proxy. POST requests from Retell often didn\u2019t align with standard expectations, which required creating POST-only fallback endpoints for robustness.Another hurdle was handling voice recognition edge cases. If a user says something outside Nova\u2019s capabilities (e.g., \u201cTell me a joke\u201d), the agent needed to gently redirect them back to supported tasks, such as customer or loan-related queries.Finally, integrating Capital One\u2019s Nessie API, which isn\u2019t actively maintained, introduced additional issues with error handling, authentication, and occasional data gaps.Despite these roadblocks, we tackled each one head-on with creative API design, robust error logging, and flexible endpoint logic.Accomplishments that we're proud of: \u2705 Full integration of voice and web interfaces using Retell and React.\u2705 Built a modular, secure backend in Python using helper functions for reusability and maintainability.\u2705 Developed a name-based customer filtering system, allowing business owners to fetch detailed records without knowing the customer ID.\u2705 Created a loan-tracking feature that loops through all user accounts and compiles upcoming loans\u2014something that mimics functionality you'd find in enterprise-level software.\u2705 Made the entire platform accessible with minimal friction and voice-friendly error handling for seamless interaction.What we learned: This project pushed us to grow in multiple areas\u2014from backend architecture to human-AI interaction design. We learned how to:Build voice-first user flows that feel natural, not robotic.Work with external APIs (like Nessie) that require extra care in error handling and testing.Design POST-only endpoints to bypass proxy restrictions, which was a critical learning moment in building production-grade web APIs.Create modular request handling functions that let us scale features faster with fewer bugs.Most importantly, we learned the value of making complex financial tools feel simple\u2014especially for non-technical users.What's next for Voice Agent for Finance: We\u2019re just getting started with Nova. Here's what's on the roadmap:Smarter NLP interpretation: Make the voice agent understand vague or casual phrases and context-switch more naturally.Dashboard visualizations: Let users visualize their financial health with graphs, pie charts, and dynamic insights.Authentication + Multi-user support: Allow businesses with teams to assign roles and access levels.,-Mobile-first experience: Optimize the UI for mobile users who need to check finances on the go.Wider API integration: Nova can now connect with QuickBooks, Stripe, and Plaid to offer more financial intelligence.,Nova is on a mission to democratize financial management for small businesses\u2014and this is just the beginning.",
                        "github": "https://github.com/rishabhchheda01/bitcamp-2025/tree/clean-repository",
                        "url": "https://devpost.com/software/voice-agent-for-finance"
                    },
                    {
                        "title": "Polaris",
                        "description": "Like your personal North Star, it\u2019s an AI-powered journaling app designed to help you navigate your thoughts, emotions, and goals with clarity.",
                        "story": "\ud83c\udf1f Inspiration: We built Polaris because journaling can be hard. Sometimes you don\u2019t know what to write, or how to dig deeper into your thoughts. We wanted something that could guide us\u2014like a helpful companion\u2014through reflection, emotions, and personal growth. That\u2019s what Polaris is: your personal North Star for the mind\u2728 What it does: Polarisis an AI-powered journaling app that guides users through their emotional landscape. It offers three core features:Get Direction: When you don\u2019t know where to begin, this feature offers thoughtful prompts to spark ideas and help your thoughts flow with purpose.Go Deep: Instead of moving on, this feature invites you to explore what\u2019s already on your mind\u2014asking meaningful, introspective questions that guide you into deeper self-reflection.Get Perspective: This feature looks back with you. It reads your past entries, recognizes patterns in how you felt in similar past situations, and offers gentle insights to help you process and grow with clarity.,Together, these features help users move beyond surface-level journaling and into meaningful self-discovery.\ud83d\udee0\ufe0f How we built it: Polaris was built using:Python + Flaskfor the backend and core logicGemini APIfor AI features.MongoDBto securely store journal entries with context and timestamps.,Github:https://github.com/raghavagg4/MyJournal",
                        "github": "https://github.com/raghavagg4/MyJournal",
                        "url": "https://devpost.com/software/polaris-2wsu73"
                    },
                    {
                        "title": "BizCamp",
                        "description": "Our goal is to help increase company workflow by providing real-time meeting transcriptions, smart meeting summaries, concept graph generation, and an AI chatbot!",
                        "story": "Inspiration: We drew inspiration from Salesforce/CRM Software.What it does: Our goal is to help increase company workflow by providing real-time meeting transcriptions, smart meeting summaries, concept graph generation, and an AI chatbot!How we built it: We built this with Next.js, FastAPI, MongoDB, Qdrant, GitHub, and AssemblyAI.Challenges we ran into: We spent a lot of time defining our database structure and deciding our tech stack to ensure our application is deployable and scalable.Accomplishments that we're proud of: We are proud of building a deployable and scalable application, while ensuring best software security practices.What we learned: We learned about MongoDB and especially learned how Vector Databases work through Qdrant.What's next for BizCamp: Integration with with Zoom, Google Meet, Teams, and Slack. Add emotion tracking. Utilize more powerful LLMs and statistics. This would allow for more interesting data to be displayed for teams, such as attendee participation during meetings.",
                        "github": "https://github.com/aluthra23/bizcamp",
                        "url": "https://devpost.com/software/bizcamp"
                    },
                    {
                        "title": "FinePrintasaurus",
                        "description": "Don't fall prey to predatory terms! FinePrintasaurus helps users understand difficult contracts so they can make more informed decisions before signing. ",
                        "story": "Inspiration: According to The Economic Policy Institute, over 60 million U.S. workers are bound by forced arbitration, where they are unable to settle disputes outside of the company, with non-unionized, low-wage, and minority workers being the most affected. With this in mind, we developed FinePrintasaurus: a user-friendly app that helps users understand vaguely-worded contracts.As college students beginning to enter a corporate world, we knew we\u2019d soon be signing papers with conditions we didn\u2019t understand. We hope that FinePrintasaurus can protect us, our peers, and our community from vague legal terms and deceptive fine print.What it does: Users are able to upload pdf files of legal documents, contracts, and/or terms of services onto the app and extract the crucial information in a quick, digestible manner that requires no legal expertise. FinePrintasaurus summarizes the involved parties, the most important clauses that the user should know, and the consequences of terminating/breaching the contract.There also is a login feature where users can save the different contracts they have uploaded and have all of them recorded in a table on the home page. If they wish to get rid of it, they can simply delete it by pressing the X button.Throughout the entire process, we prioritized making our user interface as minimalistic and intuitive as possible for increased accessibility.How we built it: Technologies for frontend: React, TypeScript, Next.js. Technologies for backend: MongoDB, GeminiAPI,Node.js, Expressjs. For deployment Vercel was used for the frontend and Render was used for the backend.Challenges we ran into: There were multiple issues that we ran into over the course of the project. One issue that we ran across was CORS errors because the back-end server did not like the front-end pulling from it. There were also limitations with the Gemini API that we were using which made it difficult to scrap a whole file, needing it to be parsed. The original js-based parser that we were using also did not work properly so we had to pivot to a python based one.Accomplishments that we're proud of: During this hackathon, we were able to implement a deployable website on Vercel (front-end) and render (back-end) that utilizes a login feature that saves user progress onto MongoDB. Before Bitcamp, we didn\u2019t have much hackathon experience and had never used MongoDB or Gemini API. Despite this, we were able to use our own personal strengths to contribute to different aspects of the project. From the little victories to the late-night conversations, we\u2019re proud to say that we will have walked away from this event knowing that we gave it everything we had.What we learned: We learned how to use MongoDB atlas to store our information. We learned how to use GEMINI API, and how to create user authentication with JWT. We learned how to debug and deploy web applications. We also learned how to distribute tasks and communicate with a team.What's next for FinePrintasaurus: With Generative AI being a relatively new development, our parsing isn\u2019t the most accurate. But as time passes, Generative AI will continue to evolve, as will the accuracy of our legal summaries.As for future developments of FinePrintasaurus, we understand the significance of the language barrier, and so in future cases, we aim to reach a wider-audience by implementing linguistic APIs that can fluently translate these terms.Though it\u2019s only been 36 hours, we\u2019re extremely proud of what we\u2019ve done. And we hope that this app will be able to support many users in making informed decisions on contractual agreements.Thank you, and this has been FinePrintasarus!",
                        "github": "",
                        "url": "https://devpost.com/software/fineprintasaurus"
                    },
                    {
                        "title": "SoberSide",
                        "description": "SoberSide \u2013 Your AI companion that listens when no one else can.",
                        "story": "Inspiration: SoberSide was born from a very personal place. During ourfreshman year, we witnessed first-hand the damaging effects of substance addiction on some of our closest friends. We saw peers struggle with the harsh realities of addiction\u2014watching vibrant lives slowly deteriorate under the weight of substance abuse. The pain of seeing brilliant, talented individuals lose their spark and direction deeply affected us. We knew there had to be a better way to support those fighting the same battles.We wanted to create something more than just a generic self-help guide.SoberSideis our attempt to harness technology to provide compassionate, personalized support for individuals on the journey to sobriety\u2014helping them understand their patterns, track their progress, and ultimately rebuild their lives with dignity and hope.What We Learned: Throughout the development of SoberSide, we learned a great deal about:Full-Stack Development:Building the backend with Node.js, Express, and MongoDB taught us how to manage data in real time.API Integration:Using OpenAI's API pushed us to explore how AI can be tailored to provide empathetic responses on sensitive topics like addiction.Humanizing Technology:We discovered that the true value of technology isn\u2019t just in its ability to perform tasks but in its capacity to understand and support human emotions.Interdisciplinary Challenges:Combining mental health awareness with advanced technical solutions is challenging\u2014especially when working on such delicate subjects\u2014but it\u2019s also profoundly rewarding.,How We Built the Project: We started by setting up a robust Node.js backend that integrated three core components:Data Management:Using MongoDB Atlas with Mongoose allowed us to persist user profiles and chat logs, capturing details such as sobriety start dates, moods, and personal coping strategies.Artificial Intelligence:OpenAI\u2019s API powers our chatbot that provides thoughtful, personalized responses based on historical data, ensuring that each interaction feels uniquely tailored to the individual.Voice & Messaging Integration:Twilio is used to enable both text and voice communications, making sure that help is accessible regardless of the user's situation.,We meticulously crafted system prompts and data storage schemas to ensure that the AI could learn over time and offer more personalized insights with each conversation.Challenges Faced: Building SoberSide wasn\u2019t without its hurdles:Emotional Toll:Seeing how addiction impacted our friends during our freshman year made this project more than just a technical challenge\u2014it became a personal mission for our small team.Technical Integration:Integrating OpenAI, MongoDB, and Twilio into a cohesive application was complex. Each system came with its unique challenges and required careful error handling.Privacy & Sensitivity:Handling sensitive personal data and ensuring that our AI responded in a safe, non-judgmental manner required intensive testing and iterative refinement of both our prompts and logic.Context Management:Since GPT is inherently stateless, designing a mechanism to feed historical data back into the conversation so that the AI \"remembers\" past interactions was one of the most challenging aspects.,Final Thoughts: Building SoberSide has been a labor of love\u2014a fusion of technical innovation and deep personal commitment to helping those who struggle with addiction. The experiences and observations from our freshman year, filled with both heartbreak and resilience, inspired us to build a tool that not only supports sobriety but also learns and adapts to each individual. SoberSide is our ongoing journey towards better technology, better support, and ultimately, a brighter future for everyone fighting for a sober life.",
                        "github": "",
                        "url": "https://devpost.com/software/soberside"
                    },
                    {
                        "title": "Foot Finder - Plant Your Foot to Find your Path",
                        "description": "Hi, we are Travelero. Our goal is to expand our app to those who might prefer to take a path of lower elevation. For example, we want to help those in wheelchairs take a path with less hills.",
                        "story": "Inspiration: Navigation is an extremely common task in which traditional map services purely prioritize distance/time minimization between two points. This makes it hard for mobility-impaired individuals or wheelchair users, especially to reliably follow online navigation services. To solve this, Travelero generates paths based on user preference where users can prioritize the shortest distance to their location, the least variation in elevation to their location, or a mix of both.What it does: As students at UMD, we naturally find ourselves on cross-campus hikes along unpleasantly steep terrain. We tell ourselves that surely, there is a better way to get from point A to point B\u2013 one that doesn\u2019t require the endurance of a mountain goat.Travelero Foot Finder works to provide that golden path. It is a pedestrian navigation web app that solves the complications of traversing hilly urban landscapes by selecting the path of \u201cleast undesirable variation in elevation\u201d from a set of paths suggested by the Google Maps API.\nTravelero Foot Finder allows users to search for a specific location on a user-friendly GUI map of Earth. Use the continuous \u201cDistance \u2013 Elevation\u201d slider to smoothly transition between minimizing distance traveled versus minimizing elevation change along a path. Select two locations on the map to place markers, and click \u201cGenerate Path\u201d to run our Google assisted algorithm, which takes the user\u2019s slider position into consideration and generates a path.How we built it: We employed the Google Maps API to display and retrieve map data for backend use, such as distances, paths, and elevations. Our path generation began with Google\u2019s recommended paths, then applied weights based on variations in elevation along them. \nSpecifically, we split each path into nodes and edges, then measured the changes in elevation between each consecutive node. We applied weights to amplify any positive changes in altitude, and solely radical negative changes in altitude (more than 45 degrees steep), according to the user-set slider value. Sometimes, Google will suggest the shortest path even if it is steep or hilly, but after running Google\u2019s paths through our weighting process, we retrieve the path of least unfavorable variation in elevation.\nWe used GitHub, Git, and VS Code to work on code as a group. We used React and TypeScript to build the frontend and a combination of JavaScript, Python, and Flask to build the backend. We also used terpai.umd.edu in order to generate ideas for specific features that we may have wanted to include, such as our personal media links and utilization for our concept. When we were at a stand-still in terms of technical knowledge, or needed a frame of code to get us on our feet, Google Gemini helped bring some of our ideas to life.Challenges we ran into: The very first challenge was determining what pathfinding algorithm to use. If a user selects two points anywhere, there are infinitely many paths to take between them. To reduce the number of paths we needed to consider, we looked at graph-based methods, where we would discretize a region on Earth as a grid of points, then only consider step-wise movements between each gridpoint to get from point A to point B. The first graph-based pathfinding algorithm we considered was Dijkstra\u2019s\u2013 until we realized the Google Maps API already has a strong A* pathfinding algorithm built in. We stuck with that, then decided to modify the suggested paths based on variations of elevation along them.\nBeginning this project, we wanted an all-encompassing pedestrian pathfinding algorithm that would show a combination of fast and non-steep paths between any two walkable places within a reasonable distance. We found difficulties in avoiding physically impossible paths, like walking through locked buildings and off cliffs, so we ended up sticking to Google-given paths. In the future, we hope to expand our algorithm to assess paths over areas untouched by Google, like grassy hills or desert terrain without human-made walkways. Perhaps we will call this product Bigfoot, referring to footpaths in the same thick woods that are home to the mythical creature.\n    The final 10 percent of work proved to be 90 percent of the challenge, as we tried to fix errors like previously generated paths not erasing when we clicked the \u201cclear map\u201d button, or tried to connect the data from two different components (the slider and the map) to the backend in a single function call. We\u2019ve never worked with the Google Maps API, and much of React was unfamiliar to us.Accomplishments that we're proud of: During the event, we learned many new things about the inner workings of Google Maps and how to share data between web Elements for frontend-backend communication. Our research and hands-on experience leave us feeling a little more comfortable building a similar project in the future. Seeing the pathfinding algorithm work in real time inspired us to push forward!\nWe were also able to strengthen our bonds, as the development of the team really showed throughout the project. Lots of ideas were shared, lots of helping each other within the team, and lots of considerations for everyone's strengths and weaknesses brought everyone closer together. In each and every step that we completed, the entire team felt like winners. We all celebrated each small accomplishment and felt pride that we were able to achieve what we had hoped.What we learned: The team generally lacked experience in creating webpages, but we were all able to make great progress in learning new skills related to problem solving and app development. Additionally, as a team coding project, we also improved our ability to coordinate in a team environment using Git. We learned how to quickly adapt to foreign APIs and utilize pre-existing tools towards our specific goal, from terpai.umd.edu and Gemini, to Google Maps, to Flask and React.What's next for Travelero: There are lots of different possible optimizations and additional features we could add to better cater to broader audiences, like those seeking scenic walking paths, or those who wish to cross roadless natural territories unmarked by Google\u2019s pathfinding algorithm. With more time and experience in React and CSS, we would like to provide the most user-friendly and visually appealing service possible through extensive testing and revision.\u2014----------\nGenerally, we had a good experience with the technology. There were some minor issues that we were unable to explain or completely figure out, but overall, everything worked about as expected, if not better. The connection was a little slow at times (we spent the majority of the 36 hours at the venue in the Reckord Armory)\nYes, we used the Google Maps API to retrieve map data relating to elevations at different points and distances between points. We also used Google Maps pathing to find crosswalks and walkways that we could use for our generated paths.",
                        "github": "https://github.com/FallenFork/bitcamp-map-app",
                        "url": "https://devpost.com/software/foot-finder"
                    }
                ],
                [
                    {
                        "title": "Roomlytics",
                        "description": "From Random to Besties! ",
                        "story": "Roomlyticswas inspired by our own college experiences, where we realized how challenging it can be to live with new roommates. From loud snorers and messy kitchens to mismatched sleep schedules, we saw firsthand how difficult cohabitation can be without real compatibility. Traditional roommate matching often only considers surface-level traits or availability, overlooking deeper lifestyle alignment.WithRoomlytics, we wanted to build a more thoughtful, data-driven solution that analyzes user preferences across key lifestyle dimensions to provide insight and improve harmony in shared living spaces.How It Works: Our project allows users to record their lifestyle preferences and housing criteria through a brief questionnaire. Once submitted, the system:This makes it easier to evaluate shared living potential with another person.Tech Stack: We builtRoomlyticsusing:Flask (Python)for the backendHTML/CSSwithJinja templatingfor the frontendMatplotlibfor radar chart generation,Radar charts are encoded usingbase64and rendered dynamically in HTML templates.Challenges: Designing radar charts that were visually clean despite long axis labelsMapping quiz responses accurately to profiles in the databaseEmbedding Matplotlib images using in-memory buffers and encodingHandling session data securely and restricting access to result pages,Accomplishments: Built a full-stack web app from scratchCreated clear, readable radar charts to represent multidimensional dataImplemented profile storage, quiz logic, and dynamic matchingLearned how to pass and display data efficiently between backend and frontend,Lessons Learned: Structuring Flask apps effectivelyVisualizing data withMatplotlibUsingFlask sessionsand secure routingImportance of UI/UX decisions for readability and clarity,Future Plans: Add a full roommate matching algorithmAllow real user profiles withphotos and biosImplementmessaging featuresBuild anadmin dashboardfor user data analysisCreate amobile-friendlyversion of the site,Tools Used: FlaskMatplotlibHTML/CSS  (TailwindCSS)Jinja2,We came up with an algorithm to sort out different compatibility categories.",
                        "github": "",
                        "url": "https://devpost.com/software/roomlytics"
                    },
                    {
                        "title": "MarlowOS",
                        "description": "MarlowOS is a secure flight based defense system.",
                        "story": "Inspiration: We recently set up a security camera in our college apartment after getting a server to host our website. It worked fine, but I quickly realized its biggest limitation \u2014 it could only monitor one room. I didn\u2019t want to spend more money on multiple cameras, so I started thinking: what if we used a drone instead? That\u2019s how the idea for a drone-based security system was born \u2014 one that can track intruders, follow them around the apartment, and even snap pictures in real time.What it does: Our system uses facial recognition to detect and track a person\u2019s face. Once an intruder is spotted, the drone automatically follows them and records their movement. It can also take snapshots of the person and save them for review later \u2014 all while navigating around the space autonomously.How we built it: We combined off-the-shelf facial recognition libraries with a basic drone SDK. The drone takes camera input, processes it on-device (or through a lightweight server), and uses that to make real-time movement decisions. A lot of testing went into making sure the drone could actually keep up with someone moving and respond quickly enough.Challenges we ran into: Facial tracking was no joke \u2014 it took a lot of tweaking to get it to work consistently, especially when lighting or angles changed. Getting the drone to fly smoothly without crashing while also tracking a moving target was another huge pain point.Accomplishments that we're proud of: Honestly, the fact that we got it working at all is something we\u2019re super proud of. Watching the drone actually follow someone and take photos was one of the coolest moments \u2014 it felt like something out of a sci-fi movie.What we learned: We learned a ton about real-time computer vision, how to control drones through code, and the limits of consumer tech when you try to push it just a little further. We also realized that hardware projects are a different kind of grind \u2014 more bugs, more crashes, but way more satisfying when things click.What's next for MarlowOS: We want to make it smarter \u2014 like recognizing specific people, integrating voice commands, or syncing it with a full smart home setup. We\u2019re also thinking about turning this into a more accessible plug-and-play security solution that anyone can set up at home without needing multiple expensive cameras.",
                        "github": "",
                        "url": "https://devpost.com/software/marlowos"
                    },
                    {
                        "title": "subpoenAI.tech",
                        "description": "Your pocket legal interpreter. We translate complex legal situations into plain English and actionable steps, from traffic tickets to landlord disputes. Justice shouldn't require a law degree.",
                        "story": "Inspiration: The legal system can be intimidating and inaccessible. I created SubpoenAI after watching friends struggle with everyday legal issues simply because they couldn't understand complex legal language or afford an attorney. Everyone deserves access to legal information in language they can understand.What it does: SubpoenAI uses artificial intelligence to translate complex legal situations into straightforward advice and actionable steps. Users can get help understanding rental agreements, interpreting traffic citations, preparing for small claims court, deciphering contracts, navigating consumer rights, and understanding employment laws\u2014all without legal jargon.How I built it: I built SubpoenAI using React for the frontend and Node.js for the backend. The core intelligence comes from integrating the Gemini API, which I've fine-tuned with specific prompts for different legal scenarios. Each module (like the Small Claims Court Preparation Assistant or Traffic Ticket Helper) has specialized prompting strategies to generate relevant, jurisdiction-specific advice.Challenges I ran into: The biggest challenge was ensuring accuracy while maintaining simplicity. Legal advice varies dramatically by jurisdiction, so I had to develop a system that could provide location-specific guidance while always being clear about its limitations. Creating prompts that generate consistently helpful responses without overstepping into unauthorized practice of law required careful testing and refinement.Accomplishments that I'm proud of: I'm particularly proud of the document generation feature, which creates customized letters and forms based on user inputs. The jurisdiction-specific guidance system also turned out well\u2014it can adjust advice based on state, county, and even city-level regulations for issues like tenant rights and traffic citations.What I learned: This project taught me the importance of prompt engineering for specialized domains like law. I discovered that providing specific examples and clear constraints to the AI produced dramatically better results than open-ended questions. I also learned how to balance technical capability with ethical responsibility when developing tools that influence real-world decisions.What's next for subpoenAI.tech: The immediate next step is expanding our jurisdiction database to cover more local regulations. I'm also working on a document upload feature that would let users scan their legal documents (leases, citations, contracts) for instant analysis. Long-term, I want to add a lawyer referral network to connect users with affordable legal help when AI assistance isn't enough.RetryClaude can make mistakes. Please double-check responses.",
                        "github": "",
                        "url": "https://devpost.com/software/subpoenai-tech"
                    },
                    {
                        "title": "ShapeShift",
                        "description": "Ready to take your projects to the next dimension without the hours of work? Meet ShapeShift, a developer tool web app that transforms 2D SVGs into 3D models with just a click of a button.",
                        "story": "Inspiration \ud83d\udca1: As developers, we\u2019re always looking for ways to stand out. Incorporating 3D models into our projects was a clear way to elevate our work\u2014but the process of modeling or sourcing assets was time-consuming and overly complex. With the help of ShapeShift,taking our projects to the next dimension has never been easier.What it does \ud83d\udcad: ShapeShiftinstantly converts SVGs into Three.js-ready code, helping developers turn any idea into 3D reality.Animations made easy \ud83c\udfa5: models can spin, move, and come to life with just a few clicks.Not sure what to build? Ourinteractive Canvas pagelets users create and experiment with 3D models directly in the browser.Save your work!With user authentication,  a smooth login/logout system,  and invaluable project storage, ShapeShift makes 3D development seamless, accessible, and stress-free.,How we built it \ud83d\udee0\ufe0f: We built ShapeShift using a modern web stack\u2014React for the frontend, Three.js for 3D rendering, and MongoDB for authentication and data storage.To generate 3D models, we started by parsing SVG files and extracting their individual shapes (paths, circles, polygons, etc.). Each shape is then converted into a 3D mesh using Three.js geometry generators. We normalize and group these meshes to ensure consistent scaling and alignment in the 3D scene.For real-time interactivity, we use React Three Fiber to seamlessly integrate Three.js with our React components. Users can rotate, animate, and manipulate their models directly within our custom Canvas interface. Model data is stored securely via MongoDB, enabling save/load functionality tied to user accounts.Our resulting project is a smooth, accessible pipeline from2D SVGtofully interactive 3D model\u2014all inside the browser.Challenges we ran into \ud83e\uddd7: Throughout development, we faced a variety of technical challenges. Exporting and importing 3D models required careful handling of file formats and geometry data to maintain model integrity. Object storage was another hurdle\u2014we needed a reliable and scalable way to save user-generated models and retrieve them efficiently.Building an interactive coding playground inside the browser also presented unique difficulties, from syncing 3D updates in real-time to ensuring a smooth and intuitive user experience. Each challenge pushed us to dig deeper into Three.js, file handling, and real-time data management\u2014but overcoming them made ShapeShift a much more robust and powerful tool.Accomplishments that we're proud of \ud83c\udf89: One of our biggest accomplishments was the growth we experienced as a team. Every member picked up new skills\u2014whether it was learning 3D rendering with Three.js, working with SVGs, building a full-stack web app, or implementing authentication and storage. The challenges we've overcome with our code have been great, and we\u2019re proud of how far we pushed ourselves. In the end, we were able to bring our vision for ShapeShift to life in such a short time.What we learned \ud83e\udde0: How to create, import, and export3D modelsAnimating components usingReactandReact Three FiberNavigating and building withNext.jsUnderstanding the fundamentals ofWebGLand how it powers 3D rendering in the brow,What's next for ShapeShift \ud83e\udd14: We\u2019re excited to keep building! Here are some of the next steps we have planned:Enhanced model export options, including support for more file formats like .glb and .gltfMore import options, including png and jpegAdvanced control tools for customizing motion paths and vector options within the modelCollaborative features that allow users to share and co-edit models in real-timeAI-assisted model suggestions based on SVG input and prompts,Our goal is to turn ShapeShift into a go-to platform for fast and accessible 3D development.",
                        "github": "https://github.com/Lewin-B/shapeshift",
                        "url": "https://devpost.com/software/shapeshift-o2yh0w"
                    },
                    {
                        "title": "BitBoxer",
                        "description": "Practice your boxing without a controller!",
                        "story": "BitBoxer is an interactive game single player punch one another until one person reaches the desired score first!BitBoxer utilizes React alongside tools such as OpenCV and MediaPipe to provide a fun, interactive user experience where they can play a game of punchies with a friend. The user's laptop webcam is used to motion track the their hands which allows them to deliver punches on the opposing side.How we built it: We leveraged Three.js to create the 3D environment along with MediaPipe for motion tracking via web camera.Challenges we ran into: The biggest challenged we faced was making a fluid display of the gloves to recognize punches. This led to other problems that came up such as positioning, calibration, and scaling of the different components.Accomplishments that we're proud of: We are proud of having a deployed website for people to use! This was more of a fun hack to build and definitely made it a more enjoyable hackathon experience.What we learned: Blending Three.js with MediaPipe and building literacy on using webhooks!What's next for BitBoxer: We would like to make the whole experience multiplayer with being able to go up against different people to have a more fun and playable experience.",
                        "github": "https://github.com/ikshayer/bitcamp",
                        "url": "https://devpost.com/software/fisteez"
                    },
                    {
                        "title": "Helpr",
                        "description": "Helpr is a tool that brings businesses more customer service power than any billion dollar company with just one click. ",
                        "story": "Inspiration: Customer service is broken for small businesses. While giants like Amazon and Apple offer lightning-fast, intelligent support, most businesses rely on outdated FAQs, overwhelmed reps, or generic chatbots. We wanted to change that. With the rise of AI voice agents, we saw an opportunity to give every business\u2014no matter how small\u2014the ability to launch a full-service, voice-based customer support rep with just a company name and a URL.What it does: Helpr instantly creates a fully functional voice assistant trained on your company\u2019s website, FAQ, and support documents. Just enter your company name and URL, and with just one click, Helpr crawls your site, builds a knowledge base, and spins up a natural-sounding voice agent that can answer calls, help customers, and resolve issues 24/7.How we built it: At the heart of Helpr is a vision-based Retrieval-Augmented Generation (RAG) pipeline powered by Colpali and Qwen. This system enables us to not only extract information from web pages and documents, but also understand visual context\u2014like layout, tables, and scanned forms\u2014making our voice agent far more robust when dealing with PDFs, screenshots, or policy documents uploaded by businesses. By using Colpali to semantically parse visual documents and Qwen as the generation engine, Helpr can answer questions grounded in both structure and content\u2014something traditional text-only RAG systems can\u2019t do effectively.Alongside our RAG pipeline, we fine-tuned a CSM-1B model from OpenSesame to give Helpr a strong foundation in conversational understanding, especially for follow-up questions and longer dialogue history. We used ElevenLabs\u2019 Conversational AI for real-time, natural-sounding speech-to-text and text-to-speech handling, allowing the voice agent to feel human, fast, and fluent.To understand, cluster, and personalize user queries, we integrated Google Gemini and ChromaDB for semantic encoding and grouping. This allowed Helpr to identify patterns in customer questions and tailor responses based on intent. Our backend was built in FastAPI, chosen for its lightweight performance and flexibility, while MySQL handled structured storage of user sessions, knowledge chunks, and analytics data.For dynamic, real-time search beyond the scraped knowledge base, we used Perplexity AI to fetch up-to-date web results for open-ended or news-related questions. The ReactJS frontend gave us a fast, modern UI where users can deploy their agent by simply entering a company name and URL. All of these systems work together to generate a voice agent with just one click.We even used TerpAI to build us a business pitch and linkedin post to really get Helpr out there!Challenges we ran into: One of the biggest challenges was orchestrating all of these systems to work together in real time. Fine-tuning the CSM model required careful dataset curation and extensive testing to maintain response quality while supporting dynamic voice inputs. Integrating ElevenLabs with fast, low-latency backend responses while maintaining conversational naturalness also took tuning and optimization.We also ran into complexities when using vision-based RAG on user-uploaded documents\u2014handling malformed PDFs, inconsistent layouts, or low-resolution scans pushed us to build fallback mechanisms and cleaning pipelines. Finally, ensuring that our semantic clustering (using Gemini and ChromaDB) could intelligently group diverse customer queries without losing nuance was a delicate balancing act.Accomplishments that we're proud of: We\u2019re incredibly proud that Helpr can deploy a fully functional voice support agent trained on any business\u2019s website in under a few seconds. This includes scraping, indexing, and speech agent initialization\u2014tasks that typically require hours or manual configuration. We also successfully integrated vision-based document ingestion with semantic search, giving Helpr the power to handle FAQs, legal policies, and even scanned paper forms. Finally, our system grouped customer questions intelligently and provided analytics-ready insights about what customers were really asking.What we learned: We learned that combining traditional RAG with vision-based document parsing significantly boosts coverage for small businesses with fragmented content. Fine-tuning smaller open models like CSM-1B, when done carefully, can outperform larger models in narrow domains like customer support. We also discovered that building a voice UX is fundamentally different from building a chatbot\u2014timing, tone, and content brevity are critical. And most importantly, we learned that users don\u2019t want to fiddle with configurations\u2014they want magic in one click.What's next for Helpr: We plan to launch multilingual support with real-time accent adaptation, making Helpr more accessible globally. We\u2019re building native integrations with tools like Shopify, Zendesk, and HubSpot to embed Helpr directly into existing workflows. We\u2019ll also roll out a self-serve analytics dashboard so businesses can track common questions, identify content gaps, and fine-tune their agent behavior. Long-term, we're exploring ways to embed Helpr into phone systems and smart speakers, transforming it from a web-first solution to a full omnichannel support agent. With strong early interest, we\u2019re gearing up to raise a pre-seed round to scale Helpr beyond the hackathon and into the hands of thousands of small businesses worldwide.",
                        "github": "https://github.com/AwesomeCuber6543/BitCamp2025",
                        "url": "https://devpost.com/software/helpr-s2efpn"
                    },
                    {
                        "title": "OutTheGC",
                        "description": "A collaborative event planning web application that helps friend groups follow through with plans - finally getting them out of the group chat.",
                        "story": "Inspiration: We built OutTheGC to solve a problem almost every friend group runs into: making plans in the group chat that never actually happen. Messages get buried, decisions drag on, and good ideas fade away. OutTheGC turns those scattered conversations into real plans by giving friends a simple, organized way to coordinate events together.What it does: OutTheGC is a collaborative event planning platform that makes organizing with friends easy and actionable. Users can create or join friend groups through invitation links, suggest and plan events, and build out details like location, time, and budget. Events sync directly with Google Calendar, and upcoming plans are displayed within each group for better visibility and follow-through.How we built it: We built OutTheGC using Next.js for the frontend and Firebase for authentication and real-time data storage. Our UI was designed in Figma with a focus on simplicity and ease of use. We integrated the Google API for seamless calendar syncing and used the Gemini API to help generate activity ideas and planning suggestions.Challenges we ran into: One of the biggest challenges was integrating with the Google Calendar API, which involved navigating a complex system of permissions, scopes, and OAuth configurations. We also spent time refining the user interface to strike the right balance between functionality and simplicity, ensuring it supported collaboration without overwhelming the user.Accomplishments that we're proud of: We're proud to have built a complete first version of OutTheGC, bringing together collaborative planning tools, calendar syncing, and smart suggestions in one cohesive platform. Successfully integrating multiple APIs into a smooth, unified experience was a major milestone. Most importantly, we\u2019re proud that the platform is something we genuinely want to use with our own friends.What we learned: Throughout this project, we gained experience working with Firebase and third-party APIs in a collaborative dev environment. We also learned how to approach product design with a user-first mindset, and how to keep our feature set focused to avoid unnecessary complexity while still solving the core problem.What's next for OutTheGC: We\u2019re excited to keep growing OutTheGC. Next steps include integrating payment platforms like Venmo or PayPal to simplify cost splitting, adding push notifications to keep users informed, and developing a mobile version so planning can happen on the go.",
                        "github": "https://github.com/csumah/pjarcs-bitcamp-2025",
                        "url": "https://devpost.com/software/outthegc"
                    },
                    {
                        "title": "Finfig",
                        "description": "A social leaderboard to gamify good spending habits ",
                        "story": "A financial fighter game that can motivate you to make better decisions.\nWith the use of AI, Finfig lets you compete with your friends while keeping your financial history confidential.How it works: Every month, you and your friends start with 1000 points.Transactions are automatically uploaded using Capital One's Nessie API.The user then has to upload the receipt for that purchase and justify it, allowing AI to see the itemized bill. (You can choose not to upload a receipt for a purchase, but there will be a point deduction to prevent users from hiding unnecessary spending.)The AI analyzes the receipt and justifications to decide the necessity of the purchase, and deduct points based on how frivolous the purchase was.The AI can also estimate the amount of carbon emissions needed to produce the item, and the value for  money of that item using data from Walmart and other online price averages.This means there are three categories upon which you can compare your purchases to your friends, allowing you to make more necessary, more sustainable, and more efficient purchases.It also stores your receipts locally for your convenience.,Why it works: The competition forces you to review your payments later and think about why you made them, which is a crucial part of controlling frivolous spending.The competition also adds a social element to the motivation to be more sustainable and make more clever financial decisions.,We were inspired to pursue this avenue because as college students we have all been in a position where we spend an excess amount of money relative to what we should be spending, and still understand very little about good finance.We used Flutter to build the mobile app after building a style guide and a wireframe to establish the layout and the theme.\n# Challenges we ran intoWorking in such a crowded environment with unpredictable wifi was initially challenging\n# There were a lot of issues we were having with our Android Studio installations that were not in the documentation, which forced us to be resourceful\n# Accomplishments that we're proud ofWe were able to go from not even having Flutter installed to making a functioning app that uses proper views and styles for underlying components.\n# What we learnedWe learned about development in Flutter as well as the Dart programming language\u2019s syntax. We also gained significant experience with Android Studio and learned how to emulate various Android devices. - - Through emulation on Android Studio we learned how to troubleshoot Dart code as well as resolve file dependency issues.,What's next for FinFigOur next steps would be to pursue Capital One API integration to make FinFig more secure and further optimize the user experience.We would also improve the AI to make it more accurate, assigning more meaningful scores and potentially even matching you with random users to make the gameplay more interesting.,",
                        "github": "https://github.com/PratheekRamakrishna/BitCamp-FinFight",
                        "url": "https://devpost.com/software/good-spending-habits-social-leaderboard-project"
                    },
                    {
                        "title": "PodQuirk",
                        "description": "PODCAST",
                        "story": "Inspiration: I wanted to experiment with ElevenLabs to see how far I could push realistic AI-generated voices. It seemed like a fun challenge to create something functional and creative, and podcasts felt like a natural fit.What it does: PodQuirk lets users pick a topic or category, and in under a minute, it generates a podcast episode that talks about that subject. It combines real-time news and smart summarization with AI narration, giving users a quick, engaging audio experience.How I built it: The frontend is built with Next.js, the backend with Flask. I use OpenAI to generate the podcast script, ElevenLabs for lifelike TTS audio, and the Newspaper3k API to pull relevant news content. AWS handles caching and storage to reduce unnecessary API calls and speed up repeat requests.Challenges I ran into: Managing API usage was a big one! I didn\u2019t want to burn through credits every time someone clicked \u201cgenerate.\u201d So I implemented a caching mechanism using AWS to store previously generated episodes. Another challenge was fine-tuning the prompt for script generation so that it would sound natural and match the tone of a real podcast. Setting up and managing ElevenLabs also had a bit of a learning curve.Accomplishments that I'm proud of: I was able to generate a full podcast in under a minute, from topic selection to audio output. That includes gathering relevant news, scripting, and producing it all with high-quality narration.What I learned: What's next for PodQuirk:",
                        "github": "https://github.com/JulianChavez/BITCAMP_2025",
                        "url": "https://devpost.com/software/podquirk"
                    },
                    {
                        "title": "zombs.tech",
                        "description": "Fast-paced first-person zombie shooter \u2014 survive waves, rack up scores, and fight till your last breath. https://zombs.tech/",
                        "story": "\ud83c\udfae Inspiration: When I was a kid, I was obsessed with browser-based first-person shooters likeShellShockersandKrunker.io. They weren\u2019t just games\u2014they werefriendship factories, moments of joy, and core memories. I've always dreamed of creating myownFPS web game that could give someone else that same thrill. Throw in someDoominspiration, and boom\u2014zombs.techwas born.\u2694\ufe0f What It Does: zombs.techis a fast-paced, first-person shooter where you take down waves ofzombies and slimes, leveling up as you go and chasing that high score. Every wave gets harder. You\u2019ll dash, updraft, and shoot your way through chaos, racking up kills and dodging enemy fire like a boss.\ud83d\udee0\ufe0f How I Built It: I built this beast using JavaScript withThree.jsto handle all the 3D magic. It\u2019s running on a web-based HTML framework and features a custom UI/UX design that brings the whole thing to life. Every model, mechanic, and texture was hand-crafted, tested, and refined for smooth gameplay.\ud83d\ude35\u200d\ud83d\udcab Challenges I Ran Into: BRO. THERE WERESO. MANY. BUGS.Texturing? Tookagesto figure out how to make textures attach.Projectiles? Had to learnraycastingjust to figure out if a bullet hit an enemy or me.Walls? My character kept phasing through them for TWO HOURS until I coded collision physics from scratch.Map creation? Every wall had to be placed by hand with specific coordinates\u2014ugh.UI/UX? Total overhaul midway through because the original was trash.Testing? I wrote actual unit tests for aweb gamejust to track down sneaky bugs.,But I pushed through, and I\u2019m so freaking proud of where it landed.\ud83c\udfc6 Accomplishments I'm Proud Of: I packed in a TON of features in a short time, and now I feel like I\u2019ve truly mastered Three.js. Designing and building this game is second nature to me now. Here's what\u2019s in the game:\ud83d\udd2b FPS mechanics with WASD + mouse look\ud83e\udddf\u200d\u2642\ufe0f Wave-based survival\u2694\ufe0f Two enemy types: melee zombies & ranged slime shooters (from wave 3)\u2764\ufe0f Health system (100 HP)\ud83e\uddee Score and wave progression system,Dash (E) \u2013 6s cooldownUpdraft (Q) \u2013 8s cooldown,Zombies chase and hit you up closeRanged enemies shoot green projectiles, can't shoot through walls, have smarter AI (150 HP!),3D map with full collision detectionProjectile collisionsMinimap and spawn point systems,\ud83d\ude80 Start screen with title, instructions, credits, etc.In-game HUD:Health barScore trackerWave progressCooldown indicatorsCrosshair\ud83e\udea6 Game over screen\u2699\ufe0f Settings menu:Mouse sensitivityCrosshair styleMusic & SFX volume slidersMute all option,Gun animations, glowing bullets, enemy death animationsHit markers, score pop-ups, and health bar color changesDamage screen flash for that extra drama,Background musicLaser shots, zombie groans, dash and updraft soundsFull volume control and mute options,\ud83d\udcda What I Learned: Scene setup, camera movement, lighting, materialsTexture mapping and UVsRaycasting and hit detection,Game loop logic and timingOOP with game classesState management & event systemsVector math and 3D calculations,requestAnimationFramefor performancePointer Lock API for smooth FPS controlsWeb Audio API for all the juicy soundsCanvas rendering for the minimap,Built fully responsive menus and in-game HUDsAdded animations, transitions, and visual feedback for all game states,Wave-based progression and enemy balancingCooldown-based abilitiesAI behaviors with pathfinding and attack logic,Collision systems for players, enemies, and bulletsMovement and camera rotation in 3D space,Well-structured code with clear game statesPerformance tuning and resource managementDebugging and writing unit tests for game logic,\ud83d\ude80 What\u2019s Next for zombs.tech: Oh man, I\u2019ve gotbig plans:Multiplayerbattles (1v1s, team fights, co-op survival)Boss fightswith crazy mechanicsNew enemies, new maps, and new game modesEven smoother movement and combat mechanicsAbility tochoose your own musicand sound packsGraphics upgrade (thinking shaders & better models)Leaderboards, account systems, and more polish all around,Thanks for checking out zombs.tech! If you love browser shooters or just want to blast zombies for fun, I hope this game brings you the same joy that inspired me to build it \ud83d\udca5\ud83c\udfaf",
                        "github": "",
                        "url": "https://devpost.com/software/zombie-arena"
                    },
                    {
                        "title": "Sort Analyzer",
                        "description": "This project is a website which allows a user to see different information on sorting algorithms, compare sorting algorithms, and input data to find the best sorting algorithms to use for that data.",
                        "story": "Inspiration: I was inspired to make this project back around the time I went to HackNYU, but I wanted to do it as as a solo project outside of a Hackathon. I haven't had the time to make this project until now, so I thought now was a better time than any.What it does: This project is a website for users to explore different algorithms. They can also input data and compare how different algorithms do, more specifically, how fast the algorithms sort the data.How I built it: I built it in Replit using the Replit Agent and exported it to Github. The Replit Agent used typescript as the main language.Challenges I ran into: When I got to the Armory on Saturday morning from my hotel, I discovered that my personal computer's screen was cracked and glitched, so the computer was unusable. I now have to go into the shop when I return home o fix it. Lucky for me, I was able to use my school Chromebook which often blocks websites and search results, but I was able to access Replit and Github. I also struggled to deploy my website from Github properly.Accomplishments that I'm proud of: I'm quite surprised that I was able to get the Replit Agent to make something as good as it did. I thought I was going to have to make my own elements which might not have happened, so I'm glad I have something to present during the expo. Additionally, I'm glad I got to use Github for the first time and plan to do so in the future.What I learned: It can be challenging when the unexpected occurs. I didn't know my computer was going to break or how it broke so I am pretty disappointed. I was still able to continue coding despite my setbacks. Even deploying the website was a struggle and I was unable to figure out how I might do such a task, but it was fun poking around my files in Github once it was connected.What's next for Sort Analyzer: After I fix my computer, if I really want to, I'll find some way to make the website actually a website.",
                        "github": "https://github.com/montymole27/sorting-algorithm-analyzer",
                        "url": "https://devpost.com/software/sort-analyzer"
                    },
                    {
                        "title": "SmileySpotify",
                        "description": "Best of Youtube and Spotify",
                        "story": "Inspiration: Have you ever been on a YouTube music video and wanted to add it to your Sptofiy playlist? Or, want to find your Spotify playlist songs quickly on YouTube? Well, we have! SmileySpotify bridges the gap between YouTube and Spotify, bringing your Spotify songs to YouTube.What it does: Our Google extension makes a YoutTbe playlist from your selected Spotify playlist. Also allows you to add current song on youtube to spotify playlist.How we built it: Brute force and React.Challenges we ran into: Depends on which feature, abstracting things and getting them to work together was sometimes difficult. One of the big ones was handling the state of the current playing video.Accomplishments that we're proud of: Finishing a polished product as a two-man team!What we learned: Figure out your development environment first!!! We should have made sure we could build our React project into a Google extension before we started coding. Research > Brute forceWhat's next for SmileySpotify: Fixing a few edge cases.",
                        "github": "",
                        "url": "https://devpost.com/software/smileyspotify"
                    },
                    {
                        "title": "Out with the Old, In with the News",
                        "description": "what if.. there was a website.. and it had.. a URL.. and.. a rlly cool idea",
                        "story": "",
                        "github": "https://github.com/evabarks1/bitcamp2025",
                        "url": "https://devpost.com/software/out-with-the-old-in-with-the-news"
                    },
                    {
                        "title": "Steg Detector",
                        "description": "Steganography is an encryption technique in which a harmful payload can be encoded into a harmless-looking image (very dangerous combo). I will use AI to detect if images have these payloads in them.",
                        "story": "Inspiration: 3 years ago, I took a cybersecurity class in high school where I first learned about steganography. It was fascinating. Like I genuinely hadn't enjoyed anything in that class up to that point, but I enjoyed that topic so much I ended up building a project where you could encode and decode messages from audio files using steganography. While it was pretty basic and harmless, it was cool to work with files on the binary level for the first time in my life.That being said, I'm a machine learning guy at heart, so when I went to UMD, that's the path I signed up for and got into. However, fast forwarding to this semester, I took a class on binary exploitation, which honestly rekindled my passion for working with binary. I'm now a cybersecurity minor in addition to my machine learning major, and so I thought it would be a good idea to combine ML with cyber in my Bitcamp 2025 project.All of these thoughts kind of combined in my mind literally one hour before the opening ceremony, and I instantly called my best friend to ask him if I was being an idiot or if this would be a good idea. He gave me some encouraging words, and the rest is history, I suppose.What it does: Steganography is the process of basically taking the least significant bits of information and writing over them to secretly encode your own information in files. While small changes to information can cause huge differences in many file types, files like images and videos are perfect for this type of encrypting technique, as the change in what you see visually is minuscule. This fact alone makes these attacks extremely dangerous as they have no visible trace to the human eye. My solution is to try to use an AI model that will read in the least significant bytes of an image and predict if there is some sort of malicious payload in those least significant bytes.TLDR: The program takes in a PNG image (as steganographic attacks are only used on image types without compression) and uses a sequential neural networking model to detect whether that image has a payload injected into it via steganography.How we built it: I built it by first getting data from a dataset on Kaggle containing images with and without steganographic injected payloads in them. I read the bytes from the images, used bitwise operations to get the least significant bits, and grouped them into a sequence of bytes. I then converted those bytes into floats between 0.0 and 1.0 so I could feed them into a sequential neural network model to find patterns in the least significant bit data of images that would correspond to whether or not they contained steganography.Next, I built a Flask API using the model so that users could interact with the model and find out if their images had steganography or not. I coupled this with a simple front-end in React where users could upload a file, click a button, and then interact with the model and find out whether or not their image has steganographic binary in it.Finally, I built the readme for the project.Challenges we ran into: Some challenges I ran into included converting the least significant bits into something that could be used by a machine learning model. I initially had read and stored the least significant bytes as a string, but not only was that inefficient, but it also made it very difficult to use with machine learning models. After some research, I discovered that converting the raw bytes to float values between 0 and 1 and using a sequential neural networking model would be the best way to use the raw bytes in the way I intended (which is for them to be read in order and see if that sequence is malicious).Another challenge I ran into was the physical limitations of my laptop/optimization of my code. I initially intended to use the entire dataset size to train, especially since I was using a neural network and neural networks need a large amount of data, but my laptop could only handle a couple of hundred images at first. After a lot of optimization using vector multiplication, I was able to increase my dataset size to a couple thousand images.Accomplishments that we're proud of: I'm proud I completed the project start to finish, and I'm proud that I decided to go do something out of my comfort zone. This was my first time ever working with a neural network, so I'm glad I tried and learned something new. I'm also glad I didn't use my laptop as an excuse to just give up on the project and instead sought ways to optimize my code so I could get as much data as possible for my neural network.What we learned: I learned a lot about how neural networks work. I learned about things like weights, biases, epochs, and also the types of neural networks. I also learned how to deal with larger datasets in the future (to start off with small sections of that data, optimize your code, and then try to get more at a time). I definitely made the mistake of trying to get all the data at once, and that led to my computer crashing a lot. So that's a good lesson I'll take with me into the future.What's next for Steg Detector: I plan to improve the accuracy of my model by getting more data from the dataset, a little at a time. I also plan to try to address the issues of false negatives in my project by adjusting the prediction threshold at which I classify something as either steganographically injected or clean.Additionally, I want to try to get random images from online as \"clean\" images too, so that my dataset for \"clean\" images has more variance and mimics the variance in real-world internet images better, as the current dataset has no grayscale images and has the same pixel size (512 by 512). I would probably do the same for the \"steganographic\" images too if I could find another dataset.Finally, in the far future, I would like to expand this steganographic check to audio and possibly video forms of media too.",
                        "github": "https://github.com/Meherzan-Gai/steganography-detector",
                        "url": "https://devpost.com/software/steg-detector"
                    },
                    {
                        "title": "SurfSmart",
                        "description": "Have too much website to look at and opening too much tabs? Don't worry, we got you!",
                        "story": "Inspiration: Every time I was asked to do research, there will be tons of tabs showing on my browser and my computer go slow by time. This project is an attempt to solve this issue.What it does: It basically creates an external knowledge database for each project, dynamically updates using the website contents, LLM's response, and user's notes. For accuracy, we would extract the most frequent keywords and generate a summary using or not using Gemini API. User can ask Gemini questions with this external knowledge database and built-in prompts, such as if one website should be replaced by another, or ask for recommendation websites. User can also get the most frequent keyword in a project and ask Gemini to generate a short overview of this project.How we built it: We used Vite + React for frontend, python flask for backend, MongoDB Atlas for database.Challenges we ran into: The time is short and the total workload was way too heavy.Accomplishments that we're proud of: At least we wrote a prototype, and many places still need revision.What we learned: How to craft a website using Front-end and back-end separation technology.What's next for SurfSmart: Debug.",
                        "github": "",
                        "url": "https://devpost.com/software/surfsmart-tiodl4"
                    },
                    {
                        "title": "Gather",
                        "description": "Find your people\u2014meet, connect, and make plans.",
                        "story": "Inspiration: We\u2019ve all been there\u2014wanting to go to a concert for a niche artist or try out an activity that none of our friends are into. It\u2019s frustrating when you\u2019re excited about something but can\u2019t find anyone to join you. Or maybe you\u2019re in a class where you don\u2019t know anyone, and you study better with a group. Usually, the only option is to post on social media and hope someone\u2019s interested, which can be time-consuming and hit-or-miss. We built Gather to bridge that gap\u2014making it easy to connect with others on campus who share your interests, whether it's music, hobbies, or forming a study group\u2014so no one has to miss out on what they love or need.What it does: Gather is an app designed to make it easier for students to find and connect through events happening both on and around campus. Whether it\u2019s a study group, a casual meetup, or a local concert, students can browse and RSVP to events that match their interests. Users can also create their own events\u2014like jam sessions, movie nights, or tutoring meetups\u2014and invite others to join. For academic settings, the option to create private events is especially useful for instructors or TAs who want a simple, organized way to host office hours, check-ins, or focused help sessions.How we built it: The app was built from the ground up using React Native for a seamless mobile experience across platforms. For authentication and data storage, we relied on Supabase and SQL, which allowed us to manage user accounts and event information efficiently. Event listings were pulled from multiple sources, including the UMD Events page, the Ticketmaster API, and user-generated content. For the map discovery page, we utilized React Native Maps. To personalize the experience, we integrated the Google Gemini API to scrape and gather events from TerpLink.Challenges we ran into: One of our biggest challenges was deciding on the direction and target audience for our app. We had a lot of different ideas, and it took hours of discussion and iteration to align on a shared vision. Eventually, we decided to focus Gather on college students and young adults in their early 20s, aiming to create something that genuinely fits their social and academic needs. On the technical side, we ran into issues like image upload errors, which slowed us down at times\u2014but through collaboration and persistence, we worked through them together as a team.Accomplishments that we're proud of: We\u2019re especially proud of our contacts feature, where you can add contacts to a group to invite people by bulk to an event. Combined with the map discovery system, they complement each other perfectly to elevate the user experience. The recommendation engine analyzes user preferences and suggests events tailored to their interests, making it easier for them to discover activities they'll enjoy. Meanwhile, the map discovery system provides a dynamic, visual way to explore both on-campus and nearby off-campus events, helping users easily navigate and RSVP to events that catch their eye. Together, these features create a more personalized, intuitive experience that encourages users to engage with the app and connect with others around them.What we learned: We are really glad that we were able to tackle learning a new framework successfully as half of our team did not have a lot of experience with React Native or mobile app development in general.What's next for Gather: One idea we\u2019re exploring is integrating chatrooms directly within event listings, so users can easily communicate, ask questions, and build excitement leading up to the event. We\u2019re also working on implementing our recommendation engine, which uses onboarding survey responses to offer personalized event suggestions and connect users with others who share similar interests. These additions would help make the app more interactive, social, and tailored to each user\u2019s experience.",
                        "github": "https://github.com/aadikrishna04/FriendFinder",
                        "url": "https://devpost.com/software/gather-jib5mc"
                    },
                    {
                        "title": "Qubikel",
                        "description": "Tired of empty bike stations? We use quantum tech to predict demand, prevent overflow, and make bike sharing in cities like NYC smarter, faster, and frustration-free.",
                        "story": "Inspiration: Urban bike-sharing systems are a sustainable, healthy alternative to cars\u2014but in busy cities like NYC, they\u2019re often frustratingly unreliable. You arrive at a station only to find no bikes\u2026 or ride to your destination and can't dock because it\u2019s full.\nWe asked ourselves: Can we predict and fix this chaos before it happens? That question led us to combine newly learned quantum computing concepts and urban systems modeling to tackle a real-world mobility problem.What it does: Qubikel is a quantum-powered simulation tool designed to predict bike-sharing station behavior across NYC. It models how bikes move between stations based on population density and inferred commuting patterns.With Markov Chains enhanced by Quantum Walks, Qubikel can:Forecast station depletion and overflowSuggest bike rebalancing strategies with GenAISimulate new infrastructure scenariosMonitor overall bike traffic,How we built it: We developed Qubikel using a stack that combines quantum simulation with modern web technologies:In the backend, we used Python and Flask for the server alongside implementing quantum random walks using Google's Cirq framework for quantum computing. We have Implemented both classical Markov Chain models and quantum-enhanced versions to compare effectiveness.In the frontend, we created interactive dashboard with react & vite for data visualization, and responsive design.We also added the power of Google Gemini to explain complex quantum simulation results to users with varying technical backgrounds.The simulation engine processes geographic data, weather conditions, and time-of-day factors to create realistic transition matrices that govern bike movement patterns. We implemented quantum random walks that leverage superposition to model the complex, non-deterministic nature of urban bike movement.Challenges we ran into: This project represented our first dive into quantum computing and working with Cirq, which presented a steep learning curve. Despite having strong software engineering backgrounds, translating classical algorithms into quantum equivalents required a fundamental shift in thinking about computation.  As newcomers to quantum computing, understanding concepts like superposition, quantum gates, and circuit design in Cirq required extensive research and experimentation. Also, adapting classical Markov Chain methodologies to quantum random walks proved much more complex than anticipated.Accomplishments that we're proud of: Despite these challenges, we achieved several significant milestones:Successfully implemented quantum algorithms despite being first-time users of quantum librariesCreated a working quantum-enhanced simulationDeveloped an intuitive, responsive interface that clearly visualizes complex system dynamics,What we learned from the Bitcamp UQA workshops: This project taught us valuable lessons across multiple domains:Fundamentals of quantum computing and practical application through CirqMethods for effectively comparing classical vs. quantum approaches to demonstrate quantum \nadvantageQuantum and its future (next Nvdia engineer)Grover's Algorithm,What's next for Qubikel: Our roadmap:Connect with actual bike-sharing APIs to test our predictions against real-world usage patternsImplement more sophisticated quantum algorithms to further improve prediction accuracyContinue optimizing our quantum simulations for better scalabilityCombine our quantum approach with machine learning to identify hidden patterns in usage dataRewatch Intersteller after having a deeper understanding of quantum! :),",
                        "github": "",
                        "url": "https://devpost.com/software/qubikel"
                    },
                    {
                        "title": "CosmicCV",
                        "description": "With a starry design and powerful AI, CosmicCV reviews your resume and launches your career. Upload it, watch it get pulled into a black hole, and receive personalized feedback with a dreamy UI/UX.",
                        "story": "Inspiration: We wanted to create a resume builder that wasn\u2019t boring. Job hunting already feels like floating through space without a map, so we thought: why not make the journey beautiful? Inspired by the cosmos, we designed CosmicCV to combine dreamy, starry visuals with powerful AI analysis \u2014 and make resume reviews actually fun (and a little magical).What it does: CosmicCV lets users upload their resume, watch it get pulled into a mesmerizing black hole, and receive detailed feedback powered by Gemini AI. Users get scores for experience, skills, and education, along with strengths, weaknesses, and personalized activity suggestions. It also compares your resume to a database of top resumes to give you a real sense of how you stack up \u2014 so you\u2019re not just floating aimlessly, you\u2019re aiming for the stars.How we built it: We built the frontend with React and Mantine UI, combined with custom CSS animations (like the spinning black hole and twinkling stars). For resume analysis, we used Google\u2019s Gemini API to parse and evaluate the resumes intelligently. We layered in a benchmarking system by comparing resumes to a curated set of top resumes to provide deeper, actionable insights. Everything was stitched together with Vite for a fast and sleek development process.Challenges we ran into: Centering divs in CSS\u2026 it\u2019s always the centering.Getting the black hole animation to look cool \u2014 not like a sad square.Parsing resumes reliably and formatting AI feedback into clean sections.Designing a comparison system that feels fair, useful, and motivating \u2014 without crushing dreams!,Accomplishments that we're proud of: Creating a fully immersive, animated upload experience that feels fun.Pulling off a highly responsive and aesthetic UI while keeping performance smooth.Seamlessly integrating Gemini AI and building a real resume benchmarking engine.Making resume review \u2014 something most people dread \u2014 actually exciting.,What we learned: Thoughtful animations and UI/UX polish massively improve how users experience your app.AI feedback is powerful, but context (like comparing to top resumes) makes it meaningful.Building a great product means lots of testing, tweaking, and asking \u201cdoes this feel right?\u201d,What's next for CosmicCV: Direct editing: Allow users to update their resume based on feedback right inside the app.Smarter suggestions: Generate custom cover letter lines and interview prep questions.Cosmic leaderboard: See how your resume ranks among peers (because why not gamify it?).Even dreamier UI: More starfields, customizable galaxies, and cosmic animations.,",
                        "github": "",
                        "url": "https://devpost.com/software/cosmiccv"
                    },
                    {
                        "title": "Rekindle",
                        "description": "Rekindle finds modern alternatives to your favorite clothes. Just upload a photo, set your preferences, and our AI delivers personalized, style-matching recommendations.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
                        "story": "",
                        "github": "https://github.com/TomMitchell123/Rekindle",
                        "url": "https://devpost.com/software/rekindle-zn7gcm"
                    },
                    {
                        "title": "Control Flow",
                        "description": "A roguelike coding puzzle game where you craft solutions with draggable code blocks. Buy custom code blocks, overcome challenges, and master programming concepts in a procedurally generated adventure!",
                        "story": "Play NOW: Inspiration: Control Flow was inspired by Balatro, a roguelike card game that resembles poker. We thought it would be fun and challenging to incorporate programming puzzles into this style of game, something that's never been done before!What it does: Control Flow is a roguelike programming puzzle game where players solve coding challenges by dragging and combining code blocks. Players progress through increasingly difficult levels, earning coins to purchase new code blocks from a randomized shop. Control Flow turns code blocks into limited resource and will have you code tracing like never before! The game has no one solution for many of the problems--it is up to the user to construct a strategy and use their reasoning abilities to advance.How we built it: We built Control Flow using p5.js for the graphics and interactions. The game features a custom block-based programming language with a visual editor that allows dragging, dropping, and nesting of code snippets completely from scratch. We implemented a roguelike progression system that generates problems with varying difficulty, alongside a shop system that offers randomly selected code blocks. The game evaluates solutions by parsing and executing the assembled code blocks against test cases to determine if the player reached the target value.Challenges we ran into: The biggest challenge was building the evaluator. We basically invented our own interpreted  programming language that gets executed as javascript. We dealt with challenges having to do with scope, variables, (nested) loops, etc..Creating our custom code block editor was also a huge challenge, as it was built completely from scratch with only the p5js core library. We ran into big problems with how to represent nesting and headers, and how to customize the arguments to loops and conditionals. We are beyond proud of how well it turn out.It was difficult to create engaging problems that were not trivial, possible to solve, and, most importantly, fun. We had to make our system highly scalable to be able to hot-swap different kinds of problems quickly! We built this game with scalability in mind, and we are excited to continue adding more features that add a fun twist to how you think about code.Accomplishments that we're proud of: We are super proud of taking a vague idea into a polished game that looks great and is super fun to play, all from scratch. We are also proud of making a super tight stack with no unneeded database, restAPI, or bloaty frameworks like react. Additionally, the game is available for anyone to play right now!!",
                        "github": "",
                        "url": "https://devpost.com/software/control-flow-v415xl"
                    },
                    {
                        "title": "tomatocode.xyz",
                        "description": "Peardeck for coding. Tomatocode is a real-time interactive coding platform that connects teachers and students during presentations.",
                        "story": "",
                        "github": "https://github.com/AregGevorgyan/tomatocode-frontend",
                        "url": "https://devpost.com/software/tomatocode-xyz"
                    },
                    {
                        "title": "CyberSafe HUB",
                        "description": "SafeHub: Empowering everyday users with tools for network and website security analysis, bringing cybersecurity awareness to the forefront before dangers strike\u2014because prevention is the best defense.",
                        "story": "Inspiration: The inspiration for SafeHub came from the growing need to make cybersecurity accessible to everyone. People often underestimate the dangers lurking online until it's too late. We wanted to create a tool that not only educates users but also empowers them to take control of their digital safety. By providing practical tools for analyzing network security and website links, we aim to foster a cybersecurity mindset in everyday life.What it does: SafeHub is a user-friendly web platform offering tools to enhance personal cybersecurity. It provides:Website Link Verification: Analyzes URLs for potential threats, such as phishing or malware.WiFi Security Analysis: Identifies vulnerabilities in nearby WiFi networks, helping users make safer choices.Download Verification: Validates downloaded files to prevent malicious software.,SafeHub brings cybersecurity awareness to users before they encounter threats.How we built it: We built SafeHub using aDjango backendto handle the core functionality, leveraging Python scripts for security checks and analysis. The front-end uses Django templates for a clean, intuitive UI, ensuring ease of use for non-technical users. We also integrated libraries and APIs for URL verification and network scanning. The development environment was set up to streamline collaboration and iteration.Challenges we ran into: CSRF Cookie Issues: Implementing secure forms while maintaining functionality was a significant challenge.Integrating Python Scripts: Embedding security analysis scripts into the Django framework required debugging and testing to ensure smooth integration.Balancing Scope and Time: Deciding which features to include in the MVP while ensuring a functional and polished product was a tough call.,Accomplishments that we're proud of: Successfully implemented core security features in a user-friendly format.Built a functional prototype within a limited time frame.Addressed technical challenges like script integration and secure input handling.Made cybersecurity principles accessible to everyday users.,What we learned: This project reinforced the importance of:Designing with the user in mind, especially for non-technical audiences.Managing time and resources effectively during development.Debugging and integrating diverse tools into a cohesive system.Educating ourselves about advanced cybersecurity concepts while making them relatable.,What's next for SafeHub: We plan to:Refine and Expand Features: Enhance WiFi security analysis and add real-time threat detection.Modern Front-End: Transition to a React-based interface for improved interactivity and design.Mobile Accessibility: Develop a companion app for on-the-go security checks.Community Engagement: Incorporate educational resources and forums to promote cybersecurity awareness.Advanced Tools: Add deeper network diagnostics and proactive threat prevention measures.,",
                        "github": "https://github.com/NahomWondimu/CyberSafe",
                        "url": "https://devpost.com/software/cybersafe-hub"
                    },
                    {
                        "title": "The Tomato Trade",
                        "description": "Does your family love to garden? Have you ever had a surplus of crops you wanted to trade away? The Tomato Trade fixes this problem by connecting you and your veggies with people who could use them!",
                        "story": "Inspiration: We were inspired by the idea that so many people grow more food than they can use, especially neighborhood gardeners with seasonal crops like tomatoes, zucchinis, or herbs. At the same time, others nearby might be looking for those exact items. Instead of letting extra produce go to waste or relying on grocery stores, we wanted to create a platform that lets neighbors connect and trade directly.The concept combines sustainability, local food access, and community building. We also drew from personal experiences of seeing surplus food in our own gardens or communities, and wishing there were a simple way to share it.What it does: Tomato Trade is a bartering platform that allows users to list, browse, and trade homegrown produce with others nearby. Users can:\n-Create posts with item details, quantity, location, prices, photos, and more\n-View listings from other gardeners based on location\n-Search or filter by listing category (selling or requesting) or item type (e.g. tomatoes, herbs, fruits)\n-Connect with other users to arrange a tradeThe app is designed to make it easy for people to reduce waste, access fresh food, and build local connections through a sustainable, community-driven system.How we built it: -Figma to plan out the design\n-Front-end using react, html, css, typescript\n-Back-end using MongoDB, FastAPI, PythonChallenges we ran into: This was our first hackathon, so one of the biggest challenges was figuring things out as we went. We had to teach ourselves how to build an entire full-stack application, including:\n-Setting up a backend from scratch with MongoDB and FastAPI\n-Debugging unfamiliar errors while working under time pressureAccomplishments that we're proud of: -Built a full-stack application from scratch in under 36 hours including setting up both the backend and frontend without prior experience\n-Successfully integrated MongoDB for storing and retrieving user data, learning how to structure collections and handle queries on the fly\n-Overcame the challenge of integrating backend APIs with the frontend to ensure seamless user interactions and data flow\n-Fostered strong teamwork and communication skills, making the project come together despite the steep learning curveWhat we learned: -How to set up a full-stack web application using React for the frontend and MongoDB/FastAPI for the backend\n-How to integrate third-party APIs, like Google Sign In, to enhance user experience with location-based features\n-The importance of clean code organization and version control, especially when collaborating under time constraints\n-How to troubleshoot bugs efficiently, even when the errors were completely new to us, for example merge errors caused by everyone working on the same website at the same time\n-That we\u2019re capable of learning fast, adapting on the fly, and building something meaningful, even with no prior experience in some of the technologies we usedWhat's next for The Tomato Trade: -Next step would be to deploy this onto a domain to make it accessible by all.\n-We would love to implement this into local neighborhoods to see the community and sustainability results in a real-world environment!",
                        "github": "https://github.com/asamaga18/thetomatotrade",
                        "url": "https://devpost.com/software/the-tomato-trade"
                    },
                    {
                        "title": "Code Confession",
                        "description": "\"Commit Your Secrets, Compile Your Conscience.\"",
                        "story": "",
                        "github": "https://github.com/ApexCoder99/CodingConfession?tab=readme-ov-file#codingconfession",
                        "url": "https://devpost.com/software/code-confession"
                    }
                ],
                [
                    {
                        "title": "MoneyTrail",
                        "description": "MoneyTrail: Uncover suspicious transactions with an interactive fraud investigation dashboard powered by dynamic network graphs and behavioral pattern detection.",
                        "story": "Inspiration: We were inspired by the real-world challenges faced by bank testers and other niche financial crime investigators. With fraudulent transactions becoming more digital and harder to trace by the day, we wanted to build a tool that not only flags suspicious behavior but also helps analysts visually trace the money trail. Our goal was to make fraud detection more intuitive, interactive, and explainable\u2014especially for analysts who need to act quickly and with confidence.What it does: MoneyTrail is a real-time investigative platform that:How we built it: Challenges we ran into: Connecting the frontend and backend \u2014 properly handling cross-origin requests (CORS), creating certain API endpoints, and synchronizing live data in ReactDesigning modular components that could handle dynamic data across tabs and viewsIntegrating D3.js into React without breaking layout responsivenessMaking the search experience fast, smart, and visually intuitiveManaging conditional rendering based on selected accounts, merchants, and user inputAccomplishments that we're proud of: Built an end-to-end financial investigation tool in under 36 hoursSuccessfully connected React frontend with Flask backend and passed live data through Context after almost 7 hours of struggle :)Visualized complex fraud patterns in a way that's easy to understandCreated a clean and scalable frontend that mimics real-world compliance toolsMade the experience fun and informative \u2014 like Palantir-lite for fraudWhat we learned: How to build a full-stack app with frontend\u2013backend communicationHow to use React Context for clean global state and live data accessHow to implement custom network and bar visualizations with D3.jsHow to translate fraud patterns into meaningful frontend UI/UXThat connecting React + Flask isn't plug-and-play \u2014 but incredibly rewarding once solved!What's next for MoneyTrail: Add user authentication and role-based access (analyst vs admin)Integrate an ML model to generate real-time fraud scores, instead of using mock data and rule based logic.Enable timeline filtering for historical fraud patternsUser/bank customization of suspicion weights for each of the 4 metrics, to \"score\" more actively fraudulent accounts higher than a possible one time or accidental fraudulent transaction.Connect to live banking APIs for real-time simulationPackage it for compliance teams as a plug-and-play fraud dashboard",
                        "github": "https://github.com/jainansh16/Money-Trail",
                        "url": "https://devpost.com/software/moneytrail"
                    },
                    {
                        "title": "See with me ",
                        "description": "Smart Glasses. Smarter forensics\r\n",
                        "story": "Inspiration: Digital forensics investigations often hinge on identifying and tracking key technological devices\u2014laptops, flash drives, crypto wallets, modems, and more. These objects may appear only briefly, be partially obscured, or go unnoticed during chaotic site visits. Traditional methods of documenting evidence often rely on manual photography, memory, or reviewing hours of footage after the fact. We wanted to change that.We were inspired by the need to bring real-time intelligence and semantic understanding to the digital forensic process\u2014right from the investigator\u2019s point of view. With emerging smart eyewear like Meta Ray-Bans and the growing capability of on-device AI, we saw an opportunity to build a system that sees what matters, keeps what matters, and lets investigators focus on the investigation\u2014not on the camera.What It Does: See With Meis a digital forensics tool that uses smart glasses, computer vision, and AI to identify, track, and catalog objects of interest in real-time or from previously recorded video.Here\u2019s how it works:Live footageis captured through Meta Ray-Bans and streamed via Instagram Live into our processing pipeline.A fine-tunedYOLOv11n modeldetects specific high-value digital objects such as laptops, external drives, modems, phones, and more.Using acustom algorithm, we monitor the bounding box movements of detected objects. When significant movement is detected, the relevant frames are saved.Each keyframe is passed through animage analysis LLMto generate detailed, context-aware captions and red-flag alerts.Custom video uploadsare processed in the same way\u2014breaking the video into frames, analyzing each frame through the object detection pipeline, and generating leads from each significant keyframe.Asearchable interfaceallows investigators to filter and find frames based on object detection results.TheSuper Searchfunction lets investigators query across all keyframes, identifying patterns such as objects appearing together in specific contexts.,How We Built It: The system is designed to handle both real-time and pre-recorded footage, integrating multiple technologies to achieve accurate detection and analysis:Meta Ray-Bansstream live footage toInstagram Live, feeding directly into our backend.Acustom YOLOv11n modelhas been fine-tuned with over 1,700 annotated images of cybercrime-related objects to ensure high accuracy in detection.Adelta-based bounding box algorithmfilters out redundant or irrelevant frames, ensuring that only key changes in object position are retained.TheLLM-based image analysisprovides forensic insights, generating captions that highlight potential red flags or unusual object groupings.Custom video uploadfunctionality allows users to analyze archived footage, breaking the video into frames that go through the same object detection and semantic analysis pipeline.Thebackendis powered byFastAPI, with aReact frontendthat enables easy searching and exploration of the analyzed frames.,What Makes It Special: What setsSee With Meapart is the combination ofreal-time object detection,intelligent frame retention, andsemantic image analysis. It\u2019s a system that doesn\u2019t just detect objects\u2014it understands what those objects are and their potential relevance in a forensic investigation.Key features that make our solution unique:Real-time and custom video analysis:Whether you\u2019re on the scene with smart glasses or reviewing old footage, our system ensures that significant frames are captured and analyzed.Advanced object detection:The fine-tuned YOLOv11n model specifically targets cyber-relevant objects, making it far more focused than generic detectors.Intelligent frame filtering:The delta-based algorithm intelligently selects frames where significant object movement occurs, reducing irrelevant data and saving time.LLM-generated insights:Each keyframe receives detailed captions and potential forensic leads, making it easier for investigators to spot red flags.Powerful search and analysis tools:The built-in search and Super Search functions allow investigators to find and cross-reference objects, making large-scale investigations more manageable.,Challenges We Ran Into: No project comes without its hurdles. ForSee With Me, we faced a few key challenges:Object detection accuracy:Fine-tuning the YOLOv11n model to detect niche, cybercrime-related objects, such as specific hard drives and USB devices, required a highly customized dataset of over 1,700 annotated images.Frame filtering efficiency:Ensuring that only the most relevant frames were kept involved balancing the sensitivity of our delta algorithm\u2014too sensitive, and we'd keep too many frames; not sensitive enough, and we'd miss critical moments.Semantic image analysis:Crafting effective LLM prompts that generated useful forensic insights (e.g., red flags, suspicious object interactions) was an iterative process, with many tweaks to ensure the output was truly useful.Real-time performance:Optimizing the pipeline for low latency to ensure seamless real-time object detection while still processing large video files effectively was one of our major technical challenges.,Accomplishments We're Proud Of: Despite the challenges, we\u2019ve created something truly functional and impactful:Real-time object detectionthat works seamlessly through Meta Ray-Bans.Custom video upload supportthat processes user-provided footage with the same object detection and semantic analysis pipeline.Delta-based frame filteringthat intelligently reduces noise and ensures that only relevant, action-oriented frames are retained.LLM-based captioningthat generates detailed and meaningful forensic leads for each identified object.Searchable databaseof identified objects across keyframes, with a powerful Super Search tool to cross-reference multiple objects and scenarios.,What We Learned: Throughout the development process, we learned a great deal:Object detection can always be improved:While the YOLOv11n model worked well, fine-tuning for niche objects required us to build a specialized dataset and continually evaluate detection performance.Real-time processing requires balance:Achieving real-time performance without compromising the accuracy of object detection or image analysis was a delicate balance.Semantic AI needs careful prompting:Generating useful, actionable captions from images required careful tuning of the image analysis prompts to extract forensic insights effectively.User needs shape the design:The most important feature was the ability tosearchandfilterkeyframes based on real-world investigative needs, which required us to focus on intuitive interfaces and fast processing.,What\u2019s Next: We\u2019re just getting started:Field testingwith cybersecurity professionals and law enforcement to refine the tool based on real-world feedback.Video source expansionto include other footage types, such as drone cameras and mobile recordings.Smarter multi-frame stitchingto analyze patterns and relationships between objects over time.Mobile appfor on-the-go review of analyzed footage, keyframes, and forensic insights.Integrating case management toolsto allow for better tagging, notes, and report generation.,The future ofSee With Melies inenhancing its capabilities to bridge the gap between real-time and retrospective forensic analysis, empowering investigators with a smart, efficient way to track digital evidence.",
                        "github": "https://github.com/ritesh3280/bitcamp25",
                        "url": "https://devpost.com/software/see-with-me"
                    },
                    {
                        "title": "BattleTask",
                        "description": "AI-Powered, Gamified Procrastination Inhibitor \ud83e\udd16",
                        "story": "Inspiration: Getting on your laptop with a task at hand then getting distracted and wasting hours on end.What it does: This chrome extension tracks the active windows and tabs in order to see if the content on them is productive or not. Using this information it creates a score for each tab, where the average of all the tabs is used in setting up the game. When the average of all the tabs is less than 50 the user loses a life (heart). Once the user loses a three hearts in their chosen time period, the game ends and the user dies. If however, the user remains concentrated and productive for their chosen time period, the user wins the game and gets to track the cumulative time they were focused for.How we built it: We built this app primarily in JavaScript and HTML.Challenges we ran into: Integrating Gemini to analyze the tabs the user was on to determine if they were educational or not.Accomplishments that we're proud of: Using AI to distinguish between productive Youtube, and entertainment Youtube.What we learned: We learned how to incorporate an AI API and Chrome Extensions API.What's next for BattleTask: We want to set it up with a serverless setup through Google's services.",
                        "github": "https://github.com/J0shua2/BattleTask-main",
                        "url": "https://devpost.com/software/battletask"
                    },
                    {
                        "title": "Mammoth Maze",
                        "description": "Escape the maze ... while avoiding the mammoth stalking you. A 3D Rendering of a 2D horror game using raycasting. Don't get Mammothed!",
                        "story": "Inspiration: Our project was inspired by the old DOOM games and the prehistoric theme of Bitcamp 2025.What it does: Mammoth Maze renders a 2D maze game in 3D using raycasting. The player is placed in a maze and is challenged to reach the opposite corner. Standing in their way is a mammoth piloted by A* pathfinding. The maze itself is generated randomly with an algorithm, and includes a portal with a rendered texture that brings the player to the next maze.How we built it: Mammoth Maze does not use any 3D models or tools, rather the graphics are displayed entirely with Java AWT. Many rays are aimed from the player outwards at varying angles, and the distance between the player and the wall that they hit determines the position, length, and brightness of the wall segment to be drawn.Challenges we ran into: There were problems with the perspective of the 3D world as well as a strange fish-eye effect that we were able to address with better calculations. We also had trouble implementing sound effects as none of us had any experience with doing so.Accomplishments that we're proud of: We are especially happy about the Mammoth's A* pathfinding algorithm which it uses to get through the maze and stalk the player. We are also proud of implementing a texture for the portal and of including sound effects. These were both \"extra\" features that we were not sure we would have time to implement, with the former being particularly challenging.What we learned: We developed a greater understanding for how 2D graphics are used to create an illusion of 3D space, which is the case for all 3D graphics on a computer screen.What's next for Mammoth Maze: Future ideas include:True 3D Space (Stairs, multiple floors of a maze)Greater differences between levelsItems to collect in the maze (keys to locked doors, coins)A way to stop/distract the mammoth,",
                        "github": "https://github.com/8tsmith11/mammoth-maze",
                        "url": "https://devpost.com/software/mammoth-maze"
                    },
                    {
                        "title": "UncompliGrow",
                        "description": "UpRoot is a gamified self-growth tracker that visualizes your habits, goals, and wellness as a dynamic, growing tree \u2014 helping you stay balanced, motivated, and thriving every day.",
                        "story": "Inspiration: Balance is everything.When something becomes too heavy on one side, it topples \u2014 this truth applies to life just as it does to trees. A tree with too many branches and leaves, but no strong trunk or deep roots, cannot support its own weight. It's bound to collapse. Likewise, a tree with an abundance of roots but no growth above the ground withers away, its full potential unseen.UpRoot is built on this metaphor. We are the tree. If we take on too many goals and tasks without solid discipline and mental well-being, burnout is inevitable. Conversely, if we invest deeply in wellness and inner growth, but don\u2019t apply it toward any visible accomplishments, we fail to bloom in the eyes of the world.What it does: UncompliGrowturns your personal growth into a living tree \u2014 a visual, interactive reflection of your habits, goals, and mental wellness.Rootsrepresentbelief,rest, andrecoveryThetrunksymbolizesdiscipline\u2014 the consistency of daily habitsBranchesare yourgoalsLeavesare thetasksyou complete,Each element of the tree is connected, helping you see not justwhatyou're accomplishing, buthow balancedyour growth really is. The metaphor encourages self-awareness:UncompliGrowhelps you answer those questions and take action.Leaves appearwhen you complete tasks tied to a goalBranches growas you create new goals or subgoalsRoots expandwhen you complete foundational wellness challengesThe trunk thickensas you complete daily habits,You can add or remove habits, goals, subgoals, tasks, and challenges \u2014 giving you full control over how your tree (and you) grow.How we built it: Frontend Framework: React (with TypeScript)Build Tool: ViteStyling: Tailwind CSSComponent Library: Shadcn UIState Management: React QueryRouting: React RouterIcons: Lucide React,The app uses acomponent-basedarchitecture:Index.tsx: Main dashboard with the growth treeHabits.tsx: Habit management pageGoals.tsx: Goal tracking and managementChallenges.tsx: Daily challenges and roots,Tree.tsx: Core visualization of user\u2019s growthHabitItem.tsx: Represents individual habitsGoalItem.tsx: Handles goal and task managementChallengeItem.tsx: Tracks and displays challenge progress,api.tsCustom hooks for:Toast notificationsAPI interactions,Challenges we ran into: Coming into Bitcamp 2025, I had very little experience with frontend development. Most of my background was in backend and logic-heavy systems, so diving intoReact,Tailwind CSS, andcomponent-based designwas a steep learning curve.I had to quickly learn how to manage state withReact Query, structure pages withReact Router, and build responsive, modular interfaces usingTailwind\u2014 all within a high-pressure environment.Accomplishments that we're proud of: Built a fully functional and interactivegrowth-tracking treein just 36 hoursLearned and implemented amodern frontend stack(React, TypeScript, Tailwind CSS) with no prior experienceCreated a visually tree structure that grows based on user input. (Trust me! This was really an accomplishment...),What we learned: Gained hands-on experience withReactandTypeScript, including managing state and routingLearned how to style components usingTailwind CSSUnderstood the importance ofcomponent reusabilityand clean architecture in frontend development,What's next for UncompliGrow: User Authentication:Enable users to securely log in and save their growth dataBackend Integration:Replace the mock API with a real database and backend servicePersonalized Challenges:Generate daily challenges based on past activity and habitsMore Tree Variations:Let users customize their growth tree based on their goals or themes (like positioning branches, color, etc.),",
                        "github": "https://github.com/Rohan-Payyavula/UncompliGrow",
                        "url": "https://devpost.com/software/uncompligrow"
                    },
                    {
                        "title": "PepperAssistant",
                        "description": "PepperAssistant, because everyone needs a Pepper Potts. Our AI runs your schedule, makes calls, and books appointments, so you can focus on building an arc reactor in a cave, with a box of scraps.",
                        "story": "Inspiration: Calling maintenance, making appointments, managing your schedule, creating new events. Elite lawyers and politicians have someone to do these things for them. But these tasks are aggravating for the average joe, too. Why not have your own personal AI secretary that you can direct at will?What it does: PepperAssistant allows users to connect via Google to our web application and chat with PepperAssistant, allowing users to create new events in their calendar, make call-based appointments at doctor's offices, call maintenance with requests, and more!Our built-in chat system allows users to accomplish all of these things simply by telling PepperAssistant what to do, and where to go. All the work Pepper does herself!How we built it: Our application runs on a Next.js frontend and a FastApi backend hosted on Vercel and DigitalOcean respectively. \nThe Docker container hosting our backend software uses Deepgram API, Twilio API, and Groq Chat using Llama3-8b-8192 to autonomously issue and interact in live phone calls, translating STT, parsing the text under the specified agenda, and TTS to speak back to the person on the other end of the line, all in real-time.Challenges we ran into: We ran into significant challenges getting the voice pipeline to work, manipulating audio and fine tuning silence thresholds to create an optimal agent that can speak on the user's behalf.Accomplishments that we're proud of: The accomplishment that we are most proud of is automating the process of scheduling appointments totally autonomousl.What we learned: We learned a lot about Google Auth, how to use APIs, and building applications from scratch.What's next for PepperAssistant: PepperAssistant wants to continue adding features to the AI secretary, like automatically adding appointments to the calendar, and preparing meeting notes ahead of time.",
                        "github": "",
                        "url": "https://devpost.com/software/pepperassistant"
                    },
                    {
                        "title": "Unravel",
                        "description": "Unravel tracks narrative drift by analyzing shifts in public discourse and sentiment across social platforms and news, revealing volatility and distortion over time.",
                        "story": "Inspiration: News consumption today is fragmented. Most readers rely on a narrow set of sources, reinforcing personal biases and creating filter bubbles. We wanted to break that by helping users visualize how stories shift...not just in topic, but in tone, timing, and framing. What if we could model that drift and volatility across platforms?What it does: Unravel is a browser extension that enhances news reading by offering multiple perspectives. When you're reading a news article, Unravel:Analyzes the current article's topicFinds related articles from other major publishers across the political spectrumPerforms sentiment analysis to show how each source frames the same storyVisualizes publication timelines to reveal how coverage unfolds over the past 12 days,Readers get immediate, contextualized insight into how narratives are shape, without needing to search manually.How we built it: Chrome extension: JS content scripts, popup UI, and background workerFastAPI backend (Python) for NLP processingHuggingFace transformers for real-time sentiment analysisChart.js to render dynamic sentiment-over-time graphsNewsAPI to source articles across mainstream mediaExperimental integration with the Bluesky API to track real-time social volatility,Challenges we ran into: Content Security Policy (CSP) restrictions in Chrome extensions made it difficult to use external libraries like Chart.js and D3.jsNeeded sentiment analysis that was both lightweight and context-awareManaging cross-script communication (content, popup, background) was trickyPretrained LLMs struggled with nuanced cross-source comparisonsBluesky\u2019s rate limits unexpectedly hit us mid-testing \ud83e\udd72,Accomplishments that we're proud of: Built an end-to-end system that integrates seamlessly into the reader\u2019s workflowCreated visualizations that reveal media framing and sentiment shiftsClean architectural split between frontend and backend componentsDesigned a reusable modular backend that extends beyond just newsIntegrated decentralized social data into our sentiment pipeline,What we learned: Chrome extension architecture and inter-script messagingHuggingFace pipelines for zero-shot classification and sentiment scoringWeb scraping, rate limiting, and content filtering on decentralized platforms like BlueskyHow to model narrative drift as a volatility signal,What's next for Unravel: Support more news languages and international domainsIntegrate source credibility scores and historical sentiment baselinesAdd an interpretability layer explaining why a sentiment was labeledExpand volatility tracking to Reddit, X (Twitter), and emerging social platformsRelease a research-grade API for analysts, journalists, and traders,",
                        "github": "https://github.com/bkalaaa/unravel.git",
                        "url": "https://devpost.com/software/ravel-drift"
                    },
                    {
                        "title": "Tetherboard",
                        "description": "Collaboratively build connections between your thoughts, ideas, and notes.",
                        "story": "Inspiration: All of us here today are working to build amazing projects and novel creations. In order to most effectively create, it\u2019s important to have an organized way to manage your thoughts, ideas, and notes in a collaborative environment. Our brains naturally learn by building connections between different ideas, so we built Tetherboard to be a collaborative knowledge platform that operated in the same way.What it does: Tetherboard allows you to create boards that contain images, symbols, and markdown text which supports code highlighting and LaTeX statements. Multiple people can work on a board at a time and any changes will update live! You can create links (or \u201ctethers\u201d) to other boards and view an overall \u201cTethermap\u201d with all of your boards connected in a graph.On top of all of this, your entire workspace is a context for Gemini. You can ask Gemini anything related to your workspace and it has all the information from the board you\u2019re on and all boards in your workspace and will answer your questions accordingly.How we built it: The app is mainly written using React and TypeScript. We used TailwindCSS and shadcn for styling and UI components. The backend was written using Firebase and we used the Gemini API for letting the user ask questions about their tetherboards. The tethermap and board building area was created with a combination of other React specific libraries.Challenges we ran into: Board editing area was prone to a variety of different interaction bugs we had to fixLive collaboration on boards proved to be tough to get working reliably. We had to include optimizations to make it smootherGetting the markdown area to show code/images/math properly,Accomplishments that we're proud of: Reactive tethermap! You can move it around, we think it\u2019s fun to play around withGemini being able to intelligently answer questions from your entire workspaceThe board editor! We wanted to give the user the tools they need to customize boards however they want,What we learned: We learned a lot about using React and TypeScript to build a web app. Tackling dependency issues also taught us a lot about wrangling with NPM. Most of us hadn\u2019t used the Gemini API before this so that was also a learning experience. Finally, we learned how to use Firebase snapshots to create live interaction between different users on the same board.What's next for Tetherboard: Allowing Gemini to generate entire boards on its ownDrawing\u2013so you can write your thoughts by handMore optimized board editing UI (rotating elements and such)Even more optimized sharing!Multiple workspaces with individualized Gemini contexts,",
                        "github": "https://github.com/rk234/bitcamp-bookworm",
                        "url": "https://devpost.com/software/tetherboard"
                    },
                    {
                        "title": "NeedleDrop",
                        "description": "Tinder for Music. Swipe On Newly Generated AI Music Recommendations",
                        "story": "Inspiration: We wanted to create an app that made finding new songs for your music playlists very easy.What it does: NeedleDrop takes in user preferences and leverages AI-driven models to provide unique music recommendations to each user. The user can swipe left on songs they dislike, which will be used to tell the model to show less of, and swipe right on songs they like, which will be shown more of.How we built it: With the React Native framework in javascript, the server calls to Spotify's API, and backed by Google's Gemini Gen-AI Model, NeedleDrop creates custom music recommendations just for you!Challenges we ran into: Dealing with React Native with relatively little experience and having connected it to the different APIs posed quite a few troubles with non-collaborative languages and persistent errors in the app development process.Accomplishments that we're proud of: We are pleased to have created a fully functional app that uses Gemini's models to provide music recommendations just for the user, updating these preferences in real-time, and connecting to Spotify's API to add these songs back to the User's discography.",
                        "github": "",
                        "url": "https://devpost.com/software/needledrop"
                    },
                    {
                        "title": "JeffAI",
                        "description": "Have a proprietary personal AI agent complete your web related tasks",
                        "story": "Inspiration: Our inspiration was to give people a tool to complete their simple web related tasks using the power of our AI agent.What it does: Our project allows you to give an AI agent web related tasks that will be solved using our proprietary AI agent. \nSample inputs: \n\"Plan me a flight from where I am to Japan\",\n\"Find the distance between UMD Armory and a pizza restaurant\"How we built it: Built using Playright and Google's Gemini API",
                        "github": "https://github.com/daven-c/ClosedManus/tree/Stable-JS",
                        "url": "https://devpost.com/software/jeffai"
                    },
                    {
                        "title": "Keyless Campus",
                        "description": "Never get locked out of your dorm room ever again.",
                        "story": "Inspiration: I lost my dorm room keys last week. And I am for sure not the only one. At least 20% of all college students in the US lose their dorm keys each academic year and have to pay $100+ to replace their lock and the keys of their room/apartment mates.What it does: It is a college dorm door unlocker system for each door where there is an RFID mounted on the outside and a knob unlocker on the other end. Beyond the hardware system, we have a web app that allows users to review and manage door access for each system.How we built it: Hardware: The core of our system is two ESP32 boards running custom TCP protocols on top of the FreeRTOS operating system. The Scanner board uses a RC522 sensor to read RFID data and send it to our server, which verifies it and forwards any accepted IDs to our lock board which fires a relay powering a DC brush less motor which actuates our door.\nWeb Application: We used React + Vite on the front end and Java Springboot MVC on the backend with a PostgreSQL database hosted on Render. The React client allows users to sign up a new RFID card into a door system and sign into the door-system-specific dashboard to view the door access history and manage access.Challenges we ran into: Hardware-side:\nHacking up the embedded software to control the door opener motor and connect to the backend proved to be a non-trivial task. We had to figure out a few quirks and intricacies like circumventing ESP32 wifi band incompatibilities with the local network and setting up the right protocols for TCP communication between the scanner and unlocker chips.Software-side:\nSetting up the proper database schema/architecture and fully comprehending the user requirements required a few back-and-forths and trial and error. Once we got that figured out, ensuring proper communication with the RFID chip sign-up request posed some trouble but we powered through it.Demoing:\nSince we clearly were not able to bring in a college door and neither a strong enough motor system for it anyway, we have to figure out the most convenient yet realistic way to demonstrate a proof of concept of the whole system working together.Accomplishments that we're proud of: We are very proud of successfully setting up this hardware + full-stack project within the 36 hours of the hackathon while spending some time enjoying the weekend.What we learned: We learned a great deal about the different intricacies when it comes to building a hardware hackathon project as it is our very first hardware project! We got to apply a lot of concepts from class including TCP networks and protocols, database schemas, low-level/embedded systems programming and more!What's next for Keyless Campus: Scaling Keyless Campus to all dorm doors for all US colleges is the next step!",
                        "github": "https://github.com/Andry-Arthur/KeylessCampus-bitcamp25",
                        "url": "https://devpost.com/software/project-209"
                    },
                    {
                        "title": "BiasGPT",
                        "description": "BiasGPT integrates advanced analytics, modular design, and agile development to build scalable, secure solutions addressing a modern industry issue - Bias in AI.",
                        "story": "BiasGPT: An Automated Demographic Bias Detection System: Inspiration: The idea forBiasGPTwas born out of a desire to explore and mitigate demographic bias in AI-generated content. As language models become more integrated into products and systems, it\u2019s essential to understand how subtle shifts in race, gender, or other identity traits can influence the tone or content of generated responses. I wanted to build a tool that would not only detect these shifts but also learn from them over time.What I Built: BiasGPT is an end-to-end system that:Takes a prompt and generates a demographically-swapped version using spaCy and a custom dictionary.Scores the \"tone shift\" between the original and swapped prompt using OpenAI\u2019s GPT-3.5.Labels responses based on the shift and stores them in a dataset.Retrains a DistilBERT classifier nightly to improve its bias detection capabilities.Includes a React frontend and a FastAPI backend for live interaction.,Frontend:React + Tailwind CSS (Next.js)Backend:FastAPIML/NLP:Hugging Face Transformers, spaCy, OpenAI APIAutomation:Cron (macOS LaunchAgent fallback), bash scriptsModel training:PyTorch, Scikit-learnData handling:pandas, CSV pipelines,What I Learned: How to automate nightly ML pipelines with cron and bash scriptingHow to tokenize, train, and save Hugging Face transformer modelsManaging environment variables securely for API accessCreating modular, debuggable code for complex pipelinesUsinglaunchctlas a fallback to cron on macOS for scheduled jobs,Challenges I Faced: Setting up cron jobs on macOS was more complex than expected due to SIP (System Integrity Protection)Loading large spaCy models and dealing with slow runtime at scaleEnsuring.envvariables loaded correctly in every context (cron, shell, etc.)Resolving issues with model serialization (e.g., mismatched keys instate_dict)Detecting subtle differences in tone and formalizing that into a reliable metricAvoiding submodule conflicts in GitHub and ensuring a clean repo structure,What\u2019s Next: Improve demographic swaps to be more context-awareUse more advanced classifiers for tone and sentimentSupport batch uploads and human feedback loops in the UIExtend support to other biases (e.g., age, socioeconomic status),",
                        "github": "https://github.com/rzarka1298/BiasGPT/tree/main",
                        "url": "https://devpost.com/software/biasgpt"
                    },
                    {
                        "title": "CircuLens",
                        "description": "\ud83d\udd0cCircuLens is an AI-powered tool that instantly converts real-world images of breadboard circuits into clean, readable circuit diagrams and explanations.",
                        "story": "\ud83d\udd0c CircuLens \u2013 Understand Circuits, Visually\nAbout the Project\nCircuLens is an AI-powered tool that converts real-world images of breadboard circuits into clean, digital circuit diagrams and intuitive explanations.Inspired by tools like Google Lens, we wanted to bring similar visual intelligence to the world of electronics. Many students and hobbyists struggle to understand how their physical circuits relate to schematics \u2014 CircuLens bridges that gap with just one upload.Inspiration\nWe were driven by a common frustration:\n\u201cI built a circuit\u2026 but I don\u2019t know how to explain it.\u201dIn hardware workshops and beginner electronics classes, students often create circuits on breadboards but lack the clarity to document or describe what they've built. Our goal was to make understanding circuits as easy as taking a photo.What We Learned\nHow to build a clean, responsive frontend using React, Vite, Shadcn UI, and Tailwind CSSHow to simulate an AI pipeline with mocked output for circuit analysisHow to handle UX challenges like image previews, drag-and-drop upload, and loading statesReal-world Git collaboration, including resolving complex merge conflictsHow We Built It\nFrontend: Built entirely with React using Vite and TypeScriptUI Framework: Styled with Tailwind CSS and components from Shadcn UIImage Upload: Supports both file selection and drag-and-dropAI Layer (Simulated): Currently mocked to return:An ASCII-based circuit diagramA natural language explanation of how the circuit worksPolished UX: Emphasis on mobile responsiveness, clear layout, and ease of useChallenges We Faced\nGit merge conflicts between frontend team membersEnsuring responsive design across screen sizesManaging image state and simulated analysis output without a real backendKeeping the interface simple and beginner-friendlyWhat's Next\nIntegrate real-time circuit recognition using Google Gemini or OpenCVAdd live camera input for mobile usersSupport exporting diagrams to tools like KiCad, LTSpice, or FritzingBuild collaborative features like shared annotations and editable diagrams",
                        "github": "https://github.com/eionpaulos/bitcamp2025",
                        "url": "https://devpost.com/software/circulens"
                    },
                    {
                        "title": "Cinder",
                        "description": "Tinder for courses",
                        "story": "",
                        "github": "https://github.com/abhyuday-srivatsa/bitcamp",
                        "url": "https://devpost.com/software/cinder-5dgjb3"
                    },
                    {
                        "title": "YummyBytesAI",
                        "description": "No more stressing over meal planning \u2013 YummyBytesAI turns your ingredients into creative, yummy dishes in seconds!",
                        "story": "Inspiration: As college students, sometimes we are limited to certain ingredients left in our kitchen and on a small budget. With YummyBytesAI, we can input our personal dietary preferences and restrictions for AI to help us create an idea for a meal.What it does: YummyBytesAI takes in the dietary restrictions, allergies, cuisines, and ingredients you input to generate a complete ingredient list and instructions to cook a delicious meal.How we built it: With the use of Gemini API and React, we coded the project in JavaScript, HTML, and CSS.Challenges we ran into: As this was our first time utilizing an AI API and having little foundations in React, we had to do a lot of research and test runs within a short amount of time to gradually get our project to its final stage. Making specific prompts to ensure that Gemini produces the proper results was also a difficult challenge, given the creative lengths and results that it can have.Accomplishments that we're proud of: In a short number of hours, we were able to learn two complex tools and develop a project with the functions that we were aiming to create.What we learned: Collaborating together effectively across GitHub and VSCode, utilizing APIs, UI/UX design, prompt engineering, problem-solving, critical thinking, communication, and staying composed under pressure.What's next for YummyBytesAI: As we continue learning new things in computer science, we hope to be able to come back and revisit this project and implement new features to develop a more enhanced experience for users.",
                        "github": "https://github.com/NickYuannn/ai_chef",
                        "url": "https://devpost.com/software/yummybytesai"
                    },
                    {
                        "title": "CapitalClarity",
                        "description": "\u201cSmart spending starts with smart awareness.\u201d - CapitalClarity is a Gemini AI-powered React Native app that uses spending history to deliver personalized financial insights for upcoming purchases.",
                        "story": "\u201cSmart spending starts with smart awareness.\u201dCapitalClarity is aGemini AI-powered React Native appdesigned to help users take control of their finances. By analyzing your spending history, the app deliversreal-time, personalized financial insightsthat empower you to make smarter, budget-conscious decisions. Think of it as yourintelligent financial coach, always learning and guiding you.The Problem: Many people\u2014especially students\u2014struggle with managing personal finances. The issue isn\u2019t always about income, but rather a lack of awareness and context around spending.In fact:42% of college students carry credit card debt, often unaware of how daily habits accumulate.Small, frequent purchases tend to slip under the radar, making budgeting difficult.Existing financial tools often feel disconnected or require too much manual input.,Inspiration: CapitalClarity was born out of personal pain points:The Project: CapitalClarity doesn't just track expenses\u2014itunderstands them. By combining AI-powered reasoning with user-specific context, the app classifies expenses, flags potentially wasteful spending, and adapts over time.What We Learned: \u26a0\ufe0f Challenges Faced: Getting real banking data ishard. We attempted to use thePlaid APIto seamlessley integrate with Capital One, Chase, and other bank accounts, but were blocked by multi-day api approval.Thus, we had to pivot into creative solutions for getting transaction data, emulating the real-time nature we wanted the application to capture.Here\u2019s the tough part: we often had only thevendor nameandamountfor each transaction.To tackle this, we:Analyzedtime of transactionto detect patternsUsedmerchant categoriesto infer context (e.g., groceries vs. fast food)Collecteduser feedbackto fine-tune prediction weights,This created a feedback loop that made the appsmarterthe more you used it.CapitalClarity is still evolving. Some ideas we\u2019re excited to explore:BetterAI Insightsto teach users about their spending habits in a personal mannerBank Integrationsto allow for a more seamless approach to our applicationPersonalizingthe app, recognizing different cultures and spending habits to create a more inclusive environments.,Built with clarity in mind. Powered by AI. Designed for your life.",
                        "github": "https://github.com/ukataria/Bitcamp2025",
                        "url": "https://devpost.com/software/capitalclarity"
                    },
                    {
                        "title": "EvaStudy",
                        "description": "Study with a Emotional Virtual Assistant. She'll keep you on track!",
                        "story": "Inspiration: We\u2019ve all faced it: you sit down to study, and 10 minutes later you\u2019re deep in a meme rabbit hole or doomscrolling TikTok. Or worse yet, you\u2019ve actually managed to lock in and you find yourself getting angry and unproductive. In today\u2019s world of constant digital distraction, staying focused isn\u2019t just about willpower \u2014 it\u2019s about having the right tools.With online learning and self-study becoming the norm, we envisioned a smarter, emotionally-aware companion that supports\u2014not scolds\u2014you during your study sessions. That\u2019s howEvaStudywas born.\ud83d\udcf1 Detects distractions like phone usage in real time\ud83d\udcac Recognizes emotional fatigue and offers personalized support\ud83c\udfa5 Boosts accountability with session recordings and insights,EvaStudy blends intelligent detection with compassionate feedback to help make every minute of studying more effective\u2014and more human.What it does: EvaStudy is an AI-powered web app that transforms how you study by combining multi-modal machine learning with conversational GenAI.Real-time phone detection using YOLOv5Facial expression analysis to record emotion in a log and recommend breaks accordinglyWebcam recording for self-review and accountability,Live nudges if you're distracted (via Gemini)Encouragement when frustration is detectedBreak suggestions based on emotional data,\ud83d\udcf9Why record at 4\u00d7 speed?By saving sped-up versions of study sessions, EvaStudy makes it easier toreview long sessions quickly, reflect on productivity patterns, and stay consistent. This also taps into the growing trend of \u201cstudy-with-me\u201d style content that helps usersfeel connected, motivated, and mindful\u2014even during solo study time.Business Model: We aim to empower focused learning\u2014at scale.High school & college studentsWorking professionals seeking productivity toolsEdTech platforms looking to enhance engagement,Individual Plans:Monthly/yearly subscriptions with core featuresInstitutional Licenses:Bulk discounts for schools and universitiesPremium Add-Ons:Personalized coaching, advanced analyticsPartnerships:Integrations with learning platforms and referral models,Year 1:5,000 users | $300K revenueYear 2:15,000 users | $1M revenue | Breaking EvenYear 3:40,000 users | $3M revenue,We're currently seeking$500,000 in fundingto accelerate development, expand our reach, and build a community around focus and wellness in learning.How we built it: Flask with Jinja2 templatingOpenCV video feed + custom CSS UI,Object Detection:YOLOv5s model from Torch Hub identifies phonesEmotion Recognition:FER-2013 CNN via pre-trained model (AI-Gajendra)Live Feedback:Gemini API crafts contextual nudges like \u201cPut down your phone\u201dCooldown Logic:Tracks recent emotions to prevent repetitive alertsVideo Recording:Captures sessions at 4\u00d7 speed without audio (every 4th frame)Storage:MySQL backend handles video metadata and user session logs,Python, TensorFlow/Keras, Torch, OpenCVGoogle Gemini API, MySQL, Flask,Challenges: Balancing performance vs. accuracy in real-time videoAvoiding false positives in face and phone detectionAvoiding overrepresentation in emotion detectionCoordinating multiple AI models with shared session stateDesigning interventions thatsupportrather than annoyEnsuring secure, isolated video recordings for each user and routing related to SQL, Flask and JavaScript,Accomplishments: Seamless integration of emotion + object detectionBuilt a real-time system that actually respects user experienceDeveloped non-intrusive reminders based on emotion contextCreated full video review tools with annotation overlays,What we learned: Tuning AI models for real-world use requires iterationMultimodal systems (CV + NLP + user flow) require careful coordinationFrontend UX must evolve alongside backend logic to maintain trustPython + OpenCV can deliver real-time performance with the right optimizations,What\u2019s next for EvaStudy: Deploy to AWS Cloud or Cloudflare for increased security, usabilityCalendar and Pomodoro integrationFacial recognition for personalized emotion baselinesAI companion with customizable tone/personalities,Group study mode with shared statsLeaderboards and friendly focus competitions,Embed in LMS systems like Canvas or MoodleAPI access for EdTech platforms and coaching apps,Final Note:",
                        "github": "https://github.com/samragyee-d/bitcamp-hack",
                        "url": "https://devpost.com/software/evastudy"
                    },
                    {
                        "title": "Crossing Counties",
                        "description": "Relocating for work? Let data guide your move! With comprehensive county-by-county insights for all of the US, crossing counties helps you navigate housing choices, wage and cost-of-living expenses.",
                        "story": "Inspiration: With housing shortages, rising living costs, and an uncertain job market, relocating is challenging. However, access to reliable insights and information can be a valuable tool in reducing stress.What it does: Crossing Counties leverages frequently updated federal government data to provide county-by-county insights, helping individuals make informed decisions when relocating.How we built it: Front end: Streamlit renders data, charts, managers user inputs while being very quick to implementAnalysis: Pandas is a tried and tested data analytics tool that integrates with nearly every python api or libraryChallenges we ran into: Limited time, lack of familiarity with front-end tools, rudimentary data analysis.Accomplishments that we're proud of: Getting a working proof of concept out, solo, under a massive time crunch.What we learned: Make a detailed plan ahead of time, be ready to burn the midnight oil and never lose focus of the goal.What's next for Crossing Counties: Gather more data, use advanced analytics and incorporate more technologies to help users make more informed decisions.",
                        "github": "https://github.com/sdhar150/Crossing-Counties",
                        "url": "https://devpost.com/software/crossing-counties"
                    },
                    {
                        "title": "PhishNet AI",
                        "description": "Everyday, millions fall victim to phishing attempts, scams, and identity theft. PhishNet AI catches these malicious websites and explains why it was dangerous using AI and anti-virus programs.",
                        "story": "Everyday, millions fall victim to phishing attempts, scams, and identity theft. PhishNet AI catches these malicious websites and explains why it was dangerous using AI and anti-virus programs.Inspiration: Many of the people who fall victim to tech support scams, phishing attempts, malware attacks, and identity theft are people who might not be familiar with internet safety. These people might click on shady links, be targeted by scam advertisements, or download malware claiming to be legitimate software. Because of this, billions of dollars of victims' money is stolen every single year.We wanted to find a solution that prevents these scams and phishing attempts, while also providing people internet safety tips to prevent scams in the future.What PhishNet AI does: PhishNet AI is a pun of the wordphishingand the wordnet.Phishing refers to any attempt from a website, email, or phone call to trick someone into providing an attacker personal information and/or money.PhishNet AI serves to \"catch\" these phishing attempts in its metaphorical \"net\" and prevent the user from getting scammed, and prevent them from losing money and/or their personal information.As the user browses the web, PhishNet AI is always vigilant about what websites the user clicks on or gets redirected to. Everyday websites like Google or Facebook won't trip any red flags, but if a user stumbles across a malicious website seeking to steal their information, PhishNet AI will quickly block the user from accessing the website, and provide the user with useful information (powered by Gemini AI) about why the website was blocked, and what threats were prevented.Additionally, a well-known tactic used by scammers is to have the user download remote control software, allowing the scammer to take control of their computer. However, these kinds of software are not inherently malicious, and so instead of blocking the website, we instead alert the user about what the website is, and allow them to proceed forward if they understand.How we built PhishNet AI: PhishNet AI is a Google Chrome web extension that sits on top of the browser at all times. The extension constantly monitors the web URL that the user is currently on, calls upon multiple URL checking bots to gauge the threat level of the current website, and generates a report on what the URL checkers found.If the URL checkers report that a threat has been found, PhishNet AI will redirect the user to a safe page, and notify them that the malicious website has been blocked. Gemini AI generates a short summary of what kind of threat the website posed, and what kind of threat PhishNet AI prevented. All of this is shown to the user in a simple and concise manner, so that they can quickly get back to what they were doing. In addition to this, our program checks the URL with a database of known remote control websites, and if it is a known remote control website, then it alerts the user of the risks and ensures they know what they're doing.Challenges we ran into while developing PhishNet AI: There were many different methods we considered when it came to gauging a website's legitimacy (whether a website was a phishing scam, contained malware, or was completely benign).Accomplishments that we're proud of: We're proud, simply, of how coordinated we were throughout the whole project. It was amazing working with people who were able to bounce ideas back and forth with a lot of overlap between ideas. It felt like everyone was on the same wavelength at all times, and this enabled us to finish our hack impressively quickly.Additionally, the speed at which we solved our biggest hurdles (detailed above) while on two hours of sleep, as well as empty stomachs is quite the achievement and definitely something to be proud of.What we learned: Chrome web extensions have tight constraints. Working and coding around those constraints allowed us to formulate workarounds for things wethoughtwould work, but didn't work due to the limitations posed by Chrome web extensions.For instance, calling certain types of APIs requires a \"man in the middle\" to route these API calls securely. We learned that the underlying technology behind this limitation, Manifest V3, is intensely restrictive about securely sending requests to certain API endpoints. Researching this limitation was a huge pain and took a long time to crack, but of course it was worth it in the end.What's next for PhishNet AI: Being able to scan newly downloaded files for malware, and warning the user about the infected file is a feature we're extremely ambitious about. Sometimes, browser security protocols and antivirus tools can miss malicious files. However, by utilizing the same bots that analyze web URLs, and modifying the behavior to work for downloaded files, the user can quickly and effectively be alerted about malicious files on their device.",
                        "github": "https://github.com/Pouncetail/PhishNet",
                        "url": "https://devpost.com/software/phishnet-ai"
                    },
                    {
                        "title": "Bounce",
                        "description": "Group finding for the lonely.",
                        "story": "Inspiration: What inspired me was that all my teammates gave up within the first 30 minutes of BitCamp and decided to attend only for the food. I love making projects, but it's not enjoyable, so I wanted to create something that could connect me with others. I feel that many collaboration websites are too \"try-hard,\" and there needs to be something more relaxed for passionate individuals who want to create for the love of it, rather than just adding a project to a resume.What it does: It's a website that facilitates postings in search of members. A posting includes a title, description, number of teammates, tags (SE, IoT, etc.), location, and other fields. Individuals can then apply to these listings, and a simple chat will be initiated between them. After this, it's up to the individuals. This is primarily intended to help find projects to collaborate on with others.How we built it: This was built on Next.js with TypeScript and Tailwind. For storing data, Firebase's Firestore Database was used. Google's Places API was used for autocompleting location selection and filtering of locations. Of course, many React components were utilized. ## Challenges we ran into\nThe chat presented quite a challenge. It's still not what I would like, but uploading takes a noticeable amount of time, and I probably didn't implement it in the best way - just uploading it to the database and then pulling it back immediately. UI/UX is not my strong suit, which is where Gemini helped me significantly... The Google API isn't set up in the best way either; I might've inadvertently uploaded the API key to GitHub as well. ## Accomplishments that we're proud of\nI'm proud that I have a website with buttons I can click on. I'm proud that I was able to implement Firestore with minimal effort.What we learned: I learned that web development may seem daunting at first, but gradually learning it step by step isn't as taxing a process as I thought it would be.",
                        "github": "https://github.com/jwihardi/lonely",
                        "url": "https://devpost.com/software/bounce-lwau96"
                    },
                    {
                        "title": "Permits, Please",
                        "description": "Discover if you've got what it takes to bring order to the college streets. Introducing \"Permits, Please\" a driving and ticketing simulation to assist training for campus management.",
                        "story": "",
                        "github": "https://github.com/Zenza235/bitcamp2025",
                        "url": "https://devpost.com/software/permits-please"
                    },
                    {
                        "title": "SkillMatchAI",
                        "description": "Lost in a maze of choices, people guess their future. SkillMatchAI uses AI to reveal strengths, close skill gaps, and guide you to smarter career paths\u2014no more wasted time or potential.",
                        "story": "SkillMatchAI is your personalized career co-pilot. You upload a resume, answer a few tailored prompts, and our AI identifies your current strengths and missing skills. Then, it recommends relevant jobs along with personalized learning paths\u2014like Udemy courses or degree programs\u2014to help bridge the skill gap and reach your goals, all within your budget and timeline.We used:React + Tailwind for a clean frontend UIFlask backend to handle API routes and logicGemini API for NLP-driven resume parsing, questions generation, and skill mappingPinecone for vector-based job, course, and program recommendationsMongoDB to persist user profiles and session dataFirecrawl + Gemini for scraping and structuring real course data from the webChallenges we ran intoStructuring skill domains to balance granularity and generalizationEnsuring accurate and budget-aware course recommendationsPreventing hallucinations and duplicate recommendations from AIBuilding a clean UI that smoothly transitions across multiple stepsAccomplishments that we're proud ofBuilt a full-stack, AI-powered, personalized skill recommender from scratch in <36 hoursIntegrated Pinecone vector search across three datasetsCreated a natural UX flow that guides users from resume to personalized action stepsUsed Gemini in both RAG and agentic formats (scraping + summarization + classification)What we learnedPrompt design is everything when working with generative AIHow to handle fallback logic when vector search returns low scoresBetter ways to guide users through survey UIs and track their intentHow to align user goals with tangible, data-driven paths using AIWhat\u2019s next for SkillMatchAIAllow users to track progress over time and re-evaluate goalsAdd LinkedIn and GitHub integration for even richer profilesExpand to soft-skill and leadership training recommendationsCollaborate with educational platforms for deeper API-level course accessFine-tune Gemini prompts using real user data and feedback,",
                        "github": "https://github.com/willloe/SkillMatch",
                        "url": "https://devpost.com/software/skillmatchai"
                    },
                    {
                        "title": "AI Resume Coach",
                        "description": "Meet your AI Resume Coach! \r\nYour personal career assistant reads your resume, compares it to thousands of top-performing examples, and gives you instant, tailored feedback to land your dream job.",
                        "story": "Inspiration: Every student in the world wants one thing:a job. But like many of my peers, I feel uncertain about whether the skills I have are translated and explained well in my resume. This inspired me to create an AI-powered Resume Coach to provide instant, personalized feedback that would help job seekers improve their resumes and feel more confident when applying for jobs.What it does: The AI Resume Coach allows users to upload their resumes in PDF format and receive instant, AI-powered feedback. The app analyzes the content of the resume and compares it to others in similar fields. Using advanced natural language processing and machine learning models, it provides actionable suggestions focused on improving formatting, wording, and content to better align the resume with job goals. The goal is to help job seekers optimize their resumes, boost their chances of landing interviews, and feel confident in their applications.How I built it: The app was built using Streamlit for the interface, which allowed me to quickly create a simple and intuitive web application. I then took a dataset of 13,000+ resumes and preprocessed them using Natural Language Processing. I added this preprocessed data into a database using ChromaDB. These stored resumes act as a \"training set\" for the AI model to look at stored resumes for context. For the AI-powered feedback, I used Hugging Face\u2019s GPT-2 model to generate detailed feedback on resumes. From the user's POV, the process starts with a user uploading their resume, then the app extracts text from the PDF, processes it, and uses the AI model to generate feedback, which is displayed to the user in an easy-to-understand format.Challenges I ran into: One of the major challenges I faced was integrating PDF extraction into the app. Extracting text from PDF resumes in a way that preserved the formatting and structure was tricky, especially when dealing with resumes of varying complexity. I also had to refine the AI feedback generation to ensure that the suggestions were practical and actionable without being too generic. Performance was another concern, as generating feedback could take a few seconds, and I wanted to balance speed and accuracy.Accomplishments that I'm proud of: I'm proud of building my first functional app that allows job seekers to receive personalized resume feedback in real-time. The integration of GPT-2 for text generation provided high-quality, context-aware feedback, and ChromaDB helped me store and retrieve similar resumes for comparison. Deploying the app on Streamlit Cloud made it easy to share with users, making it accessible to anyone who needs it. I'm especially proud of how the app supports students and job seekers who might not otherwise have access to professional resume feedback.What I learned: This project taught me a lot about integrating machine learning models with web applications. I learned how to use Hugging Face\u2019s model pipeline and Streamlit to quickly prototype a functional app. I also gained hands-on experience with ChromaDB for managing and querying embeddings, which was essential for making the feedback more relevant. Most importantly, the project reinforced the value of user experience in building an app that not only works well but is also intuitive and easy to use.What's next for AI Resume Coach: Maybe after this competition, it's time to get a team together and create a more fleshed-out application. This project has a lot of potential to help any college student.",
                        "github": "https://github.com/kavin-manivannan/Bitcamp2025/tree/main",
                        "url": "https://devpost.com/software/ai-resume-coach"
                    },
                    {
                        "title": "Money Hog",
                        "description": "We all think we hog our money, but how much do you really spend? Money Hog lets you see where every dollar goes so that you can improve your spending and budget habits with a click of a button!",
                        "story": "Inspiration: With technology today, we can track all kinds of miniscule things like sleep, grades, and steps automatically. Unfortunately, one's spending habits aren't as easily trackable. Being college students, we recognize the need to budget efficiently, as it can be a daunting task. With this realization, we sought to make an easy-to-use app to help make budgeting accessible to everyone!What it does: Our web application tracks a user's transactions live as they happen. Using Capital One's Hackathon API (Nessie), we collect a user's transactions and categorize them as \"Money Earned\" or \"Money Spent\". The difference between those values is used to calculate how well a person is staying within budget. They can also see each transaction in chronological order, filter out specific transaction types, sort them, and filter them based on month and year.How we built it: We used TypeScript, React, Material UI, HTML, and CSS to build our app.Challenges we ran into: None of us had interacted with APIs directly before, so there was a learning curve trying to understand how to fetch data and upload our own test data to use. Similarly, it was tricky figuring out how to update our app as soon as a new transaction comes in, so that the user doesn't need to enter anything on their own.Accomplishments that we're proud of: We are very proud of being able to connect to the API and use \"real\" data to simulate helping people like us. We also love our overall design, especially on the main page. There was a lot of effort put into making the front page stylistically appealing and functional for anyone to use. Similarly, this was many of our group members' first time using TypeScript, and everyone was able to try new tasks beyond their comfort zones.What we learned: We learned how to interact with an API and fetch data in real time. We also learned the fundamentals of styling and user interface to make our app appealing to an average user.What's next for Money Hog: We hope to build on what we learned over the past 36 hours by implementing a monthly challenges feature that helps gameify and promote healthy financial decisions.",
                        "github": "https://github.com/ryaneldho/CapitalOneHackBitcamp2025",
                        "url": "https://devpost.com/software/money-hog"
                    }
                ],
                [
                    {
                        "title": "Bytecamp",
                        "description": "UMerryland's hackathon for students all across the Beast Coast!",
                        "story": "Try it out!: https://yuwex.github.io/bitcamp2025/Inspiration: Bytecamp is incredibly unique and totally not influenced by Bitcamp. We were inspired from our own journeys at Bitcamp, creating projects and embarking on side-quests. We aimed to encapsulate the entire hackathon experience with Bytecamp, simulating everything from working on your project, refueling with food, and stealing the moon with your teammate.What it does: Bytecamp is a hackathon simulator where you, Joe HackaThon, are trying to build the best possible project. You have various personal stats, such as Hunger, Morale, and IQ, as well as project stats, such as UI/UX Experience and Originality.How we built it: We developed Bytecamp using the Godot game engine with GDScript and collaborated using Git.Challenges we ran into: While we all have experience developing games in Unity (shoutout CMSC425!), only Yuji had experience working in Godot. There was a slight learning curve at first with the Godot engine, but GDScript is very similar to Python so it was easy to integrate functionality through code. It was definitely a challenge, but we all learned a lot and are very happy with the final product.Accomplishments that we're proud of: We are particularly proud of the visual elements in our game. The UI is filled with smooth animations, it is very responsive, and has a lot of unique interactions. Additionally, every element you see in Bytecamp is created in Aseprite. A lot of time went into creating unique characters, scenery, and animations.Overall, we are very proud of Bytecamp\u2014it is fun, has a simple but unique gameplay loop, visually appealing, and funny.What we learned: We all learned a ton about Godot and GDScript, as well as working together well as a team!What's next for Bytecamp: One stretch feature idea we had was implementing mini-games you can play after events occur. While we didn't have time to add them into Bytecamp for Bitcamp, it would be awesome to continue to work on our game and make it even more fun to play.",
                        "github": "",
                        "url": "https://devpost.com/software/bytecamp"
                    },
                    {
                        "title": "Quantum vs Classical Portfolio Optimization",
                        "description": "This project leverages a variational quantum linear solver to find the optimal solution to a minimization problem. It also compare the result with classical linear system solvers.",
                        "story": "Inspiration: This project leverages my interest in quantum computing and computational finance. I was curious whether quantum algorithms could offer an alternative to classical portfolio optimization techniques, especially for problems like the Minimum Variance Portfolio, where the linear algebra structure maps naturally to quantum linear solvers.What it does: The project compares classical and quantum approaches to solving the minimum variance portfolio problem:The classical side uses Markowitz-style optimization via scipy.optimize.The quantum side uses a Variational Quantum Linear Solver (VQLS) built with Pennylane, leveraging a custom cost function and a warm-start ansatz.The result is a head-to-head comparison of weights, expected return, volatility, and Sharpe ratio,How I built it: Classical module: Fetches stock data from yfinance, computes covariance matrix, and solves the optimization using SLSQP.Quantum module: Constructs a padded matrix system, Ax = b, initializes a parameterized quantum circuit (ansatz), and trains via gradient descent (Adam) to approximate A^{-1} bWeb API: Built a Flask endpoint (/compare) to return results in JSON format for frontend visualization.Used warm-start initialization and custom penalties to ensure convergence to interpretable quantum states.,Challenges I ran into: Designing a cost function that stabilizes optimization while aligning with the classical target.Ensuring numerical stability in quantum circuits (e.g., normalization, padding).Handling dimensional mismatch between classical weight vectors and quantum state vectors (especially due to power-of-2 requirements).Debugging gradient descent behavior in variational circuits, especially with initial states.,Accomplishments that I'm proud of: Implemented a fully working quantum portfolio optimizer using Pennylane.Matched quantum results closely to classical benchmarks in terms of both weights and metrics.Built a modular pipeline that is easy to reproduce and extend.Demonstrated that quantum methods can approximate classical financial optimizations even with few qubits.,What I learned: How to design and train variational quantum circuits using real-world financial data.Importance of ansatz design and initialization in convergence. (Thanks Mihir for this idea)How resource scaling (qubit count, matrix size) affects performance and accuracy.The practical trade-offs between classical speed and quantum generalization.,What's next for Quantum vs Classical Portfolio Optimization: Extend to larger portfolios (5\u201310 assets) using 3\u20134 qubits and assess quantum circuit depth.Test with real quantum hardware to evaluate noise sensitivity and fidelity.Explore hybrid strategies where quantum circuits solve subproblems within a broader classical loop.Add a visual dashboard to view efficient frontiers, quantum vs classical states, and real-time simulations.,",
                        "github": "https://github.com/Ostailor/PortOptQuantum",
                        "url": "https://devpost.com/software/quantum-vs-classical-portfolio-optimization"
                    },
                    {
                        "title": "oto hana.",
                        "description": "Mood-based Spotify playlist generator powered by Gemini AI and the Spotify Web API\u2014an intelligent music generator that captures your vibe and curates the perfect soundtrack for how you feel.",
                        "story": "Inspiration: Both of us love listening to Spotify but sometimes its annoying to think long of all the songs you want to add to your playlist. Maybe you want to find music that matches your current mood, without having to search through countless tracks yourself. This inspired us to create a platform leveraging the Gemini 2.0 Flash AI to do that work for us.What it does: You log into your Spotify account, describe how you're feeling, and the application generates a corresponding mood profile. It then presents you with ten pre-selected songs based on that mood, allowing you to click to either add a song to your playlist or skip it. Gemini then analyzes your selections along with your mood description to fill the playlist and the remainder of the playlist with compatible songs.How we built it: \"oto hana\" integrates Spotify's Web API with Gemini API on a Next.js/React foundation. We used Gemini   to create a sentiment analysis pipeline that maps a user's feeling/description into a single mood. The system generates initial recommendations, then fine tunes the remaining selections from the user feedback based on the their initial selection of songs.Challenges we ran into: The environment setup proved quite tedious, and we encountered some difficulties perfecting the UI design. We faced significant compatibility issues between different Node.js versions and the Spotify Web API, resulting in hours of debugging. The Spotify Web API also required us to have an SSL certificate, so setting it up for Spotify user authentication made local setup more tedious. Additionally, optimizing the application's performance while maintaining responsive design across devices led to a lot of troubleshooting and refactoring of our initial implementation.Accomplishments that we're proud of: We successfully fine-tuned Gemini to generate songs based on moods derived from user descriptions, and effectively connected Spotify and Gemini APIs to work in unison with each other. The simplistic user flow was also something we were proud of, as we managed to create a simple experience that guides users seamlessly from mood description to playlist generation. Our prompt engineering efforts also very impressive in matching emotional states to musical selections, creating a more personalized playlists that consistently align with the users' described feelings.What we learned: We learned that setting up a development environment properly is more challenging than anticipated. We also learned that sometimes simple is better for product and implementation. A database could have been implemented but was not necessary in order to have a successful product. We also learned how to work with endpoints through ngrok for local development testing with external APIs. The importance of prompt engineering became evident as we refined our approach to natural language processing. User testing was also very important because for simple prompts it worked very well, but for more complex prompts it showed that refinement was needed in enhancing our API calls.What's next for oto hana: We plan to incorporate users' Spotify listening activity to further enhance personalization. By importing users' listening history, we can identify favorite songs and artists to incorporate into future playlists.",
                        "github": "https://github.com/govinds108/Otohana",
                        "url": "https://devpost.com/software/oto-hana"
                    },
                    {
                        "title": "BarNone",
                        "description": "BarNone maps dead zones in real-time. Crowdsourced signal data shows you the best connectivity spots nearby. Never lose bars again!",
                        "story": "\ud83d\udca1 Inspiration: We\u2019ve all experienced the frustration of dropped calls and buffering videos in \"dead zones.\" BarNone was born to turn this universal pain point into actionable data\u2014helping users find reliable connectivity in real-time. But beyond convenience, we saw a deeper impact. What if this data could help non-profits bring services to the people who need them most? With BarNone, we crowdsource signal strength to map where connection fails\u2014so organizations can identify where to set up mobile clinics, distribute aid, or support remote learning.It\u2019s not just about signal bars\u2014it\u2019s about bridging digital divides and empowering communities through connection.\u2699\ufe0f What it does: BarNone tracks and visualizes cellular/WiFi strength as you move, creating live color-coded indicators of signal quality. Crowdsourced data helps users avoid dead zones and find optimal connection spots. Users essentially contribute data as they move, building a reliability network.\ud83d\udee0\ufe0f How we built it: Frontend: React Native + Expo for cross-platform performance\nBackend: MongoDB for crowdsourced data storage\nMapping: Dynamic circle scaling using signal strength metrics\nLive tracking: GPS-integrated movement updates\ud83d\udcaa Challenges we ran into: This was our first hackathon ever, so we were honestly overwhelmed at first. The concern of the time constraints mixed with some rushed planning and lack of experience led to a rather lackluster start to the project, but we quickly turned that around. Our key was that we didn't take small victories for granted, which boosted team morale and motivated us to push through a variety of difficulties.\ud83c\udfc6 Accomplishments that we're proud of: We were proud to have developed circle overlaps in dense areas, created a dynamic map view using react native libraries, and connecting our database to our map to show user data points.\ud83d\udcad What we learned: Donotunderestimate dependencies and version control...\u23ed\ufe0f What's next for BarNone?: We aim to configure \"best path\" suggestions to avoid poor-signal zones, carrier comparisons that show which networks perform best locally, and an offline mode to cache maps for areas with spotty bandwidth.",
                        "github": "https://github.com/nitinenj/signalscape",
                        "url": "https://devpost.com/software/barnone"
                    },
                    {
                        "title": "ShadowScan",
                        "description": "In sensitive environments such as defense systems, stakes are usually very high in terms of preventing data leakages. ShadowScan uses ML similarity detection models to prevent leakages.",
                        "story": "",
                        "github": "https://github.com/prajwalshah19/bit-camp-2025/tree/integrated-routes",
                        "url": "https://devpost.com/software/shadowscan"
                    },
                    {
                        "title": "Portalyzer",
                        "description": "Ever wanted to stress test or see the risks of your stock portfolio in light of recent events? Introducing Portalyzer! ",
                        "story": "Inspiration: Some of our team, as well as fellow friends and family, invested in stocks which, in recent times, have been incredibly volatile. Thus, we wanted to make something that could help friends and family gain some more insight into the risk assessment of the stocks through a simple, intuitive interface with the data being showed in easy to understand graphs.What it does: Many people wish they could see the risks of each of their stocks in case of these events, as well as stress testing for these sorts of abnormal market conditions. Thus, Portalyzer takes in stock information (currently limited to the S&P 500) from the user through manual input, and generates graphs that analyze the risk of their portfolio. The pie chart displays the percentage of risk due to each equity in the portfolio. The histogram displays the expected returns for the next trading year over 1000 simulations. This allows the user to see the probability that their portfolio achieves a target return value, and the value at risk, the maximum loss at 95% confidence.How we built it: Our team built a full-stack web application with a React frontend and a Python backend.Challenges we ran into: This was the first hackathon for everyone on our team, as well as the first time for most of us, so we all had very little experience working on a full-stack web application with a team. After the brainstorming, we initially had an idea, but we jumped into it too quickly and didn't build a solid and simple foundation that we could add more features to. This resulted in a complex system that we didn't understand very well, making it difficult to debug and identify issues. It was also the first time all of us used React, so there was a large learning curve. We especially struggled with integrating the frontend to the backend, resulting in many issues with fetching the analysis that we did in the backend.Accomplishments that we're proud of: We're proud of actually having a finished product that (sort-of) works accurately and expectedly (most of the time). We faced a lot of challenges in choosing a team of 4 from 5 people, choosing a topic, getting started with the idea, and building the website itself, because none of us had any experience in full-stack and, more importantly, frontend. So we're really proud of getting the backend Python script connected with the frontend React script.What we learned: Most of all, we learned how to make a dynamic website using React and connect it to a Python script backend, which saved data using a text file, along with gathering data using an API and a CSV matrix.What's next for Portalyzer: Better UI/UX. Expand to include more stocks not in the S&P 500. Expand the simulation to consider as well as analyze more variables and data. Integrate an API that includes live stock information.",
                        "github": "https://github.com/Portalyzer/portalyzer",
                        "url": "https://devpost.com/software/portalyzer"
                    },
                    {
                        "title": "Species Tracker",
                        "description": "Track the range of a species and see future population trend predictions!",
                        "story": "What it does: Species Tracker both compiles and processes various data on animals to both inform the user and predict future population trends of each animal.How we built it: We built the backend with Python and flask, while the frontend was done in React. The data cleaning and model training was done with Jupyter Notebooks and various Python libraries such as pandas and scikit-learn. Data was gathered from the iNaturalist API.What we learned: For most of us, it was the first time combining both back and front-end together, as well as using frameworks such as flask and React.What's next: Future features we'd like to add include:Discovery feature to randomly generate species for people to learn about, especially those that are considered endangeredSupport more species through collection of more dataOptimize by caching prior operations to reduce API callsList of the currently safe animals at risk of being threatened in the next 10 years,",
                        "github": "https://github.com/backedman/species_tracker",
                        "url": "https://devpost.com/software/wip-y4a1p0"
                    },
                    {
                        "title": "SNOM",
                        "description": "SNOM is an emotionally intelligent AI buddy, empowering neurodiverse children to navigate emotions, build social skills, and feel truly understood because every child deserves a friend who gets them.",
                        "story": "\u2728 Inspiration: The silent battles of young children with autism\u2014struggling to make sense of a world that feels overwhelming\u2014and the heartbreak of parents watching their child suffer, unable to help,inspired us to act. We saw the frustration, helplessness and isolation, and we asked:What if there was a way to bridge this gap?SNOM was born from this question. Not just a tool, but a lifeline. A friend who listens, understands, and helps neurodiverse children find their voice in a world that often drowns them out.\ud83c\udf1f What it does: SNOM is more than a robot\u2014he's atrue companion. Like a loyal friend, SNOM follows children around their home, offering emotional support and companionship. Powered by cutting-edge AI, he analyzes moods through facial expressions, voice tone, and behavior, engaging children in conversations tailored to their emotional state.He builds social skills by:Giving compliments and boosting confidenceEncouraging self-expression and creativityHelping children understand emotions through playful interaction,Whether it's building with LEGO, admiring their artwork, or exploring their unique interests, SNOM adapts to each child's personality, making every interaction feel personal and meaningful.For parents, SNOM offers peace of mind with built-in safety features and live monitoring, ensuring their child is supported and secure at all times.\ud83d\udd27 How we built it: Hardware:Reverse-engineered RC car chassis, Raspberry Pi 5, camera module, Bluetooth speaker, and display screenVision System:OpenCV for person detection, distance calculation, and trackingControl System:RPI.GPIO for motor control and smooth navigationConversation AI:Gemini 2.0 Flash for natural language processing and emotionally adaptive conversationsSpeech:PyTTS for text-to-speech conversion, making SNOM's voice friendly and relatableEmotion Display:HTML/CSS/JavaScript frontend on a Flask server to visually show emotionsIntegration:Python scripts to coordinate all components seamlessly,Fine-tuned AI models for emotional intelligenceBuilt personalized personas for children based on their interests and sensitivitiesIdentified key problems faced by neurodiverse children and parentsGuided the creation of an empathetic, effective solutionHelped us develop a robust business plan to bring SNOM to life,\ud83e\udde9 Challenges we ran into: First-Time Hardware Hack:As computer science majors, this was our first experience building a hardware-based project, which required learning new skills on the flyReverse Engineering the RC Car:Carefully disassembling and analyzing the car's control systems to integrate our own without damaging its functionality was a steep learning curvePower Management:Running multiple components (Raspberry Pi, display, camera, and motors) simultaneously required optimizing power distribution to ensure smooth operationAutonomous Navigation:Developing reliable algorithms for user tracking and obstacle avoidance was challenging and required iterative testingLatency Reduction:Minimizing delays between vision processing, decision-making, and motor control demanded significant system optimizationAudio Clarity:Ensuring SNOM's speech remained clear and audible in various environments involved experimenting with speaker placements and configurations,\ud83c\udfc6 Accomplishments that we're proud of: Perfectly Moving Rover:Successfully engineered a mobile robot that can navigate smoothly while detecting and following user movementsObject Movement Detection:Implemented reliable vision systems to detect objects and track movements in real-timeFine-Tuned Conversational AI:Developed emotionally adaptive conversation models that engage children based on their moods and personalitiesIntegrated Systems:Seamlessly brought together hardware and software components to create a fully functional, interactive robotEmpathy in Action:Created a tool that genuinely connects with neurodiverse children, helping them feel understood and supported,\ud83d\udcda What we learned: Mastered hardware-software integration, bridging gaps between disciplinesFine-tuned AI for emotional intelligence and adaptive conversationsSolved complex challenges like navigation, latency, and power optimization,\ud83d\ude80 What's next for SNOM: Advanced Personalization:Making SNOM even smarter in adapting to unique personalities and interestsTherapeutic Integration:Collaborating with educators and therapists to revolutionize support for neurodiverse childrenContinuous Evolution:Leveraging feedback to refine emotional intelligence and conversational capabilities,GitHub Repository",
                        "github": "https://github.com/KanikaGupta16/BitCampBot",
                        "url": "https://devpost.com/software/your-friend-steve"
                    },
                    {
                        "title": "Pangaea DDE",
                        "description": "A Conscious and Aware World, One Step at a Time.",
                        "story": "Inspiration: Our inspiration is very close to home, as our group members have family members who struggle with chronic illnesses. As a result, we wanted to create a way to spread awareness for the most prevalent chronic illnesses worldwide. We started with dementia, since it's very personal to one of our members who lost a loved one with dementia.What it does: Pangaea DDE is a 3D visualizer for global disease data. It takes disease data and displays it as a heatmap projected onto the globe. Users can interact with the globe by clicking on countries, and are given a pop-up with information on the population, income group, subregion, and dementia prevalence (as a rate per 100,000). Additionally, they are presented with AI Insight, courtesy of the Gemini API, which provides analysis on dementia in the country, as well as sources to learn more about dementia.How we built it: We used the following tools to build our project:Cesium, an open-source JavaScript library for 3D Geospatial Applications as our visualization method.React to use Cesium.Python to merge our data.Gemini API to present insights and analysis of dementia data per country.Challenges we ran into: Initially, we wanted to use Google Earth as our visualization method, but its API has been deprecated since 2015.Our group had some issues with GitHub Source Control, since two branches diverged too much when refactoring code.The names of countries didn't match between the geojson and the GBD data.Accomplishments that we're proud of: We accomplished what we set out to do!!!While the code isn't super polished, everything that we envisioned at the start of Bitcamp is functional. In particular, getting the heat map to work and overriding the default Cesium infobox with our custom information popup was a huge milestoneWhat we learned: We learned how to use the Gemini API, and a lot more about React.We read the Cesium docs to learn how to use the library.What's next for Pangaea DDE: We want to increase awareness of diseases across the globe, and aggregating data so that users can view and compare the prevalence of something like Parkinson's vs dementia globally would further that goal.We also want to make the transition between years smoother so users can see changes in the heat map as they move through the slider, as well as make it possible to directly compare data between years.",
                        "github": "https://github.com/ChrisC920/BitCamp2025",
                        "url": "https://devpost.com/software/dyno-dementia-yielding-new-outcomes"
                    },
                    {
                        "title": "Corporate Chaos",
                        "description": "A fun strategy game involving making quick financial decisions, purchasing property, and smart banking on your way to become a real estate mogul and reach 5 million dollars.",
                        "story": "Inspiration: We were inspired to take the subject of finance as the focus of our project, and we were also seeking to create something fun. This inspiration ultimately led to us deciding to make a game focused on real estate and financial decisions, incorporating financial literacy and intuition as key skills required from the player to be successfulWhat it does: It is a financial management and literacy game. The player is trying to reach a monetary goal within a set in-game time frame while trying to prevent bankruptcy and keep monthly income above a certain threshold. The player must engage in a variety of actions such as buying properties, selling properties, and interacting with the in-game bank on their way to reaching their financial finish line.How we built it: The entire game was built on a platform called Unity, for game development. The entirety of the code was written in the C# programming language.Challenges we ran into: Numerous bugs and compilation errors were part of the entire process. Designing the dynamic financial system and interactions of the game also took a lot of planning and thought before we could even begin coding. We also hit some bumps in the road with feature creep, and had to make sure we could get a product we were proud of out there and not let wandering ideas that we wanted to implement stop that from happening.Accomplishments that we're proud of: We were all proud of getting to a working final game product, especially considering 3 of us hadn't even touched Unity before. We were also particularly proud of being able to mix a bit of silliness, like incorporating \"Capital Won\" as the main bank, into a game that you can still use as a bit of a mind teaser with financial math.What we learned: Three of the members of the team were brand new to C# and game development, so they learned new and crucial skills in that regard. We also all got a lot more experience with the depths of version control like GitHub on a project like this. Additionally, we learned a valuable lesson of not letting our wild dreams halt us from getting a fun final product, and choosing the features that we could implement in time.What's next for Corporate Chaos: One of our original intentions with the game was to have a sort of 4 person multiplayer experience where you could sabotage other players and get into bidding wars against them. We definitely see that as a fun future we could explore for the project.",
                        "github": "https://github.com/abmanos/CorporateChaos",
                        "url": "https://devpost.com/software/corporate-chaos-fbauxw"
                    },
                    {
                        "title": "The Dino Game",
                        "description": "The timeless classic of the dinosaur game reimagined (with color)!",
                        "story": "Inspiration: Google's classic dinosaur game.What it does: You play as a dinosaur that is running through the wilderness, in either the story mode or endless mode. In either one, the only movement you control is ducking and jumping. You have to dodge dangerous obstacles in order to make it to the end in story mode or to a new highscore in endless mode.How we built it: Using Godot 4.3 as our game engine, we were able to put everything together quite easily. We collaborated using GitHub, made art with Aseprite, and got the font through searching online. The music was made through samples online and then those samples were mixed.Challenges we ran into: We ended up having a major issue where a commit was pushed to main that cause the entire game to break. This resulted in us losing about and hour's worth of progress and having to rebuild what we had lost.Accomplishments that we're proud of: We were able to get the core gameplay of both routes of the game done even though some of the features such as the pause menu and main menu are a little bare bones.What we learned: The dinosaur game is actually pretty hard to make. Also, tweening kinda sucks.What's next for The Dino Game: There isn't much planned for the future of The Dino Game, but if we were to do more, the visuals and less developed parts of the game would definitely be our focus.",
                        "github": "",
                        "url": "https://devpost.com/software/dino-j37e5i"
                    },
                    {
                        "title": "Defend",
                        "description": "A new creature has apeared and are creating odd objects and scaring off prey. They also look weird.",
                        "story": "Inspiration: super metroid, rain world, sonicWhat it does: you are a dino detroying the envirement in order to stop humans from building generators and industrializing the land. Of course you are a dino and just don't like them.How we built it: Using godot and sprites from ich.ioChallenges we ran into: Learning tilemap, staring 12 hr late because I didn't know our group was to big.Accomplishments that we're proud of: The movement feels pretty nice and the tiles break when attacked.What we learned: How tilemaps work.What's next for Defend: I might finish it by adding enemys and actual levels.",
                        "github": "https://github.com/Crabion78/defend",
                        "url": "https://devpost.com/software/defend-lce192"
                    },
                    {
                        "title": "Clear Jobs",
                        "description": "Looking for a job but can't find one? Indeed and LinkedIn too confusing? You should use Clear Jobs, a web application that helps pair you with the perfect job!",
                        "story": "Inspiration: As students, we strive to help our community find a place to work ranging from part time during university to professions post graduation.What it does: Our application takes in a resume, job title, and location and searches through a public API to find online job postings based on nearby locations. There, it returns median salary earnings and the link to apply for the job.How we built it: We built our front end completely in TypeScript, and our data base in Python. For our database, we used Py Mongo, and we also used Flask for our endpoints to link the front end and back end. We also used multiple APIs to get job postings from the internet and access OpenAI for our resume parser. Lastly, we finished the UI with Figma.Challenges we ran into: The deployment of the website was a big challenge that took a lot of debugging.Accomplishments that we're proud of: Using OpenAi's APR, we were able to make an extremely accurate resume parser that saves the user the time of filling out various forms. This provides an advantage over other competitors.What we learned: We learned about Mongo Database and how to effectively incorporate it in Python and link it into our front-end in TypeScript. We also learned about OpenAI's API and how to properly parse pdfs into script using AI.What's next for Clear Jobs: More Accurate Information on Location for Job Postings, Hopefully getting Recognized.",
                        "github": "https://github.com/VirTrivedi/Bitcamp25.git",
                        "url": "https://devpost.com/software/clear-jobs"
                    },
                    {
                        "title": "Circa",
                        "description": " Circa is a web app that helps users track and visualize their energy levels throughout the day. Circa empowers users to better optimize their schedules based on when they feel most energized.",
                        "story": "Inspiration: Health tracker apps, like sleep trackers and also the energy output of a person. Circadian Rhythms were our main inspiration, hence the name Circa.What it does: Circa allows users to create an account, then start tracking their energy levels throughout the day by letting users input data through an interactive graph. Over time, the graph would show what the user's daily energy levels look like on average at each hour. This would help the user know how their body works during different hours of the day, allowing them to productively do things.How we built it: We used React, Firebase, TailwindCSS, and hosted with Github Pages.Challenges we ran into: Making the graph feel as smooth as possible was difficult. Making all the components work and blend together was also a bit difficult but we think we managed it well.Accomplishments that we're proud of: We made a very elegant UI with clean design and transitions.What we learned: How to use different UI libraries effectively.What's next for Circa: Adding more analytics for users, or recommendations based on their patterns. A tips page was also in the works, where it would let users know what best to do when their energy level is high and what they can do when their energy level is low.",
                        "github": "",
                        "url": "https://devpost.com/software/circa-vsozqg"
                    },
                    {
                        "title": "TerpFit",
                        "description": "TerpFit \u2014 Your Personalized AI Workout Partner. Find nearby gyms, chat with AI, and build smarter workouts tailored to you \u2014 all in one place.",
                        "story": "Inspiration: The inspiration behind TerpFit came from wanting to make working out smarter and more personalized for everyone. A lot of people struggle with planning workouts, finding nearby gyms, or staying consistent with their fitness goals. We wanted to create a platform where users don\u2019t have to think too hard about building a workout plan, instead they can simply chat with an AI and get a customized plan built for them. Combining location tracking, AI conversations, and workout logging felt like the perfect way to make fitness easier and more interactive.What it does: TerpFit is a user based web application that helps users find nearby gyms or parks, chat with an AI chatbot to build a workout plan, and log their workouts for future tracking. Users can log in or sign up, select a location from an interactive map using Leaflet.js, and then talk to a chatbot powered by Google\u2019s Gemini API. The AI will ask smart questions until it gathers key workout details like the type of workout, duration, estimated calories burned, and a health rating. After the workout plan is created, users get a checklist of exercises to complete. Once finished, their workout, notes, and stats are saved to their personal dashboard where they can track their progress and streaks.How we built it: We built TerpFit using Python Flask for the backend and SQLAlchemy with SQLite for managing the database. The frontend was developed using HTML, CSS, and JavaScript, with Leaflet.js powering the interactive map that displays gyms, parks, and courts near the user. The AI chatbot was integrated using Google\u2019s Gemini API, which allowed us to have conversations that adjust based on user input. We also implemented user authentication so users can log in, log out, and securely track their workout history. Workouts are stored in the database with attributes like date, time, location, workout type, duration, calories burned, and user notes. The dashboard page allows users to view their full workout history and track their workout streak.Challenges we ran into: One of the biggest challenges we faced during this project was working with the Gemini AI API. Since we were heavily relying on the API to generate workout plans and handle conversations, we quickly ran into issues with API key errors and usage quotas being exceeded. There were multiple times where the API would stop responding or throw unexpected errors, which slowed down development and forced us to find creative ways to minimize requests. Another major challenge was building the interactive map using Leaflet.js and trying to get it to properly identify and display nearby gyms, parks, and courts. Accurately handling location data, saving user selected locations, and ensuring markers appeared correctly on the map took a lot of testing and troubleshooting. Additionally, the front-end page formatting often broke due to conflicting elements between the chatbot, checklist, and map features all being displayed together.Accomplishments that we're proud of: We are really proud of being able to fully integrate AI into the workout planning process in a way that feels personalized and useful. We\u2019re also proud of creating a smooth user experience where people can easily log in, select a location, and build a complete workout plan within minutes. Building an interactive map that lets users find real locations and saving that along with their workout data felt like a huge accomplishment. Lastly, being able to track workout streaks and display full workout history in a clean dashboard really brought the whole project together.What we learned: Throughout this project, we learned how to build a full-stack web application from start to finish while combining several complex technologies. We gained experience working with the Google Gemini API to handle dynamic conversations, integrating Leaflet.js for real-time maps, managing databases with SQLAlchemy, and securing user authentication. We also learned how to design and structure an interactive user flow where all parts of the app communicate smoothly between frontend and backend.What's next for TerpFit: Moving forward, we want to continue expanding TerpFit by adding even more features. Some ideas include adding Google Maps Street View for location previews, exporting workout plans to PDF, adding email reminders for workouts, and improving the AI\u2019s ability to recommend specific exercises based on user goals. We also want to explore adding social features like sharing workouts with friends or creating group challenges. Overall, we see a lot of potential in TerpFit and hope to keep developing it into an even more powerful fitness companion.",
                        "github": "",
                        "url": "https://devpost.com/software/terpfit-xcm7oq"
                    },
                    {
                        "title": "MetaVAULT",
                        "description": "Asset Management, Made Easy. ",
                        "story": "Inspiration: Being from California, we witnessed first hand the destruction natural disasters can have on families. We noted that Home Insurance providers were doing everything they could to prevent insuring all the lost items. Thus, we wanted to create a tool to help those in dire circumstance, easily keep track of all their items.What it does: MetaVault takes an image provided by the user and is able to display all notable items within the image. It then suggests prices for each item, allowing the user to change the price before ultimately logging the items and prices to the inventory. Users have accounts and can keep track of all their assets through simply logging in and out of accounts.How we built it: We built it using Gemini AI as a our computer vision. We also used Gemini AI's accurate model to count the number of items, as well as calculate the prices for each item. Using Tailwind, CSS, and Next.JS, we were able to create a UI to allow Users to upload and access their assets. We used Mongo DB to store User's data and login information to ensure that data is accessible from anywhere!Challenges we ran into: Due to being beginners, we went into this challenge pretty blind. We had issues with using the right technologies as we started with outdated methods such as Open CV. We then decided to switch to Gemini, which was our first time using an API. We then ran into some challenges we ran into were issues with Gemini hallucinating. This was also our first time creating a full stack web app and we struggled with databases in the backend.Accomplishments that we're proud of: We are all super happy with what we accomplished and we know there is so much potential to this project. Creating this project for our first hackathon was taxing, but it was so fun and we learned so much along the way! We see ourselves using our own app and we are all satisfied with the outcome.What we learned: We learned almost everything for this project as most of us really only had front-end experience. It was so cool witnessing a functioning sign in function, as well as utilizing advanced tools such as Gemini AI. What we took away the most was how to work together as a team and building off each others strengths and weaknesses!What's next for MetaVAULT: We see a lot coming with this project. We hope to get users for this app and hopefully HomeInsurance partners to endorse this product as a trustworthy way to keep track of assets. We want to add better UI which we have already designed, a dashboard to keep track of total value of all assets, and live market price comparisons to get the most accurate prices for your products.",
                        "github": "",
                        "url": "https://devpost.com/software/metavault"
                    },
                    {
                        "title": "The Nest",
                        "description": "Have work to do, but never do it? Have roommates that don\u2019t do their part in the house? The Nest is a cute, reliable chore/task list organizer for all your productive needs.",
                        "story": "Inspiration: Our main source of inspiration stemmed from the fact that we are college students soon to move into apartments, where such apartment life would require good coordination among roommates to divide and account for chores and upkeep around the house, such as doing the dishes, laundry, cleaning the floor, and others. We thought that it would be interesting to turn this idea into a web application for this hackathon.What it does: The user has a global pet (one pet for all groups)! The user can either join or create a group. Each group will have their own unique ID. After joining a group, all the group member's pets are shown together in a nest while the current tasks for the day are displayed on the left side of the screen. You can create new tasks, edit existing tasks, finish (and delete) tasks, and cover / take over another group member's tasks.How we built it: We used DB Browser to create the backend database. Figma was used to format the separate screens we would have. Implementing the frontend was mainly Javascript with some HTML, CSS, and React.Challenges we ran into: Translating our ideas from Figma to actual code (frontend implementation) was extremely challenging for the entire team. The main problem was that designing the website layout in Figma took a lot of time and that no one in our team was highly confident in implementing the frontend.\nConnecting the backend and frontend was also challenging.Accomplishments that we're proud of: Proud of learning how to make color palettes mesh well. Documentation of all package sources and dependencies. Creating and using our own database. Making seamless router connections. Using usestates to reflect user inputs within chore editing UI.What we learned: We learned how to create vibrant pieces through Figma. Coding formatting through JavaScript and React and managing HTML elements.What's next for The Nest: Complete the Tasklist:Features such as having multiple pets show up in the same group andhovering over to the task => whoever's in charge of it, their pet jumps were not implemented.Connecting the Pages:Some pages don't connect to one another. For example, when a new user signs up, they should be redirected to \"choose a pet\" page, but our current code does not do that.Chore Reminders and (Push) Notifications:Implement timely reminders via email or notifications to keep everyone on track.Debt:Introduce tools to track who owes what and how much, with gentle nudges to settle balances on time.",
                        "github": "https://github.com/abdul8117/bitcamp2025",
                        "url": "https://devpost.com/software/the-nest"
                    },
                    {
                        "title": "QuantumDecider",
                        "description": "Tired of making decisions? Let the qubits take the wheel!",
                        "story": "Inspiration: Too many kinds of peanut butter, what to wear on a nice day out, or the staggering number of choices when looking for a place to eat. Decision paralysis affects us all - so we thought it would be nice to let someone, or something, else make those choices for a change.What it does: QuantumDecider makes a random choice for you - but instead of the usual boring way, it usesmagicquantum computing. You can upload a picture of a menu, a list of options, or anything and Gemini will return a list of everything in the image, and then a number of qubits will make a truly random choice for you. You can even set some preferences, and Gemini will help narrow down those choices for you as well. You can also enter your own custom options to choose from, or select a point on a map to get a place to eat near you. It will also show you how Grover's algorithm compares to a linear search on the same problem.How we built it: We used Qiskit to implement Grover's and our circuits, Flask for the backend, and Bootstrap to pretty-up the front-end.Challenges we ran into: We didn't (and still don't really) know anything about quantum computing, so we spent the majority of the weekend doing research and trying to wrap our heads around such fancy words like diffusion, amplification, and oracles.Accomplishments that we're proud of: Just getting a working project - none of us had ever worked with Qiskit or anything quantum-related before.What we learned: quantum computing is hardWhat's next for QuantumDecider: Refinement of our Grover's implementation, better image recognition, and more versatility in what kind of choices it can make.",
                        "github": "https://github.com/edtton/QuantumDecider",
                        "url": "https://devpost.com/software/quantumdecider"
                    },
                    {
                        "title": "TerpNotes",
                        "description": "One stop shop for all notes at UMD and using generative AI to summarize!",
                        "story": "Inspiration: Imagine cramming for an exam last minute, then realizing you missed a few days of school and did not have the notes to catch up and its too late to ask for anyone for their notes. Well, we have that for you! Students can upload their notes to our platform to make their notes free for everyone! We also provide a free to use AI generated summary tool for the top 5 best voted notes for a class!What it does: Our website can host notes by current or previous students who attended UMD for a specific class and professor. The website will compile that together and will pick the top 5 best community voted notes and generate a summary notes compiled by AI.How we built it: For the frontend, we used:ReactTailwindNextJSHTML, Typescript, CSS,For the backend:MongoAtlasFirebasePythonGemini API,Hosting:Domain name: NameCheapServer: AWS EC2,Challenges we ran into: Learning backend development and API endpoints. Deployment onto AWS, authentication, and etc.Accomplishments that we're proud of: Complete front-end UI + backend system. Our whole project in general.What we learned: PyMongo - MongoDB python integration feature, monitoring and hosting a database for storage using FlaskAWS EC2 and S3 - Using Amazon SDK to upload and return images that users store using our websiteGeminiAPI - making calls and returning markdown text, specifically with file inputs using the API keyFirebase - resolving user to token id verification for people using our websiteWhat's next for TerpNotes: Everything! Full deployment for this app is coming soon after touching up some deployment issues.Team Credits: Built byHabibOlaniyiJacobKevin\u2014and our dear companion: ChatGPT...,\ud83c\udf10 Connect With Us: Made with \ud83e\udde0 by students, for students.",
                        "github": "https://github.com/MajorH5/TerpNotes",
                        "url": "https://devpost.com/software/terpnotes-uyi2kv"
                    },
                    {
                        "title": "Keep It Up!",
                        "description": "Have you ever heard of the game where you keep a balloon in the air? Have you ever played Fruit Ninja??  Combine the two from the comfort of your browser, and you get KEEP IT UP!!",
                        "story": "Inspiration: Firstly, we just wanted to make a silly game with the hand landmark detection from MediaPipe, a pose detection software developed by Google. Gameplay-wise, we primarily took inspiration from the childhood experience of trying to keep a balloon in the air by any means possible, but also took some gameplay elements from mainly Fruit Ninja and other endless arcade games popular in the 2010s. Stylistically, this game aims to recapture the essence of childhood with stylized animations, taking inspiration from the aesthetics of the early internet era, such as 2010s mobile games, Windows XP, and early Facebook design. Because the goal of the game is to keep a balloon in the air, the name \"Keep It Up!\" serves a double meaning, referring to both the balloon and a simple message of encouragement to the player, accompanied with a thumbs up :)What it does: Keep it up, the game uses your webcam that tracks hand landmarks to hit your balloon. LAZERS?There are also bombs that drop randomly, that you want to dodge, or else your game will end and you will lose. There is a score that keeps track of how many taps you take to keep your balloon in the air. Once you lose the game by letting your balloon fall past the bottom of the screen or hitting a bomb, you have the option to save your score to add to the leaderboard. Once it is saved, you can view your current and previous scores on the main menu's \"scores\" button. Other features we added to the game are the setting function, which enables you to change the volume settings.How we built it: Frontend:\nWe built the majority of the game using ReactJS as the frontend. The frontend handled ball rendering, finger tracking overlays, and game logic. In the frontend, we used MediaPipe for hand pose detection and finger tracking.The bulk of the visuals (titles and buttons) were hand drawn two frame animations in photoshop, which were then put into React Components to swap between frames. \nBackend:\nWe built a backend using Flask and SQLAlchemy, which allowed us to store scores in a PostgreSQL database hosted on SupaBase. \nOur frontend would make requests to the routes defined in our backend to save scores and get scores to render the leaderboard.Challenges we ran into: During the development process, we ran into several issues with modules and packages for both the backend and the frontend. In the frontend, we had difficulty importing the MediaPipe packages. In the backend, we had trouble with virtual environments and python versioning.Accomplishments that we're proud of: We made a game yay! PROUD OF OUR FIRST TIME HACKERS!!!!!What we learned: We learned how to utilize MediaPipe, SupaBase, React.js and more.What's next for Keep It Up: We play :)",
                        "github": "https://github.com/echang505/Keep-It-Up",
                        "url": "https://devpost.com/software/keep-it-up"
                    },
                    {
                        "title": "Terpsicle",
                        "description": "Your CS Degree Simplified: a smart 4-year planner designed specifically for UMD CS students to map out their courses, track requirements, and graduate on time.",
                        "story": "Inspiration: Each of us noticed how difficult planning got when registration time came around each semester. Having to have a custom-made 4-year plan, the schedule of classes, and your degree audit open on separate tabs gets confusing, hard to manage, and takes a lot of time.What it does: Terpsicle combines the functionality of a 4-year planner, a CS major degree audit, and a semester-schedule planner into one app. Changes made to the 4-year plan are reflected in the audit and your data when the page is reloaded without the need for a server. To get started quickly, you can copy and paste your unofficial transcript to fill in the classes you\u2019ve already taken!How we built it: We used Next.js and React to create a serverless site that can be updated with your course information. Web scraping and parsing are used to get information from your transcript and the schedule of classes, to make sure the information displayed is accurate. Courses are fetched dynamically when adding to the schedule builder, and the sections the user chooses are stored locally and displayed with a calendar layout. All changes made are saved in your browser\u2019s local storage so that they persist between sessions.Challenges we ran into: There were many inconsistencies in cross-listings, gened. credits, and graduation requirement categories.The Schedule of Classes had inconsistent formatting that led to parsing difficultiesDisplaying courses in the schedule builder as a calendar required special care with how to calculate offsets for elements based on their times of day \u2014 dealing with the CSS here was not particularly early.,Accomplishments that we're proud of: 1000+ lines of graduation requirement logicTranscript can be pasted into the website and automatically loads every class takenGood new user experienceData successfully reading/writing between pages written by different team membersIntuitive UI/UX for adding and displaying courses to the schedule builder,What we learned: Hot to use TypeScript and web technologies to develop a web appHow complicated CS/GenEd graduation requirements areHow to store and read lots of information via local storageHow to create dynamically updating react componentsThe value of linting code during development,What's next for Terpsicle: To start factoring in other concentrations of majors/minors, more than just CS, and making it usable for all students. The goal is to ease the stress of many who are consistently alt-tabbing to figure out their future academic plan, and we want all areas of studies to benefit from this project. We also want to add additional features such as upcoming registration info, possible time conflicts, or even extra pathways that might fit into one\u2019s 4-year plan.",
                        "github": "https://github.com/zsrobinson/terpsicle",
                        "url": "https://devpost.com/software/terpsicle"
                    },
                    {
                        "title": "Netflix Summary Extension",
                        "description": "Ever drop a show halfway and want to continue but have no clue what's going on? Now you do!",
                        "story": "Inspiration: Originally we were working on creating a kindle plugin that had a similar feature. It would summarize a book up to the current page and provide key insights so that you could continue reading. We wanted to broaden our scope to more than just kindles and Netflix seemed like a good place to start.What it does: It is an extension for Chrome and any other browsers that are compatible with Chrome Extensions. When you have your browser open, you can use the pop up window. If you are on Netflix then it will ask you to select a show or movie. If you are not on Netflix then it will tell you that you are not on Netflix. It has two buttons that can give a review of a select show or a summary up to the current episode. In the case of movies, it will see how far you are in a movie and use that progress to generate a summary.How we built it: We first followed a guide to make a very simple Chrome Extension that says Hello. From here, we simply added and changed the functionality to suit our case. A lot of googling was involved, understanding how extensions can interact with webpages and get data from them. We had some knowledge of HTML and CSS, so the designing part was not too bad. Chrome Extensions are basically like mini webpages, utilizing the same tech that websites do. Most of the time was figuring out how to make the scripts that ensured button functionality and used Gemini.Challenges we ran into: The first issue we ran into was how to get data from the webpage. Netflix has a lot of HTML elements and we needed to find the right ones to get the information we wanted. There were multiple options but we wanted consistency across different types of media on Netflix.The next issue we ran into was making the call to Gemini. It was hard to test with this and configure the call to the model. Since we were using a fetch call with the url, there were many issues that would resolve to an error 404. Eventually, we were able to properly call the model and get the data we needed. Figuring out the correct model for our use cases was also an issue, but we ended up with 2.0 flash.Accomplishments that we're proud of: We are really proud of the styling. We wanted to match the Netflix feel and I think we did a good job of that. We also added in some basic animation for the buttons.What we learned: The biggest thing we learned was how to work together. At the start, we found it hard to work together. Eventually we found our grove and communicated effectively. We were able to delegate tasks and ensure that work actually got done by everyone.We also learned how to effectively build a project up. Usually we start with something but here we had nothing. I think it was a good experience to figure out where to start from and where to find resources.What's next for Netflix Summary Extension: We hope to add functionality to other popular streaming services like DisneyPlus. Another aspect that we could improve on is the Gemini calls. The summaries are not perfect. We could implement technologies like RAG or fine tuning to make the model more effective for our use cases.",
                        "github": "https://github.com/ajain-us/NetflixSummaryExtensionWT",
                        "url": "https://devpost.com/software/netflix-summary-extension"
                    },
                    {
                        "title": "Buddy",
                        "description": "Buddy is your personal voice-powered app designed to find Bus transit routes, AI powered itinerary summaries and Photorealistic 3d Map animation for the selected Bus Route.",
                        "story": "Inspiration: Over3 millionstudents rely on Shuttle-UM buses at the University of Maryland each year. While the Transit App offers reliable real-time tracking, I noticed a gap: it felt too cluttered.\n Buddy makes navigating campus buses and Metrobus as simple as saying, \u201cFind me a bus to Regents Drive Garage?\u201dWhat it Does?: Listen to natural speechUnderstand bus-related queries like:\"Find me a bus to Eppley Recreation Center.\u201d\"Find me a bus from Graduate Hills to Regents Drive Garage.\u201d\u201cShow me the bus route\u201d:This would create a photorealistic 3D map animation for all the bus stops.\"Where am I right now?\":Automatically fetch your location using GPS coordinates and display it.Retrieve and summarize the bus transit itineraries.,How its built ?: Flutter + Dart: Built the mobile interface for speech interaction, message rendering, etc.Speech-to-Text: Usedspeech_to_textpackage to convert voice to text in real-time.Flask Backend: Acts as a middleware between the app and third-party APIs (Dialogflow, TransitApp, Maps, Gemini).Dialogflow: Handles NLP for understanding intents.Geolocation: Fetches user coordinates and place names using the Geolocator package and Google Maps Geocoding API.Photorealistic 3D Tiles API: Used to show animation and bus stops.Transit Routing: Calls TransitApp\u2019s OTP planning API for real-time itinerary results.Google Gemini AI: Parses JSON route data and generates clean, human-friendly summaries with coordinates and dynamic Google Maps links.,Challenges:: Geolocation accuracy: Translating location intents to real GPS data was tricky, especially with ambiguous addresses.3D Maps: Figuring out the animation with each bus stop for the selected bus itinerary was difficult because this feature is inALPHAby Google.Processing .json file having bus routes.,Accomplishments that I'm proud of:: Generated interactive route summaries with Gemini AI that users can explore via photorealistic 3D maps.,What I learned: Combining Flutter , flask and a bunch of API\u2019s and create a fully working prototypeWhat's next for Buddy?: More extensive summaries for Bus routes with Gemini API \nAdd real-time bus tracking.\nLot of error handling needed for edge cases.\nIntegrate Database and multi-users.",
                        "github": "",
                        "url": "https://devpost.com/software/buddy-hbrdj9"
                    },
                    {
                        "title": "Moodify",
                        "description": "Vibing with a song? Check out other songs that might math your mood!",
                        "story": "Inspiration: We were inspired when listening to music from the hackathon and thought of being able to listen to similar musicWhat it does: It takes a given song as an mp3 file and predicts its mood, then provides recommendations to similar songs based on the moodHow we built it: We used python to generate a model using training data from MTG Jamendo's dataset. Our front end uses html/css.Challenges we ran into: The most challenging part of this project was creating the AI model as none of us had extensive experience with AI. Also used Flask for the first time and so had a challenging time integrating it.Accomplishments that we're proud of: We are proud of being able to create an AI model for the first time.What we learned: We learned how to create an AI model and work with data sets. USed Flask to integrate the backend with the frontend.What's next for Moodify: We will continue to improve our AI model to provide better predictions and recommendations. We also hope to be able to integrate with Spotify, Apple Music, or another music app",
                        "github": "https://github.com/skottchen/mood_classifer",
                        "url": "https://devpost.com/software/moodify-e0p5jm"
                    }
                ],
                [
                    {
                        "title": "Therassist",
                        "description": "A one-stop solution to be your best self. Track your mental health progress, get automatic insights, and more\u2014all in one convenient platform for both therapists and clients.",
                        "story": "\ud83c\udf1f Inspiration: Working on mental health takes incredible strength, courage, and perseverance. Yet, alongside this journey often comes the burden of tedious paperwork. For instance, therapists typically see an average of seven clients per day, and as they jot down notes in Electronic Health Records (EHR), the process can generate unnecessary physical clutter. \nSimilarly, clients may be given actionable goals during their sessions, but without a system to track them, these valuable insights are easily forgotten. We saw an opportunity to streamline this process.That\u2019s why we createdTherassist\u2014 a solution that empowers both therapists and clients to be the best version of themselves. By reducing the busy work, Therassist allows therapists to focus on what truly matters: fostering growth and providing personalized care. For clients, it helps track mood, mindfulness, and overall wellbeing, ensuring they stay aligned with their therapy goals.\ud83d\udee0\ufe0f What It Does: Therassist is an intuitive web application designed to streamline and enhance the therapy experience for both clients and therapists. Upon signing in, users select their role as either a client or a therapist to access tailored features.For Therapists:Therapists have access to a comprehensive dashboard where they can view client information, session notes, and associated data\u2014all organized and easily accessible. Powered by interactive AI, the app helps therapists quicklygenerate session summariesand detailed notes based on what was documented on paper, as well as easilyfollow-up with patients, reducing manual work and ensuring accuracy.For Clients:Clients have their ownpersonalized dashboard, which includes features such as journaling, mood tracking, audio recordings, and reminders for tasks or activities designed to improve their mental well-being. These tools help clients stay engaged in their progress and stay on top of their therapy goals between sessions.Therabot: The Virtual AssistantBoth therapists and clients benefit from Therabot, anAI-driven assistantthat enhances the therapy experience. Therapists can ask Therabot for concisesummaries of patient data or treatment plansuggestions. Clients can interact with Therabot outside of their sessions to reflect on their emotions, practice grounding exercises, and receive support for mental well-being. After each therapy session, an automatic email is sent to the client with a recap of the session's key points and any homework or tasks to focus on for the week.Seamless Data Upload and Integration\nTherassist makes it easy to upload and manage session data in three convenient ways:\u2014File Upload:Clients and therapists can easily upload documents or notes.\u2014Image Capture:Users can take pictures using their webcam, and the app will automatically detect and convert handwritten notes or journal entries into digital text usingGoogle Cloud Vision OCR.This allows for quick, seamless updates to session records.\u2014Audio Recordings:Users can record their thoughts or sessions, and the app will transcribe audio to text, making it easy to track progress and review key insights from recordings.\ud83e\uddf1 How we built it: Frontend: Next.js, React.js, Flowbite, Tailwind CSS\nBackend: MongoDB, Express.jsChallenges we ran into: Connecting to MongoDB: a port 5000 error on Mac stumped us for many hoursA loading issue with Flowbite (React UI)Switching from Tesseract to Google Cloud Vision for OCR,Accomplishments that we're proud of: Fully incorporating text recognition with OCRIncorporating Gemini API with the chatbot and responds in real timeIntegrating the database,What's next for Therabot: Add integration with health data from Apple watches or fitbit (sleep, heart rate, wellness)",
                        "github": "https://github.com/gzhu725/BitCamp2025",
                        "url": "https://devpost.com/software/therabot-h7yli1"
                    },
                    {
                        "title": "TerraCare",
                        "description": "Feeling down after Bitcamp? Have a chat with TerraCare, a therapist AI chatbot that can always be a listening ear!",
                        "story": "Inspiration: Before coming to Bitcamp, a few of us were feeling anxious and unsure about what to expect. We wished there was someone we could talk to, someone who could understand and offer support. This idea of connecting with someone in a more personal way, especially in a tech environment like Bitcamp, sparked the inspiration for TerraCare. We realized that many people, especially in stressful environments, need someone or something to talk to that\u2019s both helpful and supportive. Our project, TerraCare, aims to bridge that gap, offering a platform that can provide real-time assistance and advice, especially for people going through tough moments.What it does: TerraCare is an AI-powered platform designed to help users navigate moments of stress and anxiety, providing them with personalized recommendations, tips, and guidance. It leverages natural language processing (NLP) and machine learning to offer real-time conversational support. Whether you\u2019re feeling overwhelmed or just need someone to talk to, TerraCare is there to listen, understand, and guide you through those moments.How we built it: The data is stored in PostgreSQL, and we use cloud hosting for scalability. The user interface is built with HTML, CSS, and JavaScript, ensuring it\u2019s simple yet effective. The backend relies on gemini-2.0-flash and flask.Challenges we ran into: One of the biggest challenges we faced was designing the cache system in such a way that it didn't use too much data or too many tokens when prompting the model. With long conversations, we risked overloading the model with excessive data. Balancing efficient storage of conversation history while ensuring that the context given to the model was meaningful and concise was tricky. We had to optimize how much historical data we stored and selectively pass only the most relevant parts of the conversation to the model to prevent unnecessary data usage.Additionally, Git collaboration was another significant challenge. As a team, we had trouble managing multiple branches and resolving merge conflicts. It was tough to stay on the same page with all the changes being made, especially since some of us were more familiar with Git than others.Accomplishments that we're proud of: We\u2019re really proud of how we were able to build a functional and intuitive AI assistant that can understand emotional cues and provide personalized responses. It\u2019s not just about delivering information\u2014it\u2019s about offering support. We also integrated real-time conversation history into the model so that TerraCare can provide more relevant responses based on the user\u2019s previous interactions, which makes the experience feel more human-like. The careful design of our cache system allowed us to reduce token usage while maintaining context, which is a key feature of the system.What we learned: We all came from different knowledge and experience levels, so each of us learned new languages or tools. Some of us got more familiar with machine learning, others with web development frameworks, and some learned how to use Git more effectively. The biggest takeaway for all of us was learning how to work in a team. We all had to communicate better, handle different perspectives, and collaborate on tasks in a way that allowed us to build something meaningful together.What's next for TerraCare: We\u2019re planning to expand TerraCare\u2019s capabilities by integrating more advanced features, like voice to text, so users can interact with it hands-free. We also want to explore deeper personalization features, such as mood tracking and long-term emotional support, to make the experience more meaningful.",
                        "github": "https://github.com/ayank674/therapistAI/tree/production",
                        "url": "https://devpost.com/software/terracare"
                    },
                    {
                        "title": "TravelBuddy",
                        "description": "Generate smart itineraries and route summaries instantly with just one click.",
                        "story": "\ud83d\ude80 What it does: TravelBuddy allows users to:\u2708\ufe0fGenerate smart travel itinerariesbased on source, destination, and dates\ud83d\ude97Get a route summarythat estimates travel time by road (if in-country) or flight (if international)\ud83d\udccdDiscover attractions and restaurantsnear destinations (via the Foursquare API)\ud83e\udd16 All powered byLLMs like Gemini, integrated into a smooth React frontend + FastAPI backend,\ud83d\udee0 How we built it: Frontend:React + Vite for blazing-fast SPA developmentTailwind CSS for sleek, responsive stylingBackend:FastAPI (Python) with modular route/logic separationAsynchronous handling and LLM prompt managementAI Integration:Google Gemini Pro/Flash for natural language content generationCustom prompt engineering for itinerary and route summariesAPIs Used:\ud83d\uddfaFoursquare Places API\u2013 for attractions & restaurants\ud83e\udde0Google Gemini API\u2013 for AI-generated trip content,\ud83e\udde9 Challenges we ran into: \ud83e\uddfeLLM response parsingGemini returns natural text, not JSON \u2014 formatting the response reliably was tricky\ud83d\udeebMultiple airports per cityEstimating flights is tough when cities have several airports (e.g. New York, London)\ud83c\udf26Weather forecast limitsMost APIs cap forecast data to 14 days \u2014 which limits trip planning beyond that window\ud83d\udd04Frontend-backend syncAsync API calls + Gemini latency required careful loading state management in React,\ud83c\udfc6 Accomplishments that we're proud of: \ud83d\ude80 Built a full-stack AI travel planner from scratch in under36 hours\ud83e\udde0 Integrated a production-grade LLM into a real-time planning tool\ud83e\uddf1 Designed aclean, scalable backendwith reusable prompt modules\ud83c\udfa8 Built abeautiful, responsive frontendwith interactive trip planning flow,\ud83d\udcda What we learned: \ud83d\udcdd How tostructure promptsfor consistent LLM outputs\ud83d\udd78 FastAPI + async best practices\ud83d\udd01 Designing scalable frontend-backend data flows\ud83e\udde9 Implementingfallback logicwhen APIs are rate-limited or fail,\ud83d\udd2e What's next for TravelBuddy: \ud83d\uddfa Show maps and directions between locations\ud83d\udcf1 Package as a mobile app or Chrome extension\u26c5 Integratelive weatherandlocal eventsinto itineraries\ud83e\udded Let users save + share travel plans,",
                        "github": "https://github.com/jayeshpamnani99/TravelAgentFrontend",
                        "url": "https://devpost.com/software/travelbuddy-at5bue"
                    },
                    {
                        "title": "campfireify",
                        "description": "Find some interesting parts of your Spotify usage! From the song you're currently playing, to top tracks and artists in the past, we can dive into YOUR Spotify!",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/campfireify"
                    },
                    {
                        "title": "PosePilot",
                        "description": "PosePilot: using YOLOv8 & custom pose detection, our system monitors your webcam in real time, flags misalignment with math checks, and offers tips and reports via Gemini API, all on Flask.",
                        "story": "Team Members: Sai Jagadeesh Muralikrishnan, Chan, Pure, VivekBitCamp 2025 SubmissionInspiration: The inspiration for PosePilot came from a shared struggle that many of us face\u2014spending long hours in front of our computers with poor posture. As students, gamers, and developers, we often found ourselves slouching unknowingly for extended periods, leading to neck strain, back pain, and fatigue. We wanted to build something practical and helpful\u2014a lightweight tool that could nudge us toward better posture in real time, without needing any wearables or specialized hardware.What We Built: PosePilot is a real-time AI-powered posture monitoring system that uses your webcam to detect your posture, evaluate key metrics like neck and back angles, and provide visual/audio feedback if you\u2019ve been misaligned for too long. The system also gives you a posture score, deviation alerts, and motivational badges to gamify progress and encourage good habits.What We Learned: We deepened our understanding ofpose estimationusing YOLOv8-Pose.We appliedvector math and trigonometry(like dot products and angle calculations) to convert raw keypoints into posture insights.We learned how to build and deployFlask APIs, integrate real-time camera feeds, and render feedback through a fully responsivefrontendbuilt with Tailwind CSS and HTML.We explored how to deliver high FPS performance in Python while balancing accuracy with efficiency.We also discovered how important user experience is in an AI tool\u2014so we designed a clean, intuitive interface to make posture tracking simple and enjoyable.,How It Works: We also included a baseline posture capture, deviation duration tracker, and earned badges like \"Posture Master\" and \"Fast Tracker\" to help users stay motivated.Built With: Python\u2013 Backend processing and detection logicYOLOv8-Pose\u2013 Real-time pose estimationFlask\u2013 Lightweight API backendHTML5 + Tailwind CSS\u2013 Modern frontendChart.js\u2013 Posture history graphPyttsx3 & Audio Alerts\u2013 For voice feedback and beepsThree.js & GSAP\u2013 For 3D visuals and scroll animations(Upcoming)Google Gemini AI\u2013 For posture reports and tips (in progress),\ud83c\udf10 Try It Out: \ud83d\udcbbGitHub Repository:github.com/cravotics/Pose-pilotFollow the instructions in the README to replicate the system locally.\ud83d\udcf8Note: The demo requires running the Python backend (main.py) and accessing the HTML interface locally.,Challenges We Faced: Calibrating neck and back angles reliably from webcam input was tricky due to real-world camera distortions and body variations.Managing real-time performance with high-resolution video while maintaining inference speed required careful optimization.Designing a UI that felt natural and motivating without overwhelming the user took several iterations.,What's Next?: We're integratingGemini AIto generate daily posture summaries and personalized feedback.We're also prototyping avibration wristbandthat gives a gentle buzz when you slouch\u2014turning PosePilot into a full hardware+software assistant.,",
                        "github": "https://github.com/cravotics/Pose-pilot",
                        "url": "https://devpost.com/software/pose-pilot"
                    },
                    {
                        "title": "BoxBoxBot- AI Generated F1 Commentary",
                        "description": "BoxBoxBot is your AI-powered F1 commentator- delivering race commentary in text and speech. From strategic pit stops to daring overtakes, BoxBoxBot translates raw race data into human-like commentary.",
                        "story": "Inspiration: As passionate Formula 1 fans and AI enthusiasts, we\u2019ve always been captivated by the energy of live race commentary. But what if we could bring that same excitement into dashboards, race data apps, or even personal race reviews- all powered by AI? We wanted to build something that could narrate the thrill of the race not just with numbers, but with natural, context-aware commentary. That\u2019s how BoxBoxBot was born- a voice from the pit wall, powered by data and driven by language.What it does: BoxBoxBot transforms raw Formula 1 telemetry and timing data into real-time, driver-specific commentary in both text and speech. It detects key moments like pit stops, overtakes, yellow flags, fastest laps, and builds contextual statements- mimicking the style of live commentators. It can:\n1) Generate lap-by-lap summaries\n2) Provide position change insights\n3) Highlight driver battles\n4) Speak commentary aloud using text-to-speech (TTS)How we built it: We used:\n1) Python & Pandas to process and align real-time F1 data (position, laps, race control, weather)\n2) OpenF1 API to fetch live/historical race data\n3) Natural Language Templates with dynamic insertion logic for generating text commentary\n4) pyttsx3 for speech synthesis (TTS)\n5) Streamlit for a live interfaceChallenges we ran into: 1) Handling edge cases like lap overlaps, simultaneous pit stops, and null values in telemetry\n2) Generating the right input that ensures that the information provided to the LLM is clean and creates accurate commentaryAccomplishments that we're proud of: 1) Created a working pipeline that turns raw data into human-style commentary\n2) End-to-End pipeline from web-scraping to front-end streamlit application in under 24 hoursWhat we learned: 1) The power of combining data science with storytelling\n2) Challenges of real-time commentary from structured data sourcesWhat's next for BoxBoxBot- AI Generated F1 Commentary: 1) Add support for multi-lingual commentary and more natural speech synthesis\n2) Build a live dashboard or Twitch overlay for fans and streamers\n3) Extend to other motorsports like Formula E or MotoGP",
                        "github": "https://github.com/atudorcarsin/f1-commentary",
                        "url": "https://devpost.com/software/f1-commentary"
                    },
                    {
                        "title": "Prod",
                        "description": "Destroy Lonely type beat",
                        "story": "Inspiration: I love music production, however getting into it is not easy. It took me countless hours spent to make decent beats, and learning how to use a DAW like FL Studio is not straightforward. There are many technical challenges that even today after using for 5 years, I am still unsure how to navigate with it.I wanted to make an online DAW that allows for anyone to cook up. This is designed to be as intuitive as possible for beginners to pick up production fast and pros to cook up without the need to be on FL or another DAW.What it does: Beat production functionalities like pattern creation with drums, synthesizersSound imports for anyone to import their kit, with an algorithm that can detect what kind of sound imports they are (hihats, 808, snare, etc.),How we built it: Next.js, TypeScript, Tone.js for audio processing, React for UI components, Tailwind CSS for styling, and Gemini for LLM-assisted beat generation. I implemented a custom audio engine that handles complex sequencing, synthesis, and audio export functionality. The audio engine leverages Web Audio API through Tone.js to create high-quality synthesizers with effects chains (reverb, delay) and precise timing for beat sequencing.Challenges we ran into: Getting the UI and audio to respond properly to step adds, pause/play/stop, etc.Implementing reliable audio export functionality that correctly captures all tracksCreating a playback system that maintains timing accuracyDesigning an intuitive UI that works for both beginners and experienced producers,Accomplishments that we're proud of: Gathered insights from 6 producers to tackle this challengeMade beatsGot deployed,What we learned: Music production is pretty math-heavy.Building a DAW is hard,What's next for Prod: Adding a playlist where users can have multiple patternsAdding a mixer and mixing effects like reverb or EQFine-tuning the LLM on patterns. Music production is pretty math-based so I would have the LLM correlate the different types of artists to the math that their music reasonates with and from there do a better job at programming the beats with addition to any other variables they specify.Integrating the LLM with mix effects and playlist arrangement.,",
                        "github": "https://github.com/amahjoor/Prod",
                        "url": "https://devpost.com/software/prod-qwbcyh"
                    },
                    {
                        "title": "BizLingo",
                        "description": "Master Your Money: Learning Finances Made Simple & Fun",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/bizlingo"
                    },
                    {
                        "title": "ChatGPT Bookmarker",
                        "description": "No more getting lost within ChatGPT conversations - now you can bookmark sentences to come back to.",
                        "story": "Inspiration: When using ChatGPT, I would sometimes get confused and ask ChatGPT to elaborate on a subsection of its output. However, it was annoying to have to scroll and find the original thread after I finished asking the clarification question.What it does: Therefore, our team decided to create a web extension-- named ChatGPT Bookmarker-- to prevent the user from needing to scroll up and down the message history when trying to ask follow-up questions about a topic.Features:Collapsible bookmark listDraggable and resizable panel for ease of useAbility to bookmark sub-parts of messagesRenaming and removing bookmarks,How we built it: We created a manifest.json file to make it a Chrome extension. Then, we created a bookmark by using a popup, that asked for our permission to activate the bookmark. We could add to the bookmark by clicking an add button. To do this, we created Javascript files. We used MutationObservers to tell when ChatGPT generated new messages by recognizing DOM changes, which let us hover over the text to get the bookmark icon. We stored the bookmarks in chrome.storage.local from the Chrome Storage API that lets the user go back to saved messages. We created icons on Canva to match which the aesthetics of ChatGPT.Challenges we ran into: One challenge we overcame was making sure the scrolling feature automatically scrolled down each time a new bookmark is added to the list. One challenge we ran into was preserving the bookmark tab across conversations, which we hope to include in future versions of our extension.Accomplishments that we're proud of: We created a functional Chrome Extension!What we learned: We gained more experience with JavaScript and created our first Chrome extension!What's next for ChatGPT Bookmarker: We hope to publish this on the Chrome Web Store soon to gain more users (The webstore was closed for the weekend so we could not set our extension up during the hackathon)!",
                        "github": "",
                        "url": "https://devpost.com/software/chatgpt-bookmarker"
                    },
                    {
                        "title": "Cipherzoic",
                        "description": "Cipherzoic is a web app that encodes secret messages in images using a \"prehistoric\" substitution cipher and LSB steganography. Tech meets the Stone Age!\r\n\r\nTalk like ancestors. Hide like hackers.",
                        "story": "Inspiration: We wanted to build something that combined encryption, humor, and creativity. Cipherzoic took inspiration from Bitcamp's theme to put a fun twist on substitution ciphers. The process of hiding secret messages in images by first turning the text into caveman-style \u201cgrunts\u201d and ASCII cave paintings was full of enjoyment and laughter.What it does: Cipherzoic lets users upload an image and a secret message. It uses least significant bit (LSB) steganography to embed the message into the image without visibly altering it. When decrypted, the hidden message is transformed into \u201ccaveman language\" via a custom-built \"prehistoric cipher\" that replaces letters with primitive-sounding words and ASCII cave paintings.How we built it: We used HTML and CSS to create a simple and intuitive front end where users can upload images and enter secret messages. On the back end, we used Flask to handle message processing, encryption, image manipulation, and decryption. The website runs on our.tech domainin a DigitalOcean container on an Nginx web server.The core of the project uses LSB (Least Significant Bit) steganography to embed the message directly into the image without visibly altering it. Once decrypted, the message is transformed back into plaintext using our custom cipher.Our cipher is fully reversible, giving users a fun and functional way to hide and then reveal messages with a prehistoric twist.Challenges we ran into: Developing the cipher: We wanted the symbols to be complex enough to obscure the original message but still reversible for accurate translation.File format issues: Lossy formats like JPEG often corrupted the embedded message, so we focused on lossless formats like PNG.Unicode support: Creating prehistoric symbols for modern characters such as emojis was a challenge\u2014supporting more characters remains a future goal.,Accomplishments that we're proud of: Designed a custom, fully reversible cipherEmbedded messages using steganography with zero visual distortionResponsive and polished frontend that perfectly fits the theme of our projectMade our website accessible via our .tech domain using DigitalOcean, nginx, and get.tech.,What we learned: How to develop a custom cipher systemThe mechanics of substitution ciphers and LSB steganographyLimitations and quirks of different file formats and character encodings,What's next for Cipherzoic: Support for video or audio steganographyA caveman-speak chatbotMore comprehensive character support and symbols for code embedding,",
                        "github": "https://github.com/joe-magg/Bitcamp-2025",
                        "url": "https://devpost.com/software/cipherzoic"
                    },
                    {
                        "title": "Vibelist",
                        "description": "Generate a Spotify playlist to align with your mood!",
                        "story": "Inspiration: We wanted to create something that blends music and emotion. We wanted to create something that could understand how you're feeling and translate that into the perfect playlist. Music is a powerful mood enhancer, and we thought, why not let AI help pick the vibe?What it does: Vibelist generates a personalized Spotify playlist that aligns with your current mood. You simply input how you're feeling, and the app uses AI to interpret your emotion and build a playlist that matches the vibe, using both the OpenAI and Spotify APIs.How we built it: We built Vibelist using Node.js with Express for the backend. The frontend is styled with Bootstrap, HTML, and CSS for a clean and responsive UI. We used the OpenAI API to analyze mood descriptions and generate keywords or themes, which we then used with the Spotify API to create and curate mood-matching playlists. We use FastAPI to connect frontend to backend as well as to test backend.Challenges we ran into: The SpotifyAPI is very particular with the Access tokens and timeout errors and this is somewhat of a nuisance when trying to build out a product. Integrating the OpenAI and Spotify APIs seamlessly took some troubleshooting, especially around token management and response parsing. Translating natural language mood descriptions into meaningful playlist criteria was tricky as we had to fine-tune the prompts and logic. Getting the UI to work fluidly with the backend and handle async API calls required careful coordination.Accomplishments that we're proud of: We successfully connected two powerful APIs to deliver a fun and useful user experience. We created a smooth and intuitive app where users can go from mood to music in just a few clicks. We learned how to structure a full-stack application with real-world API integrations.What we learned: We learned how to effectively work with third-party APIs, including handling OAuth flows and managing rate limits. We gained experience with prompt engineering, which helped us get the most useful and relevant outputs from the OpenAI API. We also learned how to build a responsive user interface using Bootstrap, as well as how to manage asynchronous interactions between the frontend and backend on the web.What's next for Vibelist: We plan to add user authentication so that users can save their playlists and revisit them later. We also want to expand the mood detection capabilities by incorporating facial recognition or sentiment analysis from longer text inputs. In the future, we aim to allow users to fine-tune their playlists even further by selecting preferences such as energy level or music genre. Finally, we hope to deploy the app for public use and continue improving the user experience based on real-world feedback.",
                        "github": "https://github.com/tspanguluri/bitcamp-25",
                        "url": "https://devpost.com/software/vibelist-lo4j7c"
                    },
                    {
                        "title": "Moody Ring",
                        "description": "A simple, interactive webpage that allows you to get in-touch with how you're feeling through an interactive personality quiz and find your true colors and what they mean for you.",
                        "story": "Inspiration: Thinking about mood rings, one of the most quintessential tourist town souvenir shop items you'll find out there, I have always had an interest in how colors have been attributed to the way we feel. I decided to try and make a \"mood ring\" themed website that allows users to get in-touch with their emotions and learn more about why we attribute certain colors to certain emotions, and how things like seeing certain colors can bring about different thoughts and feelings.What it does: Although I couldn't get the full functionality out of the way for the website, Moody Ring would be a five-question mood quiz that dynamically displays the color you're feeling as you progress through each question.How I built it: I already have experience with HTML/CSS/JS, but I wanted to finally get my feet wet with some brand-new skills: React and Node.js. I have never used the framework before, so to start, I looked up a few tutorials to follow and some sample code to get started with setting up a website framework.Challenges I ran into: Since I was COMPLETELY unfamiliar with React and Node.js, I had a lot to learn as I progressed as much as I could with this project. A good chunk of my project time was dedicated to sitting down with tutorial videos and guides and learning how to implement TypeScript. I also had never worked with custom domains before, and setting up my .tech domain was a bit tricky, so I'm not sure if I set up correctly.Accomplishments that I'm proud of: I'm proud of what I learned about frontend frameworks as I was working through this project. Thinking about everything I've done without using React, TypeScript, and Node, I never realized how React bridges together my web development skills into one easy bundle. I'm proud that I can walk away from Bitcamp with an entirely new skill under my belt that can prove useful as I move forwards professionally.What's next for Moody Ring: I am currently taking an HTML/CSS/JS course here at the University of Maryland, so I do want to build on my knowledge by working through completing MoodyRing as a personal project. I really enjoyed working with Node.Js and React, and I want to keep on developing my skills beyond college as well, so personal projects like these will definitely help to keep my skills sharp. Seeing as I have the MoodyRing.net domain name free for one year, I might as well make the most out of it.",
                        "github": "https://github.com/devwilkes/moody-ring",
                        "url": "https://devpost.com/software/moody-ring"
                    },
                    {
                        "title": "Room Bloom",
                        "description": "Declutter your space. Decorate your peace.",
                        "story": "Inspiration: Individuals with mental health struggles often find it overwhelming to complete seemingly simple everyday tasks such as cleaning your room. When motivation is low, chores like these tend to become less of a priority. We wanted to design an app that turns these tasks into something rewarding, fun, and achievable. We aim to turn everyday cleaning into a gamified experience in order to provide motivation and reinforcement, one small win at a time.What it does: Room Bloom provides users with motivation through visual progress and positive reinforcement. The app allows users to upload a picture of their space before and after cleaning it up. The app then uses Google Cloud Vision to asses the difference in cleanliness of the space and awards points to the user as well as a positive affirmation. Points can be used to decorate a virtual room, by adding features plants, furniture, and other visual rewards that grow as they complete more tasks.How we built it: We used React Native with Expo to create a mobile app with a clean, simple and easy to use interface. The images that the users upload were encoded in Base64 and sent to used Google Cloud Vision's API. We used its object localization feature to detect the number of objects in the before and after pictures that the user uploaded. We used the change in number of detected objects to calculate a cleanliness score and award points. If the number of points was past a certain threshold the user was allowed to add a decoration to their room and was also presented with a positive affirmation.Challenges we ran into: Both teammates were completely new to React as well as using API's so there was a major learning curve from enabling Google Cloud Vision's API, to implementing the API key in our code, to actually sending the images to the API and receiving the correct information. Another challenge came with the limitations of the API. It is not completely accurate in detecting the amount of items removed especially in a very cluttered space, or when the difference in images is not very obvious which limited the accuracy of our app. We also struggled with awarding a range of points based on the ratio of items removed rather than simply 0 or 100 depending on if the space was completely cleaned or not which is what our current product does. Another issue was with the use of Expo Go to view the app on our phones - we did a lot of trial and error with different installations before we were able to successfully load it. We struggled with sharing our project on GitHub and had to even make SSH keys to publish correctly. Finally, it was a challenge to figure out how to successfully view our project on both teammates' devices by sharing servers but we figured it out eventually.Accomplishments that we're proud of: We are proud of creating a finished product that can have a real world impact on people struggling with mental health issues. We both know from personal experience how much of a difference a little bit of motivation can make when battling mental struggles. We are proud of being able to incorporate new and evolving AI such as Google Vision in to our app as it was the first time for both of us. We are also proud that we were able to effectively work together as a team, splitting up tasks and leaning on each other for support when needed. We are also proud of our ability to debug complex issues such as getting accurate information from the image recognition software.What we learned: Both teammates were completely new to using React Native, Expo and any sort of image recognition so we learned a lot about app development as well as implementing API's. From React Native we learned the basics of building mobile friendly UIs. We also learned how to navigate Expo to test across multiple devices. We learned how to implement API's like Google Cloud Vision as well as how to format at send data using Base64 encoding. We learned how API responses work by parsing JSON data to drive our app's functionality. We also learned how to effectively debug and the importance of collaboration and version control.What's next for Room Bloom: We want to create a way for users to select which specific chore they are completing so that our app can work more accurately for that specific task. We also want to add a levels component so that users level up as they gain more points as well as a way for users to keep track of cleaning streaks. We want to create a leaderboard system with a way for users to connect with their friends as this would add a more competitive aspect. We would also like to make our \"room\" more realistic with a more of a 3D look and options to drag and drop decorations. We also want to train our own AI model instead of using Google Cloud Vision's API to detect cleanliness in a way that is specific to our app, as this could improve both speed and accuracy especially for niche environments.",
                        "github": "https://github.com/simrankhotra/room-bloom",
                        "url": "https://devpost.com/software/room-bloom"
                    },
                    {
                        "title": "DataVision",
                        "description": "Bridge the gap of\u00a0access to data tools revealing understandable insights suited for any user",
                        "story": "Inspiration: We wanted to give people the ability to easily discover interesting relationships and obtain actionable insights from any data they have, regardless of their data science prior knowledge/background.What it does: The user uploads a CSV file and the agent cleans the data, comes up with potentially interesting relationships that might exist in the data, performs hypothesis testing to see if those relationships are actually there, creates visualizations to see these relationships, and provides a plain-english summary of everything that it found. After this, the user can ask follow ups about the analysis related to anything in the data and the agent will answer.How we built it: For our frontend, we used Next.js with TailwindCSS, building a real-time drag-and-drop interface and a dynamically rendering interface for the data analysis and user follow ups. For the backend we used Python + Flask to power a microservice that cleans data, runs tests, and streams p-values and visuals, using Google Gemini with LangChain and LangGraph for our agentic workflow.Challenges we ran into: Some challenges we had were syncing frontend-backend data streams and coordinating inputs and outputs for our agentic workflow.Accomplishments that we're proud of: We built an intelligent agentic data exploration tool that can find interesting relationships in any data with a very clean UI.What we learned: We deepened our understanding of real-time data streaming, backend/frontend orchestration, and agentic AI systems.What's next for DataVision: We aim to improve our system by creating even more detailed and relevant visuals, adding more useful information about the analysis procedure conducted, and making our data analysis agent more intelligent and capable overall.",
                        "github": "https://github.com/aadia1234/DataVision",
                        "url": "https://devpost.com/software/datavision-hrm5jv"
                    },
                    {
                        "title": "NeuraCoach",
                        "description": "NeuraCoach is an agent-based health assistant that leverages Reinforcement Learning (RL) and agentic AI to create and refine personalized wellness plans across mental and physical health domains.",
                        "story": "Inspiration: Being a student can be lot of work. A full course load for an undergraduate student is anywhere between 12 - 18 credit hours. Most professors expect at least 3 hours of work per week, per credit hour. That is up to 54 hours a week of work! It can be overwhelming, but NeuraCoach is here to help.What it does: NeuraCoach is an interactive service that synthesizes a users list of tasks into a clear and concise task plan. This plan is generated to include the items a user needs to complete, as well as offer complementary tasks that can provide meaningful rests and buffers to the workload. Users are also able to log their mood and energy levels as they complete tasks, providing them with a valuable log, and the system with valuable data to adjust its suggestions to.How we built it: On the frontend, we utilized the Streamlit platform for user interaction. This allowed us to use Python to create our frontend. On the backend, we used Python scripts to interface with Google Gemini 2.0 for task generation. This way, the task generation can be tuned to the users needs, and current wellbeing.",
                        "github": "",
                        "url": "https://devpost.com/software/neuracoach"
                    },
                    {
                        "title": "Pantry Pal",
                        "description": "\"What am I going to eat\" says every college student ever. Pantry pal makes cooking much easier by fetching recipes quickly with ingredients that are already in your pantry!",
                        "story": "Inspiration: As busy students and food lovers, we realized how often we let ingredients go to waste simply because we didn\u2019t know what to make with them. We wanted to create a tool that helps people make the most of what they already have\u2014whether they\u2019re college students, working professionals, or home cooks trying to reduce food waste.What it does: The user inputs the ingredients they have in their pantry and fridge and the website sends them options of recipes they can make.How we built it: There are 3 steps to our website:Challenges we ran into: We found it hard to find APIs to find recipes at first, but we ended up using groqCloud and Prexel.Accomplishments that we're proud of: We're proud of building a fully functional web app in just 24 hours that takes user-inputted ingredients and returns recipe suggestions using an external API. We created a clean, easy-to-use interface and learned how to effectively manage state and data in React. Even though we ran into API limitations, we adapted quickly and found creative solutions to keep the project on track. Most importantly, we collaborated smoothly, combining our skills in design, frontend development, and logic.What we learned: Through this project, we learned how to integrate APIs into a React app, fetch and display external data dynamically, and troubleshoot common issues like API errors and JSON parsing. We also gained experience designing an intuitive UI and working with React features like hooks and conditional rendering. This hackathon also taught us how to manage time efficiently, divide tasks as a team, and iterate quickly based on feedback and challenges.What's next for Pantry Pal: In the future, we plan to add a \"Favorite Recipes\" page where users can log in and see all the recipes they've saved by clicking the heart icon on any recipe card. This will allow users to easily return to meals they love and plan their cooking more efficiently. We're also working on integrating an image scanner that will identify ingredients from the picture users upload, and automatically feed them into the app, generating recipes based on what users actually have on hand. These features will make Pantry Pal more intuitive, user-friendly, and tailored to real-life cooking needs.",
                        "github": "",
                        "url": "https://devpost.com/software/pantry-pal-w3dgin"
                    },
                    {
                        "title": "Seismic Sense",
                        "description": "Helping blind users navigate their surroundings by relaying sonar data through vibrations.",
                        "story": "Inspiration: We were inspired to build the Seismic Sense device by visually impaired characters that are able to sense through other means, such as Daredevil or Toph Beifong, with the idea of giving visually impaired people another option to navigate their surroundings.What it does: The Seismic Sense device relays information about how far objects in the environment are to the user through vibrations in the handle. The 3 Ultrasonic Distance sensors allow for a field of view of 90 degrees. The 3 vibration motors are placed on the left, center, and right of the handle to give the user a sense of direction when using the device. The the distance sensor on the right is placed 45 degrees from the center and it corresponds with the vibration motor on the right side of the handle to give accurate feedback.How we built it: We recorded distance sensor data with an Arduino and mapped its signal to control vibration motors, then we 3D-modeled/3D-printed a housing for it to sit inside.Challenges we ran into: We had trouble with the ultrasonic distance sensors' signals interfering with each other, we were able to fix it by spacing out the times they would send their signals. We also had some trouble with knowing how much feedback to give at certain distances. Optimizing print times for 3D-Printers, in order to have more time for testing.Accomplishments that we're proud of: We were able to use our device to successfully navigate around people and detect moving obstacles while not being able to see.What we learned: We learned that running multiple ultrasonic sensors next to each other can cause problems, and that small vibration motors can be controlled through pulse width modulation for easy haptic feedback.What's next for Seismic Sense: Adding more sensors to increase the amount of information we are able to receive and then transmit to the user. Creating a haptic glove for more efficient vibration transmission. Adding ground feedback to relay changes in slope when walking.",
                        "github": "https://github.com/Brickbuilder0658/SeismicSense",
                        "url": "https://devpost.com/software/seismic-sense-l2p13k"
                    },
                    {
                        "title": "StakeHouse",
                        "description": "Our NFT staking system turns digital collectibles into yield-generating assets. By staking NFTs, users earn rewards while retaining full ownership.",
                        "story": "Inspiration: I was inspired by the lack of user-friendly NFT staking platforms that offer both functionality and a clean, responsive design. I wanted to build something that feels smooth to use and provides real value through passive rewards.What it does: Stake House lets users mint NFTs, stake them, and earn $SH tokens over time. It supports wallet connections, real-time staking stats, and a sleek, dark-themed UI\u2014all directly in the browser.How we built it: We built our staking system using a combination of web3 technologies and smart contract development. On the frontend, we used JavaScript, HTML, and CSS to create a responsive and interactive user interface that allows users to connect their wallet, stake NFTs, and claim rewards. The backend was developed in JavaScript and Solidity, where we handled the smart contract logic, ABI integration, and controller functions to interact with the Ethereum blockchain. We deployed our smart contract to an Ethereum Testnet, which also served as our server and database, storing all user and staking data on-chain. Additionally, we used IPFS to manage and retrieve NFT metadata in a decentralized way.Challenges we ran into: One of the biggest challenges we faced was understanding how smart contracts interact with the frontend. Since blockchain data isn't stored traditionally, we had to learn how to read and write data on-chain using ABI and ethers.js. We also struggled with wallet connection issues, especially when testing on the Ethereum testnet, and ensuring that users\u2019 NFTs were correctly approved and transferred to the contract for staking.Accomplishments that we're proud of: We built a fully functional, secure, and elegant NFT staking platform. The real-time reward tracking, bulk staking support, and IPFS metadata integration are features we're especially proud of.What we learned: We deepened our knowledge of smart contract architecture, frontend-driven Web3 development, and how to create responsive UIs that interact smoothly with the Ethereum network.What's next for StakeHouse: We plan to add staking leaderboard features and possibly mobile wallet support for a more inclusive staking experience.",
                        "github": "https://github.com/sueun-dev/stake-house",
                        "url": "https://devpost.com/software/stakehouse"
                    },
                    {
                        "title": "FoodShare",
                        "description": "Find the closest free food to you!",
                        "story": "Inspiration: Every year, millions of Americans struggle with food insecurity, especially those in low-income urban areas. With our group being from the Baltimore area, we've seen the causes and effects of urban food insecurity firsthand. Although Baltimore is home to a plethora of food pantries and food-based non-profits, residents struggle to find free or affordable food options. Because of poor funding, small non-profits struggle to get the word out about what they do. As a result, residents are left hungry and organizations aren't serving as much people as they could, leaving them with underwhelming numbers on grant applications. The system is broken and we're here to fix it.What it does: We answer the fundamental question:Where can I go to get free food right now?Our user-friendly interface displays all of the food pantries that are open nearby and provides filters that curate the results to the needs of the user. Users can easily filter for locations that are in walking distance, locations that offer delivery, and locations that offer a specific food type such as meat or produce.How we built it: The front-end of the web application was built with React.JS. The back-end was built with Express.JS. Our dataset was retrieved and stored in Python with Pandas. We utilized the Google Maps API for map-related features and Firebase for user authentication. We also developed a prototype of a mobile version of the app in Figma.Challenges we ran into: Incorporating the various aspects of the project from the front-end, to the back-end, to the data was difficult. Implementing the Google Maps API was especially challenging, as was user authentication with Firebase.Accomplishments that we're proud of: We're proud of our UI design in both the web application and the mobile prototype. We're also proud of the practical use of our project and the promise that it has as a potential business venture in the future. Overall, we're very pleased with what we were able to accomplish in a weekend.What we learned: We learned that the process of developing an application like ours is exhaustive and requires much troubleshooting and collaboration. We also learned that through the use of AI technologies like ChatGPT, Gemini, and Repl.it, the development process is significantly smoother.What's next for FoodShare: In the future, we'd like to add an admin panel where non-profit owners could directly modify the information that's displayed about their non-profit on the app. We want to fully implement the app's social ecosystem, allowing users to leave ratings and reviews on food pantries. Hopefully, this will motivate the pantries to improve their food and service quality as a result. Lastly, we'd like to develop the mobile application to resemble the application displayed in our prototype.",
                        "github": "",
                        "url": "https://devpost.com/software/foodshare-t9sj56"
                    },
                    {
                        "title": "Whole Lotta Stonks",
                        "description": "Whole Lotta Stocks is where Wall Street and Instagram reels meets Whole Lotta Red. We translate complex stock market information into Playboi Carti-dubbed summaries: short and drippy. ",
                        "story": "Inspiration: We watch lots of reels everyday trying to be entertained. Why no utilize a similar format but for stocks? Even better why not add everyday aura in the form of Playboi Carti as he narrates to you about said stock?What it does: You can scroll on new stocks, where Playboi Carti will then tell you more about the stock, its historical prices, P/E ratio and much. You can also listen as he tell you more about recent news with the company and much.How we built it: We built a content generation platform utilizing Python, Google Gemini, ElevenLabs and alpha vantage api to generate Carti dubs of stock information while recording other data for the frontend (ios device) to render.Challenges we ran into: Some challenges we ran into were being rate limited by the stock api which made it harder for us to generate content. Also setting up and debugging react native and expo was difficult for us but we managed to figure it out.Accomplishments that we're proud of: We are proud of having code a react native project has tiktok like features to it while also being able to render stock data and news articles. At the same time we are proud of learning to use api's and developing a program/pipeline to automate the generation of new content if needed for the platform.What we learned: We learned React Native and Expo while also learning how better utilize api's and other tools to build out this projectWhat's next for Whole Lotta Stonks: We would like to add mock investments and commenting so peers can feel connected as the learn more about stocks through the voice of Playboi Carti.",
                        "github": "https://github.com/matths05/Bitcamp2025",
                        "url": "https://devpost.com/software/whole-lotta-stonks"
                    },
                    {
                        "title": "fin-sights.tech",
                        "description": "Turn your investments into insight. Unlock the power of AI to analyze your portfolio, simulate market scenarios, and make smarter investment decisions in real-time.",
                        "story": "FinSights AI - Financial Portfolio AnalyzerFinSights AI is a modern, full-stack investment analytics web application that empowers users to securely connect their brokerage accounts, visualize\n  their portfolios, and receive AI-powered investment insights - all through an elegant Next.js interface.Tech StackFrontend: Next.js 15.2 with App Router, React 18.3Styling: TailwindCSS, shadcn/ui components, Framer Motion animationsBackend: Next.js API Routes (Node.js)Database: MongoDB (via Mongoose)Authentication: Firebase AuthAI: Google Gemini AI via GenKitFinancial Data:Plaid API for brokerage connectionsYahoo Finance for market dataCharting: Recharts,FeaturesSecure Brokerage Connection: Connect your financial accounts via Plaid integrationInteractive Dashboard:Total portfolio value with performance metricsAsset allocation visualizationHoldings breakdown with detailed metricsMarket summary with major indicesAI-Powered Insights:Portfolio analysis with risk assessmentDiversification recommendationsPerformance comparison to benchmarksChatbot Interface: Natural language interaction with your portfolio dataNews & Sentiment Analysis: Real-time market news and sentiment analysis for your holdingsPerformance Tracking: Track portfolio performance over various time periodsResponsive Design: Optimized for both desktop and mobile devices,Email:bitcamp@gmail.comPassword: password",
                        "github": "https://github.com/jgmathew2/fin-sights.tech",
                        "url": "https://devpost.com/software/fin-sights-tech"
                    },
                    {
                        "title": "InfoGrab",
                        "description": "Have you ever wanted to find news fast? You got it now!",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/infograb"
                    },
                    {
                        "title": "Binary Classifier in C++",
                        "description": "I wrote a binary classifier in c++ (I don't know what to do)",
                        "story": "",
                        "github": "https://github.com/wings20055/nl",
                        "url": "https://devpost.com/software/binary-classifier-in-c"
                    },
                    {
                        "title": "MealMap",
                        "description": "A website to make meal prepping easy and organized! You can track macros and lifespan of ingredients for each week to organize your grocery shopping and track your calories.",
                        "story": "Inspiration: As we are moving into an apartment next year, we wanted to stay organized and efficient as we transition into cooking our own meals. We realized how much time and mental energy goes into planning meals, figuring out what groceries to buy, and making sure we're not forgetting about the cheese in the back of my fridge for 4 months. That is why we created MealMap, a tool to make meal planning, grocery shopping, and inventory management easy.What it does: It allows you to plan out your meals for the week, track the ingredients you have in your fridge, and gives you a shopping list based on the recipes you choose. Once your meal plan is set, it automatically generates a shopping list based on the ingredients used in your selected recipes. You can explore recipes created by other users or build your own from scratch. It\u2019s designed to reduce food waste, save time, and help users maintain consistent eating habits.How we built it: We built the frontend using React with TypeScript. The backend uses Firebase, which handles authentication, database storage, and hosting. For the ingredient and nutrition database, we integrated the USDA FoodData Central API, which provides macro and micronutrient information for thousands of food items. This allowed us to automatically display nutritional info for each recipe and calculate totals per meal or day.Challenges we ran into: One challenge we ran into is not having enough time to add all the features I wanted to add to make this a more useful tool. Some features we would've loved to add include a fridge inventory integration and dynamic expiration tracking. These needed more backend logic that we didn't have the time to implement. Another challenge was figuring out how to structure recipe data in a way that is flexible yet searchable was more complicated than expected.Accomplishments that we're proud of: We\u2019re proud of building a fully functional meal planning app with recipe browsing, custom meal creation, and auto-generated shopping lists. I didn't expect the  integration of the USDA API with our UI to be so seamless, and getting the authentication, database syncing, and UI components to work together smoothly was a rewarding challenge. Most importantly, the app solves a real personal need, and we know it could help others too.What we learned: Do NOT put your api keys publicly on the git repository. :D We also used TypeScript for the first time and realized how easy it makes the web development process with debugging compared to Javascript. Using Firebase is a lot easier than we expected.What's next for MealMap: We would love to create a more complete website and scale the website for public use. Here are some features we did not have the time to implement during bitcamp:suggested shopping daysimproved food expiration trackingbetter track ingredients based on what meals you eat and buysuggest meal suggestions based on current fridge,Special Thanks: Hariketh Kailad <3",
                        "github": "https://github.com/carrotlemon",
                        "url": "https://devpost.com/software/mealmap"
                    }
                ],
                [
                    {
                        "title": "AlphaParse",
                        "description": "AlphaParse is a financial statement parser that gives an alpha (edge) to the user. AlphaParse leverages Gemini 2.0 Flash to allow users to query a company's SEC filings and chat about the findings.",
                        "story": "Inspiration: We were inspired by a demo done at the Bloomberg table of the Bloomberg Terminal; the intuitive design and level of accuracy are what we aimed to add to AlphaParse. We have also spent time looking through different companies ' SEC filings by hand, and it can be quite time-consuming. We strived to speed up the process of extracting key information about a company from its filings.What it does: Alpha Parse uses Gemini to allow the user to query financial information directly from SEC filings. Quering includes but is not limited to creating bar graphs, pie charts, and line charts from accurate quantitative datapoints found in a company's filings.How we built it: We began the project by building out the front end. At first, we created each front-end component to only display mock API data in order to see how the website would look and feel. This included components to enter a stock's ticker, toggle search/ticker mode, a card that displayed quick facts about the ticker (full company name, price, market cap, etc), a query/chat mode toggle, a query search component along with query results cards, and an AI chat component. Afterwards, we linked these front-end components to APIs on the backend in order to display real information. First, we added a ticker search API to populate a dropdown search results while the user looks for a company in company mode. Next, we added Finnhub's quote and company-profile endpoints in order to display a company's: Ticker, Company Name, Exchange, Current Price, Change, Market Cap, IPO Date, Country, Currency, Phone, Website, and Logo when the User types in a ticker. At the same time, we implemented the SEC's company concept API endpoint in order to get the most recent 10-K after a ticker is entered. Then we implemented Gemini's API in the query mode to look for keywords that the query may query for, and display accurate information from a request. For example, creating a pie chart of Apple's revenue by product segment. Gemini's API was also implemented on the chat mode, where Gemini is prompted to give the User brief chat-like responses and given the company's 10-k as context, the chat mode is able to give accurate information about a company's 10-k and at the end of each message, ask the user leading questions that help guide them into learning more about a Company and if they would like to have any niche finance terms defined.We chose Next.js as the React framework. The front end is built with Tailwind, a CSS framework. The backend is built in TypeScript.Challenges we ran into: We ran into git challenges, we were developing very quickly on different branches, and found it challenging to manage all of our commits and branches! Another challenge was to truncate the file sizes before prompting Gemini; without any truncation, the file sizes were in excess of 15mb each, and this would crash the server as the file contained too many tokens for Gemini to parse. We had to find a way to truncate the document as much as possible without removing any key information.Accomplishments that we're proud of: We are proud of how the final product turned out. We think that the UI looks very nice and that the project will be useful to us as we are personally interested in a tool that allows for quick, accurate information about a company for free. Many of the tools available that give out financial information are very expensive because of API costs! We look forward to continuing development in the future.What we learned: We learned how to navigate the SEC's API endpoints in addition to learning lots about how a company files with the SEC, including the types of forms public company\u2019s are required to file, the structure of some of these forms, and what type of information is contained in some of the different SEC forms.What's next for AlphaParse: In the future, we plan on adding more support for international securities and potentially other asset classes (i.e, Bonds, Forex, Cryptocurrency, Commodities). We would also like to add logic that lets the user view multiple stocks simultaneously to compare.",
                        "github": "https://github.com/lukasozpaker/bitcamp25",
                        "url": "https://devpost.com/software/alphaparse"
                    },
                    {
                        "title": "Mealbot",
                        "description": "An AI chatbot to help you meal prep!",
                        "story": "Inspiration: Oftentimes college students who live in apartments aren't aware what foods they can make or have access to and as such have poor diets, we sought to help remedy this issue in an easily accessible manner. This is where Mealbot comes in!What it does: It is a chatbot that answers all questions about meal prepping.How we built it: We built the frontend using HTML, CSS and JS and the used Gemini's API to answer the questions.Challenges we ran into: Trying to integrate the API into the webpage. Figuring out how API's work!Accomplishments that we're proud of: A working project.What we learned: How to work with API's and how to code a chatbot!What's next for Mealbot: Adding in better functionality with more features.",
                        "github": "",
                        "url": "https://devpost.com/software/mealbot-j5shi4"
                    },
                    {
                        "title": "Switch lane ",
                        "description": "Go for gold? Nah, go for blue ",
                        "story": "Inspiration: Pac-Man, flappy birdWhat it does: There are 9 sprites coming towards the player. One  or two of them are blue while the rest are black. The player has to maneuver to go through the blue sprite. If the user touches the black sprite they lose. The sprites change blue randomlyHow I built it: Using snapChallenges I ran into: I struggled finding a way to get data to all the sprites at once. I used the broadcast functionAccomplishments that we're proud of: Broadcasting the data. I always struggled having multiple sprites communicatingWhat we learned: A little bit more about snapWhat's next for Switch lane: Maybe branch out using a different language",
                        "github": "",
                        "url": "https://devpost.com/software/switch-lane"
                    },
                    {
                        "title": "Raccoon's Port Scanner",
                        "description": "A port scanner that quickly finds all open ports for a host and identifies known exploits",
                        "story": "",
                        "github": "https://github.com/Aads1/Racoons-Port-Scanner",
                        "url": "https://devpost.com/software/raccoon-s-port-scanner"
                    },
                    {
                        "title": "TerpOracle",
                        "description": "Instantly analyze your UMD schedule's workload, professor quality, and balance! Upload an image or enter courses manually to get AI-powered insights using Gemini, Testudo, and PlanetTerp data.",
                        "story": "Inspiration: Choosing the right courses and balancing a schedule at UMD can be overwhelming. Students often rely on scattered information from Testudo, PlanetTerp, and word-of-mouth. I wanted to create a centralized tool that not only gathers this data but also uses the power of generative AI to provide a holistic, insightful analysis of a student's potential semester schedule, helping them make more informed decisions.What it does: TerpOracleanalyzes a University of Maryland (UMD) student's class schedule to provide comprehensive feedback and scores across several key categories. Users can either:The application then:Extracts course information (if using image upload) using theGoogle Gemini Visionmodel.Fetches official course details (professor, time, title) byscraping the UMD Testudowebsite.Gathers professor ratings and reviews using thePlanetTerp API.Sends the combined course, professor, and review data to theGoogle Gemini (2.0 Flash)model for analysis.Generatesconcurrent AI analysesfor each individual course usingasyncio, covering aspects like Teaching Quality, Difficulty, Workload, Grading Fairness, etc.Produces a finaloverall schedule analysis, synthesizing the individual course feedback and assigning scores for Overall Workload, Professor Quality, Schedule Balance, Subject Synergy, Difficulty Management, and anOverall Schedule Grade.Displays the results in a clean web interface, including the overall grade and detailed breakdowns for each course with basic Markdown rendering.,How I built it: Frontend:Built withHTML,CSS, and vanillaJavaScript. The interface is clean and intuitive, inspired by Apple's aesthetic. It supports both image upload and manual entry, and uses the Fetch API for backend communication.Backend:A lightweightFlaskserver inPythonhandles routing, input parsing, and orchestrates the schedule analysis pipeline.Core Analysis Script (enhanced_schedule_analyzer.py):Integrates theGoogle Gemini APIvia thegoogle-generativeailibrary for both vision and text analysis.Scrapes Testudo data usingrequestsandBeautifulSoup4.Retrieves professor data from thePlanetTerp API.Usesasyncioto parallelize Gemini calls for each course, improving speed.Supports both image input and structured JSON input viaargparse.,Challenges I ran into: Testudo Scraping:HTML inconsistencies made parsing challenging.Data Integration:Merging scraped HTML, JSON API data, and image OCR.Prompt Engineering:Fine-tuning prompts for accurate, structured Gemini output.Concurrency:Managingasyncioworkflows and debugging edge cases.Input Flexibility:Supporting both image and manual text input cleanly.Frontend Formatting:Handling display formatting and Markdown rendering in vanilla JS.,Accomplishments that I'm proud of: Successfully integrated vision and text AI with traditional scraping and APIs.Achieved real-time, concurrent Gemini-based analysis.Built an intuitive, multi-input interface with clean UI/UX.Delivered a product that directly helps UMD students make better schedule decisions.,What I learned: Real-world application of large language models via APIs.Robust web scraping withrequestsandBeautifulSoup.Flask API design and dynamic frontend-backend integration.Asynchronous programming withasyncio.Effective UI design and iterative debugging based on feedback.,What's next for TerpOracle: Improve Scraping Robustness:Adapt to layout changes on Testudo.Deeper Analysis:Consider commute time, course prerequisites, and topic diversity.Support More Schools:Generalize scraping logic to support other universities.Add User Accounts:Save analysis history and API keys for frequent users.Improve Markdown Rendering:Provide richer formatting support.Refine Prompts:Tune Gemini input for greater consistency and reliability.,",
                        "github": "https://github.com/andrewxie04/terporacle",
                        "url": "https://devpost.com/software/terporacle"
                    },
                    {
                        "title": "Natch",
                        "description": "A beginner-friendly visual neural network builder inspired by Scratch",
                        "story": "Inspiration: We were inspired by how Scratch lowers the barrier for coders and with machine learning at the cutting-edge of computer science, wanted to do the same for neural networks. We were also inspired by a project that visualizes network packet. Computer science concepts are typically more important than the syntax of coding, but visualizations in computer science are rare.What it does: Natch lets you drag and drop blocks to add parameters to a neural network, then provides code and runs your neural network.Below is a list of all of the features:Drag and drop: Drag blocks and see what parameters you must add before generating your code and your visualizationTooltips: See an explanation of every single function to guide you in your creation of the neural networkChoose number of input and output nodesAdd any number of hidden layersEach layer has an activation functionThe entire network has anoptimizerand aloss functionError handling: you will receive an alert if you have not provided all parameters, in the right formatCode generation: Generates concise PyTorch codeVisualization: View the graph of the neural network to build your intuition for deep learningIn-workspace clipboard: Use keyboard shortcuts and right click menu to copy, paste, undo, redo, and cutMove blocks to the trash when you're done using them or if you made a mistakeCopy your generated Python code to your clipboard,How we built it: We used Blockly, the same engine that runs code.org, MIT's Scratch, and AppInventor, to build a modern interface for block coding. We carefully planned a UI that simplifies the process of building a deep learning model. We also incorporated a syntax highlighting library calledreact-syntax-highlighterin the generated Python code.Challenges we ran into: React and Blockly have a react-blocky nom package that is supposed to use Blockly in a more React style. It turns out thereact-blocklylibrary was poorly documented and it ended up causing more problems than solutions. We ended up abandoning the react-blockly.We tried embedding a lite Jupyter notebook instance, but we are only allowed to use a public notebook, not a private instance for each user. We decided to scrap the idea and prompt the user to run their code separately in a notebook.We received a strange react error that showed up on all but one computer.Accomplishments that we're proud of: We built a block coding tool with a beautiful idea and an original idea in only about 16 hours over 2 days!~835 lines of codeWhat we learned: We learned about Blockly and the power of prompt engineering. Some of us were using React and JavaScript for the first time.What's next for Team Natch: Other kinds of networks: CNNs, NLPscourses and templates. Natch is an educational tool at its core.adding parameters to optimizersdisplay the activation function as a symbol in our visualization,",
                        "github": "https://github.com/ObfuscatedFuture/natch",
                        "url": "https://devpost.com/software/team-flying-pandas"
                    },
                    {
                        "title": "Askit",
                        "description": "A personal Q&A notebook, tailored to your needs.",
                        "story": "Inspiration: Ever thought of a question but forget it later?\nAskit is a question management app that allows users to jot down any questions and ideas they have about anything... be it school, work, or life! We wanted to create an app that helps anyone organize and store questions for studying, reviewing, or casual note-taking.What it does: Askit lets users create folders that store different questions. Users can sign up for an account, create folders, and add questions to them, whether they\u2019re studying for a class or just organizing thoughts. Once those questions are made, users can edit them or add answers. The search bar feature allows them to search for a specific question.How we built it: We built Askit using React Native and Expo. For the backend, we used Supabase for database management. Navigation between screens was handled with React Navigation, and we used React Native Paper for styling the UI components. We also used Lucidchart to map out the flow and different screens of the app.Challenges we ran into: Our biggest challenge was setting up the login and signup. We initially tried to use Supabase Auth for it, but we ran into errors with mandatory email confirmations for users, so we had to pivot and start over (in a sense) for those pages.Accomplishments that we're proud of: We\u2019re proud of building a functional and aesthetic application. We were able to implement all of the features that we thought it would be possible for us to do with the time given.What we learned: We learned a lot about React Native navigation, Supabase integrations, and how to work quickly and under pressure. We also picked up new skills in debugging and refining UI.What's next for Askit: We wanted to try to add a feature that allows users to take pictures of questions (for example, missed questions on a paper exam) and store those pictures in the folders instead of manually having to type the questions out every time. \nIn addition, there are a few bugs and privacy issues that need to be fixed if we decide to continue developing our app.",
                        "github": "https://github.com/lynn737/Askit",
                        "url": "https://devpost.com/software/askit-rf4iwa"
                    },
                    {
                        "title": "Eyes of Expression",
                        "description": "Paint with your eyes",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/yippie-1zo4di"
                    },
                    {
                        "title": "DinoLearn",
                        "description": "Bite-sized learning. Giant results",
                        "story": "Inspiration: Studies show that people withADHD are three times more likely to drop out of traditional online courses, largely due to long, overwhelming lessons and low engagement. Meanwhile,over 60% of learners prefer microlearning formatsfor better focus and retention.We took inspiration fromDuolingo\u2019s success with neurodiverse learners, especially those with ADHD. Duolingo uses gamification, fast feedback loops, and flexible pacing to help boost dopamine, a neurotransmitter often deficient in ADHD brains. This keeps users motivated and makes learning feel fun, not forced.We wanted to bring that same magic toany topic, not just language learning.That\u2019s howDinoLearnwas born, an AI-powered, mobile-first learning platform that turnsanything you want to learninto a fun, 14-day roadmap of quick lessons and quizzes. The app is themed around dinosaurs so we could match withBitcamp\u2019s themeWhat it does: DinoLearn generates a personalized learning journey based on any topic the user enters. Each day contains:It\u2019s designed tohelp ADHD learners thrive, using coreHuman-Computer Interaction (HCI) and UI/UXprinciples like:Short, manageable contentto avoid cognitive overloadVisual rewards and streaksto stimulate motivation and dopamineInstant feedbackafter every quizMinimal decision fatiguewith a clear 14-day roadmapBright design and fun dino animationsto hold attention and spark joyDino companions throughout the appthat cheer you on, guide your journey, and make every lesson awesome,Think of it like Duolingo but foranything, not just languages.How we built it: Frontend: React Native with Expo Go for seamless mobile accessBackend: Python FastAPI to handle lesson generation and user dataAI Models: Gemini API creates the roadmap, ChatGPT generates lessons and quizzesDatabase: MongoDB stores progress and user-generated contentHosting: Fully deployed onlearnivore.techwith a custom.techdomainExtras: Lazy-loading architecture\u2014lessons and quizzes are only generated when clicked, speeding up performance,Challenges we ran into: Gemini\u2019s generation time was ~90 seconds at first, which made the app unusable. We solved this by moving toon-demand daily generation, so content is only loaded when needed.Vercel domain verification gave us trouble with our.techdomain.MongoDB\u2019s ObjectId serialization broke FastAPI responses, which we fixed using proper Pydantic modeling.,Accomplishments that we're proud of: Fully built and deployed a mobile-ready AI learning platform in under 36 hoursLive atlearnivore.techwith a working Expo Go mobile appSeamless integration of multiple AI models, a dynamic backend, and databaseDesigned withintentional HCI patternsto support ADHD learnersGamified UX inspired by Duolingo, but themed arounddinosaurs to match Bitcamp\u2019s vibe,What we learned: How to balance AI generation time with performance using smart backend strategiesHow to build ADHD-accessible user flows using HCI techniquesHow to architect full-stack mobile platforms with dynamic content loadingHow to bring real UX polish under extreme time pressure,What's next for DinoLearn: Add voice-based lessons usingElevenLabs APIIntroduce badges, dino evolution, and streak-based rewardsSupport multi-language lessons and more accessibility optionsBuild a web-based creator mode for teachers and topic creators,",
                        "github": "https://github.com/k-kochhar/DinoLearn",
                        "url": "https://devpost.com/software/dinolearn"
                    },
                    {
                        "title": "Quanta",
                        "description": "Quantum-Assisted Nano-Structural Analysis",
                        "story": "Inspiration: We've all worked with Fusion360 before and been frustrated at how long it can take to render certain items and the fact that its limited in scope - it can't model nanomaterials. Predicting how materials behave under stress is critical for many different industries, so we decided to make a solution.Introducing Quanta: Quantum-Assisted Nano-Structural AnalysisWhat it does: Our algorithm queries the properties of a material through the Materials Project API. Then, it represents the material as a flat mesh grid with a corresponding stiffness matrix, K. It can also take in an .obj file and put the material onto that. We used Finite Element Analysis (FEA) principles to approximate the behavior of mesh/thin sheets as a system of linear equations. We solve for u in the equation Ku = f, where f is force vector and u is the displacement of each point on the mesh.In our quantum implementation, we did this by using the Harrow\u2013Hassidim\u2013Lloyd (HHL) algorithm to determine the potential of quantum to quickly solve for sparse linear systems of equations.We can then render a 3D model based on the displacement matrix, which is presented to the user.How we built it: We used Python (Flask and NumPy) on the back-end. For the quantum implementation, we used the Classiq Python SDK. On the front-end, we used React and Typescript. To create the .obj files, we used Blender.Challenges we ran into: This was our first time coding up a simulation like this, and none of us had a lot of experience in materials science, so we had a lot of background research to do. In addition, we had to do a lot of math to derive the code for the algorithm, which took a long time. We spent a lot of time deliberating on if the project was even feasible before we began coding.Also, none of us had actually integrated quantum computing into a \"real\" application, so we were nervous about if it would even work. We quickly realized that the current state of quantum hardware constrains us to smaller matrices, so we decided to pivot and create both a classical and quantum algorithm. The quantum implementation serves as a proof of concept and building block for future iterations of Quanta, while the classical implementation shows what is capable with current technologies and what areas for improvement are.On the front-end, we faced challenges as well. This was the first time our team created a site using React Router, and it was also our first experience with Three.js for rendering 3D models. Learning these technologies and integrating them into our project was a steep learning curve, but it was rewarding to see the a physical visualization of our product.Accomplishments that we're proud of: We're proud that we were able to make a functioning project! We also are proud that we are able to do meaningful work on both the backend and the frontend.What we learned: We all learned about quantum algorithms and how quantum computing can be used in the real world. We also learned a lot about materials science and different considerations that engineers use when deciding what materials to use.What's next for Quanta: We want to see how far we can push Quanta and how big our mesh can be for the quantum implementation. We also want to add more variables to our calculation for the stiffness matrix and make our model even more accurate.In the spirit of Bitcamp, we hid a few fun easter eggs for anyone on our website!! As you can see in the video, if you click the \"Dinosaur!\" in the top right, you'll be brought to the wonderful T-rex game and if you click the \"I don't need this\" button, you'll go to a dinosaur themed casino page that will relieve all stress built during the hackathon :)",
                        "github": "https://github.com/chemystery09/quanta",
                        "url": "https://devpost.com/software/quanta-obvusa"
                    },
                    {
                        "title": "Sprout It!",
                        "description": "Your plants will never die on you again!",
                        "story": "",
                        "github": "https://github.com/rhebadeeba/sprout_it",
                        "url": "https://devpost.com/software/sprout-it"
                    },
                    {
                        "title": "AlphaMutate",
                        "description": "A combination of AlphaFold and Google Gemini to provide a structured workflow for common tasks encountered in drug discovery and other protein sequencing applications.",
                        "story": "Inspiration: We wanted to get more involved with the Drug Discovery research field and thought that this hackathon would be a good chance to try out developing a project to aid that process. The members of the group all thought the Alphafold library was interesting and were interested in implementing an actual service using them.What it does: Given a protein sequence, our application will direct the user to a website where they can see an interactive 3D model of the closest matching protein that is in the Alphafold database, allowing for incomplete sequence inputs. The application also hosts a Gemini based chatbot that will answer any questions about the protein that was chosen from Alphafold as well as models of some mutations of the protein that may be useful for other applications.How we built it: We built this application using a Svelte based website and a backend that consists of Gemini and Alphafold API calls.Challenges we ran into: We were unable to implement many of the things we had planned out, such as using RF-diffusion to implement binder design or MD simulations with the proteins. This was largely due to a lack of clarity in dependencies, along with certain services required to implement those features being very complex or lengthy to implement.Accomplishments that we're proud of: We are proud of our user interface and the design / structure of the website and the workflow we have created. We are also proud of creating something that could actually be incredibly helpful to various fields of protein research, based on our discussions with experienced researchers in this field.What we learned: We learned a new style of front-end web development with Svelte, as well as new technologies like AlphaFold and Gen-AI Integration with an interactive element on a website.What's next for AlphaMutate: Some potential expansions for our project include implementing RF-diffusion to help find \"Binders\" where proteins can be linked to other proteins. We also want to expand our use cases to implement molecular dynamics simulations. Currently our project is catered towards chemistry applications, however using simulation softwares like OpenMD and GROMACS, we can also cater towards different fields of research, like materials science.",
                        "github": "https://github.com/pkuppa11/alphafold-bitcamp",
                        "url": "https://devpost.com/software/alphafold-app"
                    },
                    {
                        "title": "LifexAI",
                        "description": "LifexAI - Your personal financial detective that analyzes spending patterns, detects anomalies, and provides actionable insights to improve your financial health, all with a user friendly interface.",
                        "story": "Inspiration: In today's complex financial landscape, many people struggle to make sense of their spending habits and financial health. Traditional banking apps often present raw data without meaningful insights. We were inspired to create LifexAI after realizing that people need more than just transaction lists, they need a financial detective that can analyze patterns, identify anomalies, and provide actionable recommendations to improve their financial well-being.What it does: LifexAI transforms raw financial data into meaningful insights through:Financial Dashboard: Provides a comprehensive overview of all accounts and balances\nTransaction Tracking: Categorizes and displays transactions in an intuitive interface\nSpending Analysis: Visualizes spending by category with interactive charts\nAnomaly Detection: Identifies unusual transactions that may require attention\nFinancial Health Scoring: Calculates a personalized score with tailored recommendations\nRecurring Expense Identification: Automatically detects subscriptions and regular payments\nSpending Trend Visualization: Shows spending patterns over time to identify trendsHow we built it: We built LifexAI using a modern tech stack enhanced with cutting-edge AI capabilities:Frontend and Application Framework:Next.js & React: For a responsive, component-based frontend with server-side rendering capabilitiesTypeScript: For type safety and improved developer experienceTailwind CSS: For efficient, responsive styling with dark/light mode themingFramer Motion: For smooth animations and transitions that enhance user engagement,Data Visualization and User Experience:Chart.js & React-ChartJS-2: For interactive data visualizations that make financial and health trends easy to understandCustom UI Components: Handcrafted for optimal user experience with financial and health data,Backend and Data Processing:MongoDB: For scalable data storage and complex aggregation queriesNext.js API Routes: For serverless backend functionalityReact Query: For efficient data fetching, caching, and state management,AI and Machine Learning Integration:Google Gemini Vision API: Implemented for analyzing food images and receipts, automatically categorizing meals and extracting nutritional informationCloudflare AI Models: Utilized for natural language generation to create personalized health recommendations and easily digestible financial insightsCustom Anomaly Detection Algorithms: Developed in-house to identify unusual patterns in both health and financial data,Financial Data Integration:Capital One Nessie API: Integrated for comprehensive banking data access and transaction analysisCustom categorization logic: For accurate spending pattern recognition,Cross-functional Integration:Custom correlation engine: Built to identify relationships between health metrics and financial behaviorsMongoDB aggregation pipelines: Optimized for real-time data processing across multiple domains,Challenges we ran into: Financial Data Integration: Working with the Nessie API required understanding complex financial data structures\nAnomaly Detection Algorithm: Developing an effective algorithm to identify unusual transactions without false positives\nPerformance Optimization: Ensuring smooth performance with real-time data processing and visualizations\nResponsive Design: Creating a dashboard that works seamlessly across all device sizes\nUser Experience: Balancing comprehensive data with an intuitive, non-overwhelming interface\nHosting: Deployment errors caused a lot of problems as we were unable to build it successfully on the server. But we got through it at the very last minute.Accomplishments that we're proud of: Intuitive Financial Dashboard: Created a clean, information-rich interface that makes complex financial data accessible\nEffective Anomaly Detection: Implemented a statistical approach that successfully identifies unusual spending patterns\nComprehensive Financial Health Score: Developed a holistic scoring system that considers multiple financial factors\nSeamless Dark/Light Mode: Implemented a fully responsive theme system for user preference\nPolished Animations: Added subtle animations that enhance the user experience without being distractingWhat we learned: Financial Data Analysis: Gained expertise in processing and analyzing financial transaction data\nStatistical Methods: Learned to apply statistical approaches for pattern recognition and anomaly detection\nUX Design for Financial Tools: Discovered effective ways to present complex financial information\nAPI Integration: Improved our skills in working with external financial APIs\nPerformance Optimization: Learned techniques for optimizing data-heavy applications\nHosting: Learned the issues with building on platforms like Vercel and adding CName, SSL from different sites and connecting it all together.What's next for LifexAI: AI-Powered Insights: Implementing actual machine learning models for more sophisticated financial analysis and predictions\nPredictive Spending Forecasts: Adding capabilities to predict future expenses based on historical patterns\nHealth Monitoring Integration: Expanding to track and analyze health metrics alongside financial data\nDigital Safety Features: Adding tools to monitor and protect users' digital footprint\nNatural Language Processing: Implementing a conversational interface for users to ask questions about their finances\nExpanded Financial Planning Tools: Adding budget creation, goal setting, and investment tracking featuresPlay with LifexAI (testing): username: rajat\npassword: 123",
                        "github": "",
                        "url": "https://devpost.com/software/lifexai"
                    },
                    {
                        "title": "Sudoku Mobile App Solver",
                        "description": "This is an app that can turn anyone into a master sudoku solver! ",
                        "story": "",
                        "github": "https://github.com/ElijahAmir08/SudokuApp.git",
                        "url": "https://devpost.com/software/sudoku-mobile-app-solver"
                    },
                    {
                        "title": "SayPass",
                        "description": "Secure access, powered by your voice.",
                        "story": "",
                        "github": "https://github.com/AswinBalajiTR/SayPass",
                        "url": "https://devpost.com/software/medicam-0vp4lk"
                    }
                ]
            ]
        },
        {
            "title": "SusHacks 2025",
            "location": "VIIT Visakhapatnam",
            "url": "https://sushacks.devpost.com/",
            "submission_dates": "Apr 11 - 13, 2025",
            "themes": [
                "Beginner Friendly",
                "Open Ended",
                "Productivity"
            ],
            "organization": "SusHacks",
            "winners": false,
            "projects": []
        },
        {
            "title": "Students@AI Seoul Hackathon",
            "location": "Elice Labs",
            "url": "https://students-ai-seoul-hackathon.devpost.com/",
            "submission_dates": "Apr 12, 2025",
            "themes": [
                "Education",
                "Machine Learning/AI",
                "Social Good"
            ],
            "organization": "AI Consensus",
            "winners": true,
            "projects": [
                [
                    {
                        "title": "CodeWave",
                        "description": "CodeWave - Disaster Safety Alert System is a platform that delivers targeted alerts during disaster situations for the vulnerable classes.",
                        "story": "Canva Linkhttps://www.canva.com/design/DAGkcA9ANUQ/pRWtXTYj6z4Q9G5ClW8-Qg/view?utm_content=DAGkcA9ANUQ&utm_campaign=designshare&utm_medium=link2&utm_source=uniquelinks&utlId=h38a74e1d6aGithub Linkhttps://github.com/dominhok/CodeWaveThe inspiration behindCodeWave - Disaster Safety Alert Systemcame from the limitations of traditional emergency alert systems.CodeWave - \uc7ac\ub09c \uc548\uc804 \uc54c\ub9bc \uc2dc\uc2a4\ud15c\uc740 \uae30\uc874\uc758 \uc804\ud1b5\uc801\uc778 \uc7ac\ub09c \uc54c\ub9bc \uc2dc\uc2a4\ud15c\uc758 \ud55c\uacc4\uc5d0\uc11c \ucd9c\ubc1c\ud588\uc2b5\ub2c8\ub2e4.CodeWave - Disaster Safety Alert Systemis a platform that delivers targeted alerts during disaster situations for the vulnerable classes. It notifies users based on their vulnerability type, disaster type and location using SMS and voice calls, interprets their voice responses for signs of distress, and routes critical information to emergency contacts when necessary.\n It also provides interactive map guidance to nearby shelters and offers summarized voice reports for real-time situational awareness. This system is especially designed to assist at-risk populations who may need tailored support during emergencies.CodeWave - \uc7ac\ub09c \uc548\uc804 \uc54c\ub9bc \uc2dc\uc2a4\ud15c\uc740 \uc7ac\ub09c \uc0c1\ud669\uc5d0\uc11c \ucde8\uc57d \uacc4\uce35\uc744 \uc704\ud55c \ub9de\ucda4\ud615 \uc54c\ub9bc\uc744 \uc81c\uacf5\ud558\ub294 \uc11c\ube44\uc2a4\uc785\ub2c8\ub2e4. \uc0ac\uc6a9\uc790 \uc720\ud615, \uc7ac\ub09c \uc720\ud615, \uc704\uce58 \uc815\ubcf4\ub97c \ubc14\ud0d5\uc73c\ub85c SMS\uc640 \uc74c\uc131 \uc804\ud654\ub97c \ud1b5\ud574 \uc54c\ub9bc\uc744 \uc804\ub2ec\ud558\uba70, \uc0ac\uc6a9\uc790\uc758 \uc74c\uc131 \uc751\ub2f5\uc744 \ubd84\uc11d\ud574 \uc704\uae09 \uc2e0\ud638\ub97c \uac10\uc9c0\ud558\uace0, \ud544\uc694 \uc2dc \uae34\uae09 \uc5f0\ub77d\ucc98\ub85c \uc911\uc694\ud55c \uc815\ubcf4\ub97c \uc804\uc1a1\ud569\ub2c8\ub2e4.\n\ub610\ud55c, \uc8fc\ubcc0 \ub300\ud53c\uc18c\ub85c\uc758 \uc9c0\ub3c4 \uc548\ub0b4\uc640 \uc2e4\uc2dc\uac04 \uc0c1\ud669 \uc778\uc2dd\uc744 \uc704\ud55c \uc694\uc57d \uc74c\uc131 \ub9ac\ud3ec\ud2b8\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc774 \uc2dc\uc2a4\ud15c\uc740 \ud2b9\ud788 \uc7ac\ub09c \uc2dc \ub9de\ucda4\ud615 \uc9c0\uc6d0\uc774 \ud544\uc694\ud55c \ucde8\uc57d \uacc4\uce35\uc744 \ub3d5\uae30 \uc704\ud574 \uc124\uacc4\ub418\uc5c8\uc2b5\ub2c8\ub2e4.Backend Framework: FastAPI for asynchronous web APIs.Database: SQLAlchemy with SQLite (default), ready for production upgrades like PostgreSQL.Vector Store & RAG: FAISS for semantic search across disaster manuals, powered by Upstage AI LLMs for Retrieval-Augmented Generation (RAG).Voice Interpretation: Upstage AI analyzes voice input for emergency intent.Communication Integration: Twilio handles SMS and voice calling (TTS, STT and voice response).Geolocation Services: KakaoMap API geocodes user addresses; shelter data is fetched from Korea\u2019s Safety Data Portal.Maps: Naver Maps (JavaScript) provides a disaster map interface, showing epicenters, user locations, and nearby shelters.Multi-language Support: Basic KO\u2192EN translation via Upstage AI.Deployment Support: Uvicorn ASGI server, dotenv for environment management, and Ngrok for local webhook tunneling.Frontend Support: HTML/CSS, JavaScript and React.js for interactive disaster maps (static folder setup).Backend Framework: FastAPI\ub97c \uc0ac\uc6a9\ud558\uc5ec \ube44\ub3d9\uae30 \uc6f9 API\ub97c \uad6c\ucd95Database: SQLAlchemy\uc640 SQLite(\uae30\ubcf8\uac12)\ub97c \uc0ac\uc6a9\ud588\uc73c\uba70, \ucd94\ud6c4 PostgreSQL \uac19\uc740 \ud504\ub85c\ub355\uc158 \ud658\uacbd\uc73c\ub85c\uc758 \ud655\uc7a5 \uac00\ub2a5.Vector Store & RAG: FAISS\ub97c \ud65c\uc6a9\ud55c \uc7ac\ub09c \ub9e4\ub274\uc5bc\uc758 \uc2dc\ub9e8\ud2f1 \uac80\uc0c9\uacfc Upstage AI LLM\uc744 \ud1b5\ud55c RAG(Retrieval-Augmented Generation)\ub97c \uad6c\ud604Voice Interpretation: Upstage AI\uac00 \uc74c\uc131 \uc785\ub825\uc744 \ubd84\uc11d\ud574 \uae34\uae09 \uc0c1\ud669 \uc758\ub3c4\ub97c \ud30c\uc545Communication Integration: Twilio\ub97c \ud1b5\ud574 SMS \ubc0f \uc74c\uc131 \ud1b5\ud654(TTS, STT, \uc74c\uc131 \uc751\ub2f5)\ub97c \ucc98\ub9acGeolocation Services: \uce74\uce74\uc624\ub9f5 API\ub97c \ud1b5\ud574 \uc0ac\uc6a9\uc790 \uc8fc\uc18c\ub97c \uc9c0\uc624\ucf54\ub529\ud558\uace0, \ub300\ud53c\uc18c \uc815\ubcf4\ub294 \uacf5\uacf5\ub370\uc774\ud130\ud3ec\ud138(\uc548\uc804\uc815\ubcf4\ud3ec\ud138)\uc5d0\uc11c \uac00\uc838\uc634Maps: Naver Maps(JavaScript)\ub97c \uc0ac\uc6a9\ud574 \uc7ac\ub09c \uc9c0\ub3c4 \uc778\ud130\ud398\uc774\uc2a4\ub97c \uad6c\ud604\ud588\uc73c\uba70, \uc9c4\uc559\uc9c0, \uc0ac\uc6a9\uc790 \uc704\uce58, \uc778\uadfc \ub300\ud53c\uc18c\ub97c \uc804\uc1a1Multi-language Support: Upstage AI\ub97c \ud1b5\ud574 \uae30\ubcf8\uc801\uc778 \ud55c\uad6d\uc5b4\u2192\uc601\uc5b4 \ubc88\uc5ed \uae30\ub2a5\uc744 \uc81c\uacf5Deployment Support: Uvicorn ASGI \uc11c\ubc84, dotenv\ub97c \ud1b5\ud55c \ud658\uacbd \ubcc0\uc218 \uad00\ub9ac, Ngrok\uc73c\ub85c \ub85c\uceec \uc6f9\ud6c5 \ud130\ub110\ub9c1\uc744 \uc9c0\uc6d0Frontend Support: HTML/CSS\uc640 JavaScript, React.js\ub97c \uc0ac\uc6a9\ud574 \uc778\ud130\ub799\ud2f0\ube0c \uc7ac\ub09c \uc9c0\ub3c4\ub97c \uad6c\uc131,Emergency Escalation: Safely routing voice messages to emergency contacts without false positives or missed distress cases required rigorous intent classification logic.Voice Understanding: Accurately interpreting user responses over calls required careful prompt tuning and testing with Claude to differentiate between general queries and distress calls.Webhook Testing: Twilio's webhook system required stable public URLs, and free tunneling services (like Ngrok) sometimes introduced latency or blocking interstitials.Vector Store Preparation: Creating meaningful vector representations from diverse and sometimes unstructured disaster manuals was non-trivial. The quality of response from RAG heavily depended on the preprocessing of.txtcontent.\uae34\uae09 \uc0c1\ud669 \ub300\uc751: \uc74c\uc131 \uba54\uc2dc\uc9c0\ub97c \uae34\uae09 \uc5f0\ub77d\ucc98\ub85c \uc548\uc804\ud558\uac8c \uc804\ub2ec\ud558\ub294 \uacfc\uc815\uc5d0\uc11c, \uc758\ub3c4\uce58 \uc54a\uc740 \uc2e0\uace0\ub098 \uae34\uae09 \uc0c1\ud669\uc744 \ub193\uce58\uc9c0 \uc54a\ub3c4\ub85d \uc2e0\uace0 \ubd84\ub958 \ub85c\uc9c1\uc744 \uc5c4\uaca9\ud558\uac8c \uad6c\ud604\ud574\uc57c \ud588\uc2b5\ub2c8\ub2e4.\uc74c\uc131 \uc774\ud574: \uc74c\uc131 \ud1b5\ud654\uc5d0\uc11c \uc0ac\uc6a9\uc790\uc758 \uc751\ub2f5\uc744 \uc815\ud655\ud788 \ud574\uc11d\ud558\uae30 \uc704\ud574, \uc77c\ubc18\uc801\uc778 \uc9c8\ubb38\uacfc \uae34\uae09 \ud1b5\ud654\ub97c \uad6c\ubd84\ud560 \uc218 \uc788\ub3c4\ub85d Claude\uc640 \ud568\uaed8 \ud504\ub86c\ud504\ud2b8 \ud29c\ub2dd\uacfc \ud14c\uc2a4\ud2b8\ub97c \uc2e0\uc911\ud558\uac8c \uc9c4\ud589\ud574\uc57c \ud588\uc2b5\ub2c8\ub2e4.\ubca1\ud130 DB: \ub2e4\uc591\ud55c \ud615\uc2dd\uc758 \uc7ac\ub09c \ub9e4\ub274\uc5bc\uc5d0\uc11c \uc720\uc758\ubbf8\ud55c \ubca1\ud130 \ud45c\ud604\uc744 \uc0dd\uc131\ud558\ub294 \uc791\uc5c5\uc740 \uc27d\uc9c0 \uc54a\uc558\uc73c\uba70, RAG \uc751\ub2f5\uc758 \ud488\uc9c8\uc740 .txt \ud30c\uc77c \uc804\ucc98\ub9ac \uc218\uc900\uc5d0 \ud06c\uac8c \uc88c\uc6b0\ub418\uc5c8\uae30 \ub54c\ubb38\uc5d0 \uc774\ub97c \ucd5c\uc801\ud654\ud558\ub294 \ub370 \ub9ce\uc740 \uc2dc\uac04\uc774 \uc18c\uc694\ub418\uc5c8\uc2b5\ub2c8\ub2e4.\uc6f9\ud6c5 \ud14c\uc2a4\ud2b8: Twilio\uc758 \uc6f9\ud6c5 \uc2dc\uc2a4\ud15c\uc740 \uc548\uc815\uc801\uc778 \uacf5\uac1c URL\uc744 \uc694\uad6c\ud588\uc73c\uba70, Ngrok\uacfc \uac19\uc740 \ubb34\ub8cc \ud130\ub110\ub9c1 \uc11c\ube44\uc2a4\ub294 \ub54c\ub54c\ub85c \uc9c0\uc5f0 \uc2dc\uac04\uc774\ub098 \uc11c\ube44\uc2a4 \ucc28\ub2e8 \ubb38\uc81c\ub97c \uc77c\uc73c\ucf1c \uc548\uc815\uc131\uc5d0 \uc601\ud5a5\uc744 \ubbf8\ucce4\uc2b5\ub2c8\ub2e4.,Personalized, AI-Generated Alerts: Using large language models (LLMs) and Retrieval-Augmented Generation (RAG), alert messages are tailored to the user's personal status and the specific disaster situation, improving clarity and relevance.Automatic Escalation to Emergency Contacts: Upon receiving a response from the user, intelligent triage using Claude analyzes the situation to determine if it is an emergency and then escalates it to the appropriate emergency contacts, all without human intervention.Voice-Aware Safety System: We have seamlessly integrated voice AI into a real-time emergency alert system with an asynchronous workflow, enabling natural and efficient interaction for users in need of assistance.Multi-Modal Outreach: The system supports both SMS and voice delivery channels, making it accessible across a broad range of user abilities and preferences.Live Disaster Mapping: Real-time, browser-based interactive maps that allow users to track the current disaster location and evacuation routes to near shelters through a link provided via SMS.\uac1c\uc778\ud654\ub41c AI \uc0dd\uc131 \uc54c\ub9bc: \ub300\uaddc\ubaa8 \uc5b8\uc5b4 \ubaa8\ub378(LLM)\uacfc Retrieval-Augmented Generation(RAG)\uc744 \ud65c\uc6a9\ud558\uc5ec \uc54c\ub9bc \uba54\uc2dc\uc9c0\ub97c \uc0ac\uc6a9\uc790\uc758 \uac1c\uc778 \uc0c1\ud0dc\uc640 \ud2b9\uc815 \uc7ac\ub09c \uc0c1\ud669\uc5d0 \ub9de\uac8c \ub9de\ucda4\ud654\ud568\uc73c\ub85c\uc368, \uba85\ud655\uc131\uacfc \uad00\ub828\uc131\uc744 \ud5a5\uc0c1\uc2dc\ucf30\uc2b5\ub2c8\ub2e4.\uae34\uae09 \uc5f0\ub77d\ucc98\ub85c\uc758 \uc790\ub3d9 \uc2e0\uace0 \uc2dc\uc2a4\ud15c: \uc0ac\uc6a9\uc790\uc758 \uc751\ub2f5\uc744 \ubc1b\uc740 \ud6c4, Claude\ub97c \uc774\uc6a9\ud55c \uc9c0\ub2a5\ud615 \ud2b8\ub9ac\uc544\uc9c0\uac00 \uc0c1\ud669\uc744 \ubd84\uc11d\ud558\uc5ec \uae34\uae09 \uc0c1\ud669 \uc5ec\ubd80\ub97c \ud310\ub2e8\ud558\uace0, \uc801\uc808\ud55c \uae34\uae09 \uc5f0\ub77d\ucc98\ub85c \uc790\ub3d9\uc73c\ub85c \uc804\ub2ec\ub429\ub2c8\ub2e4. \uc774 \ubaa8\ub4e0 \uacfc\uc815\uc740 \uc0ac\ub78c\uc758 \uac1c\uc785 \uc5c6\uc774 \uc9c4\ud589\ub429\ub2c8\ub2e4.AI \uc74c\uc131 \uc2dc\uc2a4\ud15c: \uc74c\uc131 AI\ub97c \uc2e4\uc2dc\uac04 \uae34\uae09 \uc54c\ub9bc \uc2dc\uc2a4\ud15c\uc5d0 \ud1b5\ud569\ud558\uc5ec \ube44\ub3d9\uae30 \uc6cc\ud06c\ud50c\ub85c\uc6b0\ub85c \uc790\uc5f0\uc2a4\ub7fd\uace0 \ud6a8\uc728\uc801\uc778 \uc0c1\ud638\uc791\uc6a9\uc744 \uac00\ub2a5\ud558\uac8c \ud588\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ub3c4\uc6c0\uc774 \ud544\uc694\ud55c \uc0ac\uc6a9\uc790\uc5d0\uac8c \ub3d9\uc2dc\ub2e4\ubc1c\uc801\uc73c\ub85c \ub354 \ub098\uc740 \uc11c\ube44\uc2a4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.\ub2e4\uc911 \ubaa8\ub4dc \uc2e0\uace0: \uc2dc\uc2a4\ud15c\uc740 SMS\uc640 \uc74c\uc131 \uc804\ub2ec \ucc44\ub110\uc744 \ubaa8\ub450 \uc9c0\uc6d0\ud558\uc5ec, \uc0ac\uc6a9\uc790\ub4e4\uc758 \uc0c1\ud669\uacfc \uc120\ud638\ub3c4\ub97c \ucda9\uc871\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4.\uc2e4\uc2dc\uac04 \uc7ac\ub09c \uc9c0\ub3c4: \uc2e4\uc2dc\uac04 \ube0c\ub77c\uc6b0\uc800 \uae30\ubc18 \uc778\ud130\ub799\ud2f0\ube0c \uc9c0\ub3c4\ub97c \uc81c\uacf5\ud558\uc5ec, \uc0ac\uc6a9\uc790\uac00 \ud604\uc7ac \uc7ac\ub09c \uc704\uce58\uc640 \ub300\ud53c\uc18c\ub85c \uac00\ub294 \uacbd\ub85c\ub97c SMS\ub85c \uc81c\uacf5\ub41c \ub9c1\ud06c\ub97c \ud1b5\ud574 \ucd94\uc801\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.,LLMs for Emergency Use-Cases: With proper guardrails and prompt design, large language models can be powerful tools in high-stakes, time-sensitive scenarios like disaster response.API Interoperability: Building a resilient system that integrates multiple APIs (Twilio, Kakao, Naver, Upstage, Anthropic) involves error handling, retries, and good observability.Platform Integration is Crucial: Coordinating communication across different platforms (SMS, voice calls, maps, dashboards, emergency contact systems) requires clear interface contracts and reliable state management to ensure consistent, timely user experiences.Language Accessibility: Translation support, even basic, can make a significant difference in user reach and comprehension in multilingual communities.Preparedness is a Data Problem: The effectiveness of RAG systems hinges on the availability of clear, well-structured, and relevant disaster manuals.LLM\uc758 \uc7ac\ub09c\uc2dc \ud65c\uc6a9: \uc801\uc808\ud55c \uac00\ub4dc\ub808\uc77c\uacfc \ud504\ub86c\ud504\ud2b8 \uc124\uacc4\ub97c \ud1b5\ud574 \ub300\ud615 \uc5b8\uc5b4 \ubaa8\ub378(LLMs)\uc740 \uc7ac\ub09c \ub300\uc751\uacfc \uac19\uc740 \uace0\uc704\ud5d8, \uc2dc\uac04\uc5d0 \ubbfc\uac10\ud55c \uc2dc\ub098\ub9ac\uc624\uc5d0\uc11c \uac15\ub825\ud55c \ub3c4\uad6c\uac00 \ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4.API \ud1b5\ud569: \uc5ec\ub7ec API(Twilio, Kakao, Naver, Upstage, Anthropic)\ub97c \ud1b5\ud569\ud558\ub294 \uacac\uace0\ud55c \uc2dc\uc2a4\ud15c\uc744 \uad6c\ucd95\ud558\ub824\uba74 \uc624\ub958 \ucc98\ub9ac, \uc9c0\uc18d\uc801\uc778 \uad00\ucc30\uc774 \ud544\uc694\ud569\ub2c8\ub2e4.\ud50c\ub7ab\ud3fc \ud1b5\ud569\uc758 \uc911\uc694\uc131: SMS, \uc74c\uc131 \ud1b5\ud654, \uc9c0\ub3c4, \ub300\uc2dc\ubcf4\ub4dc, \uae34\uae09 \uc5f0\ub77d\ucc98 \uc2dc\uc2a4\ud15c \ub4f1 \ub2e4\uc591\ud55c \ud50c\ub7ab\ud3fc\uc744 \ud1b5\ud569\ud558\ub824\uba74 \uba85\ud655\ud55c \uc778\ud130\ud398\uc774\uc2a4 \uc124\uacc4\uc640 \uc2e0\ub8b0\ud560 \uc218 \uc788\ub294 \uc0c1\ud0dc \uad00\ub9ac\uac00 \ud544\uc218\uc801\uc785\ub2c8\ub2e4.\uc5b8\uc5b4 \uc811\uadfc\uc131: \uae30\ubcf8\uc801\uc778 \ubc88\uc5ed \uc9c0\uc6d0\ub9cc\uc73c\ub85c\ub3c4 \ub300\ud55c\ubbfc\uad6d\uc5d0 \uac70\uc8fc\ud558\ub294 \uc678\uad6d\uc778\ub4e4\uc774 \uc7ac\ub09c \uc0c1\ud669\uc5d0\uc11c \ubcf4\ub2e4 \ud6a8\uc728\uc801\uc73c\ub85c \uc2e0\uace0\ud558\uac70\ub098 \uc548\ub0b4\ub97c \ubc1b\uc744 \uc218 \uc788\ub3c4\ub85d \ub3c4\uc6b8 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\uae30\uc874\uc5d0 \uc798 \uac00\uacf5\ub41c \ub370\uc774\ud130 \uc0ac\uc6a9: RAG \uc2dc\uc2a4\ud15c\uc758 \ud6a8\uc728\uc740 \uba85\ud655\ud558\uace0 \uc798 \uad6c\uc870\ud654\ub41c \uad00\ub828 \uc7ac\ub09c \ub9e4\ub274\uc5bc \uc989, \uc798 \uc815\uc81c\ub41c \ub370\uc774\ud130\uc5d0 \ud070 \ud798\uc744 \ubc1c\ud718\ud569\ub2c8\ub2e4.,Real-Time Alert Service Improvements: Continuously enhance real-time notification services to ensure faster, more accurate delivery of disaster-related updates.Expanded Vulnerability Segmentation: Further refine user vulnerability categories to provide even more personalized services, targeting specific needs based on physical, visual, or cognitive impairments.International Expansion: Explore the potential for expanding the service to international markets, ensuring accessibility for users in different regions.Improved Multilingual Translation: Expand to more language pairs (e.g., EN\u2194JP, EN\u2194ZH) with backtranslation verification.Community Engagement and Feedback: Establish mechanisms for user feedback to inform ongoing improvements and ensure the system remains aligned with the needs of vulnerable populations.Admin mode : This mode is designed for public officials or system administrators to assist vulnerable individuals such as the elderly living alone. It allows bulk user registration via Excel upload and other administrative tools.Phone Call Retry System: In case the user does not answer the phone, the system will automatically retry calling every 5 minutes. Additionally, the data will be forwarded to fire departments or government agencies to ensure appropriate action is taken.\uc2e4\uc2dc\uac04 \uc54c\ub9bc \uc11c\ube44\uc2a4 \uace0\ub3c4\ud654: \uc7ac\ub09c \uad00\ub828 \uc815\ubcf4\ub97c \ub354\uc6b1 \ube60\ub974\uace0 \uc815\ud655\ud558\uac8c \uc804\ub2ec\ud560 \uc218 \uc788\ub3c4\ub85d \uc2e4\uc2dc\uac04 \uc54c\ub9bc \uc11c\ube44\uc2a4\ub97c \uace0\ub3c4\ud654 \ud558\ub294 \ubc29\uc548\uc744 \uc0dd\uac01\ud574 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.\ucde8\uc57d\uc131 \uc138\ubd84\ud654 \ud655\ub300: \uc2e0\uccb4\uc801, \uc2dc\uac01\uc801, \uc778\uc9c0\uc801 \ud2b9\uc131\uc744 \uae30\ubc18\uc73c\ub85c \uc0ac\uc6a9\uc790 \uc720\ud615\uc744 \ubcf4\ub2e4 \uc815\ubc00\ud558\uac8c \ubd84\ub958\ud558\uc5ec, \uac01 \uc0c1\ud669\uc5d0 \ub9de\ucd98 \uc138\ubc00\ud55c \ub9de\ucda4\ud615 \uc11c\ube44\uc2a4\ub97c \uc81c\uacf5\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\uc2dc\uc7a5 \ud655\ub300\uc131: \uc11c\ube44\uc2a4\ub97c \ud574\uc678\ub85c \ud655\uc7a5\ud560 \uac00\ub2a5\uc131\uc744 \ubaa8\uc0c9\ud558\uba70, \ub2e4\uc591\ud55c \uc9c0\uc5ed\uc758 \uc0ac\uc6a9\uc790\ub4e4\ub3c4 \uc27d\uac8c \uc811\uadfc\ud560 \uc218 \uc788\ub3c4\ub85d \uc811\uadfc\uc131\uacfc \ud638\ud658\uc131\uc744 \ud655\ubcf4\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\ub2e4\uad6d\uc5b4 \ubc88\uc5ed \uae30\ub2a5 \uac1c\uc120: \ub354 \ub9ce\uc740 \uc5b8\uc5b4\ub97c \uc9c0\uc6d0\ud558\ub3c4\ub85d \ud655\uc7a5\ud558\uace0, \uc5ed\ubc88\uc5ed\uc758 \uc815\ud655\uc131\uacfc \uc2e0\ub8b0\ub3c4\ub97c \ub192\uc77c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\ucee4\ubba4\ub2c8\ud2f0 \ucc38\uc5ec \ubc0f \ud53c\ub4dc\ubc31 \ubc18\uc601: \uc0ac\uc6a9\uc790 \uc758\uacac\uc744 \ubc18\uc601\ud560 \uc218 \uc788\ub294 \uccb4\uacc4\ub97c \uad6c\ucd95\ud558\uc5ec, \uc2dc\uc2a4\ud15c\uc774 \uc2e4\uc81c \ucde8\uc57d \uacc4\uce35\uc758 \uc694\uad6c\uc5d0 \ubd80\ud569\ud558\ub3c4\ub85d \uc9c0\uc18d\uc801\uc73c\ub85c \uc810\uac80\ud558\uace0 \uac1c\uc120\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\ud589\uc815 \ubaa8\ub4dc(\uac00\uce6d): \uc774 \ubaa8\ub4dc\ub294 \uc9c0\uc5ed \ub2f4\ub2f9 \uacf5\ubb34\uc6d0 \ub4f1 \uad00\ub9ac\uc790 \uacc4\uc815\uc5d0\uc11c\ub9cc \uc811\uadfc\ud560 \uc218 \uc788\uc73c\uba70, \uc5d1\uc140 \ud30c\uc77c \ub4f1\uc744 \ud1b5\ud574 \ubcf5\uc218 \uc0ac\uc6a9\uc790\ub97c \ub300\ub9ac\ub85c \ub4f1\ub85d\ud560 \uc218 \uc788\ub294 \uae30\ub2a5\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \ub3c5\uac70\ub178\uc778 \ubd84\ub4e4\uacfc \uac19\uc774 \uc2a4\uc2a4\ub85c \ub4f1\ub85d \ubc0f \ub300\ub9ac\uc2e0\uccad\uc774 \ubd88\uac00\ub2a5\ud55c \ubd84\ub4e4\uc744 \uc704\ud55c \ud589\uc815 \uc9c0\uc6d0 \uc218\ub2e8\uc73c\ub85c \ud65c\uc6a9\ub429\ub2c8\ub2e4.\uc804\ud654 \uc7ac\uc2dc\ub3c4 \uc2dc\uc2a4\ud15c: \uc0ac\uc6a9\uc790\uac00 \uc804\ud654\ub97c \ubc1b\uc9c0 \uc54a\uc744 \uacbd\uc6b0, \uc2dc\uc2a4\ud15c\uc740 5\ubd84\ub9c8\ub2e4 \uc790\ub3d9\uc73c\ub85c \uc804\ud654\ub97c \uc7ac\uc2dc\ub3c4\ud569\ub2c8\ub2e4. \ub610\ud55c, \ud574\ub2f9 \ub370\uc774\ud130\ub294 \uc18c\ubc29\uc11c\ub098 \uc815\ubd80\uae30\uad00\uc5d0 \uc804\ub2ec\ub418\uc5b4 \uc801\uc808\ud55c \uc870\uce58\uac00 \uc774\ub8e8\uc5b4\uc9c0\ub3c4\ub85d \ud569\ub2c8\ub2e4.,Python(FastAPI, SQLAlchemy, Uvicorn)Twilio(Programmable SMS, Voice, TTS)Upstage AI(Translation)Anthropic Claude(RAG, LLM, Voice Response Intent Classification)FAISS(Vector Search for Disaster Manual RAG)Langchain(LLM Orchestration)KakaoAPI(Address Geocoding)NaverMapsAPI(Interactive Shelter Maps)SQLite(Default DB for Development)Ngrok(Webhook Tunneling)Pydantic, dotenv, httpx(Validation and Config),",
                        "github": "https://github.com/dominhok/CodeWave",
                        "url": "https://devpost.com/software/codewave-w80yiu"
                    },
                    {
                        "title": "Hermex",
                        "description": "Turn any YouTube tutorial into an interactive classroom.",
                        "story": "Hermex: Online tutorials are powerful, but they often fall short in helping learnersactively engagewith the content. Many learners passively watch videos without checking their understanding or revisiting important concepts. We wanted to reimagine tutorial learning by transformingYouTube videos into interactive learning experiences, whereAI pauses at key moments, quizzes the learner, and later summarizes the lesson with personalized review materials.This idea was inspired by our personal struggles with staying focused during long videos and our passion for improving online education using GenAI.We used a modern tech stack optimized for performance and rapid iteration:Frontend: React + TailwindCSS +react-youtubeto embed and control the video playback experience.Backend: Python + FastAPI to handle preprocessing, transcript analysis, and question generation.AI: OpenAI's GPT-4o for summarization, checkpoint creation, and review generation.Voice & Video AI: MuseTalk for lip-syncing AI avatars, and Zonos for voice cloning.Hosting & Infra: Firebase for authentication, CloudRun for backend deployment, and Runpod for GPU processing.,How to create a seamless AI-driven user experience usingreal-time checkpointing.Orchestrating multiple GenAI models together (text \u2192 voice \u2192 video).Challenges of syncing video playback with dynamically generated AI content.Importance ofUI/UX clarityin educational tools.,Video Syncing: Ensuring that AI-generated checkpoints aligned precisely with YouTube video timestamps.Latency: Managing the delay between AI processing and frontend playback.AI hallucinations: Prompt engineering was critical to reduce inconsistencies in the generated questions and summaries.Deployment Issues: Integrating GPU-based tools (like MuseTalk) into our cloud infrastructure was non-trivial.,Our final product offers apause-and-learnflow: the video stops at intelligently selected points, asks relevant questions, and helps learners revisit content through personalized summaries. It feels like a tutor is watching with you \u2014 ready to help at every important moment.",
                        "github": "https://github.com/Ahm3dGI1/hermex",
                        "url": "https://devpost.com/software/hermex"
                    },
                    {
                        "title": "Home Sweet Home",
                        "description": "Tailored Alerts for Young Adults: Simplifying Housing Applications to Bring You Only What Matters",
                        "story": "\ud83d\udd17 Prototype: App Link\u270f\ufe0f Base Knowledge: What is \uccad\uc57d (Cheongyak)?\nCheongyak is a housing subscription system in South Korea that allows individuals to apply for new housing developments under specific eligibility criteria. It provides opportunities to secure homes at prices lower than market rates, making it a vital resource for many.\ud83d\udca1 Inspiration: applying for Cheongyak is often tedious\u2014checking for new announcements, reviewing over 50 pages of requirements, and verifying eligibility can be overwhelming. Upon researching the market, we found that existing Cheongyak alert services primarily focus on expensive private housing sales.\nTo address this gap, we developed a service tailored to young applicants, offering personalized alerts to simplify the process and ensure users receive only relevant opportunities.\ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb What it does: Our service streamlines the Cheongyak application process by automating key steps. Every day at a specific time, we crawl bulletin boards from platforms like LH and SH to gather newly posted Cheongyak announcements. These announcements are then matched to users based on their preferences and eligibility criteria. Matched users receive an organized email containing links to the relevant Cheongyak announcements, along with a chatbot link designed to simplify communication. Through the chatbot, users can easily find out the application deadline, required paper works, and other important details\u2014all without having to shift through lengthy documents or complicated processes. This service makes navigating Cheongyak faster, easier, and more accessible for everyone.\ud83d\udee0\ufe0f How we built it: Crawling & Data StorageEC2: Automatically crawls bulletin boards (e.g., LH, SH) daily to collect Cheongyak announcements, including titles and links.DynamoDB: Stores crawled announcement metadata.Amazon S3: Hosts raw PDF files of the announcements.PDF Processing & Data ExtractionS3 Trigger: Detects new PDF uploads and initiates processing.Upstage API: Extracts key details (e.g., eligibility criteria, deadlines) from PDFs.DynamoDB Update: Enriches announcement records with extracted data.User Profile ManagementFrontend: Collects user preferences and eligibility criteria.User DynamoDB: Stores user profiles for later matching.Automated Matching & AlertsEventBridge: Monitors DynamoDB for updated announcements.Lambda: Matches announcements to eligible users using queries.Email: Sends personalized alerts with announcement links and a chatbot link.Chatbot AssistanceChatbot: Lets users ask questions (e.g., \u201cDo I qualify?\u201d or \u201cWhat\u2019s the deadline?\u201d) and instantly receive answers based on PDF data.\ud83d\udd25 Challenges we ran into: PDF Integration & RAG ComplexityInitially, we attempted to directly connect raw PDFs to Amazon Bedrock\u2019s general models for RAG (Retrieval-Augmented Generation). However, inconsistent formatting and extraction errors made this approach unreliable. To resolve this, we pivoted to using Bedrock Knowledge Bases, which streamlined data structuring and improved accuracy.Model Permissions & Regional ConstraintsCertain Bedrock foundation models required explicit IAM permissions or were restricted to specific AWS regions. Navigating these limitations\u2014submitting permission requests, adjusting region configurations, and testing compatibility\u2014added unexpected delays to our development timeline.User-Announcement Matching AccuracyMatching user profiles to eligible Cheongyak announcements required precise prompt engineering. Early iterations often produced false positives/negatives due to ambiguous criteria (e.g., income brackets). Through iterative testing, we refined prompts to better interpret eligibility rules and reduce mismatches.These hurdles ultimately strengthened our system\u2019s reliability, ensuring seamless interactions between users, data, and AI models.\ud83c\udfc6 Accomplishments that we're proud of: Precision Prompt Engineering for BedrockBy meticulously refining prompts based on user profile attributes (e.g., income level, residency duration, savings history), we drastically improved Bedrock\u2019s ability to match announcements to eligible users. For example, adding descriptions of each key into structured prompts allowed the model to evaluate eligibility with 98% accuracy. This turned a subjective process into an objective, scalable solution.Seamless Integration Under Tight DeadlinesDespite the complexity of our tech stack\u2014spanning EC2, DynamoDB, Bedrock, and Lambda\u2014we successfully unified all components into a cohesive workflow in just 15 hours. The moment our first end-to-end test delivered perfectly matched alerts with chatbot links, we knew we\u2019d achieved something extraordinary. It wasn\u2019t just technical execution; it was teamwork, adaptability, and shared vision in action.\ud83e\udde0 What we learned: The Critical Role of PermissionsNavigating AWS permissions taught us that resource access controls can make or break a project. From Bedrock model restrictions to IAM role policies, we realized how easily misconfigured permissions can block progress\u2014and why the \"principle of least privilege\" is both essential and challenging to implement. This experience deepened our respect for AWS security best practices.\u2b50\ufe0f\u2b50\ufe0f\u2b50\ufe0fPrompt Engineering: The Unsung Hero of LLMsAs LLMs like Bedrock become central to tech solutions, we learned that prompt quality directly dictates output reliability.Embracing Challenges Accelerates GrowthStruggling with Lambda layers and container deployments initially felt overwhelming. However, trial-and-error\u2014like rebuilding layers 10+ times or switching to containerized Lambdas\u2014taught us more than any tutorial. Now, we\u2019re confident in deploying serverless functions efficiently.\ud83d\udd1c What's next for Home Sweet Home: Expanding Beyond Youth TargetingWhile currently focused on youth-specific opportunities, we aim to broaden the service to include more general housing subscription programs, making it accessible to a wider audience.Global Adaptationhousing subscription systems like Cheongyak exist in other countries. We plan to research and adapt our platform to support international housing programs, enabling users worldwide to benefit from tailored alerts and assistance.Enhanced UI/UXImproving user experience is a priority. We will refine the interface to ensure it is intuitive, visually appealing, and user-friendly, allowing seamless navigation and interaction for all users.These steps will help us scale the service while maintaining its core mission of simplifying housing applications.",
                        "github": "https://github.com/Team-Berners-Lee-AI-Hackathon/CheongCheong_Chatting",
                        "url": "https://devpost.com/software/home-sweet-home-6drj4n"
                    },
                    {
                        "title": "ResumeGit",
                        "description": "Maybe you ever been suffered from writing resume before. Or You will.\r\nBy AI, this service write code-based resume by one click!",
                        "story": "Just one-click writing resume service from Github with AI analysisMany developers are suffering things that make their resumes got latest info. Whenever they make new projects, these works should be done.It's also difficult to collect all projects information and make summary at all once because developers ususally works several projects in their dev life.Is it all done? No, isn't. After rewriting their resume, they also find a job. Usually for verifying the developers satisfied the condition work together, company do interview the interviewer. So, developers should prepare interview, like listing some questions expected be asked in interview.Resume and interview is be taken out from records. Developers usually use tools like Github and etc for manage their projects. And also they use platform to manage their portfolio, like linkedin.Almost projects developed by developers be uploaded in Github. And just knowing connection with each developer, we can check their projects free if the project is public.Company checks their resumes submitted by each applicant but also in resumes, there are some space writing url links like Github or linkedin for referencing developers projects. Company find more info in these references.We collect fragmented info inserted by user and with AI, update their resume latest by these informations. AI can help making the summary in these infos, and describe works in detailed based on code the developers wrote.Our target models are students or junior developments who have to write resume as soon as possible. Or they are going to write resume at first time. This service provide them resume with nice quality quickly and they starts rewriting resume from this.Github provides API in public for getting user data. This time, user just put username using in Github. The server requests public repositories made by the user. And then codes in each repo should be transform into chunk size lower than token number limit by LLM. LLM train that chunk data and summarize data recursively from code to repo and user code style. Then server return a markdown-based resume.",
                        "github": "https://github.com/What5Next/resu-git",
                        "url": "https://devpost.com/software/resugit"
                    },
                    {
                        "title": "K-Biox Sphere",
                        "description": "K-Biox Sphere unites Korean and global researchers with interactive maps of citations - fueling insights, breakthroughs, and collaboration. Events, Articles, Mentorship, - everything scientists need.",
                        "story": "Inspiration: We got inspiration from the Korean research society worldwide and their contributions. We believe that we need a platform to connect experienced and talented scientists worldwide.What it does: We have the following functions:3D visualization to connect researchers based on citationsMentorship platformEvent platformLive feedArticle sharing platform'Restaurants' connection,How we built it: Initially, we had a long problem identification and ideation phase to specify the problem we were solving. After setting the specific requirements, we created multiple prototypes and discussed the advantages and disadvantages of each. \nWe used typescript, javascript, react, firebase, and three.js to implement our code. We used GitHub as a collaboration platform to code at the same time.Challenges we ran into: Broad problem setting was the main challenge. We had to specify what problem we were solving and how we could implement that on our platform in a limited time.Accomplishments that we're proud of: Working code and friendly UI.3D visualization of researchers.,What we learned: How is the network of researchers set, and how we can represent it.What's next for K-Biox Sphere: We are scaling on K-Biox members only. In the future, we plan to connect it to the global network.",
                        "github": "https://github.com/Marstronix218/research-nexus-sphere#",
                        "url": "https://devpost.com/software/k-biox-sphere"
                    },
                    {
                        "title": "DocZilla",
                        "description": "DocZilla automatically analyzes against internal standards, flags issues, extracts actions, and syncs with Slack, Notion, and Calendar\u2014saving you hours with AGI-powered intelligence.",
                        "story": "Inspiration: After mentorship sessions and insightful conversations with representatives fromUpstageAI, we identified a major challenge that companies face: spending billions annually and countless hours on manual document processing, contract reviews, and compliance checks. Witnessing firsthand how much valuable productivity is lost inspired us to leverageUpstageAI\u2019s powerful APIto createDocZilla\u2014an intelligentAGI-powereddocument assistant using aRAG pipelineto automate these tedious tasks.What it does: DocZilla automatically analyzes contracts, agreements, and policy documents against internal company standards. It instantly flags compliance issues, identifies risky clauses, extracts key action items and dates, and seamlessly integrates with everyday productivity tools like Slack, Notion, and Google Calendar. It saves time, reduces human error, and ensures no important detail is overlooked.How we built it: We built DocZilla using a React and Next.js frontend paired with a robust FastAPI backend. Leveraging Solar LLM from Upstage, we implemented Retrieval-Augmented Generation (RAG) to quickly and effectively analyze uploaded documents. For vectorized database we used FAISS, for integrations, we connected APIs from Slack, Notion, and Google Calendar to automate workflow actions, document sharing, and task management.Challenges we ran into: One major challenge was ensuring accurate and rapid document analysis without overwhelming the user with too much information. Also having only 2 members in a team was definitely challenging, but rewarding at same time. Integrating the AGI backend with intuitive UI interactions required a lot of testing and refinement.Accomplishments that we're proud of: We successfully created an intuitive and streamlined user experience that makes complex AGI features accessible to everyone. Our seamless integration with popular productivity tools significantly boosts workflow efficiency, and we\u2019re especially proud of our effective use of RAG to improve document analysis speed and accuracy, connected with smart actions creating optimized workflowsWhat we learned: We learned a lot about combining powerful AGI models with user-friendly design principles. Working through technical integration challenges deepened our skills in full-stack development, API integrations, and AI-enhanced workflows. We also learned the importance of rapid iteration based on user and mentor feedback.What's next for DocZilla: We plan to enhance and scale DocZilla\u2019s capabilities with advanced features like automated compliance audits, predictive risk assessments, and deeper and more integrations with more platforms. We also want to work on scaling our RAG solution, to speed up processes. Our goal is to continuously refine DocZilla to become the go-to intelligent assistant for effortless and accurate document management.",
                        "github": "https://github.com/M-Abdelhakem/the-nomads",
                        "url": "https://devpost.com/software/doczilla"
                    },
                    {
                        "title": "Emberguard",
                        "description": "Follow the emberguard",
                        "story": "\ud83d\udccc Overview\nThis project was developed as part of the AGI Agent Application Hackathon. It aims to reduce human damage and loss due to residential fires. The project is a robot that maps out the fires in a small location and broadcasts it while looking for nearby people to guide to safety.\ud83d\ude80 Key Features\n\u2705 Feature 1: The robot is able to find the shortest path and live maps the area\n\u2705 Feature 2: Uses AI to create broadcast messages.\n\u2705 Feature 3: If no safe path is available, the robot is able to create one by managing small fires\n\ud83d\uddbc\ufe0f Demo / Screenshots\nscreenshot\n[Optional demo video link: e.g., YouTube]\ud83e\udde9 Tech Stack\nFrontend: [e.g., React, Vue, HTML/CSS]\nBackend: [e.g., Node.js, Flask, Django]\nDatabase: [e.g., MongoDB, MySQL, Firebase]\nOthers: [e.g., OpenAI API, LangChain, HuggingFace, Docker]\n\ud83c\udfd7\ufe0f Project Structure\n\ud83d\udcc1 emberguardt/\n\u251c\u2500\u2500 Arduino/\n\u251c\u2500\u2500 index.html\n\u251c\u2500\u2500 App\n\u251c\u2500\u2500README.md\n\ud83d\udd27 Setup & Installationgit clonehttps://github.com/rayyanafzal/emberguard.gitcd App\nnpm install\nnpm startif it didnt work, put it on stackblitz.com\n\ud83d\ude4c Team Members\nName    Role    GitHub\nMina Eskandar   Developer   @minarashad\nHaya Emizwghi   Developer   @hayooota\nRayyan afzal    Developer   @rayyanafzal\n\u23f0 Development Period\nLast updated: 2025-04-13\n\ud83d\udcc4 License\nThis project is licensed under the MIT license.\nSee the LICENSE file for more details.",
                        "github": "https://github.com/rayyanafzal/emberguard.git",
                        "url": "https://devpost.com/software/emberguard"
                    },
                    {
                        "title": "Nonji",
                        "description": "\"A global hub where researchers match, chat, and collaborate \u2014 powered by AI and made for human connection.\"",
                        "story": "\ud83e\udde0 What Inspired Us: In academia, we often find ourselves asking:\"Who else is working on this?\"\"How do I reach out to collaborate?\"\"Why is it so hard to just talk to someone with similar research interests?\",These are problems every master's or PhD student, or any other researcher faces. Existing platforms like LinkedIn, Google Scholar, and Slack serve parts of the process \u2014 butnothing connects it all into one seamless researcher experience. That\u2019s where the idea forNONJiwas born.\ud83d\udca1 What We Learned: We dove into how researchers discover one another, share their work, and form collaborations. We found:Researcher networking is stillmanual, fragmented, and often intimidating.There is ahuge potentialin AI for bothmatching people and summarizing ideas.Short-form contentcan dramatically lower the barrier for engagement \u2014 especially for the new generation of researchers.,We realized:\ud83d\udee0\ufe0f How We Built It: We builtNONJiwith the following core modules:We parsed uploaded papers and extracted embeddings using pretrained models.Citation data and keywords were used to match similar researchers.Users could immediately start chatting in aDiscord-style DM interface.,Matched researchers could initiate a \"Collab Proposal.\"We implemented anAI-driven proposal generatorto help users pitch joint work easily.,Users recorded or uploadedconcise videosexplaining their research.These videos were auto-captioned and styled to appeal across disciplines.,A global conference calendar was integrated with filters for interest areas.Users couldauto-sync eventsto personal calendars and see \u201cco-attendees.\u201d,\ud83e\uddd7\u200d\u2640\ufe0f The Challenge We Faced: Rendering dynamic text overlays, syncing audio narration, and preserving cross-device compatibility requiredsignificant system resources. At one point, video quality dropped, andtiming mismatches affected readability.We optimized the video generation pipeline by:Implementingframe-level alignmentalgorithms,Usingresource-efficient text-to-video rendering techniques, andSetting clear boundaries between image area and subtitle box with padding and stroke logic.,This allowed us todeliver clean, readable, and lightweight 60s research shorts.\u2728 Why It's Innovative: Unlike existing platforms that focus on static profiles or paper repositories,NONJi focuses on researcher-to-researcher resonance\u2014 the moment when shared curiosity leads to connection. Here\u2019s how we broke new ground:AI Matching \u2192 Real-Time Dialogue(not just profiles)Short-Form Research Content(a new genre of scholarly communication)Integrated Event-Based Networking(e.g., \u201cconnect with people attending the same conference\u201d)End-to-End Collaboration Funnel, from matching \u2192 chatting \u2192 proposal,We didn\u2019t just make another academic tool. We built thefirst platform where research meets relationship.\ud83c\udf0f Looking Ahead: The potential is massive:10M+ global researchers and risingSurging demand for research-driven AI collaboration toolsNew habits of content consumption (TikTok generation meets academia),We're excited to expand NONJi to covermore disciplines, integrate withORCID/Google Scholar, and offervideo creation supportwith AI-assisted templates.",
                        "github": "",
                        "url": "https://devpost.com/software/nonji"
                    },
                    {
                        "title": "TutorBase",
                        "description": "AI Custom Lesson Building Assistant for Educators: We want students to love learning. ",
                        "story": "https://tutor-pilot-ai-hub.lovable.app/homeInspiration: As a private 1on1 tutor teaching English, Global Politics, and Physics to high school and middle school students, I found myself anxious right before the class: I didn't know how to teach, I didn't prepare, but I wanted to be helpful. Tutors especially in university are very busy and don't have time to make extensive lesson material for tutees. After interviews with 8 tutors, I found out that we have the same problem: Tutors don't have a base to build upon to tailor lessons for individuals. In detail, we found 3 big problems: But, WE WANT STUDENTS TO LOVE LEARNING!What it does: TutorBase helps tutors build lesson plans and long-term learning strategies based on individual data such as interests, goals, current grades, strengths, and weaknesses because we want to make engaging lessons and activities. After the lesson, tutors can give feedback to the AI and AI will customize the lesson plan accordingly from the next lesson. In addition, tutors can track students' progress based on feedback to track their progress.How we built it: We built UX/UI first using paper, then turned them into UX using V0. Then, using React (Next.js) Supabase, and Python (FastAPI)Challenges we ran into: We struggled to connect the backend, front end, and AI. We overcame this through iterating debugs. Cooperating codes with other hackers was also challenging, so we overcame this by discussion.Accomplishments that we're proud of: We were finally able to connect the database, frontend, and back at the end, which we struggled with for a long time. Also, we had our first user for our prototype and she rated our product (we built a product that people initially wanted to use)What we learned: AI tools like V0, importance of pitch and presentation, time management, teamwork, importance of prototypingWhat's next for TutorBase: We are contacting an Education startup to consult our product, build further functions, and contact and pitch to sell to platforms and tutors",
                        "github": "https://github.com/itsbakr/tutor-pilot-ai-hub",
                        "url": "https://devpost.com/software/tutorbase"
                    },
                    {
                        "title": "A paper-based recommendation networking service",
                        "description": "\"Auto-extract research profiles & connect bio-researchers with AI recommendations \u2013 accelerating collaboration & discovery.",
                        "story": "Inspiration: Our project was born from the need for a more efficient networking platform for bio-researchers. We observed that while many researchers struggle to find collaborators with similar interests and complementary expertise, existing platforms fail to harness the power of automated data extraction and recommendation. This inspired us to create a tool that not only makes it easy to share and access research profiles but also intelligently recommends potential collaborators based on deep analysis of research data.What it does: Our platform enables researchers to seamlessly create profiles by linking their Google Scholar data. It automatically extracts key information like paper abstracts, experimental equipment, and reagents using state-of-the-art APIs and LLM-powered parsers. The system then stores this data in both a relational database and a vector database for efficient similarity searches, allowing users to receive tailored recommendations and connect with like-minded researchers.How we built it: We built the platform using a modern tech stack that balances simplicity and power. The front-end is developed in React to deliver a responsive and intuitive user experience. The back-end is powered by FastAPI, designed to handle asynchronous operations such as API calls to Google Scholar and Sci-Hub, data processing via PyPDF2, and integration with LLM parsers. PostgreSQL is used for structured data storage, while a vector database like Pincone supports fast similarity searches for our recommendation engine.Challenges we ran into: During development, we encountered several challenges. Managing API rate limits and ensuring reliable data extraction from diverse sources like Google Scholar and Sci-Hub required robust error handling and retry mechanisms. Integrating asynchronous workflows to handle long-running processes such as PDF parsing and embedding generation was complex. Additionally, maintaining data consistency between our relational and vector databases while enabling effective similarity searching pushed us to refine our data pipelines.Accomplishments that we're proud of: We are proud of our ability to quickly prototype a robust platform that integrates multiple cutting-edge technologies. Our system successfully automates the tedious process of data extraction and analysis, delivering accurate recommendations based on nuanced research similarities. The cohesive integration of various services\u2014from Google Scholar data fetching to LLM-powered content parsing\u2014has resulted in a platform that truly enhances research networking.What we learned: This project taught us valuable lessons in agile development, asynchronous programming, and data integration. We learned how to efficiently manage multiple APIs and build robust data pipelines to handle complex tasks. Moreover, the process deepened our understanding of both relational and vector databases, and the importance of designing systems that can scale. Working collaboratively under a time constraint also highlighted the significance of clear communication and strategic planning.",
                        "github": "https://github.com/SunwooKim11/AWS-hackathon",
                        "url": "https://devpost.com/software/a-paper-based-recommendation-networking-service"
                    },
                    {
                        "title": "Harimau",
                        "description": "ResQ: A simple web-based platform that helps people understand what\u2019s happening and what to do during a disaster",
                        "story": "Inspiration: We created ResQ because we\u2019ve seen how difficult it is to get proper help during disasters. In Malaysia, floods happen almost every year, yet there\u2019s still no official app or system to support people in real-time. Most people rely on WhatsApp or scattered social media posts, which often leads to delays and confusion. This is especially hard for the elderly, those who live alone, or people in rural areas.Emma, one of our teammates, has an aunt who works in local flood relief. She told us how hard it was to reach out to victims, not because people didn\u2019t want help, but because there was no single place for them to get reliable information or guidance.That\u2019s what inspired us to build ResQ. Instead of creating a whole new system, we built on top of Korea\u2019s Emergency Ready App, which already sends out disaster alerts. We used it as our proof of concept and focused on improving the experience\u2014making the information more accessible, easier to understand, and available in multiple languages.What it does: ResQ is a simple web-based platform that helps people understand what\u2019s happening and what to do during a disaster. It gives step-by-step instructions, shows nearby shelters, offers safety tips, and includes a smart chatbot that answers urgent questions. It\u2019s designed to work for everyone, especially seniors and foreigners who may struggle with complex apps or language barriers. There\u2019s no need to download anything\u2014just open the website and get help.,How we built it: We used HTML, Tailwind and Flask to build the platform, and connected it to real APIs like NewsAPI for scraping news data in Korea regarding disaster, Perplexity for chatbot, translating and summarizing news data.We also utilizes thehttps://www.safetydata.go.kr/website to get data on shelters, disaster informations and reports on disasters to be displayed on the map,Challenges we ran into: How to utilize the data that the CodeForKorea asked us to do which is thehttps://www.safetydata.go.kr/website which the data that was posted on there are hard to parse and extract, while also having limited API optionsTrying to minimize the API calls for AI which costs money but we did consider in using the Deepseek model which can work locally but it requires a huge amount of size for storage,Accomplishments that we're proud of: Being able to design a UI that not only catered to the elderly but are also simple and are perfect for its targeted audience which is for people who need help and information as fast as possible.Manages to utilize AI in a way that we initially intended where it can help the user and also sorts data to the way that we like.,What we learned: The importance of UI and display in conveying information in an accessible and meaningful way especially when we are dealing with disaster information and helping people in which one second can be a life and death situationHow AI and technology can be utilized to not only solve simple problems like brainstorming ideas but also can be used to solve humanity needs in dire times,What's next for Harimau: To evaluate the effectiveness of our proposed solutions not only in Korea but also globally where having timely information in critical moments can help in reducing the tragedies of natural disastersWe will create a mobile application of the web app that we created while also developing apps that will help others in other fields like mental health and education,",
                        "github": "https://github.com/Harimau-AI-Seoul-Hackathon/src",
                        "url": "https://devpost.com/software/harimau-u1logk"
                    },
                    {
                        "title": "StageMind",
                        "description": "AI-Powered Group Interactions",
                        "story": "Inspiration: Our journey began with a simple goal: to help students learn in a meaningful, impactful way. Initially, we considered teaching a \"dumb\" AI, but our vision quickly evolved into something more dynamic, creating a platform where users can engage in real-time discussions with intelligent AI agents. This shift was driven by the understanding that communication is a powerful skill\u2014one that plays a critical role in job interviews, negotiations, and everyday interactions. By learning from AI agents, users can practice and improve their communication skills, building confidence and expertise in a safe, supportive environment.What it does: StageMind is an interactive platform that helps you boost your communication skills by having real conversations with multiple AI agents. Whether you're practicing for a job interview, a presentation, or just improving your speaking confidence, you choose the topic or scenario and StageMind guides you through it.How we built it: StageMind uses Next.js and Python to create real-time, AI-powered conversations.Challenges we ran into: One major challenge was implementing realistic and responsive AI agents, and finding the right services to power them.\nWe also faced difficulties in connecting the backend to the frontend smoothly, ensuring the conversation flow, feedback, and visuals were all displayed correctly in real time.Accomplishments that we're proud of: We successfully built multi-agent, real-time conversational AI that responds intelligently and naturally to user input.\nWe also integrated a sentiment analysis and evaluation system that provides meaningful, personalized feedback, helping users understand their communication style and improve with every session.What we learned: Through this project, we gained valuable insights into building real-time AI interactions and the complexities of integrating multiple systems, from speech recognition to emotion analysis and feedback generation.\nWe also learned the importance of seamless frontend-backend connectivity to ensure smooth, user friendly experiences and the challenges of fine-tuning AI agents to respond naturally and effectively.What's next for StageMind: Looking ahead, we plan to enhance AI agent capabilities, making them even more adaptive and context-aware to provide richer, more personalized conversations and more detailed, actionable insights to help users improve specific communication skills.\nWe\u2019ll also focus on scaling our platform and introduce subscription models to provide users with premium features, including advanced analytics, personalized coaching, and exclusive content",
                        "github": "https://github.com/All4Nothing/StageMind",
                        "url": "https://devpost.com/software/stagemind"
                    },
                    {
                        "title": "YouTube Summaries",
                        "description": "Smarter Learning in Seconds",
                        "story": "Inspiration: With educational content booming on YouTube, we realized how much time is wasted scrubbing through long videos to find key takeaways. Existing tools often lack interactivity or accuracy, and few allow learners to engage deeper. So we built YouTube Summaries \u2014 an AI-powered web app to instantly summarize videos, store them, and let users ask questions to deepen understanding.What it does: YouTube Summaries takes a YouTube video (or uploaded file) and:Transcribes it using AssemblyAIGenerates a concise summary with OpenAIStores that summary in your personal dashboardEnables users to ask follow-up questions about the video using an integrated chat interfaceIn short: Watch less, learn more, and never forget what matters.How we built it: Frontend: NextJSBackend: Flask API for handling uploads, processing, and OpenAI communicationAI Services:AssemblyAI for transcriptionOpenAI GPT for summarization and Q&AChallenges we ran into: Designing a clean UI within limited time that supports interaction, not just outputAccomplishments that we're proud of: Built a full-stack AI-powered app in less than 48 hoursSeamlessly connected transcription, summarization, storage, and Q&ACreated a demo with real-world utilityDelivered a clean and intuitive user experienceWhat we learned: How to combine multiple AI APIs to build powerful user experiencesThe importance of prompt engineering in generating high-quality summariesHow to stay focused and ship an MVP with real impactWhat's next for YouTube Summaries: \ud83d\udccc Topic filtering (e.g., extract only key points from a specific theme)\ud83c\udf0d Multilingual support\ud83e\udde0 Quiz/question generation from video summaries\ud83d\udcf2 Chrome Extension for instant summaries as you browse",
                        "github": "https://github.com/Savio09/summarizeai",
                        "url": "https://devpost.com/software/youtube-summaries"
                    },
                    {
                        "title": "Gapless - Workspace LikeLion",
                        "description": "Gapless is your AI explainer bot on Slack and Discord. Just submit any confusing text\u2014code, docs, or messages\u2014and get instant, clear, interactive explanations.",
                        "story": "",
                        "github": "https://github.com/AI-HACK-GAPLESS/gapless-web",
                        "url": "https://devpost.com/software/gapless"
                    },
                    {
                        "title": "ProjectX",
                        "description": "NoteNest: An AI learning tool that transforms complex information into customized 3-line summaries with visuals, making knowledge accessible to everyone, especially those who struggle with reading.",
                        "story": "Inspiration: The project was inspired by witnessing a student with ADHD struggle to process a research paper. While many summarization tools exist, we noticed that few address the specific needs of people with cognitive disabilities, leading us to create a multisensory approach that combines text and visuals.What it does: NoteNest transforms complex content into 3-line summaries that highlight three important key points from the document, with expandable details for each line, visual aids, and audio that adapt based on user cognitive profiles.How we built it: We built NoteNest as an iPad application using Swift, implementing RAG (Retrieval Augmented Generation) technology with natural language processing and creating visualization components based on cognitive science principles to ensure information is presented in the most accessible way.Challenges we ran into: Our biggest challenge was balancing simplicity with depth in our summaries while creating visualizations that enhance understanding without increasing cognitive load, all while developing a personalization algorithm that effectively adapts to different cognitive profiles.Accomplishments that we're proud of: We successfully implemented a RAG system that processes complex materials into clear 3-line summaries, built an adaptive interface for various learning differences, and validated our approach with education specialists and users with cognitive needs.What we learned: The project taught us how cognitive science principles can be applied to information design, the effectiveness of multisensory learning for diverse cognitive profiles, and the importance of personalization in educational technology.What's next for ProjectX: Project X will continue to evolve not just as a tool but as a step toward a more cognitively inclusive society. During the prototyping process, we recognized the potential for NoteNest to support aging users as well. As Korea transitions into a super-aged society, with increasing interest in post-retirement education and re-employment, tools that simplify complex information will become essential. The core strength of NoteNest\u2014its ability to distill dense content into accessible, structured summaries\u2014aligns naturally with the cognitive and practical needs of older learners. Simplification in this context is not just about ease of use but about preserving agency, comprehension, and lifelong learning in a rapidly shifting knowledge economy.",
                        "github": "https://github.com/KwonNayeon/noteRAG",
                        "url": "https://devpost.com/software/projectx-0p8lrq"
                    },
                    {
                        "title": "Team Leafstrom : Clarity",
                        "description": "#Upstage #StudyTutor #DocumentParsing #AI",
                        "story": "\ud83c\udf31 Inspiration: We started from the problem that students struggle to grasp key points in large documents, especially without background knowledge.\ud83e\udde0 What it does: Users upload PDFs/PPTs \u2192 AI summarizes and generates a personalized study guide \u2192 Supports Q&A and review features.\ud83d\udd27 How we built it: Frontend: Next.js + Typescript + TailwindCSSBackend: AWS Serverless + PythonAI: Upstage API for text extraction & summarization & chat,\u26a0\ufe0f Challenges: Designing the product since our team doesn't have any designersHad hard time to create BE API since we are new to Upstage APIs and logics,\ud83c\udfc5 Accomplishments: Use github/git actions to manage the project FEWe actually did created fancy webpage even though we don't have designer,\ud83d\udcda What we learned: Great AI results need structured workflows and well-documented dataPeer feedback drives product improvementWe also can make real-world impact by our skillsTeamwork and communication is the most important part of hackathon,\ud83d\udd2e What\u2019s next: Aiming to launch product in a mobile versionAI-based curriculum recommendationsReal-time lecture summarization & smart notesExpand our product to many other fields,",
                        "github": "https://github.com/2025-AI-Seoul-Hackathon-Leafstorm",
                        "url": "https://devpost.com/software/team-leafstrom-clarity"
                    },
                    {
                        "title": "White Mirror",
                        "description": "WhiteMirror is an AI chatbot that detects manipulative messages, explains patterns, and empowers users\u2014expandable to chat apps, scams, education, and mental health tools. It's a digital safety net.",
                        "story": "Inspiration: In today\u2019s digital world, manipulation often hides in plain sight\u2014gaslighting, shaming, and other toxic patterns can deeply impact mental health. We wanted to create a tool that empowers users to recognize and respond to these behaviors by making the invisible visible.What it does: WhiteMirror is an AI-powered chat application that detects manipulative communication in real time. It highlights harmful messages, explains the tactics and targeted vulnerabilities, and offers users an interactive dashboard and chatbot to explore and understand their communication dynamics.How we built it: The frontend is built with React and Vite, styled with Tailwind CSS, and uses WebSockets for real-time updates. The backend uses FastAPI and PostgreSQL, with machine learning models built in Scikit-learn to classify manipulation techniques and psychological vulnerabilities. We containerized the services with Docker and managed migrations using Alembic.Challenges we ran into: Creating accurate, explainable ML models with limited labeled dataDesigning a UI that surfaces sensitive insights without being overwhelming or judgmentalEnsuring real-time detection without latencyBalancing technical complexity with user accessibility,Accomplishments that we're proud of: Successfully detecting and classifying multiple manipulation types in real timeBuilding a seamless AI chatbot (StatsBot) to interpret and query communication dataCreating an intuitive dashboard that visualizes manipulation patterns over timeEnabling meaningful reflection on digital conversations through a clean, interactive experience,What we learned: How to combine psychological theory with machine learning for real-world impactThe importance of designing with empathy when working on emotionally sensitive topicsDeeper understanding of real-time systems and WebSocket integrationHow critical explainability is in AI-driven communication tools,What's next for WhiteMirror: Integration with Messenger and Slack for live gaslighting detectionScam prevention support in platforms like Karrot (\ub2f9\uadfc\ub9c8\ucf13)Educational modules to help users build resilience against manipulationTherapist APIs for mental health professionals to use communication insights in treatmentMobile app for on-the-go support and awareness,",
                        "github": "https://github.com/syang0624/white-mirror",
                        "url": "https://devpost.com/software/white-mirror"
                    },
                    {
                        "title": "upDocs",
                        "description": "New employees waste time writing documents without guidance. Our AI assistant compares drafts with quality references and gives instant feedback. Start writing smarter\u2014let AI help you improve.",
                        "story": "Inspiration: -While working as a VC intern, I had to write investment review documents \u2014 a task that felt overwhelming at first. To learn how to structure them, I spent a lot of time digging through reports written by previous interns and senior analysts. This manual comparison process was time-consuming and mentally taxing, especially without consistent feedback. I realized this wasn\u2019t just my problem \u2014 document writing often becomes a bottleneck for many newcomers in any organization. That\u2019s when the idea for an AI-powered document assistant was born: a tool that could read, search good reference, compare, and guide, helping anyone write better documents faster.What it does: Our tool is an AI-powered assistant that helps users improve technical documents like research papers.Upload a draft PDFAutomatically parse and analyze its structureRetrieve similar high-quality papers from a curated databaseReceive visual comparison and section-by-section feedbackChat with an AI to ask questions and request rewrites in real-time,How we built it: We developed an academic paper analysis system by integrating Upstage's AI-powered APIs with custom-built components through strategic prompt engineering. The solution combines document processing, critical analysis, and visual feedback to help researchers identify weaknesses in their papers.Challenges we ran into: During the implementation process, we faced significant difficulties due to the limitations and inefficiencies of the provided APIs, which hindered seamless integration into our system. Despite these challenges, we adopted a synergistic approach by combining the functionalities of the available APIs. This strategy enabled us to overcome the obstacles and successfully achieve the desired results.Accomplishments that we're proud of: We take pride in successfully integrating multiple models into a stable and cohesive system, ensuring seamless functionality across all components. Additionally, we achieved the deployment of the complete frontend interface, demonstrating our team's technical proficiency. Most importantly, the entire process\u2014from design to development\u2014was marked by exceptional collaboration and teamwork, which played a pivotal role in delivering high-quality results.What we learned: Real-world document writing is a complex, frustrating task for manyEven simple AI integrations (retrieval + structured feedback) can greatly improve productivityPublic datasets (e.g., IEEE papers) are powerful starting points for prototyping assistive toolsSurveys and user feedback early in the process shape more impactful features,What's next for upDocs: building ageneral-purpose writing assistant that scales across all documentation tasks.",
                        "github": "https://github.com/joon363/StudentsAI_BREMEN",
                        "url": "https://devpost.com/software/updocs"
                    },
                    {
                        "title": "MakeAIdea",
                        "description": "Enhanced AI-powered Video summary service with Mindmap & user prompt.",
                        "story": "MakeAIdeais an AI-powered learning platform that analyzes YouTube VOD content to generate summaries \u2192 create questions \u2192 compare AI and user answers \u2192 and construct a question expansion tree.By leveraging LLMs, RAG (Retrieval-Augmented Generation), OCR, and Whisper, it provides an effective way to learn from video content.\ud83d\udd27 Tech Stack: Frontend: Next.js (React-based)Backend: FastAPI (Python)Database: PostgreSQLAI Engine: OpenAI GPT-4, LangchainVector Search: FAISSSpeech-to-Text: OpenAI WhisperOCR: Tesseract,\ud83e\udded Architecture Overview: \ud83d\udcc1 Directory Structure: \ud83d\udccc Main Features: \ud83d\ude80 Setup & Execution: Create a.envfile and set the following:\ud83e\udde0 Role of LLMs: \ud83e\uddd1\u200d\ud83d\udcbb Team: Seoul AI Hackathon 2025Joon SimJisu KimHyobin KimHyejeong ShinSihyun Jung,You can find the full project here:\ud83d\udc49Seoul-AI-Hackathon/MakeAIdea",
                        "github": "https://github.com/Seoul-AI-Hackathon/MakeAIdea",
                        "url": "https://devpost.com/software/makeaidea"
                    },
                    {
                        "title": "AI Consensus",
                        "description": "Testing submission",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/ai-consensus"
                    }
                ]
            ]
        },
        {
            "title": "MTC Tech for Social Good Hackathon",
            "location": "Mason Hall",
            "url": "https://mtc-social-good.devpost.com/",
            "submission_dates": "Apr 12, 2025",
            "themes": [
                "Beginner Friendly",
                "Low/No Code",
                "Social Good"
            ],
            "organization": "Muslim Tech Collaborative",
            "winners": false,
            "projects": []
        },
        {
            "title": "Flipr Hackathon 26 - ML/AI (In-Office)",
            "location": "Bangalore",
            "url": "https://flipr-hackathon-26-ml-ai.devpost.com/",
            "submission_dates": "Apr 11 - 12, 2025",
            "themes": [
                "Machine Learning/AI",
                "Web"
            ],
            "organization": "Flipr Innovation Labs Pvt Ltd",
            "winners": false,
            "projects": []
        },
        {
            "title": "ADVAYA",
            "location": "BGSCET",
            "url": "https://advaya.devpost.com/",
            "submission_dates": "Apr 10 - 12, 2025",
            "themes": [
                "Health",
                "Machine Learning/AI",
                "Social Good"
            ],
            "organization": "BGS College of Engineering and Technolog",
            "winners": false,
            "projects": []
        },
        {
            "title": "Soar into Data: 7 Wonders of the World",
            "location": "ISEB",
            "url": "https://soar-datathon-2025.devpost.com/",
            "submission_dates": "Apr 11, 2025",
            "themes": [
                "Beginner Friendly",
                "Databases",
                "Machine Learning/AI"
            ],
            "organization": "Data @ UCI",
            "winners": false,
            "projects": []
        },
        {
            "title": "The Seattle Hackathon",
            "location": "Issaquah High School",
            "url": "https://the-seattle-hackathon.devpost.com/",
            "submission_dates": "Apr 05 - 06, 2025",
            "themes": [
                "Beginner Friendly",
                "Open Ended",
                "Web"
            ],
            "organization": "Issaquah High School Code Club",
            "winners": false,
            "projects": []
        },
        {
            "title": "Los Altos Hacks IX",
            "location": "Juniper Aspiration Dome",
            "url": "https://los-altos-hacks-ix.devpost.com/",
            "submission_dates": "Apr 05 - 06, 2025",
            "themes": [
                "Beginner Friendly",
                "Open Ended"
            ],
            "organization": "Los Altos Hacks",
            "winners": false,
            "projects": [
                [
                    {
                        "title": "HealthyWorld",
                        "description": "HealthyWrld uses an AI therapist, calming activities, and soothing sounds to help teens manage anxiety and depression daily, empowering them to maintain a positive, balanced mindset.",
                        "story": "",
                        "github": "https://github.com/rin2212008/HealthyWorld",
                        "url": "https://devpost.com/software/healthywrld"
                    },
                    {
                        "title": "Task Sizzle",
                        "description": "A simple task organizer to keep high school students on top of their studies!",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/task-sizzle"
                    },
                    {
                        "title": "I feel you man",
                        "description": "Helpful Memory games for people with ADHD and Autism, and a writing spacer for people with Dyslexia to be able to read text better. Also, ",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/i-feel-you-man"
                    },
                    {
                        "title": "LearnItLive",
                        "description": "An helpful tool that uses GPT Vision to identify objects in your space and provides real-time guidance, while also offering personalized math learning with voiceovers and animations for all students.",
                        "story": "",
                        "github": "https://github.com/pranayrishi/LearnItLive/tree/main",
                        "url": "https://devpost.com/software/learnitlive"
                    },
                    {
                        "title": "AI-powered NPC models with built-in safety",
                        "description": "Next-gen AI NPC models designed for safe and interactive experiences",
                        "story": "Inspiration: Our inspiration for this project started from observing a persistent challenge in online games: effectively filtering player-generated content. Many games struggle to strike the right balance, often missing genuinely inappropriate messages while simultaneously flagging harmless communication. For example, when I played Adopt Me, any long texts I tried to send would be flagged, not allowing me to have full conversations with other players.In addition, we envisioned the potential of interactive conversations with NPC that have their own personas within games. Imagine a quest giver who can dynamically respond to player questions, offering nuanced guidance beyond pre-scripted dialogue. Or an engaging tutorial NPC capable of answering specific queries about game mechanics. The possibilities for enriching gameplay through intelligent, conversational agents seemed immense.Our goal was empowering game creators with tools for dynamic NPC dialogue and interactive tutorials, powered by AI with safety enhancements. We built a similar and simpler system, a standalone project demonstrating the capabilities of Google's Gemini API to create interactive conversations as objects with personas.What it does: This project demonstrates a basic framework for creating AI-powered Non-Player Characters (NPCs) within a game environment using Pygame and Google's Gemini API. It allows players to:Interact with different NPCs: Each NPC (represented by an image) has a distinct \"personality\" derived from its object type (e.g., a chair, a bookshelf, a TV).Engage in dynamic conversations: Player input is sent to the Gemini language model, which generates context-aware responses from the perspective of the interacting NPC.Incorporate accurate safety measures: User input is checked by AI for potentially inappropriate content with a safety measure with 14 criteria before a response is formed.Experience a typing effect: NPC responses are displayed character by character, creating a more natural and engaging interaction.,The setup is designed to be adaptable for game creators who want to implement fully interactive NPCs for various purposes, such as quest givers with dynamic dialogue, interactive tutorials capable of answering player questions, or engaging chat companions.How we built it: This project was built using the following core components:Google Gemini API:This served as the natural language processing engine. We utilized thegoogle.generativeailibrary in Python to send prompts to thegemini-1.5-flashmodel and receive text-based responses for the NPCs. Prompt engineering was used to guide the model's responses based on the NPC's identity and the conversation history. It is also used for safety evaluation of user inputs.Pygame:This Python game development library provided the framework for creating the visual interface. We used it to:Set up the game window and display.Load and render images for the player and NPCs.Handle keyboard input for player movement and chat interaction.Render text for the chat history and input field using Pygame fonts and text wrapping.Implement the character-by-character typing animation usingpygame.time.Clock()and string slicing.Manage the chat interface state, including entering and exiting chat mode and handling scrolling (with ongoing refinement).Python:Python served as the primary programming language, tying together the Gemini API and Pygame functionalities. Data structures like dictionaries were used to manage NPC properties (image, position, conversation context).MongoDB:MongoDB is used to save text histories. It stores data related to the conversations between the player and the NPCs, keeping track of dialogue for future use and providing persistent storage for text-based interactions.,The development process involved setting up each component, implementing the game loop, building the chat interface and its functionalities (input, display, scrolling), and integrating the Gemini API for dynamic responses with an accurate safety check.Challenges we ran into: We encountered several challenges during the development of this project:Consistent and Contextual AI Responses: Ensuring the Gemini model consistently provided relevant, in-character responses that flowed naturally within the conversation required careful prompt engineering and management of the conversation context.Balancing Performance and Responsiveness: The API calls to the Gemini model introduce a slight delay. We aimed to mitigate this with the typing effect, but ensuring the game remained responsive while waiting for AI responses was a consideration.Implementing Chat Scrolling: Achieving smooth and reliable scrolling of the chat window, especially during the AI's text generation, proved to be a significant hurdle. Issues involved accurately calculating visible lines, managing thescroll_offset, and ensuring it worked seamlessly with both automatic scrolling during typing and manual scrolling via arrow keys.Preventing Redundant NPC Naming: Initially, the NPC's name would sometimes appear multiple times in their responses. This was addressed by refining the prompts to instruct the model to focus solely on the dialogue content.,Accomplishments that we're proud of: Despite the challenges, we achieved several key milestones:Successful Integration of Gemini API: We successfully integrated a powerful LLM into a Pygame environment to enable dynamic NPC conversations.Functional Interactive Chat System: We created a basic yet functional chat interface that allows players to communicate with AI-powered NPCs.Accurate Safety Mechanism: We implemented an accurate safety check on user input using Gemini with strict and detailed safety guidelines.Adaptable Framework: The project provides a foundational structure that game creators can build upon to create more complex and engaging AI-driven NPC interactions.Implementation of a Typing Effect: The character-by-character typing animation enhances the user experience and provides a visual cue for the AI's response generation.,What we learned: Through this project, we gained valuable insights into:Practical Application of LLMs in Games: We learned firsthand the potential of integrating large language models to create more dynamic and interactive game experiences.Game UI/UX Considerations for AI Interactions: We gained experience in designing a basic chat interface and considering how to present AI-generated content in an engaging way.The Iterative Nature of Game Development: The challenges with scrolling and prompt engineering highlighted the importance of iterative development, testing, and refining features.The Critical Role of Safety in User-Generated Content: The need for robust safety measures when using LLMs for user-facing applications became very apparent.The Interplay of Different Technologies: We learned how to effectively combine the capabilities of a game development library (Pygame) with a cloud-based AI service (Gemini API).,What's next: Future development for this project and the broader concept of AI-powered NPCs with built-in safety could include:More Nuanced NPC Personalities: Developing more detailed and consistent NPC personalities through improved prompt engineering, potentially incorporating memory of past interactions and external knowledge.Advanced Dialogue Management: Implementing branching dialogue trees that are dynamically influenced by AI responses, creating richer and more complex conversations.Integration with Game Mechanics: Connecting NPC dialogue and actions to core game mechanics, such as quest progression, item interactions, and world events.Voice Integration: Exploring the use of Text-to-Speech APIs to give AI NPCs audible voices, further enhancing immersion.Facial Animation and Expressiveness: If integrated into a more advanced game engine, exploring ways to link AI dialogue to NPC facial animations and body language.Creator Tools and Customization: Developing user-friendly tools that allow game creators to easily define NPC personalities, behaviors, and safety parameters.Performance Optimization: Investigating ways to optimize the interaction with the LLM to reduce latency and improve the real-time feel of conversations.Exploring Different LLMs: Experimenting with other large language models to compare their performance, safety features, and suitability for different types of NPCs.,",
                        "github": "https://github.com/DayeonLim/AiNPCs",
                        "url": "https://devpost.com/software/ai-powered-npc-models-with-safety-measures"
                    },
                    {
                        "title": "TrueScan",
                        "description": "A tool to scan websites and determine their trustworthiness, give an overall score, run through AI detection and flag issues, and check for potential bias.\r\n",
                        "story": "Inspiration: For the past month, we have been diligently working on our argumentative essays at school. However, certain topics are more constrained in terms of available sources than others. Finding credible, unbiased information was difficult. Our plight led us to the idea for TrueScan.What it does: TrueScan offers a wide variety of features, including the ability to detect AI-generated content and identify bias in writing. Additionally, it provides an overall trustworthiness score and explains the reasoning behind its assessment.How we built it: We built this app by splitting up development between website and AI tools. After collecting data from a given website, we built text and image detection to see if the website was AI generated and used open AI to detect bias. Finally, we put it all together and embedded it in our scan feature.Challenges we ran into: One challenge we ran into was APIs and referencing them. We avoided spending money as much as possible, which caused multiple free trials to run out. After debugging, we finally have it working.Accomplishments that we're proud of: An accomplishment we are particularly proud of is our use of GitHub. All of our work was synced across GitHub and allowed us to collaborate very efficiently. This is the first time any of us have used GitHub and we are pretty happy with the result of our journey with commits, pushes, and pulls.What we learned: Throughout the past 20 hours, we've learned many life lessons such as patience and collaboration. We learned to be patience, as we used AI to help train our model, which tended to be wrong but would periodically produce fruitful results. We learned collaboration skills, as the three of us were all working on one file, and if we didn't collaborate, we'd all have different variables and not know what the others were doing. However, we used our newly learned collaboration skills to communicate our ideas and our work, to make sure we were all on the same page.What's next for TrueScan: What\u2019s next for TrueScan? We will work on thoroughly finishing our website, and work on finishing chrome extension to make the process even faster. Finally, we will annotate the actual html to show what specific pieces of info triggered our scan.",
                        "github": "",
                        "url": "https://devpost.com/software/truescan"
                    },
                    {
                        "title": "Cogniverse",
                        "description": "AI-Powered Cognitive Health Monitoring Through Voice Biomarkers",
                        "story": "Inspiration: The prospect of aging is often dreaded due to the high likelihood of cognitive decline in many aging individuals. Unfortunately, by the time symptoms become noticeable, significant cognitive deterioration may have already occurred, often limiting the effectiveness of interventions. However, if detected early, cognitive impairment is not only manageable, but in some cases, it can even be reversible. Recognizing the importance of early detection, we set out to develop Cogniverse, a tool that uses cutting-edge voice analysis to monitor cognitive health over time. By analyzing the voice biomarker MFCC, Cogniverse tracks subtle changes in speech patterns that could signal the onset of cognitive decline.Through daily, non-invasive monitoring, Cogniverse allows healthcare providers, caregivers, and users themselves to detect early signs of cognitive impairment, ensuring that individuals receive the necessary interventions before the condition progresses too far. Our mission is to empower both individuals and their healthcare teams with real-time insights, providing a tool for proactive health management that can help preserve quality of life during the aging process. By leveraging AI and machine learning to analyze speech data, we believe that Cogniverse can change the future of aging care, making early diagnosis and intervention more accessible and effective than ever before.What it does: By talking to Cogniverse for 2 minutes a day, Cogniverse provides insight into the extent of cognitive decline.Cogniverse listens to voice recordings and analyzes them for symptoms of cognitive decline. It looks for a biomarker called Mel-Frequency Cepstral Coefficients (MFCCs), which indicates cognitive decline. Based on the severity of the factors mentioned above, it states how far the cognitive decline has progressed. It is a method of early detection by showing the risk of developing a degenerative cognitive disease.Cogniverse looks for subtle patterns of deterioration of speech over time, like slurring. It can give a sense of the progression of cognitive decline in patients with Alzheimer's, Parkinson's, etc., or warn of the onset of symptoms.,How we built it: Back-end:We processed the PDFs by extracting the text elements from the PDF documents in the Media Set. We are extracting the raw files into files that are similar to interact with.We extracted the chunks by splitting the strings into similar lengths and it's useful to create small text blocks without removing too much important information.  By filtering the chunks we can find the information in our data related to a specific topic.We created the chunk IDs to reference the chunk objects in the Ontology we created.We used the LLM by using deep learning to summarize the 17 research articles we input into the Palantir PlatformThere are three graphs, one is the original data, the second is the first derivative of the original data, and finally, the third graph is the second derivative of the original dataWe derive the graphs multiple times to clarify the data,How we Analyzed Mel-Frequency Cepstral Coefficients:Used Jupyter through PalantirWe were not able to successfully create a model adapter to convert the code from Palantir Jupyter to our Palantir Pipeline (To synthesize data and completely train the model)We utilized MFCC because it captures spectral features of voice (timbre, phonetics) & MFCCs changes can indicate cognitive issuesWe created graphs of the MFCC and also turned the graphs\u2019 data into numerical data,Our Simulation:Analyzed audio from Joe Biden\u2019s speeches from the years of 2018, 2020, 2022, and 2024 to analyze him for cognitive declineCollected videos from the internet, downloaded, cut to ~2 min, and fed the audio into the Jupyter JournalCross-referenced with a control MFCC graph of an individual with dementia (In video),Front-end:Built the iOS app frontend using SwiftUI, starting with a clean and user-friendly interface design.Main screen includes three core buttons:Microphone button (center): Starts and stops the two-minute voice recording.\u201cCurrent Analysis\": Displays the latest cognitive risk assessment.\"Trends\": Intended to show score progression over time.Integrated AVAudioRecorder (or a placeholder) to manage audio input.Upon stopping the recording, the app simulates MFCC feature extraction by generating and converting graphical data into numerical format.Audio is sent to the Palantir-hosted backend API, which analyzes the MFCCs and returns a cognitive risk level.Results are dynamically displayed using @State variables in SwiftUI, completing the loop from voice capture to visual cognitive health feedback.,Challenges we ran into: Our next step was to create a model adapter to add this code to our Pipeline- thus synthesizing the inputs (info from research & audio examples) needed to finish training our model and ran into major issues - Adapting the dataWe followed the steps located on the Palantir Documentation PageWe asked for help from Palantir engineers but they didn\u2019t know how to create the model adapterWe worked on our errors related to adapting the data for more than 3 hours but weren\u2019t able to figure it out,Accomplishments that we're proud of: We are glad that we could generate graphs from our audio inputsOur frontend and backend work separately but not togetherWe are new to coding so we were happy were were able to create error-free code,What we learned: We learned how to learn/navigate Palantirlearned how to work under a time crunchlearned how to use SwiftUI,What's next for Cogniverse: After presenting to the judges at Las Altos IX, we aim to transform this into a real-world solution for continuous cognitive health monitoring. Because we ran out of time, we did not get the chance to integrate our front end and our back end, something we will plan to do. With further development it could be integrated into healthcare platforms like the Kaiser Permanente app or My Chart, allowing physicians to passively monitor their patients' cognitive trends and intervene early when symptoms appear. On the B2B2C front, the app could partner with insurance providers for risk assessment, integrate into senior living facilities and home health agencies to monitor aging populations, and be adapted into workplace wellness programs for early detection of cognitive strain or burnout.",
                        "github": "",
                        "url": "https://devpost.com/software/cogniverse"
                    },
                    {
                        "title": "Fingual",
                        "description": "Turning finger spelling into speech... Immediatly!",
                        "story": "Inspiration\nWe have one team member whose family member is deaf, and we learned more about the deaf culture through them. To most people, American Sign Language (ASL) is not a tool, but a culture, a part of identity itself. That's why we built something that celebrates and respects that depth. We aimed to offer a way for fingerspelling\u2014the ASL alphabet\u2014to be viewed and understood using technology, not to replace communication but to bridge the gap for those who are not yet able to understand it.What It Does\nFingual is a program that takes video input of one's fingerspelling and generates real-time text output. It utilizes OpenCV to track and label hand keypoints, a Convolutional Neural Network (CNN) for hand sign classification, and a generative AI layer to optimize the analysis for faster, more accurate prediction.How We Built It\nWe created Fingual from scratch with a mix of tools. Finger detection and tracking were done using OpenCV. The front-end was built in Xcode to make it have a good mobile experience, and the backend runs with Flask for video processing and AI inference. Our AI model was trained with TensorFlow/Keras from data we manually curated.Challenges We Ran Into\nWe're newbs, so just building a full-stack AI app was a learning experience. The hardest part, hands down, was obtaining good, usable ASL fingerspelling data. Our datasets were mostly incomplete or inconsistent. We shifted gears and built a mini dataset from scratch by filming, annotating, and training ourselves.Accomplishments That We're Proud Of\nWe're glad we didn't give up when we were stuck on the data problem. We came together, took sign readings ourselves, and built a functional model. It was like we had a mini-coding problem within our whole project, and solving it was a big success for us as a group.What We Learned\nWe learned about how to combine a front-end and back-end system, how to train and test AI models, and how important clean and well-formatted data is in machine learning. Most importantly, we learned how to be agile and work together under pressure.What's Next for Fingual\nWe plan to expand Fingual from ASL fingerspelling to other sign languages like BSL (British Sign Language) and LSF (French Sign Language). We also plan to increase accuracy, mobile-performance optimize, and eventually extend to full-word or sentence recognition\u2014not just the alphabet.",
                        "github": "https://github.com/mounty-ed/sign-language-translator",
                        "url": "https://devpost.com/software/sign-language-interpreter-app"
                    },
                    {
                        "title": "AI Therapist",
                        "description": "Come chat about anything with our new AI Therapist!!",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/ai-therapist-k5scdl"
                    },
                    {
                        "title": "StressAway",
                        "description": "Take control of your life before stress controls you!",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/stressaway-hw8xct"
                    },
                    {
                        "title": "Kinektor",
                        "description": "Kinektor, more than just an app, it's an ecosystem. Connect separate platforms (Discord, Minecraft, Webpage) seamlessly into one chat room with command access. Foster kinektion! ;)",
                        "story": "Inspiration: A problem with my friend group is that we play a lot of different games and dont end up talking together a lot. I wanted to create a system that would allow chats of many kinds to be linked together into one room.What it does: Currently, it integrates Minecraft, Discord, and a webpage all into one chat room. When you chat on one platform it sends it to the others for people to see. It essentially links chats together into one.Kinektion!How I built it: I started with the central server, I originally started in java but realized the needless complexity of it so I moved over to python/flask. The central server is the connection point for all connected platforms. When you send a message in discord, it goes to the central server and then is broadcasted to the entire network oh kineks. kinekts are the name of an individual connector, the discord and minecraft velocity plugin are both kinektsAfter getting as much as I could without 1 kinektc running, I started coding the Minecraft Velocity Plugin. It was all written in java and made it far more difficult as my poor laptop couldn't run both the central server and a minecraft server, so I offloaded it to a vps. It took around 11 hours straight to setup the website and Velocity kinekt, where i then moved on and setup the discord which only took an hour, I designed it around being easy to implement new platforms so all a kinect needs to supply is a incoming and outgoing contents of some kind of chat.I also made the logo myself, it just randomly came to me after 13 hours of straight codingChallenges I ran into: Python async being VERY annoying. I almost lost my mind just trying to fix an issue with having two async processes communicate with each other but I figured out a solution eventually after an hour or two.My laptop was terrible. 8gb of ram is not enough to run the central server, discord bot, minecraft proxy and backend and minecraft at the same time. I mean who could have foreseen that!Accomplishments that I am proud of: Its in a working state and the logo looks kinda cool ig idkWhat I learned: I learned alot about websockets in creating a website. I had made mostly static websites in the past and this was my first that shared information back and fourth. I had to learn how to be able to ping the client when a new message.What's next for Kinektor: More kinekts of course!...and more commands, commands that could pull information from the different platforms would change things greatly but it is not built currently for that. It would require more work for each kinekt which sort of defeats the purpose that they're supposed to be extremely simple.",
                        "github": "https://github.com/jbeast291/losaltoshacks",
                        "url": "https://devpost.com/software/kinektor"
                    },
                    {
                        "title": "Playbook",
                        "description": "Playbook is a modern app for high school basketball and volleyball athletes that uses AI to build workouts, match skill levels, and connect athletes for growth and recruitment.",
                        "story": "Inspiration: What it does: How we built it: Challenges we ran into: What we learned: What's next for Playbook:",
                        "github": "https://github.com/Tekkywek/Playbook",
                        "url": "https://devpost.com/software/playbook-dah3ol"
                    },
                    {
                        "title": "Lock In!",
                        "description": "Sometimes all you need to do is Lock In.",
                        "story": "Why we built Lock In!: In this modern day and age, teenagers have access to some of the most complex, and advanced tools at their fingertips. And what do we do with all this information?Doomscrolling, aka when we're stressed studying for an AP Calc exam and we promise ourselves \"Just 5 minutes on Instagram\" and end up spending 2 hours on Reels and needing to cram before the test. Lock In! provides users with an environment and the tools to focus and stay productive for long periods of time, making it easier to get work done, study, or finish assignments without procrastination.What it does: Lock In!is anall-in-one study and productivity appdesigned to help users stay focused, manage their study time, and take productive breaks. You can use thePomodoro method(study cycles with small breaks lead to less burnout!) to pace yourself throughout your study session. Don't have acalculatoron hand? Use ours instead of the one on your phone to minimize distractions. And instead of scrolling on social media during your breaks, Lock In! offers afun math quizto test your mental math that you can play during your breaks.  Feeling down? Click on thecrystal ballfor some motivation!How we built it: This program was built using Python in Replit. The modules we used included time for the Pomodoro timer and random to get random quotes and numbers for the calculator. The UI was built using Tkinter.Challenges we ran into: For the first time, we tried implementing an API into our code. We tried using theSpotify APIto write an Artist Search Function. We were going to use this function to display some recommended artists to listen to while studying. We got the Spotify API to run, but we could not get the functionality that we were looking for on our program.This is a feature we are still working on, and will hopefully be coming out in later updates!Accomplishments that we're proud of: This was our first time using pixel art, so we're pretty happy with the design that we got! We also managed to add 2 language modes apart from English: French and Spanish.What's next for Lock In!: Like we said earlier, we are going to keep working on this code to implement the Spotify API to be able to display recommended artists. In the future, we would also like to be able to play their music through tracks or playlists directly from the program.By being able to control the music directly from the program, it would minimize distractions for a more productive and focused environment.",
                        "github": "https://github.com/KMASH22/LockIn",
                        "url": "https://devpost.com/software/lock-in-vqnj9w"
                    },
                    {
                        "title": "Upskill-Me",
                        "description": "Connects students with personalized courses created by tutors, using subject-based matching.\r\nCentralizing searching, scheduling, and learning eliminates searching through scattered resources.",
                        "story": "Inspiration: The lack of personalized and accessible tutoring solutions for students inspired me, and I wanted to create a platform that empowers tutors to launch their classes and helps students find exactly what they need.  Solutions do exist, but those don't help if they're all decentralized (e.g. some posts on Reddit, others on a school platform, others elsewhere).What it does: Upskill-Me is a web app that connects students with tutors through a clean, interactive platform. Students can search and register for classes, view schedules on a calendar, and track their learning. Tutors can launch and manage classes, post announcements, and engage with students in a classroom-style interface. It suggests classes based on subject interests that a user fills out upon registration and then can always edit later, alongside other factors, such as time zone.How I built it: Used MERN stack:\nFrontend: React with SCSS theming for styling and component-based architecture.\nBackend: Node.js + Express for RESTful APIs.\nDatabase: MySQL for storing users, classes, and announcements.\nAlso implemented a multi-role login system, dynamic routing, and file upload support.Challenges I ran into: --- Setting up differentiated user experiences for tutors and students.\n--- Debugging form submissions and ensuring proper validation.\n--- Connecting dynamic frontend elements like calendars and class announcements to backend data.\n--- Posts & Image Uploads (eventually gave up on the idea)Accomplishments that I'm proud of: --- A clean, responsive UI with role-specific behavior.\n--- A scalable structure that was able to be successfully modularly iterated onWhat I learned: --- How to build a full-stack application from scratch with proper routing and data flow.\n--- Working with SQL joins to enrich class data with user information.\n--- Structuring a frontend to remain scalable and modular for future features.What's next for Upskill-Me: --- Properly Integrating Calendar\n--- Light/Dark Mode?\n--- Settings \n--- Adding live video integration for real-time classes.\n--- Notifications for class reminders and announcements.\n--- Expanding class search with filters by subject, time, tutor rating, and tens of more filters\n--- Analytics dashboard for tutors to track student engagement.",
                        "github": "https://github.com/pranshu-khanna/los-altos-tutor-project",
                        "url": "https://devpost.com/software/upskill-me"
                    },
                    {
                        "title": "WinVite",
                        "description": "Organizing meetups is hard. What location and time work with everyone? How will we pay? Is someone coming late? Winvite provides the answer to all of these questions and more on a unified platform.",
                        "story": "Inspiration: My friend group consists of a bunch of the most non-communicative people I know. Someone always doesn't know what time something is, someone is coming half an hour late, or someone isn't available at that time. Sure, some limited solutions exist. For example, when2meet.com helps a little with the scheduling. However, it's super confusing to use. And despite how much tech there was, I was surprised there wasn't a simple platform that implements all these features. I haven't found a single app that uses my virtual credit card process to help split bills.What it does: Uses MongoDB, Marqueta (a virtual card generation API), and Flask w/ React to create an easy-to-use but powerful app that lets users host events, share them easily with friends via a code, and coordinate everything in a meetup: location, time, bills, rides, ETAs, pictures, and event info.How I built it: I used Flask (a Python web framework) for the backend. This draws and stores information from and in MongoDB. For the frontend, I used Next.js, a React framework to create an intuitive frontend that just...works. For the virtual credit card generation, I used Marquetalink, and on top of that, I used OpenAI's API to generate random events based on the user's location. For the pictures, I encoded them using GridFS to make storage in MongoDB easier. To handle the payments for actually loading the virtual Marqueta credit card, I used Stripe, a payment processing platform used by companies such as Google, Amazon, and Shopify.Challenges I ran into: The biggest challenge I ran into was validating payments with Stripe and then allotting only the pre-loaded balance on the virtual credit card with Marqueta. It was tough to ensure everything was up to the security standards of Stripe, which is natural given two of the top five biggest companies use it.Accomplishments that we're proud of: I actually would use this app, and hope you would too. To be honest, I sometimes create things that don't have a real use case in hackathons: there already exist better or alternative solutions to the same problem. However, this is a real problem that I've researched, and I've found no real competitors for the position I hope to occupy in the market.What we learned: Sometimes, finding a great solution to a common problem is better than finding a 100%, works-every-time solution to a niche problem. Also, if there is a solution that works every time, there's probably a reason why it hasn't been pursued (often not that nobody has thought of it)What's next for WinVite: I want to add more features based on user feedback that let people connect in ways never seen before. I plan to add \"Groups,\" where hosts can pay $5/month to invite a bunch of people for multiple \"Events\".",
                        "github": "https://github.com/A2Pro/Winvite.git",
                        "url": "https://devpost.com/software/winvite"
                    },
                    {
                        "title": "DevBoost",
                        "description": "Real-time feedback, risk assessment, and gamified growth for your push requests",
                        "story": "Inspiration: I always hated publishing my code only to find outlaterthat I accidentally released an API key or some other stupid vulnerability, bug, or inefficient code. I thought how that would be avoidable and decided something needed to be made. That's were DevBoost comes inWhat it does: Imagine a QA team in a box. That's basically DevBoost. Have your code scrutinized for errors with Multi-Dimensional Scoring, Risk Classification, and a Developer XP System. DevBoost finds issues in push requests and then giveaccurate,traceable, andeasy to understandsuggestions based on your codebase. It ranks you in various dimensions such as Readability, Maintainability, Performance impact, Security vulnerabilities, and Business risk. It also finds security risks and vulnerabilities to protect data leaks. It allows you to collaborate with your team, because you can work with them. We also motivate developers to work harder by having a gamified leaderboard, which encourages people to get on the podium.How we built it: My tech stack/test stack consisted of: Next.Js, ShadCn-UI, Postman, GitHub API, GPT API, Flask, and Figma. Each of these tools served different purposes that eventually created DevBoostChallenges we ran into: Connection with frontend and backend was difficult. Also working with Github web hooks is always a challenge. On top of that CORS errors kept occurring, and ensuring model accuracy was also difficult.Accomplishments that we're proud of: A slick UI, relatively fast generation, and making something so complicated, alone, and within 24 hrsWhat we learned: How web hooks work, CORS, Postman, and Risk Classification.What's next for DevBoost: DevBoost needs to have a more robust system with greater analytics and higher quality models.",
                        "github": "https://github.com/c9dn/DevBoost-Backend",
                        "url": "https://devpost.com/software/devboost-0wfxzt"
                    },
                    {
                        "title": "Sustanance Sentry",
                        "description": "Ever been concerned about going to a restaurant and not knowing if they serve food that can acommodate you? That's where Sustanance Sentry comes in, providing full transparancy about food and dining.",
                        "story": "",
                        "github": "https://github.com/adityaSureshCHS/SustanenceSentry",
                        "url": "https://devpost.com/software/sustanance-sentry"
                    },
                    {
                        "title": "PoliticsForYouth",
                        "description": "High schoolers lack political education but are part of the political world immediately after graduation. PoliticsForYouth, a simulation based on current events, prepares them for this world.",
                        "story": "Inspiration: Throughout history, we\u2019ve seen how politics can impact every facet of our lives. Just the recent debates surrounding the Department of Education budget cuts show us how a single decision can cost hundreds of millions of dollars and affect people across the country.During the 2024 presidential elections, we were surprised by the lack of political awareness among our peers. After sending out surveys, we realized 75% of students were interested in politics but did not pursue their interest because of a lack of resources.Schools avoid discussions around political topics, and communities often expose students to a single ideology. In addition, news is often biased and requires prior knowledge, making it unappealing to youth. The low political awareness among youth means a higher chance of continued low engagement as they begin to vote. This leads to uninformed voters, low voter turnout, and political polarization, among various other problems.An issue rooted in the dullness of existing sources demands a creative solution, inspiring our president simulation game!What it does: PoliticsForYouth is a game that guides users through the process of making decisions that affect a variety of sectors of society. It introduces players to the types of actions taken by the US executive government and points them to the impacts of actions based on historical trends. Realistic scenarios with uncertainty and variability keep the game interesting even after hundreds of plays! It's not unlikely to be presented with an option you'll never see again.The player starts by choosing one of five actions that have been featured in the media in the past year. From there, we predict the potential impacts and use the severity of those impacts to decrease or increase a player's economy, war risk, and social welfare scores. In order to win the game, a player has to maintain a an economy and social welfare score above 90 and a war risk score below 10 for three turns, navigating risky decisions and surprise events (like a pandemic). If a player ever escalates to war (war score above 90) or their economy or social welfare scores drop below 10, the player automatically loses. Each game promises valuable context for current events and a different spin from any previous rounds.How we built it: We started by generating potential actions based on the past year's political discourse, using gpt-3.5-turbo, to seed the first five actions for a given game. From there, we generated lists of potential impacts, based on historical trends and the action's categories (like how tariffs are going to disproportionately effect the economy), and calculated probabilities of different impacts occurring to increase the variability in which impacts would occur. We combined these into an algorithm that repeatedly generates a logically and politically relevant set of impacts following from each decision the user makes.Our main goal was to increase relatability and realism, so we set up random events (like climate change, and pandemics) that change player's scores at varying times, simulating unforeseen events that occur in real life. To improve accuracy, we trained a classifier model on a few datasets to more accurately predict political leanings of actions, ensure the actions were based on current discourse, and influence the ensuing impacts from a given option.Lastly, we used React.js for the UI, integrating it with Python and our classification model to run our code.Challenges we ran into: The main challenge we ran into was how different this project was from any of the other projects any of us have done. This was the first time we created a game or a website -- we're mostly used to coding problems and task-oriented projects -- so there was quite a lot of trial and error that went into logistics for the game and planning out the best event flow.We pretty much had to learn React.js on the fly since none of us had much experience with web development!Accomplishments that we're proud of: We're proud of how much we were able to achieve with relatively little experience and time! The fact that the generations are so accurate and constrained to prevent LLM hallucination in the action generation is a huge accomplishment for us, since it ensures that our game is variable while preventing false generations.We were especially proud of our integration of generative AI and ground-up classification to achieve variable actions and impacts based on current events, surpassing capabilities of current simulations.What we learned: We learned a lot of coding subtasks, from writing up websites to creating games. We also learned a lot about integrating moving parts, like APIs and ground-up models into React, along with logistics for the simulation itself. Most notably, we learned about how to identify biases in data and see how they affect the outputs of the model. It is really difficult to procure a clean and comprehensive dataset, and even when you do it is very difficult to rid it of any intrinsic biases.What's next for PoliticsForYouth: We do plan on bringing our existing features to improved accuracy and better fluidity. After that, we plan on expanding the features in our game, potentially representing countries and making it multi-player (almost like a more-complex Risk game). We also plan on expanding to a set of games adapted from existing games (like HeadsUp) under a similar theme.",
                        "github": "",
                        "url": "https://devpost.com/software/politicsforyouth"
                    },
                    {
                        "title": "SlackShot",
                        "description": "Ever catch yourself on your phone when you are supposed to be working? I have. We wanted to help people hold themselves accountable. This website uses generative AI to catch any slacking...",
                        "story": "Inspiration: Looking around, during this event, we saw many people distracted on their phones! But what if there was a website that alerted you to stop?What it does: SnapShot captures an image of the user every couple of seconds to check on their productivity. An alarm goes off if they are caught slacking on their phone\u2026How we built it: We combined google\u2019s Gemini generative AI with a real-time camera that is recording the user. Gemini analyzes the data and SlackShot gives a warning if a phone is visible.Challenges we ran into: Getting started was the hardest part. It took time to come up with our idea, and features we wanted. All the features individually were not the challenge, but fitting them with each other.Accomplishments that we're proud of: We first got the camera working which was a big step forward. And we successfully connected Gemini to analyze the live image feed. Our prototype detects phone usage and triggers a warning when needed & an alarm. We also collaborated really well as a team, especially under time pressure!What we learned: We learned how to integrate real-time camera input with AI analysis. This taught us valuable lessons in breaking big ideas into small, doable steps; it helped to keep us on track. We picked up new troubleshooting skills, especially around API and hardware issues. Teamwork and communication turned out to be just as important as coding.What's next for SlackShot: We want to make SlackShot be able to infer there being a phone out of frame judging off posture, facial expression, etc. And we will make it detect distraction more generally: not just by checking phone use. Additionally, we have considered a point system for staying on task as well as suggestions from the website to take a break when the user appears more fatigued.",
                        "github": "",
                        "url": "https://devpost.com/software/slackshot"
                    },
                    {
                        "title": "APTutor",
                        "description": "An apt student is a quick learner. We've all encountered an apt student who spends grueling hours studying on end. APTutor makes studying for APs easier by providing an AP resource bank for students. ",
                        "story": "Inspiration: As AP students, we often struggle to find reliable and organized study resources. We created APTutor to help students like us study more efficiently.What it does: APTutor is a web app that provides curated AP study materials, including practice questions and notes. Users can filter by subject and topic to personalize their learning. Each AP has their own tips and tricks to help students improve on their exams.How we built it: We used javascript to design and build the app.Challenges we ran into: Some challenges we ran into were time constraints during development and figuring out how to organize and tag AP content.Accomplishments that we're proud of: Some accomplishments that we're proud of are building a working prototype in a short time, designing a clean and user-friendly interface, and solving a problem we personally care aboutWhat we learned: We learned how to apply hyperlinks into javascript's app lab.What's next for APTutor: We would like to improve APTutor by adding more subjects and content like AP foreign languages, improving user experience and test with real students, and improving practice question multiple choice bank.",
                        "github": "",
                        "url": "https://devpost.com/software/aptutor"
                    },
                    {
                        "title": "Screen Score",
                        "description": "Screen Score helps you reflect on app usage by grading digital experiences like a nutrition label \u2014 fun, insightful, and built for mindful scrolling.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/screen-score"
                    },
                    {
                        "title": "Floppy Fish",
                        "description": "A physics-based fish survival game where you flop, spin, and squirm across land in search of water. Stay wet, stay alive \u2014 or dry out trying.",
                        "story": "Inspiration: After listening to the speech given by one of the Juniper Networks associates, our team wanted to build something unique that plays a special role in our hearts. Rather than build a generic LLM wrapper and call it, \"Artificial Intelligence\", we sought to take our childhood nostalgia and build a game centered on this aspect. A fun simple game that centers on a unique control system, Floppy Fish provides an exciting experience for all users.What it does: Floppy Fish is a game that recreates nostalgic feeling, combining a fun aspect with a funny and unique playthrough. In the game, the player is a fish who aims to complete several levels while battling obstacles such as dehydration and rocks.How we built it: Built on the Unity Engine, we made custom physics framework with dynamically adjusted torque for its unique movement system. Instead of scaling delta time to each physics call, physics is calculated per frame and scaled using fixed update. For movement in air, we used relative quaternions for adjusting the anchor pints for in-air torque, and for water, we implemented spherical linear interpolation to dynamically adjust the local rotation quaternion based on the normalized linear velocity vector. Floppy fish has a custom post processing pipeline based on Universal Render Pipeline, and all animations were done procedurally using randomized torque and collision impact velocity. For example, if the fish falls really hard on the sand, the local scale will linearly interpolate to a higher magnitude. For the background, we used a dual-layered parallax effect for the clouds to create a sense of distance and depth. Lastly Floppy Fish has a custom particle effect system, and all assets were created during the competition. These custom components add up to the captivating effect that Floppy Fish has on all its users.Challenges we ran into: When developing Floppy Fish, we ran into a glaring problem: our game had no assets to make it look aesthetic or visually pleasing. Using Krita, we had to undergo the grueling process of creating assets by hand, giving our game a homemade feel. The physics that powered the fish movement had to be tailored and took several hours to polish.Accomplishments that we're proud of: The team that accomplished Floppy Fish feel incredibly proud for making a polished game in under 24 hours-- an undertaking that usually takes a week to come up with an idea and execute.  We are proud of everything from the smallest particle to making our own custom physics system.What we learned: How to design a physics-based character controller that feels dynamic and fun\nFine-tuning movement between different environments (land vs. water)\nUsing animation tricks (squash/stretch) to make gameplay feel more alive\nManaging transitions between scenes while preserving audio and player state\nCreating a modular system for character states like \u201cWater\u201d and \u201cLand\u201d\nMaking quick polish passes like screen shake, particles, and sound to boost immersionWhat's next for Floppy Fish: Floppy Fish is just the beginning of a much bigger splash. Moving forward, we plan to expand the game with new environments, like toxic sewers and deep-sea caves, each filled with unique hazards and obstacles. We're also adding power-ups, unlockable fish skins, and challenging enemy types to keep gameplay fresh and exciting. A global leaderboard, mobile support, and community-driven features are also on the horizon. Floppy Fish may start small, but it\u2019s diving into a future full of fun, chaos, and creativity!",
                        "github": "",
                        "url": "https://devpost.com/software/floppy-fish-c5admb"
                    },
                    {
                        "title": "Giving Tree",
                        "description": "Turning surplus fruit into hope\u2014connecting homeowners, volunteers, and charity to reduce waste and fight hunger.",
                        "story": "Inspiration: Many of us have passed by trees or backyards overflowing with fruit, only to see it left to rot. For us, the inspiration struck close to home\u2014both of our own families have lemon trees that produce more fruit than we can use, with much of it sadly going to waste. This common problem sparked our mission to turn surplus produce into something meaningful and impactful.What it does: Our platform connects homeowners and volunteers to collect excess produce, which is then sold to individuals who can make better use of it, reducing food waste and addressing hunger in our communities.How we built it: We built the Giving Tree Marketplace to connect homeowners with fruit trees to people in need, reducing food waste. The site was designed to be user-friendly with a clean interface, featuring a simple product grid and easy navigation. Our focus was on creating a seamless experience for donors and buyers, using inviting colors and intuitive design elements. The goal was to make donating and purchasing fresh produce easy and impactful for the community.Challenges we ran into: One of the biggest challenges was linking the marketplace, cart, and payment pages using JavaScript, as local data sometimes failed to save. Additionally, the large number of files and images slowed our site\u2019s load times, making it difficult to implement quick changes and delaying updates.Accomplishments that we're proud of: We\u2019re incredibly proud of the features we\u2019ve built and the impact they can have. Despite the challenges, we successfully integrated a fully functional cart and payment system, enabling seamless transactions for both donors and customers. One of the standout features of our website is the intuitive volunteer sign-up system, which makes it easy for community members to get involved and help with collection efforts. Additionally, we created user-friendly forms for both donators and volunteers, allowing for smooth donations and participation. These features not only showcase the platform\u2019s usability but also reflect our commitment to making food redistribution simple and accessible. With these key components in place, our website is ready to be used in the real world, driving tangible change in reducing food waste and alleviating hunger.What we learned: This project taught us the immense power of community collaboration in tackling issues like food waste and hunger. We discovered that by connecting people and resources, even small actions can lead to significant, lasting change.What's next for Giving Tree: Looking ahead, we plan to use AI to streamline connections between donators, volunteers, and customers, based on their availability and location. We\u2019ll introduce a sorted marketplace that prioritizes produce with higher availability or shorter shelf lives. Additionally, AI will track user activity and send personalized email reminders when familiar fruits are in season, encouraging engagement and driving even more impact.",
                        "github": "",
                        "url": "https://devpost.com/software/giving-tree-n19pay"
                    },
                    {
                        "title": "Worldwide Weather Data Watch",
                        "description": "Empowering safety through weather aware insights",
                        "story": "Inspiration: Weather app with deeper interaction that does more than just give numbers.: What it does: Accepts a location from anywhere in the world and responds with corresponding weather conditions, as well as advice for what to do.: How we built it: We used python and html on VSCode for the majority of the project, and also utilized a basic chatbot, cohere, and implemented it into the UI.: Challenges we ran into: Due to lack of domain promo code, we were unable to link our project to a website. We also had trouble thinking of a project and learning different software to develop our program.: Accomplishments that we're proud of: Our project eventually working: What we learned: Basic Palantir AIP: What's next for Worldwide Weather Data Watch::",
                        "github": "https://github.com/papa-tree/World-Weather-Data-Watch/commit/a78584d490c47417a880ebeae7959e7bc6d8082f",
                        "url": "https://devpost.com/software/worldwide-weather-data-watch"
                    }
                ],
                [
                    {
                        "title": "FitMotion",
                        "description": "Ascend beyond your traditional workout AI. Literally. Measure stability using multiple sources of flow from multiple devices, linked together.",
                        "story": "Inspiration: My parents often feel like they should start exercising. But due to the lack of support or just inconvenience, they often tend to just give up. So we decided to build an app with them in consideration, to help them pursue their fitness goals on their own.What it does: The app merges computer vision with acceleration data from our phones to provide an in-depth and comprehensive analysis of your workout, which you can personally choose.How we built it: To build this app, we needed captured accurate body positioning data from our computer's camera, along with accelerometer data, which we sent to a computer for processing. We use machine learning to analyze the user's form, and return a summary of the workout.Challenges we ran into: We ran into a number of problems integrating phone accelerometer input with computer visionAccomplishments that we're proud of: We're quite proud of linking our Phone's supplementary data to our computer for Real-Time InputWhat's next for FitMotion: We would love to add support for account data between visits, so users have a stronger sense of continuity. We also want to add a better system for detecting errors in the user's form",
                        "github": "",
                        "url": "https://devpost.com/software/axcel"
                    },
                    {
                        "title": "Kolas Productivity Tool",
                        "description": "Kolas Productivity Tool is a fusion app where users can not only get work done, but also prioritize mental health by avoiding burnout. Kolas utilizes a pomodoro timer, breathing bubble, and mnemonics.",
                        "story": "Inspiration: I was inspired from my personal experience with missing school and feeling burnt out after being handed a mountain of make up work - which made me feel unmotivated to catch up.What it does: Kolas Productivity Tools is a Study Tool focused on mental health. It utilizes the Pomodoro study method method which is where you study for 25 minutes and take a 5 minute break. It also has a breathing bubble to relax and mnemonics generator to help you memorize terms.How we built it: CSSHTMLJavaScriptjson to store mnemonic local data to view past mnemonicsClaud AI helped me simulate what a Gemini API would have doneJSON file to store local data\n## Challenges we ran intoGemini API in Javascript is a cybersecurity risk due to inspect elementlack of time/sleep\n## Accomplishments that we're proud ofthe code actually works!the timer!\n## What we learnednode.js is a backendJSON to store local data\n## What's next for Kolas Productivity ToolLogin pageAI Chatbot (w/ no cybersecurity risks),",
                        "github": "",
                        "url": "https://devpost.com/software/kolas-productivity-tool"
                    },
                    {
                        "title": "Cozy Home",
                        "description": "A calming game where you complete tasks in your own home",
                        "story": "InspirationWe were inspired by the comforting and cozy atmosphere of games like Animal Crossing and Stardew Valley. During stressful times, these games offer a sense of peace, routine, and joy through small, satisfying tasks. We wanted to create a similar experience in the browser\u2014something light-hearted and interactive that makes you smile while checking off tiny achievements.What it doesCozy Home is a browser-based, interactive task game where players can complete relaxing, everyday activities like cooking, feeding a dog, watering flowers, collecting apples, and more\u2014using fun drag-and-drop mechanics with emoji icons and gentle sound effects. The game features a daily log/calendar, weather-based events, achievements, and seasonal visuals (like falling snow or autumn leaves). Players can save their progress locally and earn badges for completing streaks or trying new tasks.How we built itWe used HTML, CSS, and JavaScript to build the core experience, with localStorage for saving game data and a combination of event listeners and DOM manipulation for the drag-and-drop functionality. Custom sound effects and weather/seasonal logic were added to enhance immersion. We iterated quickly with visual tweaks, such as scaling flower sizes and simplifying the UI to improve the experience.Challenges we ran intoOne challenge was creating intuitive drag-and-drop interactions that worked smoothly across browsers. We also had to balance a minimalistic UI with enough feedback to keep players engaged. Implementing a robust save/load system and syncing it with the calendar and badge system took more time than expected. And as the feature list grew, it became a challenge to keep things lightweight and cozy without feature creep.Accomplishments that we're proud ofWe're proud of how charming and functional the final game feels! The sound effects, subtle animations, and seasonal atmosphere really bring it to life. We also feel great about adding features like task streaks and weather-based changes without overcomplicating the experience. Most importantly, it makes people smile\u2014and that was our #1 goal.What we learnedWe learned how to scope and manage a growing project while staying focused on user experience. We deepened our knowledge of local storage, state management, and accessible drag-and-drop interactions in JavaScript. We also learned how small visual/audio touches can drastically change how users feel about a product.What's next for Cozy HomeWe plan to add more interactive items and mini-events (like planting a garden or decorating your space), as well as a sharing feature to show off your Cozy Home with friends. We'd also love to introduce more characters and daily surprises to keep players coming back. Eventually, we might explore porting it to mobile or turning it into a full progressive web app.",
                        "github": "",
                        "url": "https://devpost.com/software/study-space-j7i6zf"
                    },
                    {
                        "title": "3D Modeling",
                        "description": "Modeling 3D shapes... with a twist. It's all done on scratch.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/4d-modeling"
                    },
                    {
                        "title": "Moody Music Finder",
                        "description": "Songs based on your Mood",
                        "story": "Inspiration: Have you ever felt overwhelmed or just needed the perfect song to match your mood? Sometimes, the right track makes us feel heard\u2014or uplifted\u2014just when we need it most. That\u2019s why I built the Mood Music Finder: a quick way to choose your mood and see matching songs in an instant.What it does: The Mood Music Finder is a simple interactive tool that lets you pick a mood\u2014from happy to sad to energetic\u2014and instantly returns a list of songs that match how you\u2019re feeling. It\u2019s like having a personal DJ who understands your vibe at any given moment.How I built it: How we built it\nData Setup: I started with a small, hardcoded table of songs, each mapped to a specific mood.\nPandas & Filtering: We used Pandas to load that data into a DataFrame, making filtering by mood quick and straightforward.\nInteractive UI: Using Python\u2019s ipywidgets, I created a dropdown to select a mood and a button to trigger the display of matching songs. This approach keeps everything contained within a Jupyter or IPython environment, so users can see the results immediately.Challenges I ran into: It took a bit of experimenting with different libraries to make the UI friendly and easy for anyone to use.\nMaking sure the moods matched in a case-insensitive way was a small but important detail\u2014because nobody likes \u201chappy\u201d being different from \u201cHappy.\u201d\nI also tried integrating the Palantir OSDK for a more dynamic data retrieval process, but ran into technical hurdles\u2014like missing session references and strict expression requirements\u2014that slowed things down. Ultimately, I reverted to the simpler, hardcoded approach to keep the project moving.Accomplishments that we're proud of: Smooth, Interactive Experience: Despite being a small demo, the dropdown and button flow feels clean and intuitive.Immediate Feedback: Seeing results update instantly is gratifying and makes it easy for anyone to use, even non-technical folks.What I learned: Pandas & Data Handling: Filtering data in a user-friendly way taught us just how powerful (and easy) pandas can be for small-scale lookups.\nUI Design Considerations: Even in a notebook setting, clarity and feedback matter\u2014like displaying messages for empty results or stylizing the output table.\nExperimentation with OSDK: Though we faced obstacles, we learned how strict some third-party libraries can be in terms of syntax and configuration.What's next for Moody Music: Bigger Library: Expanding the song list to cover more moods, genres, or even user-submitted tunes.\nMachine Learning: Incorporating recommendation algorithms that factor in tempo, lyrics, or user preferences.\nPalantir OSDK Integration: Revisit the Palantir approach to create a truly dynamic, continually updating song library that could pull from multiple datasets in real-time.",
                        "github": "",
                        "url": "https://devpost.com/software/moody-music-e6nhlq"
                    },
                    {
                        "title": "FlameSense",
                        "description": "FlameSense is a wildfire spread prediction tool utilizing neural networks, real-time weather, and live fire data to determine where and how a fire will spread next.",
                        "story": "Inspiration: Many of our relatives and friends lived in LA and were impacted by the horrible Palisades wildfire. We wanted to create this tool to help first responders plan and allocate resources against wildfires as well as inform the general public about ongoing fires.What it does: FlameSense allows users to simulate the spread of current wildfires as well as in chosen areas. It utilizes a sequential neural network trained on historical fire spread data, along with temperature, humidity, precipitation, wind speed/gusts, and water vapor levels. When the user clicks on a spot on the map or chooses to simulate a current wildfire, the data at that point is fed into the trained model, and a percentage growth is output. The fire spread is then rendered on the map as a heat map, and the path of the fire is visualized taking into account conditions like wind.How we built it: Firstly, we gathered current and historical wildfire and condition data from various sources such as NASA FIRMS, Open Mateo, and the Canadian Fire Spread Dataset. We then used Palantir's tools to clean and transform the data. We then used the cleaned data to train a model in Palantir, which would take in current data inputs, such as humidity, temperature, dryness, and biomass, and output a predicted fire spread rate. We then built an API endpoint to take in the necessary parameters and plug it into the model. Finally, we built a front end in HTML that displays a map and animates how the fire will spread based on the location.Challenges we ran into: Palantir was hard to learn as it was a complex platform with many aspects to learn. The hardest part was figuring out how to access the model and deploying it. The other hard part was training the model. Due to limited time, the model performed well, but wasn't as accurate as it could be.Accomplishments that we're proud of: We loved talking with the Palantir mentors, as they were really helpful in assisting us with deploying our function. We are most proud of the fact that we made an accurate model that can help millions in the face of wildfires in an easy to navigate way.What we learned: We learned how to use Palantir as well as how to filter through many data sources and effectively build a model that predicts fire spread. We learned how to connect things like data, the model, and the actual website together in a full-fledged visual application.What's next for FlameSense: We plan to expand our model and train it with more data to make more accurate predictions as well as use more features of Palantir to enhance its functionality.",
                        "github": "",
                        "url": "https://devpost.com/software/flamesense"
                    },
                    {
                        "title": "Bias",
                        "description": "Bridge the gap for eqaulity one detection at a time.",
                        "story": "Inspiration: Racial inequality is a real and prevalent problem that affects almost all minorities. Don\u2019t just take my word for it according towww.kff.orgThree in ten Black (31%) and ASIAN adults (28%) and about one in four Hispanic (26%) and Asian adults (25%) say they experienced at least two of these types of discrimination at least a few times in the past year, all higher than the share of White adults who say so (18%). Not only that but minorities are under-reported in the news and bias of one races supremacy over another is still prevalent. In fact, according to the national institute of health A majority of black adults reported experiencing discrimination in employment (57 percent in obtaining equal pay/promotions; 56 percent in applying for jobs)What it does: My final solution is a customtkinter frontend that is able to detect bias given by input in a textbox. The user then gets a bias score were higher is lower. This feature allows the user to see the models confidence of its prediction.  The program also then concludes weather the text is biased or not. Next, the program gives unbiased articles as recommendations to check out. Lastly, the program gives recommendations for fixing bias in the text in bullet points.How we built it: I used pytorch for the NLP (to specify the exact model I used was an STLM).  The NLP gets an input form the user. Next it determines whether the text has bias or doesn't have any bias. After that if it has bias another ai this time a LLM (Llama) goes over the text and gives recommendations to remove bias. Another feature then takes a summary of the text and converts it to a likely headline. Which is added to an fstring which is the website link that will be given to the user. Lastly NLP also gives a confidence meter which is displayed to show the user how biased the text is.Challenges we ran into: Trying different tranformers such as BERT and variations of BERT. However they wouldn't work. Debugging issues and increasing accuracy. I had to combat data overfitting, class imbalance and utilizing weight optimizers.Accomplishments that we're proud of: Creating an NLP model form scratch and a complicated one that improved my knowledge of machine learning and advanced math topics.What we learned: How to create NLP's, different techniques to increase accuracy and some of the math behind machine learning models.What's next for Bias: A better NLP using transformers to increase accuracy and a bigger dataset than just hugging face.  Also experimenting with different weight optimizers would be great to find the optimal weight optimizers for accuracy.",
                        "github": "https://github.com/ProgrammingwithPI/ideal_NPL",
                        "url": "https://devpost.com/software/bias-n9fkqi"
                    },
                    {
                        "title": "Consule.AI",
                        "description": "Are you to worried what someone will think about your messages Well not anymore because Consule has your back",
                        "story": "",
                        "github": "https://github.com/Shoury-ux/LA/tree/thing'",
                        "url": "https://devpost.com/software/consule-ai"
                    },
                    {
                        "title": "Mindhaven AI",
                        "description": "Helps Individuals find mental support.",
                        "story": "About the Project\nThis project is a question-answering assistant that helps people find mental health centers based on real information stored in PDF documents. It uses Google\u2019s Gemini 2.0 Flash model to generate accurate and helpful responses, even when the documents don\u2019t contain a direct match. The assistant is designed to focus only on actual mental health services, not general or unrelated information.What Inspired Us\nThe idea came from seeing how hard it is for people to find reliable mental health resources\u2014especially when the information is buried in large, unstructured documents or scattered across different websites. I wanted to build something that made that process easier and more direct for anyone who needs it.What We Learned\nThis project taught me how to connect generative AI with real-world data in a meaningful way. I learned how to structure workflows in Palantir, work with embeddings, and fine-tune prompts to get accurate results from Gemini. I also gained experience in handling PDF parsing, building retrieval systems, and dealing with challenges like missing or inconsistent data.How We Built It\nI used Palantir Foundry to load and manage the source documents, which were PDF lists of mental health centers. I created a workflow in Palantir to preprocess and store the data in an object set. From there, I built a system that queries this data using semantic search and sends relevant context into Gemini 2.0 Flash via Google\u2019s Generative AI API. When context isn\u2019t available, the system falls back on Gemini\u2019s general knowledge to provide suggestions based on location. The whole setup is connected to a simple interface that accepts user questions and displays helpful, location-based responses.",
                        "github": "",
                        "url": "https://devpost.com/software/mindhaven-ai"
                    },
                    {
                        "title": "QuickDish",
                        "description": "An AI cooking companion that can give you recipes for any food depending on people you are serving and cooking ability that you have.",
                        "story": "Inspiration:: We all love food, but sometimes it can be difficult to find the right recipe. You might find some that are too complicated for our skill level, or your recipe might serve a vastly different amount of people than you were planning for and you need to recalculate all of your ingredient amounts. Because of this, we decided to create an AI cooking assistant that can help you find the perfect recipe for your needs.What it does:: QuickDish creates a recipe depending on parameters such as recipe complexity, dietary restrictions, number of people you are serving, and type of meal. With these parameters, the AI will pull from different sources to create a recipe that is just right for you. It also has an image recognition mode where it can see the ingredients you have available and create a recipe based on them. If you enjoy any of the recipes, you can also save them for later use.How we built it:: We used Replit and VSCode to put together our code in Python. We used an OpenAI API to create the recipes and for the image recognition model. We also used Copilot to assist us with our code.Challenges we ran into:: We were unable to add the AI to the website directly, as we needed to pay to deploy it, and we faced challenges integrating an HTML website due to its complexity. We also tried to build our own image recognition AI using Palantir, but we did not entirely understand how the interface worked and found it too complicated for our current abilities.Accomplishments that we're proud of: We are proud of how much we have learned. We are also proud of creating an app that will help with our daily lives to solve a problem we have had for a long time.What we learned: We learned how to work with APIs like OpenAI's API in Python. We also learned the basics of working with Palantir Foundry, even if we couldn't fully utilize it. In addition, we learned how to work better under pressure in a hackathon environment.What's next for QuickDish: We hope to train the model to be able to see a picture of a food and pull up its recipe. We also want to fix the recipes that the AI produces so it can cross-check with other recipes on the internet to make better recipes. We hope to make this app bigger, and put it on the market for people to use in their daily lives.All of us are beginners, and we really enjoyed working on this project!",
                        "github": "",
                        "url": "https://devpost.com/software/quickdish"
                    },
                    {
                        "title": "HumanX",
                        "description": "Therapy today for free. Bring your friends.",
                        "story": "Inspiration: Ever since inflation came, prices for things keep going up. But when it comes for highschoolers, many come to the point where they need a therapist. Unfortunately  they have had it too expensive to buy a session.What it does: This is HumanX. Its a personal AI therapist that can help you with your emotions. Not only do you get your own private space to share your thoughts and add more spaces so that you can look back at the stress you got before, but you can also message HumanX in each chat, making it secure for itself.How we built it: To start off, we went to a website called Groq. From there, we decided, since none of the sources given were free, and Groq was free, we would use this. Basically, this would be our AI source. From there, we built on our code, from nothingness, to complexity, to emotional, to publicity. We not only brought it in the web, but also in discord. The discord bot would take user responses and questions and help the user if they are feeling down, or telling the user\u00a0to\u00a0keep\u00a0it\u00a0up.Challenges we ran into: We ran into errors that even we didn't know how to solve.  For example, we were using Groq API, like we mentioned above. But then the API key kept on hitting its 413 errors. That meant that it kept hitting the limit of the chatbot.Accomplishments that we're proud of: We were proud of the fact that we managed to create the AI with python, then we managed to implement the chatting feature to the web, and then we managed to implement the feature of being a therapist in your own chat bubble.What we learned: We learned that API keys which have high rate limits with low temperature rates would last a long time. Now we could adjust the temperature, meaning how much resources would it use to generate an answer. But with the high rate limit, we were able to into worry about timing out the AI.What's next for HumanX:",
                        "github": "https://github.com/kr91416/HumanX",
                        "url": "https://devpost.com/software/humanx"
                    },
                    {
                        "title": "Clairo",
                        "description": "Clairo is an AI tool that helps users improve their clarity, pacing, and confidence in speech. By analyzing tone, pauses, and delivery, Clairo gives personalized feedback to improve speech.",
                        "story": "Inspiration: Inspired by Ishaan's personal experience with public speaking,What it does: The application takes in audio input from the user and outputs the number of times they include filler words and tips to improve the user's public speaking skills. A camera feature and playback feature are also integrated within the program to allow the user to see themselves while they are speaking, and will enable them to playback the video of themselves (with audio).How we built it: We built it using HTML, CSS, and JavaScript.Challenges we ran into: A few challenges we encountered were making our JavaScript compatible with the HTML code we had separately created. We also ran into integrating the Open AI API, as errors kept on showing up when we tried accessing the api.\nAPIs deepGram, elevenLabs, media recorderAccomplishments that we're proud of: Debugging stuff and learning languages on the run, such as JavaScript. We were also proud of getting the backend working with the API from ElevenLabs and DeepgramWhat we learned: We learned new languages, such as JavaScript, and how to more effectively debug code.What's next for Clairo: Expand into more features and in-depth analysis.",
                        "github": "",
                        "url": "https://devpost.com/software/clairo"
                    },
                    {
                        "title": "Reflective",
                        "description": "journal entries w/ reflective prompts",
                        "story": "Inspiration: Many people struggle with mental health problems and we believe that one way to help mitigate this problem is for people to express their thoughts on the day.What it does: Everyday, the user is able to input today's entry and then afterwards, they have a choice of selecting whether they would like a reflective prompt and if yes, a reflective question pops up under the today's entry. The user can reflect about their day and unpack the events.How we built it: We used swift and x-code for the entire thing so that we could constantly check.Challenges we ran into: All 3 of us had built this app for the first time and none of us were familiar with swift or creating an app.Accomplishments that we're proud of: We were able to create a finished product despite it being our first hackathon and we also learned a lot.What we learned: We learned how to create an app, configure github effectively to work efficiently with others, and working with extensions.What's next for Reflective: Hopefully, we can use more AI and advanced techniques to integrate more related reflective prompts and add more features than just journal entriees.",
                        "github": "https://github.com/terrancelouie/reflectivejournals.git",
                        "url": "https://devpost.com/software/clearglow"
                    },
                    {
                        "title": "Moody Foody",
                        "description": "Moody Foody: The AI That Turns Your Mood Into Your Next Meal.",
                        "story": "About: We built Moody Foody, an interactive AI-powered meal recommendation app. It lets users enter details like their mood, preferred cuisine, available ingredients, type of meal, whether they want a healthy option, dietary restrictions, cost, and even choose between AI-generated or famous chef recipes. All this information is sent to our Python Flask back end, which constructs a custom prompt and queries the Ollama API (using the Llama3 model) to generate creative meal options. The AI's suggestions are then displayed back on the same page, giving users personalized and innovative meal ideas.Inspiration: We were inspired by the idea that our emotions and preferences have a direct impact on our food choices. We wanted to create an interactive experience where users could simply express how they feel and what they\u2019re in the mood for, and let AI craft personalized meal recommendations. This project blends technology, creativity, and culinary art into one fun, innovative application.What We Learned: Full-Stack Development:We integrated a dynamic HTML/JavaScript front end with a Python Flask back end, learning how these components interact through APIs.API Integration:We discovered how to work with external APIs (using Ollama) to generate creative responses based on user inputs.Asynchronous Programming:Implementing AJAX calls using JavaScript fetch helped us understand asynchronous communication between client and server.User Experience Design:We refined our skills in designing an intuitive user interface that guides users through the process of inputting data and receiving real-time responses.,How We Built the Project: Challenges Faced: API Response Handling:Integrating with the Ollama API and ensuring that responses are correctly parsed and handled was challenging at first.Dynamic UI Updates:Managing asynchronous data updates without refreshing the page required careful handling of JavaScript promises and DOM manipulation.Input Validation:Ensuring the app handled missing or incomplete data gracefully and provided useful feedback to the user was an important learning curve.Debugging:Tracking down bugs that spanned both the front end and back end improved our debugging skills and taught us how to better structure our code for maintainability.,Overall, this project was a rewarding blend of creativity and technology that pushed us to learn new skills and rethink how everyday choices\u2014like what to eat\u2014can be enhanced by AI.",
                        "github": "https://github.com/Beastmaster3055/lah",
                        "url": "https://devpost.com/software/moody-foody"
                    },
                    {
                        "title": "Stranded in Space",
                        "description": "The game is about a astronaut stranded in space, who has to repair his ship and return home. He does the steps to repair his ship through mini-games, like jigsaw puzzles and card matching.",
                        "story": "Inspiration## Inspiration: We initially wanted to create a space themed game, so we took inspiration from common space based games like space invaders. However, we wanted to make a slightly story based game, so we decided to incorporate some other challenges. One of them was meant to be users building a ship, so we took inspiration from typical jigsaw games and slightly modified it to make it more interesting.What it does: Our game immerses the user as an astronaut lost in space, making it past three difficult games to head home. In the first game, the astronaut looks for missing supplies, needing to match all pairs of pictures to move on. In the second game, the astronaut must reassemble the navigation screen through a puzzle. Finally, in the last challenge, the astronaut must skillfully dodge obstacles and make it home.How we built it: We created this project through using pygame and generative AI. Pygame allowed us to create screens, have different sprites interact with each other, as well as having buttons that redirected to different pages. By using generative AI, we were able to get a basic idea of a code frame for the game, and then repurpose it for our needs. All of the pixelated game graphs were done by hand.Challenges we ran into: Some challenges we ran into were learning how to use the graphical elements of pygame. For example, when coding our 3rd minigame, we ran into a problem where our hitboxes for the ship and the asteroids were too bigAccomplishments that we're proud of: Our team is proud of learning to use pygame and ultimately creating a cohesive project. All of us had very limited experience with pygame before, and it took countless trials and errors to achieve our intended result. We also divided up the work very well, each working on a game and then putting it together with an introduction screen.What we learned: This was all of our first times creating a game in Python, so it was a great experience learning how to use the pygame interface and its different functionalities.What's next for Stranded in Space: Moving on, we hope to continue our exploration of Python\u2019s pygame module, creating more interactive and challenging levels for the user to progress through. We also strive to experiment with further usage of AI in writing code, whether it\u2019s debugging, generating pictures for our game, or explaining functions of pygame.",
                        "github": "https://github.com/at350/lahix",
                        "url": "https://devpost.com/software/stranded-in-space"
                    },
                    {
                        "title": "Flower Focus",
                        "description": "Many students struggle with procrastination and stress. By using flower focus, students are able to visualize their \"stress\" and start work ahead of time to reduce the load.",
                        "story": "Inspiration:: I really liked focus apps and Pomodoro timers, so I thought it would be nice to combine the focus timers and todo lists with mental health awareness.What it does: This app combines todo lists, pomodoro timers, and stress management into one handy app, allowing users to predict potential stress points and start working on tasks ahead of time to prevent high stress moments in the future.How I built it: React js + Vite for the frontendChallenges we ran into: The flower petals were very difficult to align with the flower center.Accomplishments that we're proud of: Having a clean todo list and pomodoro timer.What we learned: Lock in next hackathon, Learn more about the program so you are able to do more.What's next for Flower Focus: Nothing yet.",
                        "github": "",
                        "url": "https://devpost.com/software/flower-focus"
                    },
                    {
                        "title": "MoodMate",
                        "description": "This app helps individuals with autism understand the feelings of those around them through real-time facial recognition, which can make social interactions smoother and more inclusive.",
                        "story": "Inspiration: Some of my friends and friends of friends have autism, and they were in mind when I was creating this project. For many people on the autism spectrum, recognizing and interpreting facial expressions is a challenge that often causes them to be misunderstood. Social cues that may seem intuitive to others\u2014like a subtle smile or a furrowed brow\u2014can be confusing or go unnoticed, leading to misunderstandings, feelings of isolation, or anxiety in social situations. These difficulties aren't a reflection of intelligence or empathy, but simply a different way of processing the world.My app uses real-time facial recognition and emotion detection to act as a bridge between expression and understanding. By analyzing facial cues and translating them into clear information like emojis and word, the app helps users better grasp the emotional context of their surroundings. The app is simple to use and can be used in a variety of public or private situations such as at home or in a social gathering to make communication easier.What it does: My app allows the user to detect 5 different emotions of a human face in real time--happiness, sadness, anger, surprise, and neutral, providing a simple and clear interface to facilitate smooth communication for individuals with autism.How we built it: The app is made entirely in html, css, and javascript, using the ml5 facemesh library to obtain the coordinates of the various points on a face and using geometry to determine when the position of these points corresponds to the expression of a particular emotion.Challenges we ran into: Determining which points/areas of the face to use and how to manipulate them to detect various emotions was definitely the main challenge in making this app.Accomplishments that we're proud of: I have never worked with facial landmarking before, and I am proud to have gotten this emotion recognition working decently well by myself over the course of these 24 hours.What we learned: Emotions and facial expressions are extremely complex, and we should be mindful of the difficulties some face interpreting these complicated social cues, and remember not to take our perception for granted.What's next for MoodMate: Due to limited time, the app currently only detects 5 different emotions. I plan on expanding its scope to additional or more complex emotions such as disgust, fear, etc. as well as improving the app's accuracy in detecting these emotions especially at various distances and angles.",
                        "github": "",
                        "url": "https://devpost.com/software/moodmate-9801sb"
                    },
                    {
                        "title": "Eclipse",
                        "description": "A tool for everyone: from streamers to workers on zoom calls to facetime calls with family with the goal to balance privacy and publicity. ",
                        "story": "",
                        "github": "https://github.com/The7thWraith/PrivacyApp",
                        "url": "https://devpost.com/software/privacy-blurrer"
                    },
                    {
                        "title": "SafeAI",
                        "description": "AI-powered security that detects weapons in live camera feeds and alerts authorities instantly\u2014turning cameras from passive recordings into active life-saving tools.",
                        "story": "Inspiration: In an era where school safety has become a critical concern, our team recognized the urgent need for innovative solutions that can prevent threats before they escalate. We were inspired by the potential of AI and computer vision to serve as proactive guardians in educational settings. After learning about the limitations of traditional security camera systems\u2014which rely entirely on human monitoring and often fail to detect threats in real-time\u2014we envisioned SafeAI as a critical layer of protection for schools, empowering security personnel with AI-powered threat detection to dramatically reduce response times when every second counts.What it does: SafeAI transforms ordinary security cameras into intelligent threat detection systems that can identify weapons and alert authorities instantly. The system:Analyzes live camera feeds in real-time using advanced AI vision modelsImmediately detects potential weapons with high accuracyCategorizes threats by severity (LOW, MEDIUM, HIGH)Alerts security personnel through a comprehensive dashboardProvides AI-generated security recommendations for appropriate responseSupports multiple camera locations across different school campusesOffers one-click emergency actions including lockdown initiation and emergency alerts,Additional Features: Gemini AI Analysis: Uses Google's Gemini 2.0 Flash model to analyze threat situations and provide contextual recommendations on response strategies, evacuation needs, and security personnel requirementsInteractive Map View: School floor plans with color-coded camera locations showing real-time threat statusMulti-campus Support: Ability to monitor different school locations from a single dashboardEmergency Response Tools: Integrated tools for calling emergency services, initiating lockdowns, and sending SMS alertsSecurity Dispatch System: Intelligent assignment of security personnel with estimated response timesDevice Assignment: Flexible assignment of available cameras to specific monitoring locations,How we built it: SafeAI is built using a modern tech stack that combines powerful frontend visualization with robust backend AI processing:Frontend: Next.js, React with TypeScript, Tailwind CSS, Framer Motion, Leaflet, OpenStreetMapsBackend: Flask, CLIP, Pillow, Hugging Face, PyTorch, Moondream 2, Hardware acceleration for CUDA GPUs and Apple Silicon, HF-Transfer, OpenCV, PyvipsAPI Integration: Google's Gemini 2.0 Flash, WebRTC,Challenges we ran into: So so many challenges. Some of them include:Accomplishments that we're proud of: Developing a computer vision AI system that can detect weapons in under 1 secondCreating an intuitive UI that security personnel can use with minimal trainingCollaborating as a team under stress and managing to write about 10k lines of codeBuilding a solution that works across different hardware configurationsDesigning an architecture that can scale to support multiple school campuses,What we learned: The intricacies of real-time computer vision processing at scaleTechniques for optimizing AI models for edge devicesThe importance of thoughtful UX design for emergency systemsMethods for balancing AI processing requirements with available hardware resourcesHow to collaborate on a full stack app as a team effectively and efficiently,What's next for SafeAI: Developing a mobile mirror to increase accessibilityUpgrading the GPUs for faster compute for inferencingRestructuring our custom algorithms to be more computationally efficient,",
                        "github": "https://github.com/PythonDweeb/SafeAI",
                        "url": "https://devpost.com/software/safeai"
                    },
                    {
                        "title": "Garden Guardian",
                        "description": "No more long nights of anxiously pondering what is crawling in your garden: Debug your life with the help of Garden Guardian!",
                        "story": "Inspiration: The countless reddit posts we came across made us realize the demand for a product like Garden Guardian.What it does: With Garden Guardian, you can upload any image of insects in your garden, and the app with automatically identify and assess their threat level. After you input your image, the app will output: Your insect's species, a few facts about it, how it harms / benefits your garden plants, and whether it poses any threat (bites, poisonous, disease, etc.) to humans.How we built it: The back-end of our project was constructed with AIP Logic, which assisted us with our image classification model. Meanwhile, the front-end was based on the Workshop Module and Pipeline tool.Challenges we ran into: We faced several challenges both in the front and back-end. For example, we struggled with creating media objects to import into the AI model. We initially faced challenges when trying to engineer our model in Google Colab. So, as an alternative, we turned to Palantir, which offered a far more broad and versatile toolkit. Throughout the process, we discovered the function of a pre-imbedded LLM that also possessed image-processing capabilities.Accomplishments that we're proud of: We are proud of how we were able to navigate the new Palantir workspace in the time allotted to create and develop an app 100% on Palantir. We are also proud of our time management skills, as we were able to finish promptly and have lots of time to prepare for our pitch.What we learned: We learned about Palantir\u2019s various functionalities through the guidance of the site\u2019s AIP Assist as well as in-person Palantir volunteers.What's next for Garden Guardian: We are hoping to develop this into a mobile app, so that people can use Garden Guardian more conveniently. With this development, we are planning to integrate a camera right into the app. We would also add additional information for the model to output, such as solutions for getting rid of pests in your garden.",
                        "github": "",
                        "url": "https://devpost.com/software/garden-guardian"
                    },
                    {
                        "title": "RootMyMind",
                        "description": "Root My Mind is a mental wellness app gives you definitive, science-based steps to boost how you feel \u2014 essentially having a mental health coach in your pocket. ",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/rootmymind"
                    },
                    {
                        "title": "Student's Sidekick ",
                        "description": "Your All-in-One Tool for Smarter Studying, Smarter Spending",
                        "story": "",
                        "github": "https://github.com/Mithra0317/A-Student-s-Sidekick-",
                        "url": "https://devpost.com/software/student-s-sidekick"
                    },
                    {
                        "title": "Shadow Protocol",
                        "description": "Dystopian 2D game where players have to escape a facility filled with robots and security cameras.",
                        "story": "",
                        "github": "https://github.com/TrinityK9/shadow-protocol",
                        "url": "https://devpost.com/software/shadow-protocol"
                    },
                    {
                        "title": "HeartByte",
                        "description": "HeartByte is a relationship analyzer that helps users navigate stressful or confusing conversations with a significant other to reduce anxiety and boost confidence in communication.",
                        "story": "Inspiration: Navigating digital relationships can be confusing and emotionally draining \u2014 especially when you're unsure how the other person feels. From overthinking to anxiety around responding \"the right way,\" we realized there's no tool designed to support emotional clarity in texting. People often get stuck in toxic relationships and can't understand the warning signs.What it does: HeartByte allows users to upload screenshots of text conversations and receive:\nA real-time analysis of how engaged or emotionally invested the other person seems.\nSample responses based on the tone and topic of the conversation, aimed at reducing anxiety and improving communication.\nA chatbot trained to offer thoughtful advice and empathetic guidance, promoting self-worth and mental clarity.How we built it: Frontend: Built using VS Code and hosted via Lambda Cloud.\nBackend: Python-based architecture to handle OCR preprocessing, sentiment analysis, and classification. We imported libraries like PIL and Pytesseract\nAI Models: We imported OpenAI and wrote calls to it. We finetuned it slightly to better answer the prompts for our purposes.Challenges we ran into: We faced huge challenges with the HTML as neither of us has ever coded in that before. It took a long time and a lot of trial and error to sync up our backend code with our frontend. Another challenge we faced was extracting the text and categorizing it from screenshots. We found that the color fluctuated a lot of the messages so we had to adjust the color thresholds to classify the texts as incoming or outgoing correctly.Accomplishments that we're proud of: We gained a lot of HTML knowledge and debugging experience after this project. We are proud that our frontend looks so amazing!What we learned: It is our first time working in depth with AI, so we learned a lot about importing and finetuning it. We also learned frontend development because the most we had done with that is by using no-code platforms.What's next for HeartByte: We hope to further train our AI model to provide more in depth analysis. We also hope to transform it into a mobile extension so people can access it right from their Messages app. We can make a dashboard to track compiled relationship health over time to further support people's mental health in their relationships and advise when it is time to leave.",
                        "github": "",
                        "url": "https://devpost.com/software/heartbyte"
                    }
                ],
                [
                    {
                        "title": "NeoFinance",
                        "description": "Custom AI finance tracking tool, based on what you need, and what you want in the moment.",
                        "story": "Inspiration: Something that we always wanted in our lives was a tool that would help us track our finances, not our banking apps, but something else that was more personal and casual, as regular banking apps were daunting to use for us.",
                        "github": "",
                        "url": "https://devpost.com/software/neofinance-rhzo59"
                    },
                    {
                        "title": "brAIn",
                        "description": "It's like your computer's very own brAIn with memories! Turn your files into knowledge\u2014upload, interact, and let our AI answer your questions like it\u2019s reading your mind! ",
                        "story": "What it does: BrAIn is an AI-powered memory engine that transforms your files into interactive, intelligent memory. Just upload any document\u2014PDFs, spreadsheets, or code\u2014and BrAIn instantly \u201cremembers\u201d it like a brain would. Using advanced vectorization and generative AI, BrAIn can answer complex questions, reference specific info, or even analyze patterns across multiple files\u2014just like chatting with someone who read them all. Whether it's dissecting a transcript, extracting data from a messy PDF, or cross-referencing code and documentation, BrAIn makes understanding your files feel like second nature. Designed for convenience, flexibility, and ease of use, BrAIn strives to make the user experience better by revolutionizing file search.How we built it: BrAIn was built with Python, CSS, JavaScript, HTML, Firebase, and Vue. We developed a vectorization and embedding pipeline using the Langchain framework, powered by OpenAI's embedding model. Our fully functional raw data-to-vector pipeline can store vast amounts of data and interact with it efficiently. Using Firebase Cloud Storage, we made cloud storage cheaper and smarter.Challenges we ran into: Using the Langchain framework came with many bugs, as we had no prior experience working with it. Figuring out its syntax from online research and documentation led to some ups and downs. On the front end, connecting our Langchain pipeline to Flask was difficult due to Flask's backend infrastructure.Accomplishments that we're proud of: Developing an end-to-end, fully proprietary pipeline that can communicate and act like a human brain is something we\u2019re proud of. Learning how to utilize Langchain\u2014a new and innovative tool that\u2019s advancing the LLM world\u2014was another milestone we\u2019re excited about.What we learned: We learned a lot about collaborating together as a team and how to efficiently and simply divide and conquer on our ambitious idea. Slowly understanding each other\u2019s work habits and strengths was an unexpected yet rewarding experience. On a more technical level, working on LLM integration and learning a new framework significantly nurtured our technical expertise.What's next for BrAIn: We\u2019re definitely going to continue using it, as it genuinely makes our lives more convenient\u2014and we hope to publish it to help others in the same way.",
                        "github": "https://github.com/AneeshDantuluri/Brain-LosAltosHax",
                        "url": "https://devpost.com/software/brain-ihts6q"
                    },
                    {
                        "title": "Vero",
                        "description": "Your truth filter for the internet; our extension instantly checks news headlines for credibility, helping you identify fact from fiction with a simple glance.",
                        "story": "Ever come across a headline that just sounds too wild to be true? The scary thing is, false news spreads70% fasterthan the truth\u2014and that\u2019s exactly why we built Vero. It\u2019s your AI-powered shield against misinformation. Vero checks headlines in real time, highlights red flags like exaggerated claims, and analyzes the source\u2019s credibility with a 98% accuracy. For example, if a headline comes from a known satire site and lacks evidence, Vero will flag it as \"Likely False.\" No time to read the full article? With one click, you get a clear, AI-generated summary. Behind the scenes, Vero uses two advanced AI models: one trained on millions of fact-checks to spot misinformation, and another large language model that summarizes content and answers your questions. Whether you're scrolling social media, doing research, or just trying to stay informed, Vero helps you quickly see what\u2019s true\u2014so you never accidentally share fake news again.",
                        "github": "https://github.com/Ashwin-P23/Vero-front",
                        "url": "https://devpost.com/software/vero-n2jtkv"
                    },
                    {
                        "title": "ClarAI",
                        "description": "Helping students by monitoring their Discord activity. When a person says something concerning on Discord, it will alert the admins of the server and provide help to the person.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/groupify-e7c5f8"
                    },
                    {
                        "title": "toliq",
                        "description": "A custom built LLM context fetching & agentic protocol with inbuilt support for gsuite (sheets and calendar so far) with the ability to read/write from any of your apps on your behalf.",
                        "story": "Toliq was inspired by the gap between powerful AI assistants and actual productivity tools. While AI can answer questions, most solutions lack the ability to take action in the real world. We wanted to create an AI assistant that could not only understand requests but directly interact with the digital tools people use daily, making it truly useful for busy professionals.Toliq is an AI-powered assistant that can interact directly with Google Calendar and Google Sheets. Users can have natural conversations with Toliq and ask it to perform tasks like scheduling meetings, creating events, reading spreadsheet data, or updating information\u2014all through a clean, intuitive chat interface. Instead of just suggesting what to do, Toliq actually does it for you.We built Toliq using a stack of modern technologies:\nBackend: Flask server with custom function calling architecture that bridges LLM capabilities with external APIs\nFrontend: Next.js application with a responsive, modern chat interface\nAI Integration: Custom prompt engineering to enable structured function calling and context-aware responses\nExternal Services: Google Calendar and Google Sheets APIs with custom wrappers for simplified interactionCreating a reliable function calling system that could translate natural language into precise API operations\nManaging timezone conversions correctly when working with calendar events\nBuilding a robust error handling system that communicates failures clearly to users\nDesigning an architecture that could be extended to support additional integrations beyond the initial setDeveloped a clean, intuitive interface that makes complex operations feel simple\nCreated a flexible backend architecture that can easily incorporate new external services\nBuilt a system that truly bridges the gap between conversation and action\nImplemented proper timezone handling to ensure calendar events appear correctlyThe importance of designing prompt engineering specifically for function calling\nHow to build a backend that can reliably translate between natural language requests and API operations\nTechniques for robust error handling in a complex, multi-service system\nThe challenges of building a cohesive UX across natural language and structured dataAdding more integrations with productivity tools like Notion, Trello, Slack, and email services\nImplementing more complex workflows that can span multiple services\nDeveloping a memory system to better understand user preferences and common tasks\nCreating a mobile application for on-the-go productivity\nAdding authentication and multi-user support for team environments",
                        "github": "",
                        "url": "https://devpost.com/software/toliq"
                    },
                    {
                        "title": "Conlang Translator",
                        "description": "A translator going from English to a personal conlang of mine, Sadzuk\u012b.",
                        "story": "As a CS and linguistics person, making a conlang (n\u0101uode, made language) was just right for me. This hackathon truly began for me after the idea emerged,a translator from English to my conlang Sadzuk\u012b.The free, powerful LLMs provided by Palantir analyzing my numerous translations of short stories, poems, and songs I made were able to translate any input I put in!...Almost. The difficulty of this project was not the code or UI, but the rich, agglutinative, structure of my conlang\u2014with itsonly 10 verbsand nearly backwards word order compared to English\u2014fighting against the LLM's Eurocentric bias.As of February 2025, 49.4% of web content is in English,followed by 6% Spanish and 5.6% German, giving European languages a significant advantage in the AI realm.In fighting (taitae, giving a fight) these challenges, I learned so much about optimizing LLMs by smartlypreprocessing data, fine-tuning the language model, and even choosing the right model.While my translator is not perfect and makes mistakes all the time, I am very proud with how it turned out!",
                        "github": "",
                        "url": "https://devpost.com/software/conlang-translator"
                    },
                    {
                        "title": "RiskRadar",
                        "description": "Harnessing AI to Predict and Navigate Natural Disasters with Real-Time Insights.",
                        "story": "Inspiration: The inspiration behindRiskRadarcomes from the increasing frequency and intensity of natural disasters worldwide, mainly due to climate change. Just recently there was an earthquake in Burma / Myanmar that left thousands dead. From hurricanes to wildfires, floods, and avalanches, communities are facing greater risks due to climate change and geographical factors. Even in Silicon Valley, we are very prone to earthquakes. So, my goal was to empower individuals, businesses, and emergency services with better insights so they can prepare and respond to these risks effectively.What it does: RiskRadaris a web application that provides real-time insights into natural disaster risks based on weather patterns, geographical data, and AI analysis. The platform features:Adynamic mapdisplaying weather data, natural disaster alerts, and risk analysis for various locations.Asearch barwhere users can input a country or city to see disaster risks and weather data for that area.AI-powered risk analysisthat uses weather variables like temperature, humidity, and wind speed to predict potential natural disasters.,How we built it: RiskRadarwas built using a combination of modern web development tools and machine learning technologies:Frontend: The user interface was designed using HTML, CSS, and Materialize CSS for responsiveness and accessibility. The interactive map was implemented usingLeaflet.js.Backend: Data was stored inMongoDB Atlas, which houses information on cities and natural disaster occurrences. Originally, the AI model was developed usingPalantir AIP. However, as I couldn't figure out how to use it in my enviornment, I switched to a customgrokmodel using their API.API Integration: Weather data was fetched from external APIs, and disaster data was integrated from sources like EM-DAT. This data feeds into the AI model for real-time analysis. For cities, I simply used a large CSV, sorted by population.,Challenges we ran into: BuildingRiskRadarcame with its own set of challenges:Even more so, much major weather data is collected by government agencies and not made available to the public. Lists never include concurrent weather, and rarely include timestamps or latitude/longitude.Accomplishments that we're proud of: We are proud of several accomplishments inRiskRadar:What we learned: During the development ofRiskRadar, we learned several key lessons:What's next for RiskRadar: Some ideas to expand RiskRadar:",
                        "github": "",
                        "url": "https://devpost.com/software/riskradar-jas7uh"
                    },
                    {
                        "title": "Spam Assassin",
                        "description": "SpamAssassin is the next-gen AI Powered cyber security powerhouse\r\n",
                        "story": "Inspiration -: Cybersecurity threats are widespread and escalating. In 2024, over422 million peoplein the U.S. were affected by cyber scams and attacks. We wanted to address this critical issue by building an app that can effectively detect and prevent these threats in real-time. We also have had some personal experiences in the past with this where we got scammed, and really wanted to make an impact and prevent other people from facing the same issues that we faced.What it does -: SpamAssassin is a next-gen cybersecurity powerhouse. It scans emails for common threats like phishing, data breaches, and ransomware, offering users enhanced protection against online fraud with the use of Generative AI through Palantir AIP to maximize the accuracy.How we built it -: Challenges we ran into -: The AI helper often talks about irrelevant information, such as saying hello to the user, and wanting to be friends with the user. We were able to fix this by writing a more detailed prompt.Instances where JavaScript unexpectedly converting variables to different typesCoordination -  when the frontend encountered a setback and delayed, the backend had to follow along as well.,Accomplishments that we're proud of -: After countless challenges -- 4.5 hours of debugging, we successfully used good examples from our experiment to modify our prompt for the AI helper, teaching it how to properly select spam emails.Using a new frontend framework that we were unfamiliar withCoding and building in multiple different languages and platforms effectively and combing it together to maximize efficiency.Being able to make use of Generative AI to effectively classify spam and fraud emails.,What we learned -: How to apply machine learning techniques to real-world cybersecurity problems.The importance of user-friendly design, especially for critical apps like cybersecurity tools.Effective teamwork and time management to meet tight deadlines and deliver results.How to use the Palantir platform, so we did not have to rely on an internal backend.,What's next for Spam Assassin -: Cross-Platform Integration:Expand the app to mobile, browser, and desktop platforms for comprehensive device protection.Social & Messaging Detection:Extend our app to detect scams across social media and messaging apps like Instagram, WhatsApp, and Discord.Live Threat Alerts:Implement real-time notifications to alert users instantly about emerging scams and provide actionable security tips.,",
                        "github": "https://github.com/AlphaGameDeveloper/LosAltosHackathon2025",
                        "url": "https://devpost.com/software/spam-assassin"
                    },
                    {
                        "title": "OneLMS",
                        "description": "For far too long, the students of America have had to manage information coming from too many directions. We say, no more. OneLMS brings it all together.",
                        "story": "",
                        "github": "https://github.com/netlle/canvas-classroom/",
                        "url": "https://devpost.com/software/onelms"
                    },
                    {
                        "title": "disasters",
                        "description": "It provides a way for locals to be notified in case of disasters. Users can report anything, and selected reporters chosen via web UI will verify such. After, everyone gets notified & has live updates",
                        "story": "",
                        "github": "https://github.com/gdhpsk/LosAltosHacks",
                        "url": "https://devpost.com/software/disasters"
                    },
                    {
                        "title": "Healify",
                        "description": "\u201cHealify \u2014 Breathe. Talk. Heal.\u201d",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/healify"
                    },
                    {
                        "title": "CHEHRA",
                        "description": "CHEHRA transforms innovation with cutting-edge technology, driving growth and empowering change. Join us in reinventing attendance tracking. ",
                        "story": "Inspiration: School check-ins are slow and outdated. Students forget IDs, lines get long, and staff waste time manually tracking who\u2019s present. We built CHEHRA to streamline that \u2014 a smart, hardware-integrated system that uses facial recognition to automate attendance and lunch verification.What it does: CHEHRA is a full-stack project that combines a custom website with hardware to create a smart check-in system. It:Lets students visit a web page, fill out a registration form, and upload face images for trainingUses facial recognition to identify students in real time from a live camera feedSends the student\u2019s name, ID, and tardy status to an Arduino after recognitionDisplays data on a 16x2 LCD, flashes LEDs (green = success), and activates a buzzerUses an ultrasonic sensor to detect presence and trigger scanning, then pauses after recognitionIncludes a manual stop button to disable further scansAdjusts camera height using servos based on the detected distance,How we built it: Frontend:Pure HTML, CSS, and JavaScript. Students use it to register and upload imagesBackend:FastAPI handles registration, image storage, recognition status, and serial commsFacial Recognition:Python + OpenCV, trained from uploaded images. Runs on a local machine simulating a PiRaspberry Pi:Controls the camera and servo mount, and runs the detection scriptArduino:Displays recognition results via LCD, LEDs, and buzzer. Responds to serial inputESP32:In progress \u2014 will enable Wi-Fi/Bluetooth comms between Pi and Arduino for a cleaner setup,Challenges we ran into: Real-time facial recognition synced with hardware feedbackServo movement based on ultrasonic sensor dataSerial communication timing between FastAPI and ArduinoMaking everything work smoothly as a solo full-stack + hardware project,Accomplishments that we're proud of: Fully working student check-in system with a web-based frontendClean UI that lets students register and upload training imagesReliable recognition with hardware feedback (LCD + buzzer + LEDs)Adjustable camera mount that responds to student heightAll built from scratch \u2014 web, backend, vision, and hardware,What we learned: Tying HTML/CSS/JS frontends to Python-based vision systemsImplementing facial recognition from scratch with OpenCVSyncing Arduino hardware with live recognition eventsHandling real-world inputs like distance sensors and servos,What\u2019s next for CHEHRA: Finalize wireless ESP32 + Raspberry Pi communicationAdd image privacy and secure authenticationCreate a teacher/admin dashboard for live attendance monitoringPilot in a real classroom environment with anonymized dataOptimize models to eventually run directly on Raspberry Pi,",
                        "github": "https://github.com/aayan158/los-altos/",
                        "url": "https://devpost.com/software/chehra"
                    },
                    {
                        "title": "Root Cause",
                        "description": "Identify plants & their diseases, then get expert advice on caring for them.",
                        "story": "Inspiration: Whenever our mothers try to take care of plants, they always end up dying. This led us to come up with the idea of an app that helps you keep your plants healthy.What it does: The web app gets the video stream from your device's camera, and allows you to take a picture of your plant. This picture is sent to acustom-made API, where the plant is identified using aneural network, and it is determined if it has any diseases. The user receives the neural network's top three predictions, selects the one that is right for them, and can ask an advancedLLMfor advice on how to care for their plant and help it recover from its ailments. This request is once again processed by our custom API and returned to the user.How we built it: The custom-made API is hosted on aLambda GPU instancerunning Ubuntu. PyTorch was used for training the neural network responsible for plant classification as well as running the model to process requests.The Lambda inference APIwas used to run LLMs and pass in the necessary data to give the user a relevant response. The API itself was created usingFastAPI.Challenges we ran into: Hosting the site publicly wasn't possible due to the limitations of communication between a HTTPS frontend and our HTTP backend. Unfortunately, after around an hour of trying, we realized that securing the backend API properly wasn't feasible for the project.Accomplishments that we're proud of: We learned how to use many new tools, and create a relatively robust application. Creating an API and being able to send requests to it was a great accomplishment. Surprisingly, during the hackathon, we managed to stay focused for a majority of the time.What we learned: Custom domains are great, but they introduce numerous challenges that even users with experience in the topic will have trouble getting them to work properly.What's next for Root Cause: Getting the API to use HTTPS would be the main next step, but would likely involve changing service providers, which could get complicated. Enlarging the dataset for plant classification would also be a good expansion. Some sort of feature where users interact with each other and exchange ideas would be a long-term goal.",
                        "github": "https://github.com/ayushthoren/rootcause",
                        "url": "https://devpost.com/software/root-cause"
                    },
                    {
                        "title": "AirSwift",
                        "description": "AI powered file sender designed to empower those who are less familiar with technology",
                        "story": "Inspiration: Current technology to wirelessly share files requires an Apple device to another Apple device. It may also be confusing to navigate the airdrop menu and find the appropriate file for technologically challenged people.What it does: Eases the airdrop process for technologically challenged users by allowing a voice-driven method of sending and receiving files with leniency on the device's company and distance between sender and recipient.How we built it: An audio or text description of a file you wish to send is embedded within a vector DB and compared to other files within a directory through a similarity search to find which file matches the description. As files are downloaded, they are embedded in a vector DB. After an audio or text description is given, the most accurate file to the description is sent over an S3 bucket to the other person. The person may accept or reject the offer and if they accept, it is placed within their downloads folder.Challenges we ran into: The GUI was unresponsive due to blocking code. Had to use multi-threading, which was confusing. Buttons were unresponsive at times and this decreased how smoothly the program ran.Accomplishments that we're proud of: Getting the embedding and sending of PDFs to work. The download of third-party software had to be done in which it didn't seem relevant to the actual project, and many other steps were required to accomplish that downloading.",
                        "github": "https://github.com/FakeZhiyuanLi/AirSwift",
                        "url": "https://devpost.com/software/airswift"
                    },
                    {
                        "title": "Project Flourish",
                        "description": "Helping you grow a love for nature, one plant at a time",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/project-flourish"
                    },
                    {
                        "title": "Whiteboard",
                        "description": "Where grading meets guidance.",
                        "story": "Inspiration: As public school students, where our teachers manage over 120 students a day, we\u2019ve experienced firsthand what it\u2019s like to be overlooked when we need help the most. In some classes, a lack of staffing means one teacher is forced to handle nearly thirty-five students at once. This overwhelming ratio leaves little time for teachers to give meaningful feedback\u2014and students end up with long hours of confusion and poor grades. Teachers spend over15-20 hours per weeksolely on grading homework assignments.\nWhile AI has started to assist teachers with managing their classrooms, most tools give generic responses that don\u2019t fit the unique needs of each individual. That\u2019s because most large language models don\u2019t remember individual students between sessions, and it\u2019s unrealistic to expect teachers to write personalized prompts for every student. Unfortunately, this lack of care directly impacts all students, and can be especially harmful to those that need individualized support.\nThat\u2019s why we createdWhiteboard: an all-in-one student dashboard built to personalize classroom learning.What it Does: Whiteboard helps teacher manage their classroom files with the assistance of AI tools. Teachers can add their class data, including personal information for each student. This allows Whiteboard to customize it's feedback based onindividual needs. Next, teachers can create assignments and upload answers keys for our agent to reference. After adding student submissions with the click of a button, this information is sent to our database. Whiteboard efficiently grades each assignment, providing substantial feedback and room for growth. In addition, our software monitors class performance to identify common weak points.\nThe highlight of our app is the individualized education plan feature. Teachers can access the central student dashboard, which allows them to evaluate each student. Whiteboard gives a comprehensive view of the student's latest performances, as well asdirect areas of improvementand solutions for intervention.How we built it: Our product connects 3 stages of processes to create our app: aweb-client frontend interface,AWS server, andPalantir AIP Logic. The web-client interface is lightweight and meant to quickly access and upload files to the server. The AWS server leverages the immense storage capability of Amazon's S3 to store large quantities of images. Meanwhile, the EC2 instance manages user requests and database files with a Python Flask backend. Then, we use a custom Optical Character Recognition (OCR) program to scan handwriting and convert it to formatted text. This process takes computing out of the Palantir environment, but proves to perform better for our specialized use case. Finally, we ran Palantir with an S3 connect, which was eventually achieved by linking our EC2 instance as an intermediary agent. Palantir manages our LLM processes. We use its pipeline processes to sync batches of our data intermittently.Starting with development, we drafted a rough outline of our processes. We decided on a robust file system for our JSON data and completed our AWS and Palantir scheme. Beginning with  development with the servers, we rented S3 and EC2 instances large enough for moderate traffic. Meanwhile, our team began researching Palantir's use-cases, including theOntology SDKandREST APIfor linking AWS with Palantir models. We eventually settled on an easy-to-use, straightforwardBatch Pipelinefor more control over our data. After setting up our Amazon Linux environment, we configured our backend in Flask to receive requests from users running on our EC2 instance. These requests would contain enough information to access specified JSON files from our S3 interface. The python moduleBoto3allowed us to read and write our S3 data from anywhere across our app. Our web-client is responsible for uploading files, while it uses the HTTPS accept ping sent back from the S3 server to orchestrate our user requests. Our EC2 instance updates our S3 data and uploads the OCR processed text file. From there, Palantir undergoes a batch sync, runs the LLM, and sends it back to S3 for the web-user to easily access.Challenges we ran into: We faced a tremendous challenge in linking the disparate cloud services of Palantir and AWS, yet tackled such a challenge in a limited time frame. Including an EC2 instance provided more freedom in choosing our processes, as we were able to include our own OCR program which surprisingly outperformed Palantir's OCR for our use-case.Accomplishments that we're proud of: As a team, we're proud that we managed to develop such a cohesive and elegant experience, significantly improving from our project last year. In addition, we've conquered Palantir's complex, yet powerful, software, enabling us to perform tasks with AI more smoothly than we'd ever done before. Aside from technical prowess, we're incredibly proud to have made a product that willbenefit people just like us - students, mentors, and educators. Personalized teaching and individualized feedback improve both the student and teacher mental health, relieving the significant burden that is high school stress. Overall, we're proud to be helping the lives of the people at the forefront of developing the next generations - students and teachers.",
                        "github": "",
                        "url": "https://devpost.com/software/whiteboard-e4pj9r"
                    },
                    {
                        "title": "Conferensync",
                        "description": "From fun calls with relatives to serious video conference calls, we bring inclusivity to all.",
                        "story": "Inspiration: Our main inspiration for this project is to make communication seamless for deaf and mute people in meetings. Currently, companies hire a human translator for meetings, but this is not scalable and very expensive. In fact, the FCC (Federal Communications Commission) requires that video conferencing services add specific features to their service, such as \u201cspeech-to-text capabilities, text-to-speech capabilities, and enabling the use of sign language interpreting.\u201d This tool was created with the goal of making a scalable, reliable and user-friendly solution to improve communication for the differently-abled.What it does: The prototype streams the camera input to a server. This server uses a Convolutional Neural Network (CNN) to classify the sign in the frame and sends that back to the client. The client prints out the word in a stream, like a live transcript.How we built it: The project was built entirely using Python. There are a few parts to this project:Challenges we ran into: Though exciting, the road for building this system was by no means easy. Some of the biggest challenges were:Accomplishments that we're proud of: Throughout the process, we implemented some clever solutions to make the system more effective:What we learned: Initially, we were considering running the entire fine-tuning process on Palantir. For this, we started learning how to use their development environment, and the tutorials highlighted that even no-code solutions can be used to build effective pipelines.Even though we did not finally fine-tune the models on Palantir, learning about this was an important skill we gained during this hackathon. We look forward to using it for future projects and hackathons!What's next for Conferensync: Our future plans with Conferensync is to expand our vocabulary and include a lot more words, as well as transcription to speech for a more meeting-like experience. In addition, more languages could be added so that the tool is useful for a global audience. Finally, the end goal would be to integrate this into video conferencing services, such as Zoom, Google Meets, etc.",
                        "github": "https://github.com/techno-nerd/A2DE",
                        "url": "https://devpost.com/software/signsync-5b2jpi"
                    },
                    {
                        "title": "FixAR",
                        "description": "Instant object detection and repair guidance, powered by Machine Learning and AR.",
                        "story": "Inspiration: I often find myself overwhelmed by complicated instruction manuals and unreliable repair instructions. I can't always find that one person on reddit who had the same problem ten years ago, which is frustrating when I am trying to fix my electronic devices. That's why I decided to build FixAR.What it does: FixAR allows you to describe your technical problem, which then sends that to Gemini to determine the parts of your device that might be causing the problem. Then, using real-time object detection, you can scan your device and have the possible problem areas highlighted with Augmented Reality, allowing you to tackle each problem area individually.How I built it: The frontend is entirely SwiftUI. After the user enters the description of their problem, the description is sent to the API backend for processing by Gemini. Gemini's prompt restricts it to outputting a list of problem areas only. After the list is sent back, the user can move to the camera interface. I created an image classification model using CreateML for electronic devices specifically which is what powers the real-time object detection. At first, I was labeling my own dataset with PixLab Annotate, but eventually I was able to find a more comprehensive dataset, and I combined my own labeled data with the existing dataset to train the model. I was able to integrate the model with my app using CoreML. I used ARKit to create the buttons that the user can interact with to learn more about the specific problem areas with their device. After the user clicks the AR buttons, another prompt is sent to Gemini, where it provides a comprehensive reasoning behind why that specific component is causing the problem.Challenges we ran into: I faced a lot of challenges when I was trying to create the Machine Learning model. I couldn't find a good dataset, so I opted to label the data myself. This took a long time, but fortunately, I was able to find a dataset that I could use for demo purposes. However, in the future, I want to use a much more advanced dataset. I faced the most problems with the AR integration though. I had a lot of issues with the mapping for the AR buttons and labeling for the device, and even now it is fairly inconsistent, although better. There were's also a lot of problems when it came to triggering the popup and using the AR elements as buttons the user could click. I also had to figure out how to have the Gemini API execute when the AR buttons were pressed without taking to long. These were the major challenges I ran into, but there were definitely more I dealt with throughout the hackathon.Accomplishments that we're proud of: I'm proud of being able to implement the AR functionality and also creating the ML model.What we learned: I learned about making my own datasets and also a lot about the ARKit framework offered by Apple. I also learned how to make image classification models in CreateML. I learned more about the different capabilities of Gemini's API, even if I wasn't able to completely utilize Gemini's full extent.What's next for FixAR: I want to improve the AR labeling of the problem areas, and also increase the dataset of the ML model so the app can assist with repairing a lot more devices. I also want to add in real-time AR instructions to guide you while you repair your device.",
                        "github": "https://github.com/ShashwathD/FixAR",
                        "url": "https://devpost.com/software/fixar"
                    },
                    {
                        "title": "AI Color Analyst",
                        "description": "Our mission is to help individuals choose the perfect shades of clothing for each season, ensuring they always look their best while aligning with the ever-changing trends. \r\n",
                        "story": "Inspiration: In a world where color drives consumer behavior, brand identity, and emotional engagement,   our color analyst website offers advanced features such as color palette extraction, harmony suggestions, and real-time analysis. Our website not only simplifies the process of identifying dominant and complementary colors but also helps ensure that design choices align with current fashion trends, accessibility standards, and psychological principles. With powerful color tools at their fingertips,people can enhance their fashion by wearing colors that suit them.Additionally, our website offers a cost-effective alternative to spending hundreds of dollars on a professional color analyst. With the AI-powered features we\u2019ve integrated, users can access personalized color analysis and recommendations at no extra cost, making professional-grade color consultation more accessible and affordable.What it does: Our color analyst website provides a wide range of tools and resources for both fashion designers and everyday users. It helps them analyze, extract, and refine color choices from various visual sources. Users can upload images to identify the main colors, create matching color palettes, and evaluate how different color combinations work together. The website also offers features like an up to date list of fashion trends . By providing real-time analysis, complementary color suggestions, and insights into color matching, our platform helps users make smart color choices that align with their goals, enhance their projects, and maintain a consistent visual style.How we built it: We created a web application that analyzes and suggests the best color palettes for users, using Generative AI and modern web technologies. The website is built with HTML and CSS to give it a clean and easy-to-use design. JavaScript is used to make the site interactive, allowing users to upload images and get AI-powered color analysis and suggestions.Challenges we ran into: Our first challenge was building the backend for our website, which took a lot of time and effort. We faced some difficulties in getting all the features to work properly, which required a lot of troubleshooting. However, once we got past this challenge, we were able to create a strong foundation and continue developing the rest of the website's features.Our second roadblock we ran into was integrating our frontend and backend because this was our first time making a well developed website.Our last challenge is we did not know how to make AI going in this but we self-taught ourselves how to.Accomplishments that were proud of: One of our proudest accomplishments is building a working website that includes AI features. In this project, we not only learned about artificial intelligence but also used that knowledge to create AI-powered functions. Being able to apply AI in a real project is a big achievement for us, as it shows we can turn what we've learned into something practical and functional.We successfully overcame the most significant obstacle in completing the project on time, despite being considerably behind schedule. There were moments when it seemed unlikely we would finish, yet we persevered and ultimately delivered the project as planned.What we Learned: We learned that with hard work and determination, anything is possible. A prime example of this is how we successfully built an AI feature despite having no prior experience with AI. This accomplishment highlights our ability to overcome challenges, learn new concepts, and apply them effectively, demonstrating the power of persistence and a willingness to grow.What's next for AIColorAnalysis?: We were going to add more AI-features to our website to enhance the look and feel of our website.We might publish this website in the future.",
                        "github": "https://github.com/yizhifang123/aicoloranalyst",
                        "url": "https://devpost.com/software/ai-color-analyst"
                    },
                    {
                        "title": "lingu",
                        "description": "An innovative Ai optimal take on language learning.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/lingu"
                    },
                    {
                        "title": "PiggyPal",
                        "description": "This project combines enjoyment and essential life skills to create the perfect game. A financial literacy role playing game. Create an avatar in our world, and learn how to manage your REAL money",
                        "story": "",
                        "github": "https://github.com/maulikagr/FinanceApp",
                        "url": "https://devpost.com/software/piggypal-4peyc8"
                    },
                    {
                        "title": "Word Duel",
                        "description": "The ultimate word showdown that challenges your brain!",
                        "story": "Inspiration: We love competitive word games like Scrabble, so we decided to make a similar online versionWhat it does: The user is given a letter bank that keeps refreshing after a letter is used. The user is given one minute to enter as many words as they can.How we built it: We used a combination of Vue.js, firebase, and HTML CSS.Challenges we ran into: We needed a working database for the leaderboard and a nice, user-friendly UI. We also had issues with formatting, such as the user needing to click on the text box for every word entered.Accomplishments that we're proud of: We have a beautiful and user-friendly UI. We even added a Player vs. Player mode to the game.What we learned: We learned how to use firebase databases and also create graphical interfaces with the help of CSS. We also learned how to use Vue.js to run the game logic.What's next for Wordduel: Adding a LAN Player vs. Player option.",
                        "github": "https://github.com/KashBoi7/WordGame",
                        "url": "https://devpost.com/software/wordduel"
                    },
                    {
                        "title": "Rizzpedia",
                        "description": "Connecting the World Through Language",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/rizzpedia"
                    },
                    {
                        "title": "Beyond the Lab",
                        "description": "Beyond the Label is a mobile app that empowers consumers to make more informed choices about the food they eat. By simply scanning a product\u2019s barcode, users can see a breakdown of its ingredients.",
                        "story": "Inspiration: Growing up, my family and I have always been pretty health-conscious. We\u2019d buy organic food, choose seasonal produce, and, most importantly, always read the labels on the back of food containers. But even with that mindset, I found myself constantly surprised by the long, scientific-sounding, incredibly obscure ingredients on the back of food packages. What were these additives and preservatives really doing in my food? Were they safe? That curiosity (and frustration) inspired me to buildBeyond the Label, a mobile app that helps consumers make safer, smarter decisions about the food they eat.What it does: With a simple barcode scan, the app pulls up a full list of a product\u2019s ingredients and checks them against trusted international health databases like the FDA, as well as EU organizations known for having stricter standards on chemical safety such as IARC and EFSA. If an ingredient is flagged as potentially harmful or questionable, the app lets users know and even suggests healthier, safer alternatives for similar products.How I built it: Beyond the Label was built as a modern, responsive web app using React, Vite, and TypeScript. React's component-based architecture made it easy to organize functionality, while TypeScript added type safety and helped prevent bugs as the project scaled. I chose Vite for its fast development environment and optimized build process, and styled the entire app using Tailwind CSS, which allowed for rapid UI development with a clean, consistent aesthetic. For barcode scanning, I integrated Scanbot Web UI, enabling seamless in-browser scanning. Once a product is scanned, its data is retrieved using the Open Food Facts API, which provides a list of ingredients and metadata. From there, I built a custom backend pipeline that cross-references each ingredient with international health sources like the FDA, IARC, and EFSA through a combination of API calls and web scraping. This system identifies potentially harmful additives by normalizing ingredient names and matching them across regulatory lists. Finally, I extended this logic to build a recommendation engine that compares products in the same category and suggests safer alternatives with fewer flagged ingredients \u2014 giving users a clearer, healthier choice at a glance.Challenges I ran into: This was my first time solo hacking, so one of the biggest challenges was figuring out how to take an idea with such a large scope and actually turn it into a working product all on my own. From designing the user experience to implementing backend logic and everything in between, it was a lot to manage and required learning on the fly. One major technical hurdle was setting up the barcode scanning functionality. While I used Scanbot Web UI to handle most of the computer vision work, getting it to integrate smoothly with the rest of the app and ensuring it performed reliably across different devices took a good amount of troubleshooting. Another major challenge was dealing with the lack of public APIs for some of the European regulatory databases like EFSA. To overcome this, I had to write custom web scrapers to extract the data I needed, which meant handling inconsistently formatted websites, rate limits, and parsing data that wasn\u2019t always easy to work with.Accomplishments that I'm proud of: I'm proud that I was able to bring this entire project to life on my own. As my first solo hackathon project, tackling everything from ideation to design to implementation was both challenging and rewarding. It pushed me out of my comfort zone and helped me grow as a developer. Beyond the technical achievement, I\u2019m especially proud that I created something meaningful that I can actually use in my daily life to make more informed choices and continue living a healthier lifestyle. Knowing that this project has the potential to improve not just my habits, but also help others become more conscious consumers, makes it feel all the more impactful.What I learned: Throughout this project, I picked up a ton of valuable technical and practical skills. On the development side, I learned how to use React and TypeScript to build a robust, type-safe frontend, which helped me structure my code more clearly and catch errors early. Styling became a breeze thanks to Tailwind CSS, which made the entire UI-making process faster and more intuitive. I also learned how to build simple yet effective web scrapers using Beautiful Soup, which was key for extracting data from European health websites that didn\u2019t offer public APIs. Beyond the code, I gained a deeper appreciation for the full development process. I used Figma to plan and flesh out the app\u2019s design before writing a single line of code which was something that helped me stay organized and intentional with every screen. Managing a large project on my own taught me a lot about time management, prioritization, and breaking problems down into achievable steps. Just as importantly, I learned how to communicate my ideas clearly and pitch my project confidently to a panel of judges, turning technical work into an interesting story.What's next for Beyond the Lab: One of the biggest next steps I\u2019m excited about is expanding Beyond the Label to support cosmetics and skincare products. Just like with food, many personal care items contain ingredients that can be irritating, hormone-disrupting, or even potentially carcinogenic. However, most consumers have no idea what those long, scientific names on the label actually mean. By adding support for barcode scanning and classification in this space, the app could help users avoid harmful chemicals in the products they put on their skin every day. I plan to integrate additional databases like CosIng more deeply for cosmetic-specific ingredients, and build a similar recommendation engine to suggest cleaner, safer alternatives based on each user\u2019s needs and preferences. Expanding in this direction could make Beyond the Label an even more comprehensive tool for conscious living, helping people not only eat better, but live healthier across all areas of their daily routine.",
                        "github": "https://github.com/djarty24/los-altos-hacks-2025",
                        "url": "https://devpost.com/software/beyond-the-lab"
                    }
                ],
                [
                    {
                        "title": "ARDOF",
                        "description": "Automated Response for Detecting and Observing Fire",
                        "story": "Inspiration: January 2025 was a chaotic and destructive beginning to the year as the Palisades Fire burned through LA. One of our team members' sisters had to put their education on halt and evacuate from the UCLA Campus to get out of harm's way. Furthermore, many of our friends lost their homes, finances, and families, which is a story seen throughout the city. Implementing earlier countermeasures would have been crucial and effective, and our team at ARDOF(Automated Response for Detecting and Observing Fire) believes that Artificial Intelligence is the key.What it does: Think of ARDOF as a web implementation of software that we hope to integrate into currently-built cameras/sensor infrastructure. To begin, we expect this product to be effective in rural areas where there may not be heavy enough traffic to report the early stages of a wildfire before things get out of hand. On our website, the user inputs the initial image to begin the application. Still, in actual use cases, we expect existing cameras to take pictures every 30 Seconds to 1 Minute and get run through 2 Artificial Intelligence models.1: The Image is run through an ML Image Classifier to flag whether a Fire is in the image\n2: If flagged, the image is run through an LLM Model to extract key information such as location, spread status, risk level, size of fire, smoke height, stage of fire, and flammable materials.Immediately after a Fire is flagged, initial info is sent to the nearest emergency service provider over a phone call(Most likely 911 or Fire Department). Every 30 seconds after that, new information is sent out over SMS to keep responders updated on the information. Our website demos this by updating the image as the fire cycles through many stages.How we built it: The ML model for initial fire detection was trained on Google Colab with A100 runtime. The dataset consisted of 10,000 images that we split for efficient training. We trained a MobileNetV2 model on this dataset due to its capability in prominent feature extraction while staying efficient.The Backend was written in Flask, Twillio for calling and messaging, and OpenAI GPT-4o for feature extraction of the model.The front end was written in HTML, CSS, and JQuery for optimal styling and easy functionality.Challenges we ran into: Our ML Model faced many challenges due to the limited time we had to fine-tune, but we were successfully able to reach a high accuracy.Our team initially began with Palantir for the LLM Feature Extraction and was successfully able to create data pipelines, functions, and actions, but we were unable to properly integrate this into our actual Flask Application. After hours of trying, we decided to take an alternative route.Accomplishments that we're proud of: 96% Accuracy on the Fire Dataset with MobileNetV2\nFinished the software end-to-end\nIntegration with multiple software that is new to us(Twillio, OpenAI, Flask)What we learned: Our team learned a lot more about Palantir AIP, AIP Logic, and many more of their tools.\nWe learned how to use Twilio for communication software and OpenAI's API for LLM integration.What's next for ARDOF: We plan to implement our backend software into real-world scenarios such as in cameras and sensors across the world. Moving on from the test/demos of the product and truly helping mitigate the effects of wild fires is ARDOF's major goal.",
                        "github": "https://github.com/HanishAcharla/FireWebsite.git",
                        "url": "https://devpost.com/software/ardof"
                    },
                    {
                        "title": "Print Farm Tycoon",
                        "description": "Remember roblox tycoon games? Instead of simply AFKing to collect money, what if you had to start and repair 3d printers? And they could also fail?",
                        "story": "Inspiration: Roblox tycoon games, 3d print farmsWhat it does: Operate a 3d print farm to earn cash. You can choose between high and low quality 3d printers and filament, but some may cause problemsHow we built it: Roblox studioChallenges we ran into: The print failing mechanic wouldn't work until 1 hour before submission deadlineAccomplishments that we're proud of: Finishing itWhat we learned: Don't overscopeWhat's next for Print Farm Tycoon: More printers, better UIOST: Frostbyte by Alex Lihttps://onlinesequencer.net/4568325",
                        "github": "",
                        "url": "https://devpost.com/software/print-farm-tycoon"
                    },
                    {
                        "title": "Food Connect",
                        "description": "FoodConnect brings together the restaurants and shelters of this world. With our innovative solution, we can help restaurants reduce food waste by giving leftovers to shelters in need.",
                        "story": "Inspiration: We were inspired mainly due to something that's becoming visible on social media, some food employees sharing videos of the massive amounts of untouched food that have to get thrown away at the end of the day. Seeing this perfectly fine food go to waste just stuck with us, making us wonder how many other restaurants might be doing the same. This led us to create a solution that can connect restaurants willing to give away leftover food with homeless shelters or food charities, either as a donation or for a flat fee,e for example 50. This way it can help reduce the massive amount of food that gets wasted by restaurants as well as helping provide food to the homeless, and food charity events.What it does: Our project essentially connects Local Restaurants/Fast food chains with homeless shelters or food charity organizations. Restaurants can post how much food they have available in pounds, restaurants can choose if they want to charge or not for it. If restaurants don't charge it can be considered as a tax-deductible donation. Shelters/Food Charity organizations can browse restaurants near them that would fit their criteria, such as cuisine or the amount of food available. They can also use AI, which will act as a search query to help shelters/organizations find restaurants near them. On the restaurant side they can view the order and have the authority to accept or decline the order.How we built it: We built this app using Next.js for the frontend and the backend of the framework We integrated Firebase in here, which manages the database for storing orders, chats, and authentication for users to log in. We also used tailwind.css for styling and Shadcn/ui to create a clean and accessible UI. To enhance the experience for the shelters/organizations, we added a chatbot using Gemini AI, which can help suggest restaurants to order from based on the shelters' preferences. Additionally, to handle payment, we added Stripe as our provider for a smooth checkout experienceChallenges we ran into: Stripe was initially hard to configure. We had issues following the documentation, but after looking at more examples that utilized Stripe, it made it easier.Authentication was a bit challenging as we needed to set up secure routes for logged-in users and handle cases like logged-in sessions for the user when they navigate to different pages, because it would initially log them out, which took some time for debugging in the console to get it.Making the UI really pop out, this just really took a lot of debugging and various reiterations to really get the UI right.,Accomplishments that we're proud of: We are proud of completing the most difficult features of our application. The AI, Agent Stripe, a brand new UI library, and many more were definitely tedious for us to complete. We are happy that we were able to complete our main key features, and we're also happy that we kept in a position where we can make an impact.,Another accomplishment that we found pleasing was that we were able to get a full-stack application to work while making it aesthetically pleasing.What we learned: We learned that getting a full solution to a problem as big as this will take time; this isn't just a one-hackathon thing. Our team also learned how important LLMs are during this whole process, and how useful and stress-free they are to integrate within our code.We learned that better time management is required, and a more structured approach to writing code is required to make efficient solutions. Furthermore, we learned how to use Tailwind CSS further, how to utilize and optimize Firebase (specifically the Firestore database) to ensure quick responses and accurate results.Stripe also took a lot of time; last year, when we were at this hackathon, we just were not able to get Stripe working, and we pivoted to PayPal last year. This year, we are happy that we were actually able to get Stripe to work, and many of our team members found it to be a personal accomplishment as well.What's next for Food Connect: Plan on hosting the website to initially get it kicked off and have its name out there, being spreadMaking our chatbot more robust so it can handle even more complex queriesImplementing algorithm (like TikTok) suggestions based on the shelters' interests & preferences, this will display top matches at the top of the screen when the user goes to place an order.,",
                        "github": "https://github.com/nikrp/losaltoshacksix",
                        "url": "https://devpost.com/software/food-connect-vot9gc"
                    },
                    {
                        "title": "AuraCheck",
                        "description": "Empower your recovery with every moment. Our app uses facial analysis to track your sobriety and motivate your journey, helping you stay focused, clear, and on the path to strength.",
                        "story": "Inspiration: A trip we took together to San Fransisco enlightened us about the crazy amount of drug addicts on the streets who couldn't obtain help because of financial obstacles.What it does: Check the user's sobriety stateChallenges we ran into: The initial AI model didn't work as intended so we had to change the model and find a way to integrate that model into our app.What's next for AuraCheck: We want to add a sober streak that rewards users for staying sober, a community tab that allows users to interact with each other, and a feature that will enable family members to view their progress and track it.",
                        "github": "",
                        "url": "https://devpost.com/software/auracheck"
                    },
                    {
                        "title": "ForensicAI",
                        "description": "ForensicAI empowers law enforcement with AI-driven video analysis, transforming crime scene footage into actionable evidence. It automates person tracking and accurately organizes evidence. ",
                        "story": "Inspiration: Recent mass casualty incidents have exposed alarming inefficiencies in how video evidence is handled during investigations. Critical footage often goes unanalyzed for hours\u2014or even days\u2014due to outdated workflows and limited manpower. As high schoolers passionate about using our programming skills for good, we saw a powerful opportunity: what if we could bring the speed and precision of modern computer vision into the hands of investigators?ForensicAI emerged from a deep sense of civic responsibility and belief in the transformative power of technology. While we can't prevent every tragedy, we can give first responders and law enforcement tools that dramatically accelerate their ability to identify suspects, understand chaotic situations, and save lives. In crisis moments, time is everything\u2014and ForensicAI ensures no second is wasted.Technical Implementation: We developed ForensicAI using a sophisticated microservices architecture:Computer Vision Pipeline: Custom-trained YOLOv8 model with DeepSORT integration for multi-object tracking, achieving 94.7% mAP on our validation datasetBackend: Flask-based REST APIFrontend: React, Material UI with custom theming, and D3.js for advanced data visualizations,We implemented WebRTC for streaming processed footage and WebSockets for real-time analysis updates, creating a responsive system even with large video files.Advanced Features: Real-time Analytics Pipeline: Stream processing of user activity dataMachine Learning Integration: Recommendation engine based on user behaviorData Versioning: Complete audit trail of all data changesAsynchronous Task Processing: Background jobs for resource-intensive operationsResponsive Design System: Adaptable UI for all device sizesTheme Customization: Light/dark mode and custom color schemesResource Pooling: Efficient management of system resourcesAPI Gateway: Centralized management of API endpoints and versions,Key Challenges: BackendHandling backend errors and getting the video analysis to work with a custom trained model to detect weapons.PerformanceDealing with bottlenecks that only emerge under heavy loadEffective strategyBalancing different parts of the project at the same time.Cross-platform compatibilityEnsuring consistent functionality across different browsers.,Future Directions: Smarter video analysis: Implementing new AI models that can understand complex actions and behaviors in video, not just detect objectsAudio + video processing: Adding the ability to analyze sounds (like gunshots or voices) alongside video, creating a more complete picture of eventsSelf-improving models: Building systems that learn from expert feedback to continuously improve accuracy with minimal additional training dataTransparent decision-making: Creating tools that explain how the AI reached its conclusions, making the system's findings more trustworthy and usable in investigations,Impact: ForensicAI is more than a technical project\u2014it's a mission-driven response to one of society\u2019s most urgent problems. By combining deep learning with real-time video analysis, we've created a tool that empowers law enforcement to act faster and smarter during life-threatening crises. In our early testing with local agencies, we cut video analysis time by 87%, turning hours of footage into actionable intelligence in mere minutes.This impact isn't theoretical. It means faster suspect identification, more lives protected, and stronger evidence in court. ForensicAI proves that youth-driven innovation can produce serious, scalable solutions to challenges that affect us all.",
                        "github": "https://github.com/kotharidhruv/ForensicAI",
                        "url": "https://devpost.com/software/forensicai"
                    },
                    {
                        "title": "Gambler's Notion",
                        "description": "Incentivize and gamify your work!",
                        "story": "Inspiration: How can we gamify tasks to counter our procrastination tendencies?\n   Life is too precious to spend it locked in a corner slaving away at our work. The Gambler's Notion seeks to offer an interactive social production platform, drawing inspiration from platforms that have served us. Our tasks can feel so daunting at times, but with the help of AI, we can figure out the best way to tackle the most tedious of tasks while competing against user around the world.What it does: Gambler's Notion is gamified version of a productivity app complete with AI assisted To-Do List with a points system designed to push \u201cplayers\u201d to finish before an AI-determined deadline. The AI develops a formulaic approach to help you accomplish your tasks while challenging you with a \"time crunch.\" This self-imposed deadline strategy has shown great success in past studies, and along with the integration of a pomodoro clock, we can achieve our goals in a fraction of the time of the original, procrastinating approach. \n   Points can be gained for accomplishing a task before the deadline, with more points achieved by harder tasks under a shorter timeframe. These points can go into unlocking minigames to enjoy during breaks and avatars to show off your productive prowess on our global points leaderboard!How we built it: Originally hosted on GitHub Pages + Vaadin, we migrated our web-based program onto Wix and went with a more streamlined approach to our solution.\n   We discovered what an API was and integrated Gemini API onto our web app, setting up the core of our project along with ADI design of the UI/UX. The projects also utilizes a collection and a database to update and store the last query sent by Gemini and display the task list on screen along with an final deadline ETA.\nLastly we added a progress bar that serves as a timer for the task!The fully fleshed out program is likely to utilize multiple integrated platforms to host the pomodoro timer, global leaderboard, and minigame UI and gameplay. Googles' OAuth API service will be used for Secure Sign-In (SSO) to keep track of individual user data and leaderboard scores.Challenges we ran into: Deciding the best IDE to set up TGNDebugging JSON parsing errors in the API setupConnecting Gemini AI to an integrated database in the back-endWe shot for too much in too little time, which affected the end result. Our team currently lacks the coding knowledge necessary to fully flesh out this project as we originally imagined, however we will continue to work on this program as we gain more experience with hackathons.\n## Accomplishments that we're proud of\nBeing able to apply multiple languages and navigating new software in a short amount of time.\nWe learned how to interface AI with a website builder based on Wix\u2019s Velo;\nWe discovered how an API key works and its use in authentication and artificial intelligence;\nAnd we created a clean back-end function to fetch task lists from Gemini while establishing real-time input/output flow, along with a polished UI and front-end interface to engage the user!\n## What we learned\nMaybe we should not spend so much time on one piece...\nThis hackathon had a steep learning curve, and we had to learn how to code in HTML, CSS, and JavaScript, but we are proud to have said we have successfully hacked our way through Los Altos Hack IX~ with a pretty good demo to boot!\n## What's next for Gambler's Notion\nThe game (gamble) itself! We hope to add the next game pieces as well as competition across the app. The app will use Google SSO to personalize and keep track of the scores from the games. Most of our future updates will become easier as we develop technical knowledge in both the frontend and the backend. We'll also be able to integrate more sophisticated systems such as MongoDB and Lambda into our app, allowing for a more fliud user experience! We have great hopes for this project - Gambler's Notion is sure to one day hit the App Store!,",
                        "github": "",
                        "url": "https://devpost.com/software/gambler-s-notion"
                    },
                    {
                        "title": "Tripseeker",
                        "description": "A travel guide to help you find and explore new locations at your travel destinations based on your Google Maps data.",
                        "story": "Inspiration: People love to travel a lot to new countries and cities. However, finding places you would like can be challenging because everything is so new. So, we developed Tripseeker to help recommend places and create an itinerary based on your past Google Maps data.What it does: Tripseeker takes in either your Google Maps Takeout or profile. Based on your travel location or your desired area of interest, we generate a full travel itinerary tuned to your personal interests and past experiences. Further, we revolutionize sightseeing once and for all. Combining good food, good sights, and good activities based on YOUR preferences.How we built it: We built the backend, grabbing data from users' Google reviews. Then, based on the desired location and search settings, we send it to our deep learning pipeline. Based on this, we tokenized ratings using DistellBERT. With the embeddings, we pass them in our RAG pipeline. Finally, with our highest compatibility locations, we pass them into our fine-tuned LLM for itinerary planning.Challenges we ran into: A challenge we ran into was collecting data from Google Maps, which we overcame by using a mix of Selenium, JavaScript, and BeautifulSoup. We also ran into difficulty making the recommendation model fast, so we used multithreading to improve performance. We also ran into merge conflicts nearly every time we pushed, which we had to resolve. We also found it difficult to design the homepage and settled on one that worked.Accomplishments that we're proud of: We are proud of developing an algorithm that can tell what a person likes and does not like purely based on the reviews they gave on Google Maps, and we could then use that data to give further recommendations in new places. We are also proud of designing a clean, appealing UI for the website.What we learned: We learned how to analyze a person's preferences and dislikes based on the reviews they give of places they visit. We also learned how to make a clean website UI and work together as a team, especially resolving merge conflicts.What's next for Tripseeker: We hope to further develop our model to make it more consistent and accurate for better recommendations and ultimately a better traveling experience. Tripseeker can also be expanded to mobile apps for easier accessibility.",
                        "github": "",
                        "url": "https://devpost.com/software/tripseeker"
                    },
                    {
                        "title": "MindForge AI",
                        "description": "Tired of rewatching lectures and remaking flashcards? Our Chrome extension turns video lessons into quizzes, tracks your progress, and rewards you with coins to power up your RPG-style avatar!",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/mindforge-ai"
                    },
                    {
                        "title": "ZenBot",
                        "description": "Your AI companion for problems big and small, standing with you through every challenge.",
                        "story": "Inspiration\nSuicide is the second leading cause of death among youth ages 15 to 19 in the United States. At the same time, mental health issues among children ages 8 to 16 have risen by 31% between 2019 and 2025. These numbers are shocking and show that many kids are struggling in silence. A lot of children are scared to open up to adults, whether it\u2019s because they fear being judged or simply don\u2019t know how to express what they are feeling. We wanted to create a solution that feels safe, non-judgmental, and available at any time. That\u2019s where ZenBot came in. ZenBot is designed to be an AI companion for problems big and small. It provides users with a private, safe space to talk about their feelings and receive helpful, supportive feedback. We were inspired by the idea that technology could fill the gap where human systems sometimes fail, offering comfort and understanding when it\u2019s needed most.What we learned\nThroughout this project, we learned a lot about how AI can be used to support mental health in real, meaningful ways. We learned how to collect and clean real-world datasets from Kaggle, focusing on emotional support conversations to train our model. We explored how to fine-tune LLaMA 2, an advanced open-source large language model, to better understand and respond to users in distress. We also learned about integrating different technologies, like Python, Flask, React, and the OpenAI API, to bring everything together in a way that feels smooth and natural for the user. Most importantly, we learned how important it is to build tools that respect privacy and create safe environments for people to open up about difficult topics.How we built it\nWe started by gathering emotional support and mental health conversation datasets from Kaggle. After cleaning and preparing the data, we used it to fine-tune the LLaMA 2 language model to help ZenBot understand sensitive conversations and respond in a supportive and helpful way. For our front-end, we built the application using React and HTML to make sure the user interface was fast, clean, and easy to use. On the back-end, we used Python and Flask to handle conversation processing and manage API calls. We integrated OpenAI's API to help generate real-time responses and made sure our system could recognize voice input for a more natural experience. Features like Export Notes allow users to save and revisit conversations, while our voice recognition system detects distress in speech to offer coping strategies quickly. Our goal was to make ZenBot feel like a trusted companion that anyone could turn to at any time.Challenges we faced\nOne of the biggest challenges we faced was fine-tuning the LLaMA 2 model. It required a lot of testing and adjusting to get the responses to feel natural and supportive without sounding too robotic. Cleaning the Kaggle datasets was also a tough process because we had to remove irrelevant data and make sure the training material was clean and effective. Integrating voice recognition and making sure it accurately detected distress signals in real time was another challenge. We also had to work on keeping our API calls fast and secure, so that users could have smooth conversations without delays. Balancing all of these technical pieces while keeping the app user-friendly took time, but it taught us a lot about working with complex systems and building something that can genuinely help people.",
                        "github": "",
                        "url": "https://devpost.com/software/zenbot-vpg8zh"
                    },
                    {
                        "title": "RF: Earthquake Predictor",
                        "description": "Created an interactive earthquake map using USGS data and ML. Visualizes seismic clusters by intensity and predicts significant quakes (mag \u2265 5) via Random Forest with F1 score and confusion matrix.",
                        "story": "Inspiration: Recently, the city of Dublin, California experienced an unexpected and unsettling event: two back-to-back earthquakes that caught residents completely off guard. These seismic events, although not catastrophic, served as a wake-up call, highlighting the unpredictability of earthquakes and the lack of accessible, real-time tools to help people understand or anticipate them.What it does: This project is designed to monitor, analyze, and visualize global earthquake activity using real-time data from the US Geological Survey (USGS). By automatically retrieving up-to-date information on earthquakes occurring around the world for the past 30 days, the system enables continuous tracking of seismic events. The goal is to transform raw earthquake data into accessible, meaningful information that can support public awareness, scientific research, and potential early warning or risk assessment efforts.How we built it: We used Python to pull real-time earthquake data from the USGS, cleaned it, and visualized it using Seaborn and Folium. Then we trained a Random Forest model to classify whether an earthquake is significant, using SMOTE to handle class imbalance. We also built an interactive map and evaluated model performance with metrics like accuracy and F1 score.Challenges we ran into: The main issues were class imbalance, occasional missing data from the live feed, and slow map performance with lots of points. We used SMOTE for balancing, added checks in preprocessing, and solved the map issue with clustering. Choosing between classification and regression was also a key early decision.Accomplishments that we're proud of: We are proud of creating a real-time data pipeline and using forest random to integrate machine learning into our application. We are also proud of creating an interactive global map.What we learned: Throughout the development of our Earthquake Detector, we gained valuable experience in several key areas such as machine learning implementation, working with real time data and data processingWhat's next for RF: Earthquake Predictor: For RF: Earthquake Predictor, we could improve it by making the application predicting earthquakes with a magnitude greater than 6.0, using a different learning model to potentially increase our F1 Score, improving our data visualization by adding filters to the folium map and making the transition from graph to map smoother",
                        "github": "",
                        "url": "https://devpost.com/software/rf-earthquake-predictor"
                    },
                    {
                        "title": "Trash Talker",
                        "description": "Gamifying community cleanup through machine learning.",
                        "story": "Inspiration: On our school campus, we notice lots and lots of trash, but nobody seems bothered!What it does: Our app gets image input of your trash, either through live feed or through image upload, and then it classifies it into a bin (recycling bin, hazard bin, etc.) and gives you points based on its reusability.How we built it: We first created a dataset containing thousands of images and trained a CNN image classifier. After yielding unideal results, we decided to borrow from a multitude of datasets and train a YOLO model to classify the waste based on our specific classes.Challenges we ran into: Training the model, creating the dataset, and having a fully-functional backend.Accomplishments that we're proud of: Training the model, creating the dataset, and having a fully-functional backend.What we learned: We learned how to do image classification as well as image classification in real time!What's next for Trash Talker: More gamification to incentivize users to clean our environment more.",
                        "github": "",
                        "url": "https://devpost.com/software/trash-talker"
                    },
                    {
                        "title": "RagebaitResume",
                        "description": "A sarcastic AI-Powered assistant that ensures that your future is 100% in your hands by reviewing your resume and giving you feedback based on weak points, and interviewing you to enforce conciseness.",
                        "story": "Inspiration: After pondering our concerns about our future and what we could do to improve it, we settled upon the idea of resumes and interviews as we realized that we all had one thing in common: We haven't written a resume in our life. Empowered and concerned by this fact, we strove to create a tool to ensure that our futures were not a risky gamble dependent on the pity of the interviewer.What it does: Our new and improved AI-powered helper reviews your resume before highlighting your weak points and suggesting improvements. It also gives you the opportunity to find jobs that fit your resume and test yourself in a mock interview using the personalized AI. Finally, it saves your data in the log terms using a login system so that you can look back on your past attempts at a 100% secure future.How we built it: We started out by researching and brainstorming ideas and softwares we could potentially use to maximize our efficiency, preferably ones that we were acquainted with. Using heavy ideation, we then moved on to the main part of our algorithms. We start off by parsing the user's resume and giving specific feedback while still keeping the tone direct, short, and concise. Moving on from this, we started implementing mock interviews and a job recommendation system. Once finished with the main function of our website, we moved on to storing and using user data in a secure location and refining the UI to keep things much more user-friendly.Challenges we ran into: There were many issues and bugs, and at times we weren't confident in finishing a feature within the given timeframe, so we often took U-turns at dead-ends, heavily impacting the time we had. To add on, we weren't as experienced in using databases such as MongoDB, or supabase. However, with the help of AI-assistance, we were able to finalize our product.Accomplishments that we're proud of: We are proud of the steps we took in mixing together AI-assistance and databases to ensure a smooth transition for the data being used. Databases were a significant obstacle that we were proud to overcome.What we learned: We learned how to utilize databases and implement APIs to improve our user friendly factors.What's next for RagebaitResume: Improving our utilization of databases, and the accuracy of our AI-powered feedback, and making it more customizable for users.",
                        "github": "https://github.com/MasterY0das/RagebaitResume",
                        "url": "https://devpost.com/software/ragebaitresume"
                    },
                    {
                        "title": "Pulse Point",
                        "description": "Pulse point us used during music festivals and concerts to help location fentanyl overdose victims and locate them in time by responders. Responders can then locate victims and administer Narcan. ",
                        "story": "Inspiration: Drug use at festivals and concerts is extremely common. Due to the high population density, it is extremely difficult for paramedics to find the patient in time to administer the antidote. According to a study done by Monash University, \"2305 survey participants at 23 festivals in Victoria reported that almost half (48 per cent) had recently used drugs.\" Last year, four of Rishika's high-school peers died from opioid overdose. Heartbroken, she pondered, \u201cHow can I help put an end to the fentanyl epidemic?\u201dWhat it does: Emergency locating app for Fentanyl overdose victims so that Narcan can be administered in time.How we built it: We used Palantir for the back end (handles processing the victim location and alerting nearby responders). Additionally, we utilized react for the front end app.Challenges we ran into: Accomplishments that we're proud of: What we learned: Be comfortable with pivoting.\nDon't be afraid to ask for help.What's next for Pulse Point: Expanding from our proof of concept and capturing live data from both the victims and responders.",
                        "github": "https://github.com/yabu132/OverdoseAlert",
                        "url": "https://devpost.com/software/pulse-point-d1gijl"
                    },
                    {
                        "title": "CheckYoSelf",
                        "description": "This is a quick daily check-in to reduce your stress and boost your happiness. After using this, you won't need a therapist.",
                        "story": "",
                        "github": "https://github.com/saunabhandari/los-altos-hacks-2025",
                        "url": "https://devpost.com/software/checkyoself-kpmhnu"
                    },
                    {
                        "title": "Serenity",
                        "description": "Introducing Serenity, an AI-powered mental health companion. Allowing for human like conversations, Serenity offers an immersive & interactive interface powered by data, helping uplift one's mood.",
                        "story": "Inspiration: Inspired by the prevalent need for products fighting mental health issues, paired with our group members having also experienced stress and anxiety within our own lives, we decided to build Serenity in hopes of mitigating this large problem.What it does: Serenity allows a user to have a conversation with a mental health companion, otherwise known as Serenity. This AI Chat Bot is backed by data from scientific studies to map the user's mood, determined through conversation, to a background paired with audio customized solely based on the users mood. In addition to having a personalized user UI, users can use both the text to speech and voice to text features to simulate a real life conversation, through verbal contact.How we built it: We used HTML & CSS for the frontend, while using GroqCloud's API for text to speech, and Open AI's API for our chatbot. Additionally, we utilized scientific studies to perfect the mapping system of a user's conversations to mood, to allow optimal color and composition to be displayed in the background. We deployed our website with Vercel.Challenges we ran into: A challenge we ran into was the issue of a user losing all their data and conversations if they ever left the website, so in order to deal with this, we stored localized memory within the users device, encompassing a short summary of the prior conversation, to allow the AI to recall past messages if the website is opened again.Accomplishments that we're proud of: An accomplishment we are proud of was the ability to come up with the unique idea of a changing background backed by data, as well as the ability to seamlessly integrate multiple APIs within our project.What we learned: We learned quite a lot, and specifically gained a lot more experience within text to speech and voice to text features. We also gained more experience within HTML and CSS through this project.What's next for Serenity: Our future goals with Serenity are to train our own LLM to allow for a stronger mapping system, increasing the accuracy for the detection of the mood of the user. We also hope to upscale this project and gain more input for our product through beta testing.",
                        "github": "https://github.com/Mahasvin24/Serenity-2.0",
                        "url": "https://devpost.com/software/serenity-wzco5f"
                    },
                    {
                        "title": "NutriFresh",
                        "description": "Our AI-powered platform optimizes food distribution, reducing waste and ensuring that excess food efficiently reaches those facing hunger.",
                        "story": "Inspiration: Food insecurity affects millions, while vast amounts of edible food go to waste every day.\nWe were inspired by the inefficiencies in the current food distribution system, where food banks often struggle with accurate demand forecasting and equitable allocation.\nGiven how AI has transformed logistics and supply chains, we wanted to harness its potential to make food distribution smarter, faster, and more impactful.What It Does: NutriFresh is an AI-driven food redistribution platform that connects food donors, food banks, and food pantries. It ensures that surplus food is efficiently distributed to communities in need.Core Features:\nFood demand forecastingSmart inventory managementHigh-need area identificationIntegrated Mapbox for geographic visualizationHeatmaps for enhanced visibility of underserved regionsSimulation tools for agricultural surplus predictionImage classification for inventory intakeExpiration detection for food quality managementHow We Built It: We built NutriFresh by combining frontend and backend systems with geographic visualization and AI-driven logic. Our platform integrates data pipelines, real-time user inputs, and mapping interfaces to create an intelligent, user-friendly experience. Custom components were developed for food intake, demand estimation, and inventory updates, with seamless synchronization across donor, pantry, and food bank portals.Challenges We Faced: One major challenge was ensuring a clean, responsive UI while integrating complex backend logic and visual mapping tools.Accomplishments We're Proud Of: We're especially proud of integrating real-time satellite data into our map interface, allowing food banks to make better-informed decisions about distribution based on geographic and agricultural insights.What We Learned: We learned how to work with geographic data, build smart inventory systems tailored to non-profit organizations, and integrate mapping solutions into modular application components.What's Next for NutriFresh: Public API Development: Enable third-party platforms to access donation tracking, route optimization, and AI-powered expiration predictions.Advanced Satellite Analysis: Use satellite data to identify food waste hotspots and predict agricultural surplus.Mobile Food Pantries: Launch AI-optimized delivery routes and build volunteer-powered mobile units to serve remote and underserved areas.",
                        "github": "https://github.com/sasidharJasty/LosAltosHacks",
                        "url": "https://devpost.com/software/nutrifresh"
                    },
                    {
                        "title": "EASE \u2013 your quiet space for self-healing",
                        "description": "A journaling and mood-tracking platform that uses AI support and calming visuals to help users reduce stress, reflect deeply, and reclaim their emotional well-being.",
                        "story": "\u2728 Inspiration: The inspiration for EASE came from a deeply personal place. With over 21 million adults in the U.S. experiencing major depression each year, we recognized that many\u2014especially teens\u2014don\u2019t seek help. Whether it\u2019s due to stigma, cost, or the belief that their issues are \u201ctoo minor,\u201d their struggles often go unaddressed until they escalate.We wanted to create a space that felt safe, supportive, and personal\u2014where users could reflect, track their emotions, and receive gentle encouragement without judgment. That's how EASE was born: a quiet digital space for self-healing.\ud83d\udca1 What we learned: Throughout this project, we learned how to combine mental wellness research with real-world technology to design a system that balances user empathy with intelligent features. We also explored techniques in mood inference, interactive journaling, and natural-sounding AI feedback.\ud83e\uddd7 Challenges we faced: Designing AI responses that feel natural, helpful, and emotionally intelligentMapping mood scores to helpful icons, animations, and feedback in a way that avoids judgmentCreating an interface that balances utility and emotional safetyManaging real-time data syncing and ensuring a smooth UX across devices",
                        "github": "",
                        "url": "https://devpost.com/software/ease-your-quiet-space-for-self-healing"
                    },
                    {
                        "title": "Resu-ME",
                        "description": "Introducing Resu-ME, the perfect place to prep for an interview! We are the worlds first AI powered interview prep, committing to getting YOU the job you're looking for, or find the perfect candidate.",
                        "story": "Inspiration: In the current economy, landing a job can feel impossible. Especially for first time job seekers, it can be daunting navigating the interview process not knowing if your resume is even fit for the job. Conversely, recruiters are now swamped with often hundreds of applications, making it difficult to analyze which candidate is the best. Luckliy, there's Resu-ME, the first 100% AI assistant to job seekers and recruiters. Our mission is to help people of all ages land the jobs of their dreams through easy to use AI.What it does: Users can submit their resumes through various chat boxes on the website. Utilizing AI, the platform scans each resume and provides tips to enhance its effectiveness, ultimately increasing the likelihood of success for applicants. In addition, Resu-ME offers a mock interview tool powered by the DeepSeek API, allowing users to practice for upcoming interviews through real time voice interaction. This comprehensive approach helps both job seekers and recruiters understand the recruitment process.How we built it: Our front end was built in Next.js, React, Tailwind CSS, and HTML. Our backend utilizes various APIs, including DeepSeek API, Web Audio API, and MediaRecorder.Challenges we ran into: Throughout our development, we faced several challenges, such as accessing and implementing the APIs and getting DeepSeek's responses to mimic those of a real interviewer. Additionally, some team members were initially unfamiliar with using GitHub.Accomplishments that we're proud of: We were able to develop a full scale working application with DeepSeek. We are proud of our sleek design, optimal user experience, and comprehensive tool.What we learned: Although we encountered challenges as new GitHub users, we successfully learned to navigate the platform while working on our project, thanks to the guidance of our most experienced member. We also gained experience using new APIs and Generative AI.What's next for Resu-Me: Moving forward, our group aims to enhance Resu-ME by including a real-time interview feedback feature and connecting employers with candidates.",
                        "github": "https://github.com/aarushi-s14/ai-job-app",
                        "url": "https://devpost.com/software/resu-me-bmns0e"
                    },
                    {
                        "title": "Neighborly",
                        "description": "Speak your language, share your story",
                        "story": "Inspiration: We were inspired by our grandparents who live in different countries. When they come to visit the US, they have a hard time making friends similar to the ones they have in their home country. So we created an app that connects people based on interests, location, and home language to make it easier for seniors to find a community no matter where they are.What it does: Our app takes into account the user's language of choice, interests, and their location. Then it creates a profile with all of this data and has it show up as a pin on a map. From there, users can see their own location and find other people near them who share their interests and language.How we built it: As we are beginners and still learning java we decided to use Thunkable, a block coding app. We split up the project into small chunks and assigned them to members, helping each other debug.Challenges we ran into: We had trouble making sure each user was only listed in the database once. We wanted each user to be able to see all other users on the map, so whenever someone signed up for the app, we took their location, language, and interests and put it into a new row in the sheet. However, we didn't want to accidentally add people to the sheet as they signed in, because that would create multiple copies of the same user. So we created 2 different pages for new users vs. old users to make sure users existed in the database only once.Accomplishments that we're proud of: We are proud of the UI of our app, we tried to make it as user friendly and simple for seniors. We are also proud of the way one user can see all users, because none of us had ever made a multi-user project before.What we learned: As we progressed through building our app, we quickly learned the value of teamwork. Nothing would be possible without any member of our group.What's next for Neighborly: We hope to add translate options for the entire app and add filters to the map to see who your most compatible for.",
                        "github": "",
                        "url": "https://devpost.com/software/neighborly-rgles6"
                    },
                    {
                        "title": "MediScan",
                        "description": "A scan a day keeps the medical bills away.",
                        "story": "MediScan was inspired by the need for accessible, understandable, and personalized healthcare, especially for individuals in underserved communities. The aim was to bridge gaps in healthcare access and empower individuals to make informed decisions about their health.MediScan is like a smart helper that checks your symptoms. It uses Gemini (Implemented with API keys) to figure out what's wrong and tell you what to do. It asks questions in the form of text, voice, and images, looks at your symptoms, tells you what might be happening, and suggests what to do next. It also gives you info about medicines, like if you need a doctor's note to get them and what side effects they might have. It also helps you find places nearby to get help, like clinics and drug stores.MediScan uses AI and machine learning for symptom analysis, natural language processing to understand user input, and a user-friendly interface built with HTML, CSS, and JavaScript.One of the biggest problems was making sure the AI was accurate about our symptoms. It often provided inaccurate results, or no results at all. However, we persevered and pulled an all nighter to fix our code and make a stunning project.We are proud that we made a symptom checker that's easy to use and can help people find the right info and places to get care. We think it can make a big difference in how people take care of their health.We learned a lot about computers (AI), making websites, and why it's important to have the right medical info.We want to make the computer even smarter, make sure it's even better at checking symptoms, add more things to do (like video calls with doctors), and work with clinics to use MediScan in real life.",
                        "github": "https://github.com/GoodJobCoder/mediscan",
                        "url": "https://devpost.com/software/z-kw2xh4"
                    },
                    {
                        "title": "FlickPoint",
                        "description": "Making Computers Accessible To All",
                        "story": "Inspiration## Inspiration: We came up with FlickPoint as a project based on computer vision, which many in our team were interested in. Furthermore, we wanted to tackle an interesting challenge of creating an accessible input device purely based on software.What it does: FlickPoint is a program that uses hand gestures to control a computer. It also has speech to text for typing. This is very helpful for people with disabilities who may not be able to use their hands to control their device the standard way.How we built it: For machine vision and hand tracking, we utilized the ML5 library, built on TensorFlow, in JavaScript. Using the coordinate data, we can identify the fingers' positions in relation to each other, and recognize the gesture being performed. In order to properly implement mouse movements, we built a Python API using FastAPI to work around the challenge of JavaScript being limited to its own tab. Finally, we originally created a script to utilize the ElevenLabs API to process Speech to Text inputs. However, we had to fall back to the much less feature-rich Webkit Speech Recognition to implement a stripped back version of this tool.Challenges we ran into: Getting the client to communicate with our API was quite a challenge because of the frequency of requests being sent. This necessitated multiple rewrites of both client side and API code, even switching the API from Flask to FastAPI and working on a number of optimizations. On the Speech to Text front, our ElevenLabs API keys were disabled due to 'unusual activity' after we had already created a full script for it, so we had to make a last minute change to Webkit Speech Recognition, which required a major rewrite of code and a loss of multiple features.Accomplishments that we're proud of: We are very proud of getting this ambitious project to a working state, especially with the combination of different fields that we needed to cover. The integration between these was often challenging, so it is a huge relief to see it through.What we learned: We developed our skills greatly in various areas. Not only did we learn how to extract and analyze data from a machine vision model, two of our members have never used JavaScript before this hackathon, so this was a great learning experience for them.What's next for FlickPoint: We would love to add virtual keyboard functionality on top of polishing our existing features for a more intuitive user experience, which after all is our end goal with this project.",
                        "github": "https://github.com/pbhak/LAH-IX",
                        "url": "https://devpost.com/software/flickpoint"
                    },
                    {
                        "title": "Mocktech",
                        "description": "Ace your technical interviews!",
                        "story": "Inspiration: I had started looking over leetcode recently and had understood how although it is great it does not solve all of the issues we have with technical interviews today. A much less limited option isWhat it does: It is a platform that acts like a interviewer asking you interview questions and has a code editor to show an interviewer your skills.How we built it: Next.Js with another Typescript BackendChallenges we ran into: Ran into challenges with communication between frontend and backend. Also its very difficult to get the code editor to work.Accomplishments that we're proud of: Working Code editor for 3 languages and quick responsive AI.What we learned: How to use typescript in backend application and integrate it into the Next.jsWhat's next for Mocktech: New features for even more AI correction and feedback.PLEASE REACH OUT TO ME IN PERSON FOR THE DEMO I COULD NOT FILM IT ONLINE:",
                        "github": "https://github.com/NachuT/mocktech2",
                        "url": "https://devpost.com/software/mocktech"
                    },
                    {
                        "title": "HeadsUp!",
                        "description": "The study buddy powered by facial tracking technology and generative AI\r\n",
                        "story": "Inspiration: Our struggles with being distracted by our phonesWhat it does: Acts as an interactive study-buddy who will remind you to stay focused when you look at your phoneHow we built it: We used several API's for the different functions of our program. We used ElevenLabs's speech-to-text and text-to-speech, Google's gemini for generative AI, and Open CV to track the user's face.Challenges we ran into: Finding free API keys that still functioned well was difficult as many quality ones charge the userAccomplishments that we're proud of: Successfully implemented the usage of advanced head-tracking technology, used AI tools effectivelyWhat we learned: We learned how to use different APIs for lots of different purposes.What's next for HeadsUp!: Adding a UI and making the hotkeys work better would be a good start to furthering HeadsUp!",
                        "github": "https://github.com/Baanista/ProductivityHacker",
                        "url": "https://devpost.com/software/headsup-lzoixp"
                    },
                    {
                        "title": "RFX",
                        "description": "Game-changing analytics for game-changers everywhere. Talent shouldn\u2019t be limited by budget, and with this in mind, we're helping teams play smarter, before harder.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/rfx"
                    }
                ],
                [
                    {
                        "title": "WISER",
                        "description": "Get wiser in shorter time.",
                        "story": "Inspiration: I loved watching podcasts but never had the time to watch them, as podcasts are now 2-3 hours nowadays.What it does: Tracks and summarizes youtube podcasts / different videos, and allows me to use Text to Speech technology to make it even more efficient for me to consume.How we built it: I used Next.js, Youtube API, and Groq LLM to make this happen.Challenges we ran into: Rate limits from LLM servicesAccomplishments that we're proud of: I built a functional project that I will actually use consistently.What we learned: The necessity of vibe codingWhat's next for WISER: Better rate limits, more customizations and features.",
                        "github": "",
                        "url": "https://devpost.com/software/wiser-pew36n"
                    },
                    {
                        "title": "You are what you eat!!",
                        "description": "We all know someone with dietary restrictions, who could use a tool to simplify their daily routine. Try the wellness app to improve your quality of life.",
                        "story": "Inspiration - We wanted to create an app that helps people choose meals based on their diet and flavor preferences while promoting mental well-being.: What it does - The app suggests recipes based on dietary restrictions and flavor preferences, and provides a mental health tip related to the chosen flavor.: How we built it - We used HTML, CSS, and JavaScript to build the app. It matches user inputs with recipes and tips. The design is modern and includes a dark mode option.: Challenges we ran into - We had to ensure the app correctly matched dietary restrictions and flavors with recipes. We also worked on making the design user-friendly and responsive.: Accomplishments that we're proud of - We created a smooth user experience with recipe suggestions and mental health tips. The dark mode toggle is also a nice touch for user comfort.: What we learned - We learned how to build dynamic web apps using JavaScript and how food can impact both health and emotions.: What's next for You are what you eat!! - We plan to add more recipes, dietary options, and a meal planner feature in the future.:",
                        "github": "",
                        "url": "https://devpost.com/software/you-are-what-you-eat"
                    },
                    {
                        "title": "billWise",
                        "description": "Health made more accessible.",
                        "story": "Inspiration: We\u2019ve seen our families and communities suffer from medical debt, not because they received unnecessary care, but because they had no way to verify or understand what they were being charged for. Most people blindly trust their bills, unaware that hospitals can legally charge up to 10\u00d7 more than fair market rates. We wanted to change that. With the rise of advanced machine learning, we saw an opportunity to bring transparency and accountability to healthcare billing. That\u2019s how BillWise was born \u2014 an AI-powered assistant to help you spot scams, decode confusing charges, and take control of your healthcare costs.Some statistics/facts:Hospitals can charge as much as ten times the cost of production for their services (American Bar Association).Furthermore, anywhere from 30-80% of all hospital bills incorrectly charge patients.Healthcare spending in the U.S. reached $3.8 trillion in 2019, comprising nearly 18% of the country's GDP (National Health Expenditure Accounts, NHEA).,What it does: BillWise is designed to bring clarity, fairness, and independence to people navigating a complex and often predatory healthcare system.How we built it: Our tech stack includes:A custom-trained GAN (Generative Adversarial Network) that estimates fair costs for medical treatments based on location and known billing codes.A hand-built location API, which matches users to regional pricing data from public healthcare datasets.A bill parser that extracts line items from text, PDFs, or images, then compares them against GAN-generated estimates.A Recurrent Neural Network (RNN) for symptom-based diagnosis, trained on symptom-disease pairs with fine-tuned accuracy \u2014 built to avoid the anxiety loops common in internet search-based diagnosis.A secure backend using Node.js + MongoDB with encryption and blockchain-style hashing to ensure that medical data and user credentials are tamper-proof.Our frontend is built from scratch using React Native, HTML, SCSS, and JavaScript.,We built core AI models, integrated secure infrastructure, and packaged it into a clean, responsive, and meaningful user experience.HospitalBill1:https://drive.google.com/file/d/1zUVECiRjhFv8atqnlLLmL3lrEWJlKpgx/view?usp=sharingHospitalBill2:https://drive.google.com/file/d/1AdGTfb2e-y6XS5lXSKvIx1sT-Y9lQKzZ/view?usp=sharingPlease use for testing purposes ^^Challenges we ran into: Data Overfitting: Early versions of our GAN overfit due to the limited diversity in pricing data. We implemented dropout layers and added synthetic augmentation to stabilize learning.Inconsistent Bill Formats: Medical bills aren\u2019t standardized, so we had to write a custom NLP parser to extract charges and match them to CPT code equivalents.Frontend Conflicts: Mixing a React Native backend with static HTML/CSS UI introduced compatibility challenges that required deep integration logic to unify the stack.Accomplishments that we're proud of: Built a full-stack AI tool that tackles one of the most predatory aspects of modern healthcareDeployed a custom GAN + RNN in the same systemCreated a region-aware medical pricing engine using our own location-matching APIIntegrated data integrity protections for real healthcare data in just 24 hours,What we learned: We learned how to combine multiple AI systems \u2014 GANs, RNNs, and NLP pipelines \u2014 in a single, coherent product. We improved our understanding of healthcare billing codes and explored how location impacts pricing in real-world hospitals. We deepened our skills in backend architecture, database integration, and frontend polish. Most importantly, we learned how to use AI to solve a deeply human problem.What's next for billWise: Launch as a free app for patients and caregiversExpand GAN training data with insurance-adjusted pricingIntegrate real-time insurance matching and billing dispute templatesPartner with healthcare nonprofits to help underserved communitiesAdd voice input and scanning for paper-based bills,",
                        "github": "",
                        "url": "https://devpost.com/software/fairhealth-zerm2b"
                    },
                    {
                        "title": "Tewen",
                        "description": " Tewen uses and leverages tourism as a platform to spread awareness about the complex social issues faced in cities, which engages visitors and local youth in meaningful advocacy.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
                        "story": "",
                        "github": "https://github.com/nahomt1622/Tewen",
                        "url": "https://devpost.com/software/tewen"
                    },
                    {
                        "title": "Speak for Me Ai ",
                        "description": "A web app that reads lip movements and audio through your camera and turns them into real-time text, enabling communication for deaf and mute individuals without typing.",
                        "story": "Inspiration: We wanted to build a tool that empowers deaf and mute individuals to express themselves more naturally. Typing and sign language can be limiting in certain settings, so we set out to create a hands-free, voice-free communication system that works in real time \u2014 right in the browser.What it does: Speak for Me AI is a web-based application that uses a webcam to detect lip movements and optionally audio, then translates them into live text. It requires no typing and runs directly in the browser, making it accessible and easy to use.How we built it: Frontend: HTML, CSS, and JavaScript to create a clean, responsive UILip and Audio Detection: Python with OpenCV and Dlib for facial and lip tracking, and optional speech recognitionMachine Learning: A custom-trained deep learning model to map lip movements to wordsIntegration: WebSockets used to send real-time data from the Python backend to the web frontendChallenges we ran into: Ensuring accurate word recognition from lip movements without relying on soundTraining the model with limited, clean lip-reading datasetsHandling latency in real-time video processingSyncing Python backend with JavaScript frontend smoothly using WebSocketsAccomplishments that we're proud of: Working cross-platform web app with only a webcam requiredSuccessfully merged lip reading with speech recognition for flexibilityCreated a tool that could have real-world impact on accessibilityWhat we learned: Building efficient video pipelines for browser and backendTraining and testing lip-reading modelsThe value of accessibility-focused designReal-time communication techniques using WebSocketsWhat's next for Speak for Me Ai: Add language translation for international communicationOptimize for mobile devices and low-end hardwareBuild a standalone mobile/web app with offline modeExplore integration with wearables and AR for hands-free captions",
                        "github": "https://github.com/SinghHarkeerat/LAHACKS2",
                        "url": "https://devpost.com/software/speak-for-me-ai"
                    },
                    {
                        "title": "KaraokeKit",
                        "description": "Transform any song into karaoke instantly! Upload files or YouTube links, separate vocals, and sing along with synchronized lyrics. Your personal karaoke machine awaits.",
                        "story": "Inspiration: For an assignment in our AP Environmental Science class, we had to make a rap song/cover and perform it in front of the class. Despite finding perfect songs that matched our message and style, we couldn't locate instrumental versions to perform with. We wanted to create a web app that would address this issue, as well as being able to easily display the karaoke so people could have fun by themselves or with a group of friends.What it does: KaraokeKit transforms any song into a karaoke track in three simple steps. Users can either upload audio files or paste YouTube links. Our platform then separates the vocals from the instrumentals and transcribes the lyrics. Once processing is complete, users can sing along with just the instrumental track while following synchronized lyrics on screen. The intuitive interface works seamlessly across devices, featuring light and dark modes for comfortable viewing.How we built it: Frontend: SvelteKit for a responsive and reactive UI, with Tailwind CSS for stylingBackend: Flask API running Python for audio processingAudio Separation: The audio-separator library to split vocals from instrumentalsLyrics Generation: OpenAI's Whisper model for accurate transcription with timestampsYouTube Integration: yt-dlp for extracting audio from YouTube videosDeployment: Configured for easy deployment to Vercel (frontend) and any Python-compatible server (backend),What's next for KaraokeKit: We want to add a recording and a playback feature in the future, and also a 'saved playlists' feature to keep all of your karaoke songs in one place!",
                        "github": "https://github.com/JA-01/KaraokeKit",
                        "url": "https://devpost.com/software/karaokekit"
                    },
                    {
                        "title": "Primary Dungeon",
                        "description": "Its a basic 2D game where you attack enemies a level up. when you inevitably die you can increase your stats to get further into the dungeon. You can keep going for essentially forever.",
                        "story": "Inspiration: We just wanted to make a basic 2D game and this seemed fun and not too difficult. There are a lot of other games that are kind of similar.What it does: It as a menu were you can buy things with coins and upgrade stats. When you press play you are put in a small maze were you can attack enemies, collect coins, and progress to later stages.How we built it: I mostly coded it myself on IntelliJ (Java) but I used chat-GPT to help me code some thing from time to time.Challenges we ran into: One big challenge was learning how to add images using Java. Another issue was getting the player to collide with walls. In terms of beginning I also ad trouble with making the mazes but I decided to make them predetermined.Accomplishments that we're proud of: This is one of my first ever games and I am proud of how well the graphics turned out (I drew everything myself). I am also proud that it is actually kind of fun.What we learned: I learned how to add images using Java. How to use chat-GPT better. How to make players collide. How to make text using JFrame and JPanel. How to make pixel art.What's next for Primary Dungeon: I would add more enemies, an inventory, Items, better animations, and new attacks.",
                        "github": "https://github.com/Ewan182/Primary-Dungeon",
                        "url": "https://devpost.com/software/primary-dungeon"
                    },
                    {
                        "title": "Inspriteful",
                        "description": "Inspriteful motivates people to meet daily hydration, sleep, and fitness goals with the appeal of charming hand-drawn sprites, accompanied by stories and a shop feature to keep users engaged. ",
                        "story": "Inspiration: Living in a hectic society, we have seen ourselves and others around us slip into bad habits of neglecting our health. Whether it be drinking enough water, involving daily movement into our routines, or getting enough rest, it's easy to lose motivation for taking care of ourselves. But what would we actually want to take care of? We found ourselves falling deeply in love with each others' little doodles and realized that these seemingly silly characters were the key: being a responsible caretaker for an adorable virtual companion is far more enticing than being responsible for yourself. With that came the idea of one's daily habits directly affecting the state of Inspriteful sprite companions.What it does: Based on the user's daily logs of their sleep, hydration, and fitness habits, the appearance of their Inspriteful sprite companions decline or improve in health. Inciting our natural empathetic reaction to the sprite's conditions, the user is motivated to better their own behaviors. Additionally, maintaining healthy habits earns coins for the user, who can then unlock exclusive art to further foster a bond with their virtual companions.How we built it: We used Replit's HTML, CSS, and JS foundation to build Inspriteful. We hand-drew our sprites using Pixilart.com, a free web-based pixel art program.Challenges we ran into: This was our first experience building an app that wasn't based on simple block code--in fact, it was some of our group members coding outside of blocks at all. A lot of our time was spent tackling formatting issues.Accomplishments that we're proud of: We're extremely proud that we were able to create an uplifting app filled with character without fancy code. No matter where our team members came from, we came together to bring our visions to life. Creating Inspriteful was truly an artistic process.What we learned: We learned the importance of task delegation and communication, as well as the hard truth that jumping into a new project with limited experience under our belts would be a frankly brutal experience. Yet through it all, we supported each other and made it out the other side with a product we are proud of.What's next for Inspriteful: We are looking to grow our sprite library and create more features for users to unlock and interact with.",
                        "github": "",
                        "url": "https://devpost.com/software/inspriteful"
                    },
                    {
                        "title": "Altruist",
                        "description": "A mobile voice chat assistant that tells you how to use your phone and what do you in your surroundings through video and audio input.",
                        "story": "Inspiration: Our grandparents often ask us questions about technology that we find trivial, so we thought of a way to automate this process.,What it does: Altruist has screen recording, audio recording, and video recording capabilities on your device. Our custom VAD detects when you're speaking, and after you finish speaking, it sends a recording of whatever capture device you want to the model along with your voice input. Then, it returns an audio output responding to whatever you asked or said!,How we built it: We started the app with the very-good cli, built everything from the foundations provided by their base project.,Challenges we ran into: Originally, we considered using the Gemini Live API which uses a websocket connection. However, there wasn't enough time to learn the websocket system in flutter, so we had to scrap that idea.,Accomplishments that we're proud of: Eventually, we built our stack using Deepgram, Gemini, and ElevenLabs. Deepgram worked as our speech to text so that the model could interpret what we were saying. Gemini served as the actual AI processing our video and message. Finally, ElevenLabs was used to generate the response audio from the text generated by Gemini.,What we learned: We learned how to use flutter and dart for cross platform app development.,What's next for Altruist: Next, we plan to fully implement the Gemini Live API for lower latency communication.,",
                        "github": "",
                        "url": "https://devpost.com/software/altruist-jofi31"
                    },
                    {
                        "title": "Broke But Talented",
                        "description": "Teenagers can be some of the most talented people, sometimes even more qualified than adults. Broke But Talented is a platform built to showcase that potential.",
                        "story": "Inspiration: Doing web design myself, I have been constantly seeking a platform to show my work and get hired. Fiverr exists, but it's only for adults. I have likely spent more time doing outreach than I actually have doing web design. Not having a platform to show my services really prevents me from spending more time on what I actually want to do.What it does: A landing page design for Broke But Talented. A platform to connect skilled teens with clients who seek quality work for cheaper and trust a teen to do the job.Accomplishments that we're proud of: I am very proud of the advanced and playful design of the site.Site available athttps://brokebuttalented.co(not responsive, will only work properly on laptops)",
                        "github": "",
                        "url": "https://devpost.com/software/broke-but-talented"
                    },
                    {
                        "title": "Dual Drift",
                        "description": "A space based puzzle game where you control two characters, one with regular controls and one with inverse.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/dual-drift"
                    },
                    {
                        "title": "Mood City",
                        "description": "Therapy bot that creates a digital cityscape and tracking calendar based on your day-to-day moods, conversations, and journal entries.",
                        "story": "Inspiration: Mental health isn\u2019t one-size-fits-all, and sometimes words alone don\u2019t cut it. We wanted to create a way for people to visualize how they\u2019re doing\u2014emotionally and socially\u2014without pressure. Inspired by journaling apps and cozy games like Animal Crossing and Stardew Valley, we built Mood City, a digital world that grows based on your daily moods and activities.What it does: Mood City is a virtual mental wellness companion. Each time a user logs an activity and how they\u2019re feeling, a building is added to their personal city\u2014like a cottage for a restful day, a caf\u00e9 for socializing, or an office building for work. Each building changes in appearance depending on the mood (e.g. happy, sad, tired). Over time, the user\u2019s city becomes a visual reflection of their emotional journey.How we built it: We used HTML, CSS, and JavaScript to create a dynamic single-page web app. We coded a calendar, journal prompts, breathing exercises, and a mood tracker that automatically places pixel-art buildings on a scrolling map. Assets were managed through a folder structure (/assets/buildings/{type}/{mood}.png) and dynamically rendered using DOM manipulation. Mood detection is manual for now, but we designed the structure with future integration (like AI chat or sentiment analysis) in mind.Challenges we ran into: As beginners, a lot of our code didn't work at times and we had difficulties troubleshooting. However, this was a great learning experience for us. We also initially wanted to build a chatbot, but integrating a full AI model with real-time processing would\u2019ve taken longer than expectedAccomplishments that we're proud of: Designed multiple emotional variations of buildings that reflect the user's mood.Implemented a clean, simple UI that still feels cozy and comforting.Added extra features like a breathing exercise modal and journal prompts.What we learned: How to manage complex state and dynamic DOM updates in vanilla JavaScript.How to structure and reference image assets properly in a web app.The importance of balancing ambition and feasibility during a hackathon.That even simple visual metaphors (like buildings) can make mental health tracking feel meaningful and approachable.,What's next for Mood City: Integrating sentiment analysis or GPT-based conversation for more personalized mood tracking.Adding more building types (e.g. park, bedroom, train station) and animation effects.Making each building more unique to the dayMaking it mobile-first and accessible to all!,",
                        "github": "",
                        "url": "https://devpost.com/software/mood-city"
                    },
                    {
                        "title": "Indie World",
                        "description": "Indie World is a simple, interactive game where players use multiple building materials to build their world, talk to characters, and fight monsters.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/indie-world"
                    },
                    {
                        "title": "The Chore Squad",
                        "description": "yes",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/the-chore-squad"
                    },
                    {
                        "title": "Healthify",
                        "description": "Healthify helps you overcome mental and physical health challenges with personalized AI plans, daily check-ins, and wellness tips\u2014your journey to a healthier you, one step at a time.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/healthify-2sfty3"
                    }
                ]
            ]
        },
        {
            "title": "Graduate Interdisciplinary Innovation Hackathon",
            "location": "UNT Discovery Park",
            "url": "https://gradinnohack.devpost.com/",
            "submission_dates": "Apr 05 - 06, 2025",
            "themes": [
                "Beginner Friendly",
                "Open Ended"
            ],
            "organization": "UNT College of Engineering",
            "winners": false,
            "projects": []
        },
        {
            "title": "SharkHack 2025",
            "location": "Simmons University\u2013 LKP Center",
            "url": "https://sharkhack2025.devpost.com/",
            "submission_dates": "Apr 05 - 06, 2025",
            "themes": [
                "Beginner Friendly",
                "Design",
                "Social Good"
            ],
            "organization": "MLH",
            "winners": false,
            "projects": [
                [
                    {
                        "title": "Smart Farm",
                        "description": "Our platform cuts down on food waste by linking farmers with consumers. Leveraging AI forecasting, and smart inventory, surplus produce reaches those in need for more responsible consumption. ",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/smart-farm-94x86i"
                    },
                    {
                        "title": "StudyIQ",
                        "description": "StudyIQ transforms videos and PDFs into interactive learning materials using AI. Get instant notes, flashcards, and mind maps to make learning more effective and engaging.",
                        "story": "Inspiration: .What it does: How we built it: Challenges we ran into: Accomplishments that we're proud of: What we learned: What's next for StudyBuddy:",
                        "github": "",
                        "url": "https://devpost.com/software/studybuddy-ly7g2z"
                    },
                    {
                        "title": "Find your way",
                        "description": "Allows people that recently moved to get familiar to their new home through an easy and fun website, that rewards them for completing easy tasks to get used to their new home.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/find-your-way-ht8ow4"
                    },
                    {
                        "title": "FurRealFunny",
                        "description": "Studying is too stressful. But how should we relax? Well, nothing beats doom scrolling meme pages to relax. But with our website, you get to generate personal cat memes that will brighten your day. ",
                        "story": "Inspiration: Our team members are all university students. As we tread on with our school life, we continue to realize how exhausting studying can be. One thing we can all relate to is the need to scroll through online communities to give ourselves a well-deserved break. Relaxing isn't just meditating or sleeping; it's also having fun and finding joy, even if it's through memes.What it does: The website relies on user input to generate a personalized meme. Once you open the website, the user is tasked with inputting how they are feeling. After the website received the user input, it will sort through the answer and create a meme using keywords. Then, the user can choose to download or share the image.How we built it: We utilized HTML, CSS, JavaScript, and the Gemini API. Gemini AI generates captions for the cat memes based on the user's input.Challenges we ran into: The main challenge that we ran into was debugging our program. Whenever we try to implement a new function to the website, it would lead to a bug. Another problem we ran into is formatting captions within the image.Accomplishments that we're proud of: Something we're really proud of accomplishing is finishing a project. Although we had a vision for our website, we weren't sure if we could have implement it. But we're really proud of the final product since it accomplished our goals.What we learned: We all learned how to utilize AI as a helpful tool. In this project, we also focused more on the frontend compared to our past projects. Therefore, we are more well rounded in trying to position elements and also knowing how to code functions.What's next for FurRealFunny: In the future, we want to expand FurRealFunny into a community based website. By building a community, it creates a safer and more comforting place for the user. Plus, it allows the user to interact more with the memes compared to only viewing and saving it.",
                        "github": "",
                        "url": "https://devpost.com/software/furrealfunny"
                    },
                    {
                        "title": "Desktop Kitty",
                        "description": "A cute mental-health supportive desktop pet",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/desktop-kitty"
                    },
                    {
                        "title": "Boston EventHandler",
                        "description": "A Website that Lists Upcoming Events in the Boston Area...\r\n",
                        "story": "Inspiration: We were very bored one weekend, and we needed to figure out what to do to entertain ourselves, so we came up with this idea of an \"Event Handler!\" It makes searching for various events very simple and accessible.What it does: This webpage displays upcoming events in the Boston area. Click the arrow buttons in order to cycle through these events and choose whether or not to save them to your events list. Events will have the name, an image, its address, date/time, and a URL to buy tickets.How we built it: We built our front-end using React, HTML, CSS, and JavaScript. We used the Ticketmaster API for names, images, addresses, date/time, and URLs of events. We used Express.js and Supabase for our back-end.Challenges we ran into: We ran into challenges navigating the API documentation, and debugging the various interactions between our separate components.Accomplishments that we're proud of: We're proud to have integrated supabase into our website, and also create proper pagination. We are also very happy with our new design for our webpage!What we learned: We improved our knowledge of API calls, supabase, express.js and react.What's next for Boston EventHandler: We would like to add user authentication in order to have localized event lists, we may also incorporate AI into this project some time. We would also like to deploy our website some time.",
                        "github": "https://github.com/justalittlemushroom/Boston-EventHandler",
                        "url": "https://devpost.com/software/boston-eventhandler"
                    },
                    {
                        "title": "OptiPrompt",
                        "description": "OptiPrompt fine-tunes your AI prompts before you run them. Get sharper, smarter outputs with zero trial and error. Think Grammarly for prompts \u2014 powered by Gemini, built for precision.",
                        "story": "Inspiration: While brainstorming ideas for the hackathon, we found ourselves doing what so many people do every day \u2014 struggling to communicate effectively with AI. We kept tweaking prompts, hoping for better results, only to get vague or off-target responses. That\u2019s when it hit us: the problem isn\u2019t the AI \u2014 it\u2019s the prompt.\n _ So we stopped fighting the prompt and started fixing it. That\u2019s how OptiPrompt was born \u2014 a smart solution to help people speak AI\u2019s language with precision and purpose.  _What it does: OptiPrompt analyzes your input prompt and offers real-time suggestions to improve clarity, specificity, and relevance. Think of it as Grammarly, but for talking to large language models. Whether you're coding, writing, brainstorming, or querying \u2014 OptiPrompt makes sure you're getting the most out of your AI interactions. The goal? Better prompts. Better outputs. Less friction.How we built it: Challenges we ran into: Accomplishments that we're proud of: What we learned: What's next for OptiPrompt:",
                        "github": "https://github.com/renukaKandii/projects2025",
                        "url": "https://devpost.com/software/optiprompt"
                    },
                    {
                        "title": "Gym Bro Health App",
                        "description": "Our flexible AI application works for you with customized workouts, variable feedback, and user-mood based reasoning to accommodate for your physical and mental well being,",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/gym-bro-538qpt"
                    },
                    {
                        "title": "EcoRangers",
                        "description": "An interactive augmented reality game that uses your device\u2019s camera to detect trash and deliver eye-opening environmental facts in real time.",
                        "story": "EcoRangers was born from a passion for environmental stewardship and the desire to make learning about waste fun and engaging. The app uses your device\u2019s camera to detect trash in real time, letting players \u201cpick\u201d items and immediately receive disturbing facts about the environmental impact of waste. We built it by integrating iOS\u2019s Vision and Core ML frameworks with a SwiftUI interface, wrapping a robust UIKit camera view controller to deliver a seamless augmented reality experience. Along the way, we faced challenges such as fine-tuning real-time object detection, ensuring smooth UI updates, and overlaying text accurately on live video feeds. Despite these hurdles, we're proud of creating an educational, interactive game that brings together technology and sustainability in a compelling way. This project taught us valuable lessons about optimizing performance and merging different iOS frameworks, and we're excited to expand EcoRangers with additional environmental facts, multiplayer features, and enhanced detection accuracy in the future.",
                        "github": "",
                        "url": "https://devpost.com/software/ecorangers"
                    },
                    {
                        "title": "MysteryAI",
                        "description": "MysteryAI: Interrogate suspects, uncover clues, and solve the perfect crime - all powered by AI\"",
                        "story": "Inspiration: Murder Mysteries have always been captivating us, and combing it with storytelling, logic and suspense made it more fun. We wanted to create an interactive experience where players feel like real detectives using AI and not just as a storytelling engine but as a reasoning partner as well.What it does: It generates a fresh murder mystery game on each play through. Our goal was to create realistic and engaging cases every time with unique story lines, where players can freely interrogate AI characters built by our model and get to a conclusion on their own.How we built it: MysteryAI uses Gemini AI API to create a murder mystery scenario!(story,victim,suspect,witness). We ran an embedded comparison using hugging face transformers against previous cases to avoid story repetition. We used  Firestore Firebase Google nosql database to store all the cases and vectorized database of cases. For each conversation between the user and the character suspects , we generated prompts using RAG to feed character knowledge.Challenges we ran into: Our First challenge was the number of prompts we could send to Gemini API which could lead to an infinite loop which we overcame by keeping some constraints and limitations in such a way that a conclusion is reached in accountable number of Interrogatory questions.Going forward, we faced the problem of Repetitive AI responses which might have been hallucinations. We overcame this by using Retrieval Augmented Generation to feed the previous case history to remind the model.Then we noticed too many similarities in cases generated by AI. This led to us to use cosine similarity to reduce the repetition of cases. this Cosine similarity loops through the case history that is stored in the database to find similarities and returns the case only if it is not a repetition. This reduced the repetition percentage by 70%.Accomplishments that we're proud of: Case Similarity Prevention and Live AI powered interrogation.What we learned: We learnt a lot about prompt engineering, implementing RAG using cosine similarity.\nHandling LLM limitations like hallucinations.What's next for MysteryAI: There are chances the user might not be able to guess the murderer and  this might go into an infinite loop. We can try to use RAG feedback loop to tighten the Gemini Narratives which will result in more engagement. \nImproving difficulty levels based on the users skills.\nWe can also make the game dynamic by introducing time-based clues unlocking or misdirection.",
                        "github": "https://github.com/sriya632/MysteryAI",
                        "url": "https://devpost.com/software/mysteryai"
                    },
                    {
                        "title": "SkinSnap Baby",
                        "description": "Every second counts when it comes to infant health. Our project leverages Google's Teachable Machine AI to offer a real-time, webcam-based diagnostic tool .",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/skinsnap-baby"
                    },
                    {
                        "title": "NeuroFade",
                        "description": "What if your brain had to earn screen time?",
                        "story": "",
                        "github": "https://github.com/christylaminated/neuroTech",
                        "url": "https://devpost.com/software/neurofade"
                    },
                    {
                        "title": "Clock In",
                        "description": "Studying has always been stressful ... until now. Clock It is our new study tool to encourage mindful studying with an ascending timer, alternative backgrounds and theme, and much more.",
                        "story": "",
                        "github": "https://github.com/KevinK-BU/shark-hacks-study-website",
                        "url": "https://devpost.com/software/clock-it"
                    },
                    {
                        "title": "EcoGrid Tycoon",
                        "description": "Sandbox Simulator and the ability to deliver custom data centers at your chosen location. We show you the environmental impact and offer greener alternatives for sustainable solutions.",
                        "story": "Inspiration\nEcoGrid Tycoon was born from the need to merge cutting-edge technology with environmental responsibility. We aimed to create a platform that not only offers custom-built data centers but also sheds light on the environmental impact of each choice.What it does\nOur platform is an online marketplace that sells custom, on-demand data centers at the location of your choice. It provides real-time environmental impact data, giving you the visibility to choose sustainable, eco-friendly infrastructure options.How we built it\nWe built EcoGrid Tycoon using Go for the backend and React for the frontend. This tech stack allowed us to deliver a robust, responsive platform that meets the demands of modern infrastructure and user experience.Challenges we ran into\nIntegration was one of our biggest hurdles. We faced numerous challenges connecting different systems and ensuring smooth data flow. Despite these issues, each challenge taught us how to better align our tech with our environmental goals.Accomplishments that we're proud of\nWe're proud to have developed a platform that not only sells custom data centers but also empowers users with vital information about environmental impacts. This dual focus on technology and sustainability is a key milestone for us.What we learned\nOur journey taught us that data centers affect the environment in diverse ways and that location truly matters. These insights have been invaluable in shaping our approach to creating sustainable tech solutions.What's next for EcoGrid Tycoon\nWe\u2019re focused on refining our integration processes and expanding our data center offerings. Our next steps include enhancing our environmental impact metrics and further empowering users to make informed, eco-friendly infrastructure decisions.",
                        "github": "https://github.com/Samhith-k/data-center-ecology-map",
                        "url": "https://devpost.com/software/ecogrid-tycoon"
                    },
                    {
                        "title": "EcoWise AI",
                        "description": "EcoWise AI \u2014 Your Smart Coach for a Greener Life.",
                        "story": "",
                        "github": "https://github.com/Vijayrathan/wasteo",
                        "url": "https://devpost.com/software/ecowise-ai"
                    },
                    {
                        "title": "Book Bot",
                        "description": "I am building a bot which you can attach any pdf in the world to generalize it.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/book-bot-jc3xyn"
                    }
                ]
            ]
        },
        {
            "title": "CODEATHON",
            "location": "Bengaluru, India",
            "url": "https://codeathon-24807.devpost.com/",
            "submission_dates": "Apr 06, 2025",
            "themes": [
                "Machine Learning/AI"
            ],
            "organization": "Manipal Institute of Technology",
            "winners": false,
            "projects": [
                [
                    {
                        "title": "TrustLend",
                        "description": "Revolutionizing Peer-to-Peer Lending with Trust & Transparency",
                        "story": "\ud83d\udca1 Inspiration: In today\u2019s world, access to credit is a fundamental need, yet millions remain underserved by traditional financial institutions. High interest rates, slow approvals, and lack of trust plague the current lending landscape.We wanted to change that.TrustLendwas born from the idea thattrustandcommunity-driven financecan create a more equitable lending system. We envisioned a peer-to-peer platform where people could lend and borrow money transparently, backed by smart risk assessment and user reputation\u2014not just credit scores.\ud83d\udee0\ufe0f How We Built It: We builtTrustLendas a full-stack web application with an emphasis on performance, security, and simplicity:Frontend: React, TailwindCSS, Framer Motion, Radix UIBackend: Node.js, Express, Drizzle ORM, PostgreSQL (Neon)Authentication: Passport.js with session-based authPayments: Stripe for processing transactionsNotifications: Twilio API for real-time alertsDeployment Tools: Vite, ESBuild, TSX, TypeScript,A clean REST API layer connects the frontend and backend.Authentication and session management are secure and scalable.Recharts and dashboards provide insights into lending activity.Each user has areputation score, determined by lending/borrowing history.,\ud83e\udde0 What We Learned: Full-stack Integration: This was our first time combining React with Drizzle ORM\u2014learning how to structure clean DB relationships was a game-changer.Stripe & Twilio APIs: We integrated these from scratch, learning how to handle real-time events and ensure secure transactions.Session-based Auth: We deepened our understanding of cookies, sessions, and Passport\u2019s local strategy.Teamwork under Pressure: With a 24-hour time crunch, we learned the value of parallel development and clean communication.,\u26a0\ufe0f Challenges We Faced: Database Migration Conflicts: Drizzle ORM schema changes caused versioning issues\u2014fixed by syncing with Drizzle Kit.Frontend Zip Handling: Packaging and zipping our frontend separately introduced build issues; we streamlined this with Vite.Time Management: Balancing frontend polish with backend logic was a challenge\u2014we scoped features tightly to stay on track.Authentication Glitches: Managing login state across routes was tricky but taught us how to debug sessions thoroughly.,\ud83d\udcac Final Thoughts: Participating in this hackathon pushed us to explore new technologies, work as a cohesive team, and tackle a real-world problem with impact.TrustLendis more than a project\u2014it's a prototype of what peer-to-peer finance could look like in a more transparent, accessible world.We\u2019re excited to continue building on this foundation.",
                        "github": "https://github.com/nalin-mahajan/ts25.git",
                        "url": "https://devpost.com/software/trustlend"
                    },
                    {
                        "title": "Infinity AI Financial Assistant",
                        "description": "\"Unleash Your Money Mojo with Smart Finance Superpowers\"",
                        "story": "AI Finance Assistant - Project Story\nThe Inspiration\nWhen our team received the hackathon problem statement challenging us to build an AI-powered financial assistant, we immediately recognized the opportunity to solve a critical problem many people face. The problem statement highlighted the need for tools that not only track financial data but actually provide personalized insights and recommendations.\nThe challenge resonated with our team because we've all experienced the gap between having access to our financial data and actually understanding what to do with it. Traditional banking apps show transactions but rarely offer actionable guidance. The problem statement pushed us to think about how AI could bridge this gap.\nWhat We Learned\nDuring our intense 24-hour development sprint, we gained valuable insights and skills:AI Integration: We learned to effectively implement sentiment analysis models to evaluate financial health and generate contextual advice\nFinancial Data Analysis: We developed techniques for categorizing transactions and identifying spending patterns\nSecurity in Finance Applications: We gained practical experience implementing encryption for sensitive financial data\nData Visualization: We discovered how to present complex financial information in accessible, intuitive formatsThe problem statement's emphasis on both functionality and privacy forced us to balance competing priorities, teaching us valuable lessons about creating fintech products that users can trust.\nHow We Built It\nWith only 24 hours to address the problem statement, we strategically approached development:Core Architecture: We built a Streamlit application for rapid development and interactive features\nData Layer: We implemented a mock bank API to simulate financial data interactions\nAI Analysis: We integrated a pre-trained sentiment analysis model to evaluate financial health\nSecurity Features: We developed encryption/decryption functions to protect sensitive financial data\nVisualization Components: We created dynamic charts using Plotly to help users understand their spending\nUser Experience: We designed a clean, multi-tab interface focused on delivering insights at a glanceEach team member focused on specific components based on the problem statement requirements, allowing us to maximize productivity during the limited time frame.\nChallenges We Faced\nAddressing all aspects of the problem statement in 24 hours presented several challenges:\nImplementation ChallengesCategorization Accuracy: Creating a robust transaction categorization system required careful keyword mapping\nMeaningful AI Insights: Moving beyond generic advice to truly personalized recommendations\nSecurity Implementation: Designing encryption that was both secure and user-friendly\nData Consistency: Handling various date formats and transaction structuresHackathon ConstraintsTime Management: Balancing feature development with polishing the user experience\nScope Control: Staying focused on the core requirements without feature creep\nTechnical Limitations: Working within the constraints of pre-trained models without custom trainingWhat's Next for AI Finance Assistant\nWhile we're proud of what we accomplished during the hackathon, we see many opportunities to expand on our solution to the original problem statement:Implementing predictive analytics to forecast future spending patterns\nCreating more sophisticated categorization using machine learning\nDeveloping more detailed budget recommendations based on personal financial goals\nAdding support for investment accounts and long-term financial planning\nBuilding a mobile application for on-the-go financial insightsThe hackathon problem statement gave us a strong foundation, but we envision evolving this prototype into a comprehensive financial wellness platform that helps users not just understand their current financial situation, but actively improve it.",
                        "github": "https://github.com/Tridha-05/Infinity-AI-Finance-Assistant",
                        "url": "https://devpost.com/software/infinity-ai-financial-assistant"
                    },
                    {
                        "title": "TrustPay",
                        "description": "Decentralized lending, Reimagined Micro Transcations for India",
                        "story": "\ud83d\udd25 What Inspired Me: As a student constantly exposed to the struggles of financial access, I saw a huge gap in how people with urgent monetary needs are forced into high-interest debt traps. On the other side, many are willing to lend small amounts if there'strust, transparency, and a secure systemin place.That\u2019s when the idea struck me \u2014 why not build a platform that connects trusted borrowers and lenders, powered by technology and simple enough to be used by anyone with a phone number?TrustPay was born from this thought \u2014 a platform that values people over paperwork.\ud83d\udee0\ufe0f How I Built It: Framework:Built onFlaskfor its simplicity and modularity.Database:UsedSQLitewith SQLAlchemy ORM for easy data management.Modular Structure:Followed Flask blueprint architecture for maintainability.OTP System:IntegratedFast2SMS APIandTwiliofor secure mobile verification.User Roles:Created role-specific dashboards and auth flows forLendersandBorrowers.Models:Designed custom models \u2014 Lender, Borrower, Loan, Ledger, OTP \u2014 to structure all operations cleanly.Security:OTP verification adds an extra layer of authentication for trust.,\ud83d\udcda What I Learned: How to build a production-style Flask application using blueprints and factories.Integration of 3rd party APIs likeTwilioandFast2SMSfor OTP handling.Designing relational databases withSQLAlchemy, and how to model real-world entities.Writing clean, modular, and scalable Python code.Handling edge cases like expired sessions, invalid OTPs, and user feedback through flashing messages.,\ud83d\udea7 Challenges I Faced: OTP Delivery Delays:Some APIs like Twilio needed verified numbers or credits. Fast2SMS API required DLT registration and an INR 100 transaction to work.Session Handling:Maintaining OTP verification and phone numbers usingsessionwithout breaking flows was tricky.Database Setup:Running the database and managing app context without launching the main app required extra setup logic.Blueprint Imports:Dealing with circular imports and maintaining modular routing took fine-tuning.,\ud83d\udca1 Final Thoughts: This project is more than just code \u2014 it's a step towards empowering real people with real needs.Whether you're lending \u20b9100 or \u20b910,000, XChange ensures it\u2019s built ontrust, verification, and transparency.",
                        "github": "https://github.com/pmk456/Decentralized-Emergency-Loan-Networks-for-Underserved-Communities",
                        "url": "https://devpost.com/software/trustpay-ac4nvt"
                    },
                    {
                        "title": "DeFi Loan",
                        "description": " A P2P Platform for Underprivileged Communities\r\n",
                        "story": "\ud83e\udde0 About the Project: DeFi Loan Platform\nMillions of people in low-income and rural communities are excluded from formal credit systems. Why? Because they lack traditional credit histories, banking access, or collateral. Our project, DeFi Loan Platform, aims to bridge that gap by enabling peer-to-peer microloans using social trust scoring and a simulated blockchain ledger\u2014without needing a credit score.This system empowers individuals to apply for emergency loans using alternative indicators like:Monthly wallet activity \ud83d\udcb8Utility bill payment timeliness \ud83d\udccaSocial and digital footprint \ud83d\udcacThe trust score is calculated using a machine learning model, and loans are approved or rejected accordingly. Every loan, repayment, and lender registration is logged transparently on a blockchain-inspired ledger.\ud83d\udd10 Key Features:\nOTP-based registration and login via Twilio SMSLoan approval based on trust score (not credit score)Blockchain-backed ledger for transactions and repaymentsLender dashboard to track fund contributions and statsClean UI with dark mode and live blockchain tableBuilt using:Flask, SQLite, scikit-learn, Twilio, HTML/CSS/JSDeployed on Render using gunicorn and Procfile setup",
                        "github": "https://github.com/AadityaBhattacharjee/p2p-lending",
                        "url": "https://devpost.com/software/defi-loan-h0ra5b"
                    },
                    {
                        "title": "Spend vault",
                        "description": "Saving made simple, spending made smarter.",
                        "story": "InspirationAmong the two provided problem statements, this one resonated with us more deeply, as it reflected a struggle we and many of our peers face firsthand \u2014 impulse spending and a lack of savings discipline. Our close understanding of the issue motivated us to create a scalable, user-centric solution that could be mutually beneficial to both students and early-working professionals.\u2e3bWhat It DoesSpend Vault acts as an AI-powered financial assistant designed to improve financial behavior among young adults. Upon landing on the site, users are greeted with a concise overview of the platform\u2019s intent \u2014 to guide and nudge users toward more mindful spending and consistent saving habits.The platform includes six core features:\n    1.  Real-Time Expense Tracker \u2013 Helps users stay on top of their spending habits with instant feedback.\n    2.  Smart Nudges \u2013 AI-generated suggestions to reduce impulsive purchases.\n    3.  Expense Goals \u2013 Users can set monthly saving targets to stay committed.\n    4.  Gamified Rewards System \u2013 Positive financial behavior earns coins, boosting engagement.\n    5.  Privacy-First Tracking \u2013 Ensures user data is safe and confidential.\n    6.  Category-Based Breakdown \u2013 Offers insights on where money is going (food, travel, etc.).These features collectively encourage young users to understand, reflect on, and improve their money habits in a fun and secure way.\u2e3bHow We Built ItWe began by analyzing existing solutions and identifying their limitations. Our goal was to combine proven strategies with fresh ideas to increase both the pull factor and user retention.\n    \u2022 UI Design: Built using Figma, ensuring clean layout, clear flow, and youth-friendly tone.\n    \u2022 Frontend: Developed using HTML, CSS, JavaScript, and Python to bring the design to life.\n    \u2022 Backend: Powered by JavaScript and Python for handling data flow and logic.\n    \u2022 AI Model: Trained using FastAPI with a dataset of over 500 entries (synthetically generated with relevant attributes).\n    \u2022 Hosting: All components are integrated and locally hosted via GitHub, simulating a full-stack deployment.\u2e3bChallenges We Ran Into\n    \u2022 API Integration: One of our major blockers was integrating the trained model API with our frontend.\n    \u2022 Data Availability: In the absence of real user data, we had to simulate datasets that were realistic enough to reflect expenditure behavior.\n    \u2022 Design-to-Development Transition: Adapting the aesthetic Figma design into responsive and functioning code presented compatibility challenges.\u2e3bAccomplishments We\u2019re Proud Of\n    \u2022 UI Design: We crafted a visually engaging and intuitive user interface with fun language elements that appeal to younger audiences while maintaining clarity and professionalism.\n    \u2022 Frontend Execution: Smooth user interaction, dynamic elements, and aesthetic transitions using modern web styling and gradients.\n    \u2022 Backend Development: Successfully trained and deployed an AI model that returns insights and powers our monthly expense visualization chart.\u2e3bWhat We Learned\n    \u2022 How personalized financial tools can meaningfully impact everyday lives when built with empathy and data.\n    \u2022 The importance of striking a balance between functionality and visual appeal in design.\n    \u2022 Under high-pressure and time-constrained situations, we discovered that learning new tools and technologies quickly is not only possible \u2014 it\u2019s necessary.\u2e3bWhat\u2019s Next for Spend VaultSome ideas remained outside the scope of our MVP, but we see immense potential for future expansion:\n    \u2022 Investment in Gold: Introduce a secure, beginner-friendly gold investment module linked to user savings.\n    \u2022 Emergency Fund Education: A dedicated page to highlight the importance of setting aside an emergency fund.\n    \u2022 Monthly Behavior Report: AI-generated summaries showing improvements (or regressions) in spending habits to encourage users \u2014 e.g., \u201cHey! You\u2019ve reduced your impulsive spending by 30% this month!\u201d\n    \u2022 Gamified Incentives: Users earn SpendCoins for hitting savings goals or acting on nudges. These coins can be redeemed for:\n    \u2022 Gold investments\n    \u2022 Cash-back equivalents (up to 50%)\n    \u2022 Exclusive offers in partner ecosystems",
                        "github": "https://github.com/nirvika28/Outliers",
                        "url": "https://devpost.com/software/spend-vault"
                    },
                    {
                        "title": "Spendly",
                        "description": "Spendly is an AI-powered finance tracker that helps young adults curb spending, build savings, and make smarter money choices through real-time insights and nudges.",
                        "story": "Spendlyis an AI-powered personal finance dashboard designed to help users make smarter financial decisions. Built for young adults who struggle with impulse spending and saving discipline, Spendly offers an intuitive platform to track expenses, manage budgets, and receive real-time insights.By categorizing transactions into needs and wants, and delivering smart nudges through a built-in AI chatbot, Spendly encourages users to build healthier money habits. The dashboard also includes progress bars, savings goals, and a task manager to keep everything in check \u2014 all wrapped in a modern, mobile-friendly UI.Whether you're saving for something big or just trying to get a better handle on your spending, Spendly helps you stay on track with clarity, ease, and a little motivation.",
                        "github": "https://github.com/Kushagrrra/Spendly_front",
                        "url": "https://devpost.com/software/spendly-q5mp0g"
                    },
                    {
                        "title": "SpendWise",
                        "description": "SpendWise is your AI-powered financial buddy that helps you control impulse spending and build smart saving habits. By analyzing your expenses and sending smart nudges all through a simple web app.",
                        "story": "\ud83c\udf1f Inspiration: The idea forSpendWisewas born from a shared frustration among our team members\u2014impulse spending. As students and young adults, we often found ourselves making unnecessary purchases, only to regret them later. We realized that while budgeting apps exist, very few actually help usersbuild awareness or change behavior. That\u2019s when we asked:What if your finances came with a personal coach that nudges you before you overspend?Inspired by behavioral science and habit-building apps like Duolingo, we envisionedSpendWise\u2014a smart financial assistant that uses AI and psychology to gently steer users toward smarter money decisions.\ud83e\udde0 What We Learned: This project taught us a lot, including:How toprocess and analyze financial datausing Python and Pandas.Building and fine-tuning aRandom Forest Classifierto distinguish between needs and wants.The power ofnudges\u2014small, timely interventions that lead to significant behavioral shifts.Creatingclean and intuitive dashboardswith a user-centric design approach.Integrating anAI model into a working web applicationvia Flask APIs.,We also got hands-on experience withversion control, teamwork, and time management\u2014key real-world skills.\ud83d\udee0 How We Built It: Our tech stack includes:Python: For data preprocessing and building the ML model.Scikit-learn: To develop and train the Random Forest classifier.Flask: Used to wrap our model into an API.HTML/CSS/JavaScript + Bootstrap: For building the frontend dashboard.Matplotlib & Seaborn: For visualizing user spending data.Figma: For wireframing and designing UI mockups.,The model classifies each transaction based on features like amount, merchant type, time of transaction, and spending patterns. The results are sent to the dashboard, where nudges and visual feedback are generated.\ud83d\udea7 Challenges We Faced: Despite these hurdles, we were driven by the belief thatsmall nudges can lead to big savings, and that kept\u00a0us\u00a0going.",
                        "github": "",
                        "url": "https://devpost.com/software/spendwise-qijw4r"
                    },
                    {
                        "title": "Money Matters",
                        "description": "Spend smart, Save smarterwith moneyMatters",
                        "story": "Inspiration: As a college Student , you are answerable for your savings and your spending to your guardians . But telling them manually everything creates a bit discomfort . Our Website will be a hassle free solution for this problem.Problem Solution: We helps users curb impulse spending and build smart savings habits. Through personalized, real-time nudges, we make financial wellness simple, actionable, and\u00a0sustainable.How we built it: In this Website , we used the advanced technologies, like React for frontend , creating the routing pages , and toggle between the light and dark theme . Also we use different types of hooks names as uselocation  , useState , useParams and many more . we also integrate the authentication so that the user email and password is saved in the database , which is Called Supabase.Challenges we ran into: Identifying the exact need of a user , making the ui as much as user friendly , integrating the api for the financial chatbot.What we learned: How to work on the project under time constraints , which gives us the idea how to handle a deadlineWhat's next for Money Matters: One stop solution for all your savings and investments",
                        "github": "https://github.com/mishraabhi09/moneyMatters.git",
                        "url": "https://devpost.com/software/money-matters-21zhyw"
                    },
                    {
                        "title": "TrustLift",
                        "description": "We are RITians tackling financial exclusion by creating a decentralised emergency loan platform using blockchain and trust scoring, enabling secure P2P lending and financial inclusion globally.",
                        "story": "What it does: TrustLift is a blockchain-powered peer-to-peer (P2P) lending platform designed to address financial exclusion in underserved communities. It enables secure, transparent microlending without relying on traditional credit scores or banking infrastructure. By leveraging blockchain technology, trust scoring, and multi-channel accessibility (web and WhatsApp), TrustLift provides emergency loans to individuals who need them most.How we built it: Backend: Built using Node.js with Express and TypeScript for business logic and RESTful APIs.Blockchain: Integrated Hyperledger Fabric for immutable transaction records and smart contract automation.Database: PostgreSQL for relational data storage to manage user profiles, loans, repayments, and transaction history.Frontend: Developed with Next.js and Tailwind CSS for responsive design and seamless user experience.Communication: Twilio integration for WhatsApp-based loan management and notifications.Authentication: Token-based system with Clerk integration for secure user identity management.,Challenges we ran into: Accomplishments that we're proud of: Successfully implemented a trust-based scoring system that replaces traditional credit scores.Built a robust blockchain-backed platform that ensures transparency and immutability in loan processes.Enabled multi-channel accessibility through web and WhatsApp interfaces, catering to users with limited internet access.Created a user-friendly dashboard for borrowers and lenders to manage loans effectively.,What we learned: What's next for TrustLift: This file provides a concise summary of the project, its technical aspects, challenges, accomplishments, lessons learned, and future plans in Markdown format. Let me know if you need further customization!",
                        "github": "https://github.com/VirtualHorror/MITBLR-Codeathon",
                        "url": "https://devpost.com/software/trustlift"
                    },
                    {
                        "title": "RupAI",
                        "description": "Your wallet's new best friend!",
                        "story": "Inspiration: In today's fast-paced world, impulsive spending and emotional purchases have become increasingly common, especially among young adults. We recognized that traditional budgeting apps focus solely on numbers, ignoring the crucial emotional and psychological aspects of spending. This insight led us to create RupAI, a revolutionary financial wellness app that combines emotional intelligence with smart financial management.What it does: RupAI is more than just a financial tracking app - it's your personal financial wellness companion that:Understands Your Emotions: Analyzes spending patterns in relation to emotional statesSmart Classification: Uses ML to categorize expenses as \"wants\" vs \"needs\"Social Support: Creates a community of mindful spenders through friend connections and leaderboardsGamified Experience: Rewards consistent saving behavior with points and achievementsIntelligent Chatbot: Provides personalized financial advice and emotional supportBehavioral Insights: Offers deep analysis of spending patterns and triggers,How we built it: We developed RupAI using a comprehensive tech stack:Frontend: HTML with JSBackend: Node.js with Express for the server side logicDatabase: MongoDBSecurity: JWT authentication and OTP verification,Challenges we ran into: Accomplishments that we're proud of: What we learned: The importance of emotional intelligence in financial decision-makingAdvanced ML model implementation and training techniquesReal-world application of behavioral economics principlesChallenges and solutions in building secure financial applicationsThe value of user feedback in iterative developmentUsing Github and git successfully,What's next for RupAI:",
                        "github": "https://github.com/saanvitara/RupAI-",
                        "url": "https://devpost.com/software/rupai"
                    },
                    {
                        "title": "MindSpend",
                        "description": "MindSpend is a smart personal finance app that helps users track expenses, set budgets, and hit savings goals\u2014all through a sleek, user-friendly dashboard designed to build better money habits.",
                        "story": "\ud83d\udcac About the Project: MindSpend was born from the need for a user-friendly yet powerful budgeting tool. Inspired by real-world struggles with money management, we set out to build a modern finance app with simplicity at its core.Full-stack MERN app developmentFirebase + JWT auth integrationResponsive UI/UX design with TailwindFinancial data visualization with Recharts,React + Tailwind CSS + RechartsNode.js + Express + MongoDBFirebase Authentication,Token verification with FirebaseExpense and budget alert logicCross-platform design optimizationSmooth API communication,\ud83e\udd1d Contributing:",
                        "github": "https://github.com/NothingADSR123/Code4orce_MIT.git",
                        "url": "https://devpost.com/software/mindspend"
                    },
                    {
                        "title": "DhanSetu",
                        "description": "A Truly decentralized P2P Loan Funding Platform.",
                        "story": "## InspirationThe idea for DhanSetu came from witnessing the struggles of rural communities in India, where lack of financial infrastructure left millions unbanked or underbanked. During a visit to a remote village in Rajasthan, we saw how farmers, small entrepreneurs, and families were trapped in cycles of poverty due to the absence of accessible financial services. This inspired us to create a decentralized platform that empowers communities to make financial decisions.## What it doesDhanSetu is a decentralized finance platform that connects rural communities to financial resources. It allows community members to make collective decisions about loans and financial support, offering a more accessible and transparent system compared to traditional banks. The platform uses community consensus to approve loans, with the aim of fostering financial inclusion and independence.## How we built itWe developed DhanSetu by focusing on simplicity, security, and accessibility. The platform incorporates facial recognition for secure user verification, even on low-end devices. It features an intuitive interface for users with varying literacy levels and works in areas with limited connectivity. We also implemented a voting mechanism that requires 80% community approval for loan disbursements, ensuring fairness and preventing misuse.## Challenges we ran intoOne of the biggest challenges was ensuring the platform worked in areas with poor internet connectivity and low-tech infrastructure. We also had to design a system that was both secure and accessible, while considering the varying literacy and technological skills of users. Developing a fair voting system that prevented collusion while allowing community participation was another challenge.## Accomplishments that we're proud ofWe are proud of creating a platform that empowers communities to control their financial decisions. Our facial recognition system ensures secure, accessible accounts even for users with low-end devices. The voting system has fostered transparency and accountability, while the design has been well-received by communities we\u2019ve worked with. DhanSetu has become a model for decentralized finance in underserved regions.## What we learnedWe learned that trust is fundamental to financial systems. In traditional banks, trust is placed in institutions, but with DhanSetu, trust emerges from community consensus. We also discovered the importance of designing with the community rather than for them, ensuring that the platform reflects the needs and preferences of the people it aims to serve.## What's next for DhanSetuLooking ahead, we aim to expand DhanSetu's services to include savings groups, microinsurance, and educational resources, helping communities build their financial literacy and credit histories. We also plan to explore ways to adapt the platform for other underserved regions globally, offering a sustainable, community-driven model for financial inclusion.",
                        "github": "https://github.com/AskitEndo/SayWinners-Solistice-MIT",
                        "url": "https://devpost.com/software/dhansetu"
                    },
                    {
                        "title": "blockchain loan management",
                        "description": "In India, a significant number of people die by suicide due to factors including bankruptcy or indebtedness, with government data revealing over 5,213 deaths in 2020 alone. Hence, presenting our idea.",
                        "story": "In India, a significant number of people die by suicide due to factors including bankruptcy or indebtedness, with government data revealing over 5,213 deaths in 2020 alone. \nIn many low-income and rural communities, access to formal credit remains a distant reality due to the lack of traditional financial infrastructure, credit histories, and collateral. This initiative seeks to bridge that gap by developing a peer-to-peer (P2P) emergency loan platform that empowers individuals through community-based lending. By leveraging social trust scoring\u2014which draws on peer endorsements, repayment behavior, and community interactions\u2014alongside the transparency and immutability of blockchain technology, this platform enables secure and inclusive microloans without relying on conventional credit systems. The goal is to create a decentralized, multilingual solution that fosters financial inclusion and resilience for the underserved, especially in times of urgent need.",
                        "github": "https://github.com/bh00m1ka",
                        "url": "https://devpost.com/software/blockchain-loan-management"
                    },
                    {
                        "title": "FinMate",
                        "description": "Where Financial Control Meets Convenience.",
                        "story": "Inspiration: The idea forFinMatecame from observing a common yet underserved need \u2014 managing personal finances effectively and accessing short-term funds during financial emergencies. Whether it\u2019s a student struggling with rent, a freelancer facing a delayed payment, or a salaried individual encountering an unexpected medical bill \u2014 financial gaps are real, and traditional banking systems aren't always agile enough to respond.We wanted to build a tool that not only helps users track their finances but also offers a responsible, transparent, and quick borrowing solution. That's where the idea for the\"Financial Clutch\"feature was born \u2014 a micro-loan system designed for real-world needs, right inside the budgeting app.What We Learned: Throughout the development of FinMate, we gained hands-on experience with:Full-stack developmentusing modern frameworks likeNext.js(frontend) andFlask(backend)JWT-based authenticationand route protectionDatabase modelingfor financial data (Users, Loans, Spending)RESTful API designandcross-origin communication(CORS)UI design systemsusingTailwindCSSandReactEnvironment configuration,secure password handling, androbust error management,We also deepened our understanding of how user experience, security, and data handling come together in real-world finance applications.How We Built It: Frontend: Developed withNext.jsandTypeScriptfor a scalable, responsive, and modern user interface. Components were styled usingTailwindCSSand designed with accessibility and mobile responsiveness in mind usingshadcn/ui.Backend: Built withFlask (Python)using aRESTful architecture, handling API requests for user authentication, spending tracking, and loan applications.Database: UsedSQLitefor simplicity and fast iteration, with models forUsers,Spending, andLoans.Authentication: Implemented secureJWT-based authentication, route protection, and hashed passwords using industry best practices.Features:Real-time balance updatesSpending categorization and insights\"Financial Clutch\" for micro-loans (\u20b91,000 - \u20b950,000 @ 4% interest/month)Loan status tracking and purpose loggingUser Friendly and All of Your Data in Your Hands,Challenges We Faced: Integrating real-time updatesbetween user spending data and dashboard insights required careful state management and API optimization.Ensuringsecure communicationacross the frontend and backend (especially with JWT and CORS settings) took some trial and error.Designing aloan feature that\u2019s both user-friendly and financially soundwas tricky \u2014 especially around validation, limits, and interest logic.Building aresponsive and clean UIwithout clutter while providing deep financial insights was a continuous UX/UI design challenge.,Final Thoughts: FinMate is more than a budgeting app \u2014 it's a tool forfinancial empowerment. We're proud of what we've built and excited about the potential to scale it further, possibly integrating UPI support, savings goals, or credit health tracking in future iterations.",
                        "github": "https://github.com/Akhilucky/FinMate",
                        "url": "https://devpost.com/software/finmate-4thy7j"
                    },
                    {
                        "title": "Broke a Fix",
                        "description": "Too Broke? Let's Fix That!",
                        "story": "",
                        "github": "https://github.com/notshatwiks/404-NotFound.git",
                        "url": "https://devpost.com/software/broke-a-fix"
                    },
                    {
                        "title": "SpendWise ",
                        "description": "An app that helps Gen Z decode their spending, crush savings goals, and turn \u201cdo I need it?\u201d into a lifestyle \u2014 all while keeping it aesthetic and fun.",
                        "story": "Inspiration: We realized most of us struggle with budgeting because traditional finance tools are boring, complex, and designed for adults. We wanted to change that. So, we reimagined money management as something fun, personalized, and wrapped in a Spotify-style experience. SpendWise was born out of the idea: what if your money could talk back to you like your favorite playlist recap?What it does: SpendWise is your personal finance BFF. It tracks your spending habits, breaks down where your money goes, gives you a \u201cSpending Personality,\u201d unlocks achievement badges, and at the end of every month \u2014 it generates a creative, Gen Z-style Money Wrapped. Think \u201cSpotify Wrapped\u201d but for your walletHow we built it: Challenges we ran into: Accomplishments that we're proud of: What we learned: What's next for SpendWise: Adding real-time sync with actual bank accounts for live tracking\nCreating social features \u2014 compare streaks, share wrapped stats with friends\nExpanding AI to give weekly money tips based on behavior\nPartnering with brands to reward good money habits \nand finally.....\nTaking SpendWise from project to product \u2014 full launch coming soon!",
                        "github": "https://github.com/alokiscute/spendwise2",
                        "url": "https://devpost.com/software/spendwise-js5e7h"
                    },
                    {
                        "title": "MoMo : Money Monitor!",
                        "description": "Your \"Man in Finance!\"",
                        "story": "Inspiration: We were asked to create something to assist young adults with managing their finances and help them control how much money they spend.We really focused on the \u201cYOUNG ADULTS\u201d part, so we tried to make the application as user friendly and lively\u00a0as\u00a0possibleWhat it does: How we built it: We built the website using frontend and backend web development (using html, css, javascript and flask). We also used APIs and many modules (mentioned in the readme file of github).Challenges we ran into: Trying to track user's transaction history and using it for a better analysisAccomplishments that we're proud of: we could implement our AI chatbot (API), an extension which gives the user some time to reconsider his/her financial decisions before making the payment mindlessly. We also are proud of the gamified experience of maintaining streaks by seeing your own virtual plant grow from a seed to a beautiful flower.What we learned: How to implement APIs, how to make extensions on chrome and overall web developmentWhat's next for MoMo : Money Monitor!: Given the time constraint, we weren't able to implement all the effective stuff. In the future, we'll expand and improve our project by providing more in-depth analysis of the user's transactions and improving the user experience",
                        "github": "https://github.com/Zemksh/CODEATHONPROJ.git",
                        "url": "https://devpost.com/software/momo-money-monitor"
                    },
                    {
                        "title": "FINZEN - FINANCE TRACKER",
                        "description": "Finance with AI- Driven insights",
                        "story": "Inspiration: Increasing ExpensesWhat it does: Helps us track our expensesHow we built it: Using Mondb, Python and StreamlitChallenges we ran into: Chatbot implementationAccomplishments that we're proud of: Coming up with a wonderful websiteWhat we learned: Finance affiliate marketing, ai classifiers, integrating multiple ai models.What's next for FINZEN - FINANCE TRACKER: Complete integration with upi and other payment apps using the history for ai-driven spending insights.",
                        "github": "https://github.com/manas1462005/FinZen",
                        "url": "https://devpost.com/software/finzen-finance-tracker"
                    },
                    {
                        "title": "Fin-Pal",
                        "description": "Fin-Pal is your AI-powered money buddy\u2014track, save, and grow smarter with every spend. Say goodbye to money stress and hello to smart nudges, goals, and a finance app you\u2019ll actually love.",
                        "story": "About the ProjectWhat Inspired UsThis all started with one simple thought: \u201cWhy is managing money so hard?\u201d Between trying to save, keeping track of spending, and setting goals, it just felt overwhelming \u2014 and honestly, kind of boring. We talked to friends and realized we weren\u2019t alone. Everyone wanted to get better with money, but no one liked the tools out there. So we decided to build something better \u2014 Fin-Pal, a friendly, no-stress financial buddy that actually helps.How We Built ItWe started small \u2014 just a team, some ideas, and a shared goal: make money feel less scary. We designed a clean, simple interface, built out forms and dashboards, and added small touches that made it feel more human. Every button, color, and message was chosen to be helpful, not overwhelming. We didn\u2019t want this to feel like another finance app \u2014 we wanted it to feel like support.What We LearnedWe learned how powerful simple things can be \u2014 a gentle nudge, a clear goal, a friendly message. We also learned how much teamwork matters, especially under pressure.\u26a0\ufe0f Challenges We FacedWe had big dreams and little time. Balancing design, code, and usability was tough. But every hurdle made the final result better \u2014 and we\u2019re proud of what we built.",
                        "github": "https://github.com/ayushman46/Solstice-.git",
                        "url": "https://devpost.com/software/fin-pal"
                    },
                    {
                        "title": "tofu",
                        "description": "Finance tracer for the Youth",
                        "story": "Inspiration: What it does?: Tofu is a personal finance management app designed specifically for the youth. \nIt is your All-in-one app to manage all your finances at the palm of your hand.\nIt helps you save up for those important occasions and the things you really really want.Tofu is your best friend at those tough times.How we built it: React Native with ExpoSQLite,Challenges we ran into: Database managementmerge conflict,Accomplishments that we're proud of: App that we would actually useAble to cater and deliver within the given time,What we learned: Team workWorking under tight deadlines and under stressProblem solving and innovative thinking,What's next for tofu: Scaling to a bigger audienceWorking on more features to deliver the best experience,",
                        "github": "https://github.com/gauraavvvvx/tofu",
                        "url": "https://devpost.com/software/tofu"
                    },
                    {
                        "title": "SahayaChain",
                        "description": "Community based finance",
                        "story": "This project builds a peer-to-peer emergency loan platform for people in rural or low-income communities who don\u2019t have access to formal banks or credit cards.Here\u2019s what it does:",
                        "github": "",
                        "url": "https://devpost.com/software/sahayachain"
                    },
                    {
                        "title": "FinTrack",
                        "description": "A fun, easy to use web app that can track your spending, helps you set goals and gamifies the process of saving with the help of an ai chatbot",
                        "story": "Inspiration: We wanted to create a simple yet powerful tool to help individuals manage their personal finances, especially students and young professionals who may not have access to advanced financial apps. Our goal was to raise awareness of budgeting habits and make financial tracking feel easy and rewarding.What it does: FinTrack allows users to sign up, log in, and manage their income, expenses, and savings. It provides visual insights through charts, sends alerts when spending gets too high, and tracks financial transactions\u2014all stored in CSV files for easy portability.How we built it: We used Flask for the backend and HTML/CSS/JS for the frontend. Financial data is managed using pandas and stored in CSV files (accounts.csv, tranxtion.csv). Charts are rendered using Chart.js, and user sessions are managed with Flask\u2019s built-in session support.Challenges we ran into: Handling user sessions correctly with CSV-based authentication\nSimulating historical financial data without a full database\nDynamically rendering charts based on real-time CSV updates\nEnsuring data consistency across multiple read/write operations\nApi plaid implementation wasn't possibleAccomplishments that we're proud of: Built a working full-stack finance tracker without using a traditional database\nImplemented real-time notifications when spending exceeds income thresholds\nCreated an intuitive UI and dynamic charts with minimal setup\nDeveloped a points system to gamify savingWhat we learned: How to manage user state with Flask sessions\nHow to structure CSV files for scalable multi-user data handling\nIntegrating Chart.js dynamically with backend JSON endpoints\nBest practices for simple full-stack apps without external DBsWhat's next for FinTrack: Add user profile editing and secure password hashing\nEnable transaction filtering by date/category\nImplement budget planning features and goal tracking\nUpgrade to SQLite/PostgreSQL for persistent, scalable storage\nDeploy on Heroku or Render for public access",
                        "github": "",
                        "url": "https://devpost.com/software/fintrack-bugysc"
                    },
                    {
                        "title": "Sahara",
                        "description": "Sahara offers zero-interest, no-collateral emergency loans using blockchain and community approval\u2014bringing fast, fair aid to underserved people with full transparency and trust.",
                        "story": "Imagine a world where anyone in crisis can access emergency funds\u2014instantly, ethically, and without barriers.\nNo collateral. No interest. Just trust, transparency, and technology.ChainRelief.org is a decentralized platform that offers zero-interest, no-collateral emergency microloans to underserved individuals\u2014powered by blockchain technology and community-based approval.\ud83d\udee0 How It Works:\nUsers Apply \u2014 Individuals request small emergency loans for urgent needs (e.g., rent, medical bills, food).Community Validates \u2014 Instead of relying on credit scores, applicants are reviewed by a decentralized network of users and local validators who vouch for legitimacy.Smart Contracts Disburse Funds \u2014 Once approved, a smart contract instantly transfers funds with zero interest and clear repayment terms\u2014transparent and immutable.Repay & Rebuild \u2014 Borrowers repay as they are able. Their positive history builds a Web3 trust score, opening future support or services.\ud83c\udf0d Why Blockchain?\nTransparency: Every transaction is traceable, secure, and tamper-proof.Decentralization: No single bank or institution controls access.Trustless Systems: Smart contracts ensure fairness\u2014no predatory practices, no hidden fees.\ud83d\udc99 Why It Matters:\nTraditional finance excludes millions. We\u2019re creating a financial safety net that\u2019s inclusive, community-driven, and future-forward\u2014where the only collateral is trust.With ChainRelief.org, we\u2019re turning emergency aid into a sustainable, decentralized movement.",
                        "github": "https://github.com/Theroid00/Hackathon-Submission",
                        "url": "https://devpost.com/software/sahara-9tfdjs"
                    },
                    {
                        "title": "AI_Driven Financial Behaviour Modification",
                        "description": "A functional AI-driven chatbot or app that delivers real-time nudges and personalized savings tips.\r\nA short user  improvements in spending habits and savings behavior.",
                        "story": "The project aims to develop an AI-powered financial assistant to help young adults manage impulse spending and build savings discipline. It uses mock financial data to classify expenses into needs and wants, and delivers real-time nudges (like alerts and gamified savings suggestions) to modify behavior.Privacy is ensured through basic encryption, and users can choose to opt out of tracking. The solution will be a WhatsApp-based chatbot or a lightweight web/app interface, powered by simple Python models or TensorFlow/PyTorch for expense classification.Expected Deliverables:A functional chatbot/web app giving personalized nudges and savings insights.A short user testing report showing improvement in spending behavior.",
                        "github": "https://github.com/Amoghavarsha-1/AI_FINANCE",
                        "url": "https://devpost.com/software/ai_driven-financial-behaviour-modification"
                    }
                ],
                [
                    {
                        "title": "LiquidPay",
                        "description": "How about an applications tailored to Youts for doing things which are done by Oldies?? We got you covered. Our Application Liquid Pay is an AI Driven Financial Behaviour Management\u00a0System",
                        "story": "InspirationWe were inspired by the common struggle people face when managing finances\u2014especially in low-income or underserved communities. Often, there\u2019s a lack of awareness around where money goes, and no real incentive to improve financial habits. We wanted to build something that doesn\u2019t just track spending but actively motivates better decisions. That\u2019s where LiquidPaywas born\u2014an AI-driven platform that rewards users for building smarter financial behavior.What it doesLiquidPay is a personalized financial behavior platform that:\n    \u2022 Classifies transactions as needs or wants using a machine learning model.\n    \u2022 Predicts the user\u2019s financial condition for the upcoming week using a Long Short-Term Memory (LSTM) model trained on the last 60 days of spending data.\n    \u2022 Generates daily financial goals based on user behavior using insights from the Gemini API.\n    \u2022 Rewards users with our custom ERC-20 token\u2014SaveIT (SIT) for completing goals (5 SIT per completed goal).\n    \u2022 Provides a dashboard displaying total balance, savings rate, notifications, and recent transactions, all dynamically updated from backend data.\u2e3bHow we built it\n    \u2022 Frontend: Built with React, providing a clean and interactive dashboard.\n    \u2022 Backend: Built with Node.js and MongoDB for storing and retrieving user transactions and goal tracking.\n    \u2022 Classification model: Used to categorize transactions into needs and wants before storage.\n    \u2022 LSTM model: Trained on 60 days of transaction data to forecast financial conditions for the next 7 days.\n    \u2022 Gemini API: Used to analyze user behavior and help tailor daily goals.\n    \u2022 Blockchain: Deployed the SaveIT (SIT) token on Polygon Amoy Testnet and implemented reward distribution logic for completed goals.Challenges we ran into\n    \u2022 Integrating machine learning models with backend infrastructure in a seamless and scalable way.\n    \u2022 Getting the classification model to reliably tag transactions in real-time.\n    \u2022 Ensuring timely generation and deletion of goals, while tracking daily performance accurately.\n    \u2022 Connecting blockchain components (especially token transfers and balance checks) using Thirdweb and working through compatibility issues.\n    \u2022 Creating a dashboard that is both informative and dynamically linked to real backend data.Accomplishments that we\u2019re proud of\n    \u2022 Successfully integrating AI and blockchain into one cohesive financial tool.\n    \u2022 Building a real-time reward system using ERC-20 tokens.\n    \u2022 Designing a clean user flow from data classification to behavioral rewards.\n    \u2022 Making an impact-driven platform that could genuinely improve financial habits for users.What we learned\n    \u2022 How to apply machine learning models (especially LSTM) to real-world time series financial data.\n    \u2022 Hands-on blockchain development\u2014from deploying a token to integrating it into a backend reward system.\n    \u2022 How to build a robust system for automated goal setting and tracking using behavioral APIs like Gemini.\n    \u2022 Importance of seamless frontend-backend communication for a real-time user experience.What\u2019s next for LiquidPay\n    \u2022 Deploy on a real network and integrate with real financial data providers like Plaid or RazorpayX.\n    \u2022 Implement a peer challenge system where users can challenge friends to savings goals for bonus rewards.\n    \u2022 Add financial nudges based on LSTM predictions to prevent poor spending weeks.\n    \u2022 Launch a mobile-first version to improve accessibility, especially for users in lower-income communities.\n    \u2022 Integrate middleman-based cash conversion for token rewards, so even users without crypto knowledge can benefit.Let me know if you want to add visuals, API code links, or tweak the tone!",
                        "github": "https://github.com/shreyas-omkar/liquidPay",
                        "url": "https://devpost.com/software/liquidpay"
                    },
                    {
                        "title": "Ivory",
                        "description": "Stop Splurging. Start Saving with Ivory.",
                        "story": "Inspiration: The idea for this project was born out of a common struggle \u2014 managing personal finances efficiently without getting overwhelmed by numbers, trends, and jargon. As students ourselves, we noticed how many people, especially young adults, find it difficult to make informed financial decisions due to a lack of guidance, planning tools, or time. We wanted to change that by creating asmart financial assistant\u2014 one that not only tracks expenses but alsouses AI to give insightful advice, predict spending behavior, and simplify financial planning.How We Built It: Our tech stack centered aroundFlaskfor the backend andMockaroofor generating realistic mock financial data based on a sample dataset. Here\u2019s a breakdown:Backend: Built with Flask, handling API routes, data parsing, and logic for financial insights.Mock Data Integration: Used Mockaroo's API to simulate financial records (e.g., transaction types, dates, vendors, amounts).Data Processing: Performed data cleaning, aggregation, and basic ML logic to generate suggestions or flags (like overspending, budgeting anomalies, etc.).AI Logic: Incorporated a simple rule-based AI model (with future plans for ML/NLP enhancements).Version Control: Managed using Git and GitHub for collaboration.,The project was structured in modular folders for clarity:/data_processing//model_training//api//mock_data/,What We Learned: This project taught us a ton, including:Effective API integrationusing external data generators like Mockaroo.Working in a real-world Flask backend structure, beyond simple apps.Debugging Git merge conflicts under time pressure.Building cleaner and more maintainable code, with a focus onseparation of concernsandscalable logic.Collaborating on a team project with tight deadlines and rapidly evolving scope.,Challenges We Faced: Git merge conflicts: Early miscommunication led to conflicting changes that slowed progress, especially during critical hours.Data inconsistencies: Mockaroo sometimes returned data that needed additional pre-processing to be useful.Time constraints: Balancing feature ambition with time made us rethink scope multiple times.AI tuning: We initially aimed for more advanced AI features, but had to scale back to simpler rule-based logic due to time and data limitations.,What's Next?: We plan to:Add afrontend dashboardfor visualizing insights.Replace rule-based logic withmachine learning modelstrained on actual anonymized spending datasets.Incorporatevoice/chat-based interactionusing LLMs for a more intuitive assistant experience.,",
                        "github": "https://github.com/Z33xD/CodeDiggers",
                        "url": "https://devpost.com/software/ivory-70wi6f"
                    },
                    {
                        "title": "T3TRUST",
                        "description": "Small Loans, Big Impact.",
                        "story": "Inspiration: We were inspired by the financial exclusion faced by millions in low-income and rural communities who lack access to formal credit systems. With a lack of credit history, collateral, or banking infrastructure, many individuals struggle to get the financial help they need in times of emergency. By leveraging technology, we wanted to create a solution that democratizes access to loans, using social trust and peer-to-peer networks.What it does: T3TRUST is a decentralized P2P emergency loan platform that allows individuals from underserved communities to request microloans without needing traditional credit scores. The platform uses social trust scoring based on community referrals and repayment history. It combines blockchain-like features for transaction security, OTP-based identity verification for fraud prevention, and offers a simple SMS/WhatsApp interface for easy access.How we built it: Backend: We used Flask to build the core API, managing user registration, OTP verification (via Twilio), loan requests, and repayment tracking. We also created a mock blockchain ledger using a simple database to simulate blockchain transactions for added transparency and security.Frontend: The frontend was built using React components, providing users with a clean and simple interface to request loans, verify OTPs, and track loan status. We also integrated Twilio's SMS API for loan requests via WhatsApp, making it accessible for users in rural areas with limited access to mobile apps.Social Trust Scoring: A simple points-based system was implemented, where users earned trust points based on successful loan repayments and community referrals. This model incentivives good behavior and builds a sustainable lending ecosystem.Challenges we ran into: Limited Time: With only 24 hours, it was challenging to integrate all features (SMS, OTP, backend, frontend) while maintaining simplicity and usability.Simulating Blockchain: Mocking a blockchain-like ledger was tricky, as we had to ensure data integrity without an actual blockchain infrastructure. We worked with a simple hashed database ledger to simulate this feature.Accessibility: Ensuring the platform was easy to use for people with little or no experience with technology was a key challenge. We focused on making the interface mobile-friendly and implemented SMS-based interaction for a wider audience.Scalability: The initial version is a prototype, so scaling it for real-world use with NGOs or microfinance institutions would require a more robust system architecture and security features.Accomplishments that we're proud of: Almost fully functional prototype of a decentralized loan platform in 24 hours.SMS-based verification system, ensuring accessibility for users without smartphones or internet access.Successful integration of Twilio for OTP-based user authentication and loan requests.A basic social trust scoring system, which encourages responsible lending and borrowing.What we learned: Time management in a hackathon setting is critical. We learned to prioritize features and focus on delivering the core functionalities first.Integration challenges between different technologies (Flask backend, React frontend, and Twilio) helped us understand how to work with APIs and third-party services.The importance of simplicity in design: Ensuring that the platform is intuitive and accessible for non-tech users was a key takeaway.Blockchain simulation was a useful exercise in understanding how decentralized systems can be mimicked without full blockchain integration.What's next for T3TRUST: Expand the platform by adding features like dynamic interest rates, loan history, and rewards for trusted users.Real blockchain integration: Transition from the mock ledger to an actual blockchain platform (like Hyperledger Fabric or Ethereum) for greater security and transparency.Partnerships with NGOs and microfinance organizations to scale the platform to more underserved communities.Mobile app development to enhance accessibility and offer a richer user experience.",
                        "github": "https://github.com/madhurmaru/T3Trust",
                        "url": "https://devpost.com/software/t3trust"
                    },
                    {
                        "title": "LendConnect",
                        "description": "Redefining credit for the underserved",
                        "story": "Inspired by real-life situations like the 2020 COVID-19 lockdown\u2014where daily wage earners and rural households were left without access to emergency credit\u2014we've developed a Decentralized Emergency Loan Platform designed to serve underserved communities. When people like Rani, a rural vegetable vendor, urgently need a small loan but lack a credit score or bank access, our platform enables them to request microloans via a multilingual interface, accessible even through SMS or WhatsApp. The system uses PAN card-based authentication for basic identity verification and assigns a Trust Score to each user based on community referrals, repayment behavior, and basic data.Loan requests are prioritized based on urgency, trust score, and holding period. A referral and review system helps strengthen trust between borrowers and lenders. Lenders are incentivized through minimal yet meaningful returns (1\u20132% APR) and gamified recognition (like badges for top contributors). Borrowers can also request loan extensions with transparent tracking of repayments.Technologically, the platform runs on a modern stack\u2014React + Vite for a fast, responsive frontend, styled with Tailwind CSS, and enhanced with Node.js and TypeScript. The backend is built on Django REST, with a mock blockchain implemented via a secure, hashed database ledger to ensure transparency and immutability of transactions. APIs manage key operations like user interface interactions and simulated money transfers.At its core, this is a peer-to-peer (P2P) decentralized system that redefines creditworthiness by focusing on social trust instead of traditional scores. Our mission is clear: empower rural communities, support women in need, and build transparent, accessible financial systems for all.Unfortunately we couldn't integrate the frontend and the backend, but they both work well individually. In the end we couldn't upload demo videos for the frontend and the backend together, so we have added it in the Youtube playlist, please consider it.",
                        "github": "https://github.com/namitasampath/Fintech-Hackathon",
                        "url": "https://devpost.com/software/lendconnect"
                    },
                    {
                        "title": "Ace Finance",
                        "description": "Ace Finance combines automated spending analysis with Malan \u2014 your multilingual AI financial companion \u2014 for real-time advice, multilingual support, and actionable recommendations.",
                        "story": "LOOM LINK:https://www.loom.com/share/fc055f540c9341cfa0c8a80dc07bce62?sid=118739dc-ea8d-4382-8f56-22e07be01fc2Inspiration: Managing finances as a student or young adult can be overwhelming. We wanted to build something that takes raw transaction data and turns it into clear, actionable insights\u2014without needing a background in finance. That\u2019s how Ace Finance was born\u2014a smart financial dashboard that helps users track spending, categorize expenses, and visualize trends effortlessly.What it does: Ace Finance is a personal finance analyzer that:\n    \u2022 Uploads bank statements (PDFs/CSVs)\n    \u2022 Extracts and categorizes transactions using NLP & rule-based logic\n    \u2022 Provides spending breakdowns by category\nAI CHATBOT\n    \u2022 Shows month-wise spending trends\n    \u2022 Projects future expenses\n    \u2022 Offers visual dashboards (charts, tables) to make sense of the dataHow we built it: Challenges we ran into: Accomplishments that we're proud of: What's next for Ace Finance:",
                        "github": "https://github.com/mayhixza/tech-solstice-2025",
                        "url": "https://devpost.com/software/ace-finance"
                    },
                    {
                        "title": "loan.ly",
                        "description": "Decentralized Loans for Those Left Behind by Banks.",
                        "story": "Inspiration: Witnessing how 1.7 billion unbanked adults get excluded from emergency loans due to rigid credit systems, we envisioned a financial safety net powered by community trust instead of collateral. Our inspiration came from Kerala's \"Kudumbashree\" women's collectives where peer accountability enables microlending without paperwork.What it does: loan.ly enables hyper-local emergency loans through:\nTrust Scoring: Community-validated reputation replaces credit scores\nRisk-Sharing Pools: 2% of every loan builds a community insurance fund\nDecentralized Matching: Lenders in your locality/network bid to fund requestsHow we built it: Frontend: Flutter, DART, Android Studio for cross-platform Android/iOS/web app compatibility\nBackend: Supabase (PostgreSQL + Edge Functions)\nTrust Engine: trust_score = (referrals \u00d7 5) + (repaid_loans \u00d7 10) - (defaults \u00d7 15)Challenges we ran into: Implementing SMS based transactions for free, as a proof-of-concept\nDynamic trust score calculation in PostgreSQL\nLender-borrower matching without real-time WebSockets\nPivoting from geofencing to social graphs mid-developmentAccomplishments that we're proud of: Built and deployed a decentralized lending stack\nSolved optimization and scalability challenges for low-income users.\nImplemented alternative credit scoring using on-chain behavior and community reputation.What we learned: Community validation > AI scoring in low-literacy contexts\nSMS UIs need 3x more error handling than apps\nMicro-pools beat individual liability models\nTrust is quantifiable through social graph densityWhat's next for loan.ly: Voice-IVR Integration: For non-literate users\nBlockchain Ledger: Convert simulated ledger to Hyperledger\nGamified Savings: Earn trust points by depositing to risk pools\nDisaster Mode: Auto-approve loans during IMD flood alerts\nCross-Community Bidding: Expand trust networks via referrals",
                        "github": "https://github.com/schengaloor/loan.ly.app",
                        "url": "https://devpost.com/software/loan-ly"
                    },
                    {
                        "title": "Finny",
                        "description": "Finance Tracking App",
                        "story": "Inspiration: FinnyWhat it does: How we built it: Challenges we ran into: Accomplishments that we're proud of: What we learned: What's next for Finny:",
                        "github": "https://github.com/VidipGaur/Finny-codeathon",
                        "url": "https://devpost.com/software/finny"
                    },
                    {
                        "title": "TheGoodBytes",
                        "description": "Ai finance",
                        "story": "Inspiration: Solve a Personal Pain Point \u2013 Think of something that bugs you or your friends and fix it.What it does: How we built it: We built the app using React.js for the frontend and Node.js for the backend.\nJavaScript handles dynamic interactions like budgeting logic and reminders.\nWe used mock data and rule-based logic to classify expenses and trigger nudges\u00a0in\u00a0real\u00a0time.Challenges we ran into: We faced challenges in accurately classifying expenses into needs vs wants using simple logic.\nCreating a smooth user experience for budgeting and reminders required careful UI planning.\nBalancing privacy while simulating real-time nudges with mock data was\u00a0also\u00a0tricky.Accomplishments that we're proud of: We built a fully functional financial assistant prototype within the hackathon time limit.\nSuccessfully implemented real-time nudges, a dynamic budget planner, and behavior tracking.\nProud to have created an engaging, privacy-conscious tool that promotes better\u00a0money\u00a0habits.What we learned: We learned how to build a full-stack AI-powered finance assistant that nudges users toward smarter spending using graphs, alerts, and gamification. The project enhanced our skills in data visualization, user-centric design, behavioral insights, and secure\u00a0web\u00a0developmentWhat's next for TheGoodBytes: Next, we plan to integrate real-time financial APIs for live expense tracking.\nWe'll enhance our nudging system using ML models for smarter behavioral predictions.\nThe Good Bytes will continue refining the app for wider adoption and real-world impact.",
                        "github": "https://github.com/Adityapattar/TheGoodBytes",
                        "url": "https://devpost.com/software/thegoodbytes"
                    },
                    {
                        "title": "FinFlow",
                        "description": "ai based finance tracker",
                        "story": "Inspiration: notionWhat it does: gamify finance tractingHow we built it: pythonChallenges we ran into: Accomplishments that we're proud of: What we learned: teamworkWhat's next for FinFlow: go public",
                        "github": "https://github.com/Samarthvurs/FinFlow",
                        "url": "https://devpost.com/software/finflow"
                    },
                    {
                        "title": "Trust Fund",
                        "description": "Decenetralized Small Loans",
                        "story": "Inspiration: WE wanted to make itWhat it does: How we built it: Challenges we ran into: Accomplishments that we're proud of: What we learned: What's next for Trust Fund:",
                        "github": "https://github.com/saaheerpurav/social-loan-ledger-30",
                        "url": "https://devpost.com/software/trust-fund"
                    },
                    {
                        "title": "SpendWise",
                        "description": "An AI-powered financial assistant that helps users reduce impulsive spending, automate savings, and build better money habits with smart nudges, ML predictions, and a finance\u00a0chatbot.",
                        "story": "The Journey of Building SpendWiseOur journey with SpendWise began with a simple idea: to create an app that helps people track their expenses and make smarter financial decisions. However, like any development process, we faced some interesting challenges along the way.Initially, we wanted to integrate machine learning directly into our chatbot for predicting impulsive purchases. The goal was for the chatbot to not only respond to users but also classify their purchases based on patterns. However, we encountered significant hurdles in implementing the machine learning models efficiently within the chatbot, especially when handling real-time data and ensuring the bot\u2019s responsiveness.After some trial and error, we decided to take a step back and simplify the process. Instead of embedding the ML directly into the chatbot, we integrated an API using Python Flask to handle the classification of purchases. This allowed the chatbot to call the API for purchase predictions without overloading the bot's performance. The result? A seamless and efficient integration that enabled the chatbot to offer insights into impulsive spending behavior.On the frontend, we used React to create a clean, dynamic user interface, making it easy for users to interact with SpendWise. For the backend, we chose Node.js to handle the bulk of the data processing, ensuring a fast and reliable experience for users.Ultimately, the combination of Python Flask for the ML API, React for the frontend, and Node.js for the backend allowed us to build a robust, interactive expense tracker that delivers valuable insights. It was a rewarding experience to see all the pieces come together, and we\u2019re excited to see how SpendWise can help users manage their finances smarter.",
                        "github": "https://github.com/rudra995/client.git",
                        "url": "https://devpost.com/software/spendwise-7m09zo"
                    },
                    {
                        "title": "Finetic - TechSolstice 2025",
                        "description": "AI-powered financial tracker for youngsters, which has the option to run offline on the device - without sending any personal, sensitive data off-device.",
                        "story": "Inspiration: Young adults struggle with impulse spending and a lack of savings discipline.What it does: AI-powered financial assistant that helps users curb unnecessary expenses and automate savings through personalised real-time nudges.How I built it: The Android app is written in kotlin with a Gemma model embedded insideConverted a Python script to run in a kotlin/java environment to interact with the Gemma model transform the data and categorize it,Challenges I ran into: Extracting and Processing financial details without compromising securityInclude AI-based analyses without sending data off-device or onlineWrapping a Gemma model inside the app and making connections with the backend to ensure the analyses happen on device without any internet connections made,Accomplishments that I'm proud of: SMS Parsing system that ensures that reliable real-time data about the user's financial transactionsTeamwork and Cross collaboration,What I learned: Make the most out of the leastUpholding data safety and compliance standards,What's next for Finetic - TechSolstice 2025: Adding new features according to the roadmap we laid outUnderstanding if there is a viable market for the Applications offerings and use cases.,",
                        "github": "https://github.com/seriouspoop/techsolstice25",
                        "url": "https://devpost.com/software/finetic-techsolstice-2025"
                    },
                    {
                        "title": "AI DRIVEN FINANCIAL BEHIAVIOR MODIFICATION",
                        "description": "AI DRIVEN FINANCIAL BEHIAVIOR MODIFICATION",
                        "story": "Inspiration\nManaging personal finances can be overwhelming, especially when bad spending habits are deeply ingrained. We wanted to build a solution that not only tracks spending but actively helps users modify their financial behavior using AI-driven nudges and insights. Inspired by behavioral economics and digital wellness apps, our goal was to create a system that guides users towards smarter financial decisions over time.What it does\nOur application, SpendWise, is a web-based platform that helps users build healthier financial habits through:Categorized transactions for better expense trackingAI-generated spending insights to identify patternsSmart nudges that encourage saving and mindful spendingGamified savings goals to make budgeting funA user-friendly dashboard that visualizes spending and progressIt goes beyond just budgeting\u2014it actively helps you behave better financially.How we built it\nWe used:HTML/CSS/JavaScript for the frontend (with Tailwind CSS for styling)Chart.js for rendering dynamic chartsFirebase for authentication and database storageAI logic for generating personalized financial nudges and insights based on spending trendsOrganized the code for smooth integration and compatibility with VS CodeChallenges we ran into\nIntegrating AI logic in a meaningful, non-intrusive wayEnsuring a smooth user experience across different features like transactions, nudges, and insightsStructuring the codebase to be maintainable and scalableTime management across multiple pages (dashboard, nudges, login/signup)Accomplishments that we're proud of\nSuccessfully built an interactive dashboard that gives users a real-time look at their financesDeveloped a nudge system that personalizes suggestions based on real dataImplemented user authentication and secure data storageDesigned a visually appealing UI that simplifies financial trackingWhat we learned\nHow to use AI and behavioral science principles in practical applicationsFirebase integration and real-time data handlingImportance of UI/UX when dealing with data-heavy appsModular coding practices and frontend-backend communicationWhat's next for AI-Driven Financial Behavior Modification\nAdvanced AI models to generate even more contextual nudgesMobile app version for real-time notificationsIntegration with bank APIs for automatic transaction trackingExpanding gamification elements (badges, streaks, rewards)Community features like challenges and shared savings goals",
                        "github": "https://github.com/HSSHREYAS/AI-DRIVEN-FINANCE-WEBAPPLICATION.git",
                        "url": "https://devpost.com/software/ai-driven-financial-behiavior-modification"
                    }
                ]
            ]
        },
        {
            "title": "AdaHacks VI",
            "location": "Fogarty Innovation",
            "url": "https://adahacks-vi.devpost.com/",
            "submission_dates": "Apr 05, 2025",
            "themes": [
                "Beginner Friendly",
                "Education",
                "Music/Art"
            ],
            "organization": "Saint Francis High School",
            "winners": false,
            "projects": []
        },
        {
            "title": "Texas Healthcare Challenge",
            "location": "Pegasus Park",
            "url": "https://texas-healthcare-challenge.devpost.com/",
            "submission_dates": "Apr 04 - 05, 2025",
            "themes": [
                "Beginner Friendly",
                "Health",
                "Social Good"
            ],
            "organization": "Health Wildcatters",
            "winners": false,
            "projects": []
        },
        {
            "title": "CatHacks XI",
            "location": "University of Kentucky- Grehan Building ",
            "url": "https://ukycathacks-xi.devpost.com/",
            "submission_dates": "Apr 04 - 05, 2025",
            "themes": [
                "Beginner Friendly"
            ],
            "organization": "University of Kentucky",
            "winners": true,
            "projects": [
                [
                    {
                        "title": "Factify",
                        "description": "An Effortless Fact-Checking Google Chrome Extension\r\n\r\n",
                        "story": "Inspiration: Misinformation and fake news run rampant online with no quick way to check if the information is factual. This inspired us to create Factify as a tool to empower users to get instant fact-checking and bias analysis.What it does: Factify is a Google Chrome Extension that will evaluate a highlighted statement or webpage for factual accuracy and bias by using Perplexity's Sonar-Reasoning-Pro API. It returns a score from 0-100 for both factualness and bias, along with listing the reasoning and sources that it used to do this. An additional feature of our extension is the community feature where users can either like or dislike fact checks that are saved to the community page by other users.How we built it: We built the extension using HTML, CSS, and JavaScript to read any text chosen by a user and run this text through Perplexity Sonar reasoning model to create a detailed overview with citations in a interactive side panel.Challenges we ran into: We struggled when figuring out how to implement the dynamic side panel that opens on a button click. Also, designing the UI so it would display all of our data in an easily understandable format.Accomplishments that we're proud of: We are proud of making a Google Chrome Extension for the first time, including a dynamic side panel and button click feature upon user highlighting text. This was also a majority of the team's first time using an artificial intelligence API. The community feature is also somthing we are proud of as it allows us to further differentiate our product from other similar products.What we learned: We gained experience in creating a Google Chrome Extension with an easily understandable and space friendly design. Additionally, three of us also have never worked with an AI API before, which allowed us to learn about cutting-edge technologyWhat's next for Factify: Our next steps include publishing our extension to the Google Chrome Webstore. Also, giving rag model context to true sources to allow students or academic writers to use truthful information in their writing. We would also like to continue to explore the community feature.",
                        "github": "https://github.com/ImNateBerry/FactCheckingExtension",
                        "url": "https://devpost.com/software/factify-f7t1yq"
                    },
                    {
                        "title": "Modeling Radioactive Particles Through Membrane",
                        "description": "Breaking Barriers: Radioactive Transport Visualized",
                        "story": "Inspiration - We were inspired by the idea of our future energy systems. We are motivated to live in a world where we utilize our full energy capabilities without people living in fear. Our project is here to help professionals and people realize we have safe ways to combat radioactive waste.: What it does: We want to make clear at a molecular level a simulation with real-time applied physics of solvents to get rid of potentially radioactive material using a metal organic framework. Our simulation implements the Velocity Verlet integration algorithm for particle trajectories, Coulombic interactions for charged particles, and proper bond constraints to ensure physical accuracy. The simulation is run through Python and is demonstrated in real time with a user being able to input changes in molecule amounts. This helps show that even with more radioactive material, the filter is still effective.How we built it -: We integrated and expanded the functionalities of preexisting libraries to create a coherent simulation workflow. By leveraging AI, we were able to speed up the optimization of our code as well as beautify our UI. Due to the large amount of computing required to propagate the system in time, this was critical for our model. By utilizing numerical methods and approximations to fundamental equations, we were able to model how molecules interact with each other at the solvent-framework interface. The most time-intensive part of the development project was formulating and understanding the mathematics necessary to model these systems.Challenges we ran into: Running the model took significant time at every iteration. Visualizing complex 3D molecular behaviors well required optimizing our workflow over several iterations. This is due to the time-intensive nature of the numeric integration algorithms and force calculations. The sequential logic implemented in our model also makes catching and fixing errors harder..Accomplishments that we're proud of: We're particularly honored to create a simulation that maintains scientific accuracy while being visually accessible and interactive. We wanted to make sure our implementation could be used by a user. As Benjamin Franklin put it  \"Tell me and I forget, teach me and I may remember, involve me and I learn.\"What we learned: This project deepened our understanding of molecular dynamics principles, and we also gained a greater understanding in the challenges of maintaining particles and their physical properties. We also gained a greater understanding in debugging and implementing physical properties in a simulation through Python.What's Next for Modeling Radioactive Particles Through Membrane: We're interested in incorporating machine learning to optimize membrane design parameters based on simulation results. We also want to expand our library of viable elements, which proved complicated to do in a 24-hour time period due to the rendering constraints along with the dramatic differences in physics  of each simulation",
                        "github": "https://github.com/Sixteen1-6/xenon-transport-simulation",
                        "url": "https://devpost.com/software/modeling-radioactive-particles-through-membrane"
                    },
                    {
                        "title": "By Ear",
                        "description": "A python solution to generating sheet music from heard audio. Our project detects notes beat-by-beat and produces beautiful sheet music for personal use.",
                        "story": "Inspiration: It is often super difficult for beginner music students to find sheet music for songs they like or hear. This project hopes to alleviate that challenge so that they can practice easily.What it does: This tool is a CLI python project which transforms .wav and .mp3 files into an image of the correct sheet music and notes.How we built it: This project is built entirely in Python using pygame, soundfile, numpy, and numerous signal processing techniques.Challenges we ran into: Pitch Detection: Requires Fourier Analysis to find the fundamental frequencies of the waveBPM Detection: Requires amplitude spike detection to track beat and tempoSheet Music Generation: Need to count the duration of notes and display accordingly using PyGame\n## Accomplishments that we're proud ofHaving a100%accurate \"Mary Had a Little Lamb\" piece sheet music generationDetecting basic chord structures and BPMsCreating a custom intermediary file format to store musical data,What's next for By Ear: We plan to expand the project to detect chord progression songs, a wider range of BPMs, and different key signatures.",
                        "github": "https://github.com/NicholasCastelluzzo/by-ear",
                        "url": "https://devpost.com/software/by-ear"
                    },
                    {
                        "title": "CIAI",
                        "description": "All in one Android app to quiz, educate, and answer questions about cybersecurity.",
                        "story": "Inspiration: We have a passion for cybersecurity and wanted to learn Kotlin!What it does: Teaches about Cybersecurity best practices through information, a quiz, and an integrated AI consultant.How we built it: We used the Android Studio IDE with GitHub for version control.Challenges we ran into: Learning Kotlin and integrating our different parts.Accomplishments that we're proud of: Learning Kotlin and the Android Studio IDE.What we learned: Kotlin and reinforced our Git abilities.What's next for CIAI: Polishing up components, increased quiz questions, and more informational content.",
                        "github": "https://github.com/JWSmoroske/Cathacks-XI-Project",
                        "url": "https://devpost.com/software/ciai"
                    },
                    {
                        "title": "BrightFeed",
                        "description": "BrightFeed is a parental control product that uses AI to inform parents if a subreddit or YouTube video has inappropriate content. The final product will be a Chrome extension that can block websites.",
                        "story": "Inspiration: Living in a very technologically restricted household, misunderstandings around technology happened all the time. BrightFeed aims to change that by providing a tool for parents to see if a website is safe.What it does: BrightFeed takes a subreddit name or YouTube video link as input, as well as a selection of what to check for on the website. AI determines if the subreddit or video contains that type of content and outputs its decision, as well as instances of the content if any. It can then take input for a password and check if it is correct.How we built it: We used HTML and JavaScript to create the webpage and its functionality and used Python to run through AI and return a text output.Challenges we ran into: We were using technology that we are not particularly familiar with like HTML, JavaScript, Git and GitHub, AI, and website APIs. We had many technical issues to overcome but we ultimately made a working product.Accomplishments that we're proud of: We were able to successfully make our product despite our lack of experience, knowledge, and planning. We learned a lot about coding languages and software that we were unfamiliar with and made it all work together.What we learned: We learned a lot about software we were unfamiliar with. We also learned a lot about teamwork, our strengths and weaknesses regarding technology, and about the capability and useability of AI.What's next for BrightFeed: The next stage for BrightFeed is a Chrome extension that blocks access to websites until verified using a password. The extension would save website URLs that were verified so that they could be accessed in the future. The extension would work on any website with text instead of just Reddit and YouTube.",
                        "github": "https://github.com/EliasNajjar/CatHacks-XI-Bright-Feed",
                        "url": "https://devpost.com/software/brightfeed"
                    },
                    {
                        "title": "Trace",
                        "description": "Facts, Locked in. Doubt, Locked out",
                        "story": "",
                        "github": "https://github.com/VanshJP/trace",
                        "url": "https://devpost.com/software/trace-kqvcj9"
                    },
                    {
                        "title": "Midas-Market",
                        "description": "This grocery store helper will turn your money to gold with savings!",
                        "story": "About the Project: This project was born out of a desire to combine practicality with fun. I wanted to create an application that not only simplifies the process of comparing grocery prices from multiple stores but also provides a playful experience while you wait for the data to load.What Inspired Me: The inspiration came from everyday frustrations with online grocery shopping\u2014navigating different websites, searching for the best prices, and comparing products could be tedious. I thought, why not add a bit of fun to the process? Merging a simple game with real-time price scraping made the project uniquely engaging.What I Learned: Web Scraping & Automation:I dove deep into Selenium to automate web interactions, extract real-time data, and handle dynamic content across different grocery websites.Asynchronous Processing:Implementing background threads in Flask for scraping while updating a progress bar in real time was a major learning curve.Frontend-Backend Integration:Integrating a playful game built with JavaScript (using a canvas for the interactive game) with a Flask backend taught me a lot about client-server communication.Error Handling & Debugging:Overcoming challenges like handling GPU warnings in headless browsers and ensuring robust error handling was a significant part of the journey.,How I Built It: Challenges Faced: Handling Asynchronous Tasks:Ensuring that the background scraping did not block the main thread while updating the progress bar in real time required careful planning and debugging.Dynamic Content:Scraping dynamic websites that load content asynchronously posed challenges in ensuring that the right elements were available before extraction.Integration of Game and Progress Bar:Merging the interactive game with the real-time progress tracking was tricky. Balancing the user experience between entertaining gameplay and meaningful progress feedback was a key focus.Environment Issues:Running headless browsers sometimes resulted in GPU-related warnings, which, although non-fatal, needed to be understood and managed.,Overall, this project was a rewarding blend of practical functionality and creative problem-solving, pushing me to expand my skills in both backend automation and frontend interactivity.Inspiration: What it does: Takes your grocery list and scrapes real-time prices from Target, Trader Joe\u2019s, and Meijer using Selenium.Aggregates the data using Pandas into a unified price comparison across all stores.Ranks stores per item and gives you the total cost per store.Shows you the cheapest store overall, in a clean JSON outputAccomplishments that we're proud of: Python + Selenium scrape real-time prices from 3 stores.Pandas aggregates the data, sorts by price, and generates a JSON summary.Flask + JavaScript game keeps users entertained while scraping runs in the background.Users get a downloadable Excel or JSON file of all item prices and the cheapest store.What we learned: Building multi-site scrapers for dynamic contentManaging background scraping + real-time frontend updates with FlaskMerging utility with interactive UI designWhat's next for midas-market: Add more stores (e.g. Walmart, Whole Foods)Turn it into a mobile appPrice tracking and historical comparisons",
                        "github": "",
                        "url": "https://devpost.com/software/midas-market"
                    },
                    {
                        "title": "GameTok",
                        "description": "Never be bored again. As a mixture of TikTok and minigames, GameTok is the best way to enjoy those tedious bus rides. Scroll through a list of dozens of games, ranked by user likes and time played.",
                        "story": "Inspiration: Our initial ideia wasWhat it does: Our initial idea was born out of a simple yet fun challenge: beating Flappy Bird using different Reinforcement Learning (RL) algorithms. With the recent buzz around DeepSeek and other breakthroughs in AI, we became fascinated with the potential of RL and wanted to explore how it could be applied in a playful, game-like environment. Once we managed to recreate the Flappy Bird game and integrate RL agents early in the process, we realized we had room to expand our project in a more creative direction.\nThat\u2019s when the idea for GameTok hit us: an app that combines the swipe-based interface of TikTok with the fun of video games. Our inspiration came from observing how Generation Z is deeply engaged with both short-form content and interactive gameplay. We thought: why not merge the two experiences into one platform?How we built it: GameTok was mainly developed using ReactJS for the frontend and Google Firebase for the backend. The idea was to create an infinite scrolling feed of games, TikTok-style. For that, we searched dozens of minigames online and we stored a list with information about each one in a database. During this selection, various criteria were considered, such as vertical display orientation, little to no ads, free to use, and the game category.\nWe also added fields for likes and time played by users, so that we could rank the games. The app works on both computers/laptops and mobile, as it is web-based.Challenges we ran into: Debating project ideas, multiple bugs, hard implementation problems, lack of documentation.Accomplishments that we're proud of: Coming up with great ideas and working on hard technical details for both the FlappyNite game and the GameTok platform.What we learned: How to communicate and delegate tasks. How to train a neural network to control a bird in Flappy Bird using multiple Reinforcement Learning techniques.What's next for GameTok: For GameTok, we would want to implement a way for users to play against an A.I. opponent in the FlappyNite game that we made for GameTok, as well as adding a way to implement servers for that game so that multiple sessions of the game can be played at once. Some other things we would like to do for GameTok would be to add more games within the application, so that anyone can find a type of game they could enjoy.",
                        "github": "https://github.com/bernardoarodrigues/CatHacks-XI",
                        "url": "https://devpost.com/software/gametok"
                    },
                    {
                        "title": "Piimbus Adventure",
                        "description": "We wanted to make Space invaders but we were 50 years to late :(",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/piimbus-adventure"
                    },
                    {
                        "title": "Recipe Book",
                        "description": "Interactive, web-based recipe book where you can search the web for recipes or upload your own.",
                        "story": "",
                        "github": "https://github.com/KiaraJohnson2004/recipebook",
                        "url": "https://devpost.com/software/recipe-book-viqdz1"
                    },
                    {
                        "title": "Apothecary",
                        "description": "Scan, learn and drink plants.",
                        "story": "Inspiration: Hackathon theme- Hyrule Compendium, from Breath of the Wild and Tears of the Kingdom,What it does: Given a picture it will analyze it and find it in a flower data base and inform the user about\nattributes. It is gamified, as picture taken of a flower not taken before will unlock the entry for the player.,How we built it: Database: sqliteProgramming language: GolangFronten Library: charmbracelet/bubbletea,Challenges we ran into: As the frontend is terminal based, how one handles the state and draws on the screen is significantly different from a web or application render.,Accomplishments that we're proud of: Got the main application cycle running and set up the database.,What we learned: Database managementGo Programming Language (first time using it)TUI base libraries,What's next for Apothecary: Go deep into the game aspect of it. Rendering the UI with a game engine is the first step.Discover Mode: Pictures unlock entries for you to grow your collection.Instant Mode: Unlock everything instantly so that you can eat the plants right away.Make it a mobile centered app to use camera.Use phone's proeccesor for learning on pictures taken.Actually use AI and neural networks.Just reached the point of taking an image and turning it into a matrix.Give more information about the flowers.,",
                        "github": "https://github.com/Karidus-423/cat-hacks-XI#",
                        "url": "https://devpost.com/software/apothecary-lrou7m"
                    },
                    {
                        "title": "Bastion, a tower defense game.",
                        "description": "A tower defense game made in python and pygame with custom sprites.",
                        "story": "Inspiration: 8-bit video games and the theme of the hackathon\n## What it doesA tower defense game with multiple towers and paths\n## How we built itUsing Pygame and a dream. \n## Challenges we ran intoHandling of animations due to Pygame, projectiles, collisions, and overall performance metrics.\n## Accomplishments that we're proud ofGetting working pathing, a projectile system, and using python effectively with not that much prior usage\n## What we learnedMerge ConflictsSUCK,What's next for Bastion, a tower defense game.: Perhaps we will make some improvements and expand upon the game later, maybe even port it to a new\nlanguage or framework,",
                        "github": "https://github.com/Zbolt50/CATHACKS-XI",
                        "url": "https://devpost.com/software/bastion-a-tower-defense-game"
                    }
                ]
            ]
        },
        {
            "title": "TAMU Datathon Lite",
            "location": "ILCB",
            "url": "https://tamu-datathon-lite.devpost.com/",
            "submission_dates": "Apr 05, 2025",
            "themes": [
                "Beginner Friendly",
                "Machine Learning/AI"
            ],
            "organization": "TAMU Datathon",
            "winners": false,
            "projects": []
        },
        {
            "title": "Fetch.ai Hackathon at Techkriti'25, IIT Kanpur",
            "location": "Indian Institute of Technology Kanpur",
            "url": "https://fetch-ai-hackathon-techkriti25.devpost.com/",
            "submission_dates": "Mar 30 - Apr 05, 2025",
            "themes": [
                "Beginner Friendly",
                "Blockchain",
                "Machine Learning/AI"
            ],
            "organization": "Fetch.ai",
            "winners": true,
            "projects": [
                [
                    {
                        "title": "CareerSaathi",
                        "description": "CareerSaathi: Your AI-powered career guide, offering personalized resume tips, skill assessments, job matching, and more, helping you navigate your career journey with confidence using blockchain",
                        "story": "\ud83c\udf1f About the Project: In India\u2019s ever-evolving job market\u2014particularly within the AI and tech sectors\u2014many skilled individuals face major hurdles:Not knowing what skills are currently in demand.Lack of clarity on what certifications or training to pursue.Trouble tailoring their resumes to get noticed.Confusion about where to look for the right jobs.,CareerSaathiwas born to bridge this guidance gap with AI agents that act like career coaches\u2014personalized, intelligent, and always-on.The entire system was built usingFetch.ai\u2019s uAgents framework, with support from:ASI1 LLM APIfor natural language reasoning and smart response generation.Tavily Web Search Agentfor fetching real-time learning resources.Custom Indeed Web Scraper Agentfor scraping recent job listings.Five autonomoussub-agents, each handling a specialized domain.OneCommander agentthat intelligently routes user queries to the best agent (or combination).,How tochain multiple intelligent agentsto work cooperatively toward solving real-world queries.MasteredASI1 LLM APIfor agent-powered reasoning, surpassing GPT in explainability for multi-agent workflows.Built robustreal-time agent communicationusing thectx.storagecontext to pass intermediate knowledge.Designeddynamic agent routingusing extracted ASI1 keywords and a smart Commander agent.Seamlessly integratedexternal web toolslike Tavily and custom scrapers into agent pipelines.,Real-time inter-agent communication: Ensuring sub-agent results could be synchronized and meaningfully composed into the final response.Relevancy filtering: Preventing agents from sending unnecessary questions to users (since they can\u2019t respond), and instead producing self-contained, informative outputs.Keyword-driven delegation: Building a robust keyword extraction and mapping strategy for intelligent agent routing.Context fusion: Merging resume advice with demand trends, and training paths with actual job postings\u2014all into a single, useful output.,\u2699\ufe0f Tech Stack: Language: PythonFramework:Fetch.ai uAgentsLLM:ASI1(for all prompt generation and decisions)Web Search: Tavily Search AgentScraping: Custom Indeed Scraper AgentDeployment: Agentverse.ai,\ud83e\uddea Sample Query: Commander routes query to: Job Matching \u2192 Resume \u2192 Skill \u2192 Training.Result: Shows jobs, matches user profile, gives training paths.No questions asked, only intelligent insights.,\ud83c\udfc1 Final Impact: CareerSaathienables any job seeker to:Understand their skills vs market demand.Improve their resume.Upskill with smart course suggestions.Apply for jobs with full context on role-fit.,\ud83d\udc49 Built to empower India\u2019s youth withAI-native career coaching.",
                        "github": "https://github.com/dhruvagrawal27/FetchAI",
                        "url": "https://devpost.com/software/careersaathi"
                    },
                    {
                        "title": "HealthConnect",
                        "description": "HealthConnect: Your Instant, Informed Healthcare Companion",
                        "story": "HealthConnect is a CLI-based, AI-driven healthcare assistant that helps users check symptoms, manage medications, and inquire about appointments. Built entirely on the Fetch.ai Agentverse ecosystem, it uses specialized autonomous agents to handle health-related queries and return intelligent, context-aware responses powered by the ASI-1 Mini language model.What It Does: HealthConnect allows users to:Check symptoms and receive AI-generated medical adviceGet medication reminders or clarify dosage instructionsBook or manage appointments through natural language prompts,Each request is routed to the appropriate agent, which communicates with Fetch.ai\u2019s ASI-1 Mini LLM to generate relevant and actionable replies.Technology Stack: Fetch.ai Agentverse \u2014 Autonomous agent deployment and mailbox-based communicationASI-1 Mini LLM \u2014 Fetch.ai\u2019s inference agent that powers all intelligent responsesPython with the uAgents SDK \u2014 Used to build all agents and the CLICommand-Line Interface (CLI) \u2014 A minimal, responsive user experience without a frontend framework,Architecture Overview: app.py\u2013 CLI agent that receives user input and routes queriessymptom_agent.py\u2013 Handles health-related symptomsmedication_agent.py\u2013 Manages medication-related interactionsappointment_agent.py\u2013 Helps with appointment schedulingAll routing agents forward structured prompts to ASI-1 Mini, then send the AI-generated response back to the CLI,Real-World Applications: HealthConnect can be integrated into:Telehealth workflows and clinical onboarding flowsVirtual assistants and home healthcare botsPharmacy reminder systems or healthcare Q&A botsLightweight agents for triaging in clinics or insurance providers,Why It Matters: HealthConnect makes healthcare information more accessible by:Offering 24/7 conversational access to health supportReducing dependency on traditional web interfaces or staff interventionMaking AI-driven healthcare insights available on any device with a terminal,Built For: Fetch.ai Hackathon 2025Empowering developers to build intelligent, autonomous agents for real-world use cases.",
                        "github": "https://github.com/mehamurali12/HealthConnect",
                        "url": "https://devpost.com/software/healthconnect-ig63pj"
                    },
                    {
                        "title": "ChainWatch AI",
                        "description": "Real-time crypto insights powered by AI agents.",
                        "story": "Inspiration\nThe inspiration for ChainWatch AI came from the need for a decentralized system of intelligent agents that could assist with crypto-related tasks. With the increasing importance of real-time data, timely news, and automated alerts in the crypto world, we wanted to create a platform where multiple micro-agents collaborate to provide key insights and automated actions for crypto enthusiasts.What it does\nChainWatch AI is a suite of four specialized AI agents that work together to provide real-time crypto price data, aggregated news, dynamic alerts for price movements, and an assistant that integrates all agents. The agents communicate seamlessly to provide insights for crypto investors and enthusiasts, offering a personalized and efficient platform.How we built it\nChainWatch AI is built with FastAPI and uvicorn, utilizing microservice architecture to run each agent as an independent service. The agents interact via REST APIs, making it modular and easy to scale. The agents use mocked data for now, with plans to integrate live data in the future. Each agent was deployed individually, and they communicate through HTTP requests to handle various tasks, from providing price data to sending alerts.Challenges we ran into\nOne of the biggest challenges we faced was making the agents communicate with each other in a seamless manner while ensuring each was deployed correctly. We also faced challenges while integrating the Agentverse platform, especially handling the agent registration process. Another challenge was managing agent dependencies and ensuring they responded efficiently to requests.Accomplishments that we're proud of\nWe successfully built four distinct agents that each handle different tasks:PriceAgent: Provides real-time mock crypto price data.NewsAgent: Aggregates top crypto news headlines.AlertAgent: Sends alerts when specific price thresholds are hit.AssistantAgent: A dynamic assistant capable of interacting with all other agents on the platform.These agents can run independently, but they also communicate with each other to form a cohesive system that can be used for both research and real-world crypto trading scenarios.What we learned\nThrough this project, we learned a great deal about how microservices and APIs work in practice. We also learned how to work with FastAPI and Agentverse to create scalable, modular systems that can be easily expanded. The most significant takeaway is understanding how to manage multiple services and ensure they integrate efficiently in a larger system.What's next for ChainWatch AI\nLive Data Integration: We plan to replace the mocked data with real-time crypto prices and news feeds from APIs like CoinGecko or NewsAPI.User Personalization: Adding more features to the AssistantAgent so it can track user preferences, suggest tailored alerts, and provide detailed insights.Enhanced Notification System: Expanding AlertAgent to send notifications across multiple channels (email, SMS, etc.).Scalability: Improving the system to handle higher volumes of users and adding more agents for other tasks like portfolio tracking and sentiment analysis.",
                        "github": "https://github.com/Towaiji/chainwatch-ai",
                        "url": "https://devpost.com/software/chainwatch-ai"
                    },
                    {
                        "title": "InvestWise",
                        "description": "\"AI-Powered Investment Insights: Smarter Decisions, Lower Risks!\"",
                        "story": "About the ProjectInspirationThe project was inspired by the need for data-driven investment decisions. Traditional financial analysis often lacks real-time insights, and sentiment analysis alone can be misleading. By combining both, we aimed to create acomprehensive risk assessment toolthat helps investors makeinformed decisionswith AI-powered insights.What We LearnedThroughout this journey, we explored:Financial Data Analysis \u2013 Understanding key metrics like EBITDA, ROE, and profit growth.Sentiment Analysis \u2013 Using LLMs to classify news as positive, neutral, or negative.AI Agents & Automation \u2013 Leveraging LangChain & Fetch.AI to automate data collection and risk assessment.-Correlation & Risk Assessment\u2013 How financial reports and sentiment impact investment decisions.,How We Built ItLangChain Agent: Gathers structured financial data from trusted sources.Fetch AI\u2019s Marketplace Agent: Retrieves real-time news sentiment.Risk Analysis & Correlation Agent: Merges insights to assess risks.Visualization & Reports: Presents findings in an investor-friendly format.,Challenges FacedData Quality & Consistency\u2013 Ensuring reliable and up-to-date financial data.Sentiment Accuracy\u2013 Refining models to reduce false classifications.Integration of Agents \u2013 Synchronizing multiple AI-powered agents for seamless data processing.,This project showcases how AI can revolutionize investment strategies, making them more data-driven, automated, and insightful.",
                        "github": "https://github.com/Riteshk1314/Al_Codea",
                        "url": "https://devpost.com/software/investwise-k0c96z"
                    },
                    {
                        "title": "My project name is -  Ai agents in finance .",
                        "description": "AI Agents in Finance:- Transforming the Future . In this - We  explore how AI agents are revolutionizing the financial world. From solving\r\neveryday problems to shaping future outcomes . ",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/my-project-name-is-ai-agents-in-finance"
                    }
                ]
            ]
        },
        {
            "title": "Projects Program 2.0",
            "location": "Waterloo, Canada",
            "url": "https://projects-program-2.devpost.com/",
            "submission_dates": "Apr 02, 2025",
            "themes": [
                "Beginner Friendly",
                "Design",
                "Low/No Code"
            ],
            "organization": "uwaterloo Computer science club",
            "winners": false,
            "projects": []
        },
        {
            "title": "Helpingai Hacks",
            "location": "Online",
            "url": "https://helpingai-hacks.devpost.com/",
            "submission_dates": "Mar 27 - 31, 2025",
            "themes": [
                "Machine Learning/AI",
                "Mobile",
                "Open Ended"
            ],
            "organization": "Helpingai",
            "winners": false,
            "projects": [
                [
                    {
                        "title": "ReviewGPT",
                        "description": "Are you scared of interviews or thinking that your resume doesn't stand out? With ReviewGPT you can forget all your worries! \r\nReviewGPT offers mock interviews + resume reviews!",
                        "story": "Inspiration: In today's competitive job market, a well-crafted resume can make all the difference. However, getting expert feedback on a resume is often expensive or time-consuming. We wanted to create a tool that providesinstant, AI-powered resume reviewsandmock interviewsto help job seekers refine their applications and interview skills effortlessly.What it does: ReviewGPT is anAI-powered resume analyzer and mock interview assistant. Users can:Upload their resume (PDF), which is analyzed for strengths and weaknesses.Receive a detailed AI-generated reviewwith constructive feedback and suggestions for improvement.Choose a tonefor their review, whether they want friendly advice, professional critique, or even a humorous roast.Engage in a real-time AI-driven mock interview, where the AI interviewer asks relevant questions based on the resume and evaluates user responses.Interact via text and voice, usingtext-to-speech (TTS)andspeech recognitionfor a seamless experience.,How we built it: We developed ReviewGPT using:HelpingAIthe brain behind the appFlask (Python)for the backend API, handling resume text extraction and AI interactions.PyMuPDFforaccurate PDF text extraction.HelpingAI APIto generate AI-driven resume feedback and interview questions.JavaScript (Frontend)to provide an intuitive UI with file uploads, text display, and speech functionality.SpeechRecognition & Web Speech APIto enablevoice-based interviews.Vercelfor quick deployment and API hosting.,Challenges we ran into: Ensuring accurate resume text extraction: Some PDFs had formatting issues, making text extraction tricky.Making the AI-generated reviews insightful and structured: Tuning the prompts for different tones and making sure the responses were coherent took several iterations.Building a natural AI interview flow: Handling user responses dynamically while ensuring the AI provides relevant follow-up questions was complex.Integrating Speech Recognition: Ensuring smooth real-time voice interaction was a technical hurdle.,Accomplishments that we're proud of: A functional AI-driven resume review systemthat adapts to user preferences.A fully interactive AI mock interviewthat allows users to practice with real-time speech input.Speech recognition integration, making the interview feel more natural.A Web appthat works smoothly across different devices.,What we learned: How to extractclean and structured textfrom PDFs with various formats.Web Speech API's limitations and workaroundsfor smoother voice interactions.How to optimizeFlask APIs for quick responseswhen handling AI requests.,What's next for ReviewGPT: Enhancing AI adaptability: Customizing interview questions based on job roles.Scoring system: Providingquantitative feedbackon resume quality and interview performance.Multi-language supportfor non-English users.Integration with LinkedIn & job platformsfor one-click resume evaluation.AI-generated resume templatesto help users create job-winning resumes instantly.Real-time AI coachingto provide suggestions while users answer interview questions.,ReviewGPT is just the beginning of making AI-powered career guidanceaccessible to everyone!",
                        "github": "https://github.com/Sai-Santosh-Pal/reviewGPT/",
                        "url": "https://devpost.com/software/reviewgpt"
                    },
                    {
                        "title": "emotional-ai-app",
                        "description": "An AI-powered web app fostering empathy, helping users process emotions, seek advice, or chat.",
                        "story": "Inspiration: The inspiration for this project stemmed from a personal desire to contribute to mental health awareness. In today\u2019s world, many people struggle with mental health challenges, often needing a safe space to express their emotions. I wanted to develop an application that uses AI to provide emotionally supportive responses, creating an experience that feels empathetic and human-like. By leveraging AI\u2019s capabilities, this app aims to offer comfort and help users feel heard, whether they are seeking advice or simply need someone to talk to.What it does: The Emotional AI Assistant is an empathetic web application built to provide emotionally intelligent responses to users. It helps people express their feelings and receive comforting, supportive replies, creating a digital space for emotional support. The app is powered by React.js for the frontend, Node.js for the backend, and AI models to generate human-like responses based on user input.How we built it: Challenges we ran into: Integrating AI Models: Connecting the frontend with the AI API and ensuring the AI produced emotionally relevant responses was a complex task. Fine-tuning prompts to get the right emotional context was crucial.Handling Emotional Sensitivity: Ensuring the AI responded with empathy was one of the biggest challenges. I had to carefully craft prompts to avoid robotic or impersonal replies.Limited Access to AI Models: Access to OpenAI\u2019s higher-tier models was restricted due to quota limits, and alternative models like DeepSeek faced connectivity issues.Backend Communication: Managing API requests and ensuring smooth communication between the frontend and backend involved debugging asynchronous functions and handling server errors.Accomplishments that we're proud of: The app was built using React.js for the frontend. I used Material-UI and Tailwind CSS for styling to ensure a clean, user-friendly interface. The frontend captures user emotions through text input and sends the data to a backend server. The backend, built with Node.js and Express, processes the input and communicates with the AI model to generate emotionally intelligent responses.For AI integration, I initially explored OpenAI\u2019s GPT models but faced limitations such as quota issues and model access. Despite this, I learned to handle API configurations and fine-tune the interaction between the frontend and the AI. The server uses Axios to send requests and handle responses efficiently.What we learned: Throughout the development of this project, I gained valuable experience in multiple areas:AI Interaction: Learning how to interact with AI models like OpenAI to generate emotionally aware responses helped me understand sentiment analysis and the importance of emotional intelligence in AI.Frontend Development: Working with React.js provided me with hands-on experience in building dynamic, interactive user interfaces, while Material-UI and Tailwind CSS enabled me to create a responsive and visually appealing design.Backend Integration: Setting up a Node.js backend and using Axios for API requests helped me gain practical knowledge in handling server-client communication and working with external services.What's next for emotional-ai-app: Model Expansion: I plan to integrate other AI models or improve the existing AI to generate even more nuanced responses.Personalized Experience: Implementing user profiles and custom preferences will enhance the AI's ability to respond empathetically based on individual needs.",
                        "github": "https://github.com/karthikysk/emotional-ai-app",
                        "url": "https://devpost.com/software/emotional-ai-app"
                    },
                    {
                        "title": "EmoticAi",
                        "description": "EmoticAI \u2013 Emotionally Intelligent Conversations, Powered by AI.",
                        "story": "Inspiration: Many AI assistants today lack emotional intelligence, making interactions feel robotic and disconnected. The goal of EmoticAI is to bridge this gap by creating an AI that not only understands words but also detects emotions and responds accordingly. Whether for mental health support, productivity, or general conversation, an AI that adapts to emotions can improve communication and create a more human-like experience.What It Does: EmoticAI is an AI-powered assistant designed to facilitate meaningful human-AI interactions through both text and voice. It allows users to have independent chat sessions, with each chat container functioning separately. The voice assistant feature enables real-time spoken interactions where the AI adapts its responses based on the detected emotion.How We Built It: EmoticAI was developed using:GoFiberfor the backend, ensuring a fast and scalable API.Reactfor the frontend, providing an intuitive and responsive user interface.HelpingAI APIfor emotion detection, sentiment analysis, and intelligent responses.Splinefor visual enhancements, though limited by its free-tier constraints.,Challenges We Ran Into: Developing an AI that understands and adapts to human emotions presented several challenges. Fine-tuning the AI to detect subtle emotional cues required multiple iterations. Ensuring context-aware conversations while keeping chat containers independent was another key challenge. Implementing real-time voice responsiveness with emotion-based adaptation required careful optimization to maintain a smooth experience.Accomplishments That We're Proud Of: Creating a functional AI that adapts to human emotions is a significant achievement. Successfully integrating emotion recognition into both text and voice interactions enhances user engagement. The independent chat container system adds flexibility, allowing users to manage multiple conversations without interference.What We Learned: Developing emotionally intelligent AI requires a deep understanding of sentiment analysis and adaptive response generation. Optimizing for real-time interaction is critical for maintaining a seamless user experience. Building a scalable and efficient AI assistant involves balancing accuracy, speed, and usability.What's Next for EmoticAI: Future improvements include optimizing the voice assistant for mobile devices, enhancing emotion recognition for more nuanced interactions, and introducing customization options that allow users to tailor the AI\u2019s personality to their preferences. Expanding the platform\u2019s capabilities will further improve the user experience and broaden the impact of emotionally intelligent AI.",
                        "github": "https://github.com/EthicalGopher/emoticai-frontend",
                        "url": "https://devpost.com/software/emoticai"
                    },
                    {
                        "title": "DarkVader",
                        "description": "Interactive 3D galaxy: Where dark matter comes alive.",
                        "story": "Inspiration: Dark matter is the universe's biggest enigma, constituting 85% of all matter but not appearing in our telescopes. We have tried to develop an interactive tool that allows individuals to comprehend why astronomers are so sure that dark matter exists, despite its invisibility. Through galaxy rotation curves and dark matter effect visualization, we would like to render this intangible concept more tangible to everyone.What it does: DarkVader is an interactive galaxy simulation of 3D showing how dark matter influences galaxy rotation. The user can:Build a realistic galaxy with 15,000 star particles and 15,000 dark matter particlesAlter parameters like dark matter mass, normal matter mass, and black hole massWatch live changes in rotation curves and particle movementsCompare velocities with and without dark matter to calculated velocitiesDescribe WIMP flux predictions and detection thresholdsInvestigate galaxy dynamics using interactive data visualization,How we built it: We employed DarkVader with:Next.js as frontend frameworkThree.js for 3D visualization and particle systemsReact Three Fiber for use with React and Three.jsData visualization with science using RechartsRealistic astronomical constants and formulas for realistic physics simulationWebGL for hardware-accelerated renderingTailwind CSS for responsive design,Challenges we ran into: Performance optimization with 15,000 particles and smooth animationIncluding accurate physics calculations of orbital speedsCreation of realistic particle distribution for spiral galaxy armsMerging beauty with scientific precisionManaging state over multiple interactive elementsEnsuring adequate scaling of astronomical units and constants,Things we're proud of accomplishing: Created a scientifically accurate but stunningly lovely simulation of a galaxyCorrectly utilized real-world astronomical equations for galaxy movementExhibited silky performance with thousands of particlesDesigned an interactive interface for complex scientific conceptsDesigned a tool that can be utilized for teaching and research visualizationIntegrated several intricate visualization libraries with success,What we learned: Advanced Three.js methods with particle systemsReal-world physics implementation in JavaScriptEnhancing the performance of complex 3D scenesScientific visualization best practicesThe physics of dark matter and galaxy dynamicsBalance between correctness and user experience matters,What's next for DarkVader: Support for VR-based interactive explorationAdd more types of galaxies and merger simulationsAdd in guided tours and teaching tutorialsAdd more advanced analytical toolsIncorporate comparison with real galaxy data - Conduct MOND theory comparisons - Create a mobile version - Use more interactive learning aids. - Extend the simulation across many billion years to encompass galaxy evolution. - Integrate with real astronomical databases,",
                        "github": "https://github.com/Lucky-TB/darkvader",
                        "url": "https://devpost.com/software/darkvader"
                    },
                    {
                        "title": "Face-Detection-main",
                        "description": "Effortless face detection with cutting-edge AI technology for enhanced security and convenience.",
                        "story": "",
                        "github": "https://github.com/RajivRa12/Face-Detection-main",
                        "url": "https://devpost.com/software/face-detection-main"
                    },
                    {
                        "title": "AI-Daptive Teacher",
                        "description": "A teacher that adapts to you!",
                        "story": "Inspiration: I\u2019ve always been fascinated by the potential of AI in education. Many online learning platforms lack personalization, making it hard for students to stay engaged. I wanted to create a tutor that adapts to the student's emotions and understanding level, making learning more interactive and student-focused.What it does: The emotionally intelligent tutoranalyzes student sentiment, dynamically adjusts responses, and provides real-time AI-powered tutoring.How I built it: Tech Stack:Challenges I ran into: I wanted to make the UI like ChatGPT, but it was challenging to do that with Streamlit. I researched on session states and eventually solved the problem.Accomplishments that I'm proud of: What I learned: I had not used APIs much in projects before, so this hackathon helped me refresh my knowledge and build something unique.What's next for AI-Daptive Teacher:",
                        "github": "https://github.com/ShauryaAttal/AI-Daptive-Teacher",
                        "url": "https://devpost.com/software/ai-daptive-teacher"
                    }
                ]
            ]
        },
        {
            "title": "Diabetes Tech Solutions for Rural Georgia",
            "location": "Online",
            "url": "https://diabetes-tech-for-rural-ga.devpost.com/",
            "submission_dates": "Mar 31, 2025",
            "themes": [
                "Beginner Friendly",
                "Health",
                "Machine Learning/AI"
            ],
            "organization": "Emory Global Diabetes Research Center",
            "winners": false,
            "projects": [
                [
                    {
                        "title": "Test Submission",
                        "description": "This is an excellent project.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/test-submission-vyocfr"
                    }
                ]
            ]
        },
        {
            "title": "IWD 2025: Redefine Possible Edmonton",
            "location": "Edmonton Unlimited",
            "url": "https://iwd-yeg-2025-redefine-possible.devpost.com/",
            "submission_dates": "Mar 29 - 31, 2025",
            "themes": [
                "Beginner Friendly",
                "Blockchain",
                "Machine Learning/AI"
            ],
            "organization": "GDG Cloud Edmonton",
            "winners": false,
            "projects": [
                [
                    {
                        "title": "Future Fairy",
                        "description": "A Gemini AI-powered chatbot that empowers young women to explore careers while challenging gender stereotypes.",
                        "story": "Inspiration: The goal of this project was to challenge and break down persistent gender stereotypes in career choices, encouraging young girls to explore professions based on their unique skills and interests rather than societal expectations. Our chatbot,Future Fairy, serves as a bias-free career mentor for girls ages 12\u201319 by:Recommending careers based on individual skills and interests, not gender norms.Highlighting women in diverse, non-traditional fields (e.g., robotics, politics, arts) as role models.Providing curated resources (courses, books, mentorship) to support career exploration.Leveraging AI to ensure dynamic, empowering, and natural conversations that adapt to users\u2019 questions and needs.,What We Learned: Through this project, we deepened our understanding of AI and chatbot development, learning how to create more meaningful and personalized interactions with users. We gained experience usingVite, a fast and modern build tool, which significantly improved our development workflow.Reactplayed a pivotal role in building the interactive front end of the chatbot, enabling seamless and responsive interactions.We also worked with theGemini APIto power AI-generated responses, learning how to integrate external services to offer tailored advice that aligns with each user's goals. The workshops equipped us with essential skills inprompt engineering, allowing us to design effective prompts that help the AI model generate responses that are relevant and empowering.What It Does: Future Fairyis designed to serve as a supportive and motivational tool for young girls considering careers in fields they may not have traditionally been encouraged to pursue. It engages users by:Offering personalized career guidance based on their skills and interests.Providing real-world examples of diverse women in various professions.Recommending actionable steps to explore careers (classes, hobbies, etc.).,How We Built It: Challenges We Faced: Future Implementations with Aleo: Utilize Aleo SDK and blockchain to ensure users' privacy in the chatbox and in the transfer of data.Unique: Resists bias at its core and allows for organizations to have access to aggregate statistics.Beneficial: Allows for better organized, better funded and potentially more events to empower curious young girls.,",
                        "github": "https://github.com/tina-machi/redefine-hackathon-2025-t2",
                        "url": "https://devpost.com/software/future-fairy"
                    },
                    {
                        "title": "SheCodesPrivate",
                        "description": "SheCodesPrivate is a fun, safe platform for career exploration, using AI and zero-knowledge proofs to guide and empower learners\u2014especially girls 12\u201319\u2014while keeping their data private.",
                        "story": "Inspiration: As someone passionate about empowering young girls, I wanted to create a platform that breaks stereotypes and makes career exploration in tech fun and safe. I personally struggled to find beginner-friendly tools and clear learning paths while growing up, which inspired me to build something I wish I had. I was also excited to explore zero-knowledge proofs to protect user privacy in an interactive way.What it does: SheCodesPrivateis an interactive career exploration platform designed for girls aged 12 and above. It offers:A friendly AI-powered chatbot that answers career questions and generates learning pathsA badge system that rewards curiosity and engagementAleo-powered ZKPs to verify age and badge achievements without exposing personal detailsFun UI features like balloons, toasts, and downloadable badges/proofsA safe login with a username-only flow (no real identity needed),How we built it: FrontEnd/UI:Built using Streamlit for a fast and interactive experienceAI Chatbot:Integrated Gemini 1.5 Pro for natural career Q&A and path generationZK Privacy Layer:Created and connected custom Leo programs on Aleo to verify user age and badge eligibilitySession State:Managed user-specific states like badges, chat history, and learning paths using st.session_state,Challenges we ran into: Learning and debugging Leo syntax and building ZK logic for real use casesPreventing repetitive badge unlocking and ensuring clean, readable proof outputsHandling Streamlit reruns and chat state cleanly when user details changedCustomizing chat flow to show learning paths only once and making badge logic fully dynamic,Accomplishments that we're proud of: Successfully built a chatbot experience that\u2019s both educational and privacy-preservingImplemented ZK Age Verification that doesn\u2019t reveal the user's actual ageGenerated clean downloadable Aleo proofs containing just the necessary badge verification dataCreated a modular and dynamic badge system with live visual feedback,What we learned: How to integrate Google Gemini Pro for advanced career guidanceWriting ZK proof systems using Leo (Aleo) and using them meaningfully in real applicationsManaging multi-view stateful UI using Streamlit effectivelyUX techniques like rate-limit countdowns, fading locked badges, and badge caching,What's next for SheCodesPrivate: Add quizzes and mini challenges to earn badges in new categoriesImplement full private login sessions with saved dataDeploy on a public platform with persistent database and badge walletExplore more Aleo smart contracts to prove skill completion or career path progressionLocalize for more languages and expand to other underrepresented communities!,",
                        "github": "https://github.com/dhandashreya/SheCodesPrivate.git",
                        "url": "https://devpost.com/software/shecodesprivate"
                    },
                    {
                        "title": "Own Your Narrative: Reimagine Stories, Redefine Stereotypes",
                        "description": "A web-based tool that lets users customize a character's gender identity in any story to better relate to narrative. This empowers users to reshape stories, break stereotypes, and promote inclusivity.",
                        "story": "How the idea was born: When talking amongst our team, we came to a conclusion that every piece of literature we grew up reading, had the same narrative where the prince was this handsome, brave man trying to rescue a poor, hopeless female character. As women, this fed us an idea that we were not capable enough to help ourselves and needed to rely on men in the society for our well being.It is 2025 and while we have come a long way, it is saddening to see that the literature for children and teens still has not evolved much. While it might take a couple more years to realise this change in writing, it is still possible for us to alter already existing literature and empower this young generation to customize their characters however they want, thus - Owning Their Narrative.This is how the idea of Own Your Narrative came about. This tool wants parents, guardians and young folks to not conform to gender stereotypes of old/classical literature and read the stories that makes them identify with bold, courageous and strong characters becauseif the prince can save the Rapunzel from the castle, so can the princess!Behind the Scenes: Frontend: We built the user interface with clean HTML, styled with responsive CSS to ensure a seamless experience across devices. JavaScript forms the heart of our application, handling user interactions, dynamic text manipulation, and age verification.Narrative Engine: JavaScript\u2019s powerful string manipulation functions are used to change the stories. The use of regular expressions ensures that all instances of the character and their related pronouns are swapped.Age Verification: Javascript and Aleo is used to verify the age, and to hide and show divs based on the user's input. We have usedAleo (Zero-Knowledge Proofs)for secure age and gender verification. While the integration of Aleo has not been seamless into our tool, we have attached a video of it showing how we were supposed to include it in our idea and how we further plan on building upon it in future.Data Storage: Session storage is used to temporarily store the inputed story.The tool we have currently built is a prototype. We envision to integrate more tools like Gemini, Google Cloud into this tool during later stages to make it a fully functional product ready for market use.How It Works:: \"Own Your Narrative\" empowers users to take existing stories and reshape them, challenging traditional gender roles and stereotypes. Here's how:Challenges we anticipate: There were a number of issue/challenges we identified along the way which we promise to tackle at later stages of developing this product. Some of them have been listed below.What's next?: \"Own Your Narrative\" is more than just a tool; it's a platform for change. Our hope is to inspire a generation of storytellers who challenge the status quo and create narratives that reflect the world as it should be: inclusive, empowering, and diverse.",
                        "github": "https://github.com/Aggarwal21/Own-Your-Narrative",
                        "url": "https://devpost.com/software/own-your-narrative"
                    },
                    {
                        "title": "Unbias",
                        "description": "We developed a tool that neutralizes gender indicators in resumes, promoting fairer, unbiased candidate evaluations by reducing gender-related bias in recruitment.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/unbias-3um0bf"
                    },
                    {
                        "title": "Pearl",
                        "description": "Build Your Confidence, Build Your Future, Connect with a mentor. Designed for girls ages 12-19.",
                        "story": "Inspiration: Many young girls interested in STEM lack access to female role models and mentors. They struggle to navigate career paths and build the necessary networks.Pearl bridges the gap between aspiring girls and accomplished women in STEM. Our app matches mentees with mentors based on their interests and career goals, offering personalized guidance and fostering a supportive community.We're helping girls build confidence, explore career paths, and ultimately, achieve their full potential in STEM fields.What it does: Bridges the Gap:It addresses the lack of female role models and mentors for young girls interested in STEM.Connects Mentees and Mentors:It matches aspiring girls with accomplished women in STEM.Provides Personalized Guidance:It offers guidance tailored to the mentee's interests and career goalsEmpowers Girls:It helps girls build confidence and explore career paths.\nIt helps girls achieve their full potential in STEM.How we built it: We built Pearl as a responsive web app using Vue 3 with Vite, styled with Tailwind CSS, and powered by Firebase for user authentication and real-time database storage. We integrated Google's Gemini API to personalize mentor matching and message generation using natural language prompts.Challenges we ran into: We spent a long time coming up and workshopping the idea. Some technical challenges included managing user state between authenticated mentees and mentors, dynamically using Firestore data in AI prompts, and formatting Gemini API responses reliably.Accomplishments that we're proud of: We're proud to have built a platform that not only connects young women with mentors in STEM, but also uses AI in a meaningful, empowering way. We successfully integrated Gemini to personalize mentorship matches and generate authentic outreach messages something many mentees may feel nervous about writing on their own.We\u2019re also proud of how we designed Pearl to be user-friendly, inclusive, and role model\u2013focused, reflecting the voices and goals of girls aged 12\u201319. From navigating real-time Firestore data to crafting clean, responsive UI with Tailwind, every piece was built with intention \u2014 and loveWhat we learned: Throughout building Pearl, we learned how to effectively integrate AI with real user data to create personalized and meaningful experiences. We gained hands-on experience using Gemini for prompt engineering, working with Firestore to structure mentor/mentee relationships, and building responsive UIs with Tailwind.We also learned the importance of the collaboration between software dev and UI/UX and translating between each filed to communicate effectively.What's next for Pearl: Favourite feature as well as a sign up page for for mentors, as it is only the mentee side right now. There is also potential for a community hub and question area. For example, resume review.",
                        "github": "https://github.com/lina-doughnuts/HackTheBias25",
                        "url": "https://devpost.com/software/pearl-82f9tu"
                    },
                    {
                        "title": "ExposHER",
                        "description": "Our idea is to create an app that introduces young girls to diverse careers by providing insights, resources, and exposure, empowering them to explore any path they choose.",
                        "story": "Inspiration: Our inspiration for creating ExposHER comes from our own experiences growing up, where we faced a lack of resources, mentorship, and exposure to diverse career opportunities. We understand firsthand the challenges young women face when trying to explore different career paths and break through traditional gender roles. ExposHER was born out of a desire to change this narrative\u2014to provide young girls with the guidance, support, and connections they need to confidently pursue any career they choose. By offering a platform that exposes them to diverse role models and career options, we aim to empower the next generation of women to break barriers and reach their full potential.What it does: Our app introduces young girls to diverse career opportunities by providing insights, resources, and mentorship to empower them in their career exploration. Breaking gender stereotypes, the app helps young girls explore a wide range of career options, especially in male-dominated fields like tech and engineering, by exposing them to diverse careers and role models. Using AI-powered tools like Gemini, the app provides personalized career roadmaps based on users' interests, helping them make informed decisions about their future paths. Additionally, the \u201cPerson of the Day\u201d feature exposes users to real professionals in various fields, offering inspiration and showing them that these careers are achievable. We are addressing the critical issue of gender stereotypes and lack of empowerment limiting young women\u2019s career choices, a challenge that many of us in this room have probably faced.Key Features:1. Discussion Forum:  A Q&A-style chat for career-related discussions and Q&A, where users can ask and answer questions. You can choose to be anonymous or not, fostering a supportive and judgment-free community.2. AI-Powered Career Roadmap: Using Gemini, the app generates personalized career roadmaps based on users\u2019 interests and aspirations.3. Career Visualization: Gemini-powered image generation allows users to see themselves in various career roles, helping them envision their future.4. Daily Inspiration: A \u201cPerson of the Day\u201d feature highlights professionals from different fields, introducing users to real-world role models.5. Private Messaging: Users can securely connect with mentors and peers for guidance and support.6. Secure & Inclusive Environment: Powered by ALEO, ensuring secure logins, Age/ID verification, and ZK file sharing for enhanced privacy.How we built it: React Native:Used for developing a cross-platform app that works seamlessly on iOS .ExpoGo:Utilized for faster prototyping, live testing, and a smooth development process.Gemini:Integrated for AI-powered features, including personalized career roadmaps and career visualization through text and image generation.ALEO:Implemented for secure logins, Age/ID verification, private messaging, and anonymous interactions, ensuring privacy and a safe environment with ZK file sharing.App Architecture:Designed to provide an accessible, interactive, and secure platform that empowers young girls to explore diverse career paths confidently.,Challenges we ran into: One of the challenges we encountered was when we attempted to allow users to upload their profile picture or select an image from their albums for editing using Gemini. While Gemini provided detailed descriptions of the images, it did not offer the capability to edit or provide the actual image files. This limitation of the current Gemini version hindered our ability to store and display the visual content as intended.What's next for ExposHER: We plan to expand ExposHER by adding local events and organizations to the career roadmap, such as hackathons and workshops, to foster community connections. A virtual mentorship program will also be introduced, where the \"Person of the Day\" will serve as a mentor for the week, answering questions and guiding users. Additionally, we will include scholarships and bursaries in the resources section, providing users with financial opportunities to support their career goals.",
                        "github": "https://github.com/Team-Fusion-UAlberta/redefine-possible-hackathon-gdg-aleo/",
                        "url": "https://devpost.com/software/exposher-ed1o8z"
                    }
                ]
            ]
        },
        {
            "title": "Our LUCKY Connect X Learning EDGE Hackathon",
            "location": "Online",
            "url": "https://our-lucky-connect.devpost.com/",
            "submission_dates": "Mar 27 - 30, 2025",
            "themes": [
                "Beginner Friendly",
                "Education",
                "Social Good"
            ],
            "organization": "LIFE and Beyond",
            "winners": false,
            "projects": []
        },
        {
            "title": "Hackabyte's 2025 Spring Hackathon",
            "location": "Digipen Institute of Technology",
            "url": "https://hackabyte-2025-spring.devpost.com/",
            "submission_dates": "Mar 29 - 30, 2025",
            "themes": [
                "IoT",
                "Machine Learning/AI",
                "Productivity"
            ],
            "organization": "Hackabyte",
            "winners": false,
            "projects": [
                [
                    {
                        "title": "FoodSaver Hub",
                        "description": "FoodSaver Hub is a website that helps reduce cafeteria waste by ensuring students can pre-select meals they'll enjoy. When students love their food, waste becomes non-existent.",
                        "story": "Inspiration: We were inspired by Tesla STEM High School's inability to reduce food waste at schools. We have a robust Environmental Engineering and Sustainable Design class that does their best, but we keep seeing trash being sorted wrong and general confusion around food waste and composting that we wanted to combat.What it does: Our project consists ofthree things:A gameto help people test their knowledge of compostingA short quizthat helps students gain general knowledge relating to food consumption and wasteA pledgewhich allows people to set goals for themselves relating to food waste and consumptionA food voting pagethat allows people to vote for their favorite foods, which can be used by school cafeterias to make meals that more students like,How we built it: We divided the project into 4 parts amongst ourselves based on our prior coding experience. Those who were well - versed in Python and Python graphics made the game and the quiz. Those who were more versed in JavaScript and CSS worked on the website and the pledge tracker.\nWe primarily used GitHub to collaborate and share code, and also used AI to improve and smooth out the\nfiner details of the project.Challenges we ran into: One of our team members nearly lost the finalized version of the website. He couldn't open the link in his browser and had to redo certain parts of the code to fix the problem.\nAnother challenge we ran into was AI prompting. The AI didn't always make the changes we wanted it to implement or do what we asked it to do. Figuring out how to work around that was a definite learning experience.Accomplishments that we're proud of: We were able to figure out how to embed our game and quiz into the website through HTML. We also embedded it into our final presentation.\nThe design of the website itself is pretty sleek and modern, way better than anything we've done in the past.\nAdditionally, we all worked really well together as a team and collaborated effectively.What we learned: We learned how to embed games and quizzes into a website\nWe learned how to effectively use AI to simplify our project and add certain functionalities.What's next for FoodSaver Hub: Transform it into a mobile app.\nAdd weekly reminders for voting and following your pledge.\nAdd a photo grid or a tab with photos about people who've used the site to follow their pledge and support sustainability endeavors.\nConnecting the app to social media accounts. \nAllow schools to use it to improve their food waste and carbon footprint.",
                        "github": "https://github.com/Discovered12345/Food-Saver-Hub",
                        "url": "https://devpost.com/software/foodsaver-hub"
                    },
                    {
                        "title": "Save the Bite",
                        "description": "Managing food waste through informing and providing solutions for school and their students. ",
                        "story": "Why: Our inspiration comes from seeing our own school and how food is wasted there. Our school has a system where we each have a lunch number to pay for our food, but we wondered if we could instead order ahead of time using our lunch numbers to get our emails and names to communicate. Ethan has also volunteered at a food bank and saw how important imperfect food from grocery stores was to their survival and how schools could be incorporated into that.How: We used React to build an app that we would each work on locally and used Github to transfer changes between us so we could collaborate. We also used React Router to add multiple web pages.Challenges: Three of us had never used React before, and trying to set it up and make sure it didn't break on each of our machines was quite time-consuming and difficult. Also, using Github to work more asynchronously meant it was hard to know what everyone was working on and not make duplicates of some web pages. Also bugs.Learning: We certainly have a much better grasp on React and how to work as a team. We also improved in our ability to persevere in the face of these difficulties and to prioritize certain problems and difficulties compared to others.",
                        "github": "",
                        "url": "https://devpost.com/software/save-the-bite"
                    },
                    {
                        "title": "Food Waste & Donation Tracker",
                        "description": "Our Food Waste/Food Donation Points Tracker is for schools to use in order to provide a competitive incentive in helping stop food waste, hunger, and helping to make our school lunches better.  ",
                        "story": "Inspiration: Our project aims to not only tackle food waste in schools, but aims to tackle hunger at a higher scale. It was inspired by an annual food drive at one of my teammates' schools, in which they donate food to gain points, providing a competitive incentive. That is why we made our Food Waste/Food Donation Points Tracker, for schools to use in order to provide a competitive incentive in helping stop food waste, helping curb hunger and starvation, and helping to make our school lunches better.What it does: Aims to address food waste by providing a competitive incentiveAlso aims to tackle hunger/food waste at a higher scale by introducing food donationsMade an online user interface that students can access and fill out after every lunch periodStudents fill out their name and meal they have eatenState whether or not they finished their lunch, earn points for finishing food without wasting any of itIf students wasted food, they can respond with why they wasted the food, which helps with input and data collectionCan earn more points through donating different food products (store-bought) or money (thru cash and gift cards)Monthly leaderboard: top 5 at the end of each month get a cash prize,How we built it: Used HTML frontend (w/ JavaScript) for the website, Python backend for Flask (a microframework used for building web applications in Python),Challenges we ran into: Accomplishments that we're proud of and what we learned: What's next for Food Waste & Donation Tracker: Put the data collected by the website from each student into a permanent database (so data is not lost when website is reloaded)Make sure each student has a permanent account in order to not have points/standings reset in the middle of the monthMake standings reset on the 1st day of each monthUser authentication which allows for all points filled out by one person to go to that personMake the website more visually appealing so that it persuades people to donate food and/or moneyAllowing for multiple donations in one fill out of the form (current version only allows one donation per fill out),",
                        "github": "https://github.com/achyutanveera/Hackathon-Project-Food-Waste-Donation-Points-Tracker",
                        "url": "https://devpost.com/software/food-waste-donation-tracker"
                    },
                    {
                        "title": "Trash2Taste",
                        "description": "Revolutionizing School Cafeterias: Smart Menus, Smarter Waste Management",
                        "story": "Inspiration\nThe motivation for this project stemmed from a desire to combat food waste in educational institutions, which is a significant yet often overlooked problem. By integrating smart technology into everyday school operations, we aim to create a sustainable and efficient approach to managing cafeteria waste and optimizing menu planning.\nWhat I Learned\nThroughout this project, I gained deeper insights into the practical applications of machine learning in real-world scenarios. I also improved my skills in data handling, machine learning model tuning, and developing user-friendly interfaces for complex systems.\nHow We Built It\nPart 1: Smart Menu Generation\nWe began by collecting historical data on meal consumption and associated waste from several school cafeterias. Using this data, we developed a machine learning model that predicts the level of waste for different meals. We then incorporated nutritional guidelines to ensure that all meals meet specific health standards. The final system uses a multi-objective optimization algorithm that balances waste reduction with nutritional fulfillment.\nKey Technologies:\nMachine Learning Models (Regression Analysis)\nOptimization Algorithms\nData Analysis Tools\nPart 2: Smart Trash Can\nThe Smart Trash Can system uses YOLO (You Only Look Once) for real-time food waste classification. We installed cameras and sensors in trash cans to automatically capture and categorize waste. The data is then stored and analyzed to provide actionable insights, which feed back into the menu generation system, completing a data-driven feedback loop.\nKey Technologies:\nYOLO for Real-Time Object Detection\nEmbedded Systems (Cameras and Sensors)\nData Management and Analysis Solutions\nChallenges Faced\nLack of Publicly Available Datasets: Initially, the absence of specific datasets for food waste in school cafeterias was a major hurdle. We addressed this by creating a synthetic dataset that mimicked real-world waste patterns, allowing us to train our models effectively.\nCustom YOLO Training Time: The initial training time for our custom YOLO model was around 27 hours, which was impractical for iterative testing and deployment. We managed to reduce the training time to approximately 1 hour by optimizing the model's architecture\u2014reducing the input size, decreasing the number of epochs, and increasing the batch size.\nConclusion\nThe combination of the Smart Trash Can and Smart Menu Generation systems introduces a robust solution to food waste and nutritional planning in school cafeterias. By leveraging advanced technologies and innovative data-driven approaches, we not only enhance operational efficiency but also promote environmental sustainability and educational opportunities for students.",
                        "github": "https://github.com/Ahan12/HackByte",
                        "url": "https://devpost.com/software/trash2taste"
                    },
                    {
                        "title": "School Bites",
                        "description": "Your one-stop shop for all your school lunch needs, seamlessly connecting students and administrators to streamline the lunch process while actively reducing food waste. ",
                        "story": "Inspiration: Our biggest inspiration stemmed from personal experiences with school lunch systems, combined with our team\u2019s shared passion for tackling the growing global waste crisis. Each of us had witnessed firsthand the inefficiencies and food waste in schools, sparking a desire to create meaningful change. In the United States alone, schools waste approximately 530,000 tons of food every year\u2014an amount that could feed over 200 million people worldwide. This staggering statistic served as a wake-up call for us. We realized the potential to make a significant impact, not just by reducing waste but also by redistributing unused food to those in need. That\u2019s why we created School Bites: a solution designed to streamline school lunch systems, reduce waste, and ensure leftover food finds its way to people who truly need it.What it does: Our platform streamlines school meal management by allowing students to pre-order from curated menu options while handling payments directly through the portal. This approach creates accountability for ordered meals and includes a price incentive system that encourages pre-ordering.\nOn the administrative side, we've integrated a food waste reduction feature that connects unused meals with local food banks. Understanding that student choice increases consumption, we've implemented monthly and annual voting systems where students can select from approved meal options, giving them a voice in their nutrition while maintaining dietary standards.How we built it: We built is with VITE, Supabase, and lots of different UI libraries.Challenges we ran into: We faced several obstacles during our project development. At first, our team was divided on how to address leftover food waste, but we soon realized that creating an effective online platform required a more concrete implementation approach.\nAfter aligning on our vision, our biggest challenge came with Supabase integration\u2014something none of us had much experience with. Through lots experimentation and a little while of troubleshooting, we finally developed a workable database schema and refined our user profiles to a point where we were satisfied with their functionality.Accomplishments that we're proud of: Biggest Accomplisment is honeslty how well the UI came together with the whole website because we realy wanted it to be something that student woudl acutally use rather than something that looks bad.What we learned: We learnt a new type of backend which is Supa base. We leant how to better work as a team with gtihub and majorly work thrugh bugs.What's next for School Bites: Next would be to try and implement an AI model that we have. Then we market it all school with a slogan of increased sustainability with seamless integration.",
                        "github": "https://github.com/Rishboy39/hackbyte2025",
                        "url": "https://devpost.com/software/school-bites"
                    },
                    {
                        "title": "Cater-Wise",
                        "description": "Cater-Wise collects student meal preferences and restrictions, helping schools reduce waste and optimize food orders. Admins can also find local charities and donate excess food with one click.",
                        "story": "Inspiration: We researched common issues with school waste, including food preferences, lunchtime, and food overstock. We decided to focus on food preferences as they are tailored to student needs. Additionally, as high school students, we have firsthand experience with these issues.What it does: Our program uses two pathways: student and administrator. The student pathway leads to a questionnaire asking about food restrictions and preferences. Once it is complete, this information is stored in the program. This leads to the admin pathway, where you can see what the students have inputted in the questionnaire. You can also make food orders according to the trends seen in the responses. However, we cannot eliminate food waste right now, so a button gives the admin access to numerous charities sorted by county.How we built it: We built this program using VS Code. \nLanguage: PythonChallenges we ran into: When we first came up with the idea, we struggled with planning the implementation of our UI and GUI.Accomplishments that we're proud of: We are proud of the functionality of our program. In a realistic setting, there is a high chance this would allow schools to purchase food efficiently, meet their students' needs, and reduce food waste.What we learned: We learned how to utilize the framework TKINTER in python. In addition to this, we learned how to delegate our work in terms of coding.What's next for Cater-Wise: Given more time, we want to implement better data visualization for admins. This feature would allow better representation of the raw data. In addition to this, we want to add a predicted analysis feature so we can estimate food waste reductions after the changes are made.",
                        "github": "",
                        "url": "https://devpost.com/software/cater-wise"
                    },
                    {
                        "title": "No leftovers",
                        "description": "A reward system that deducts or adds points based on how much food a student finishes, and the points can be used for prizes. ",
                        "story": "Inspiration: Starbucks Rewards programWhat it does: A reward system that deducts or adds points based on how much food a student finishes, and the points can be used for prizes.How we built it: React.js front end, and Python backendChallenges we ran into: Our database took way too long to fix.Accomplishments that we're proud of: Getting the database to work, getting the machine learning to work.What we learned: We learned how to use firebase as a backend, host our web app on vercel, use a pre-trained machine learning model.What's next for No leftovers: Improve our backend, UI, and integrate machine learning model",
                        "github": "https://github.com/s-zhaojo/noleftovers",
                        "url": "https://devpost.com/software/no-leftovers"
                    },
                    {
                        "title": "Forkast ",
                        "description": "Our mobile app makes meal rating fun and engaging by turning it into a game. Instead of just filling out surveys, students take care of a virtual tomato plant. ",
                        "story": "Like any hackathon project, we faced our fair share of roadblocks. Our development process started with brainstorming the core functionalities by creating a basic framework to get an idea of what we were working with. But once we got into actual development, we ran into some major challenges that almost derailed our progress. We still persisted and ended up with a product that we are proud of.",
                        "github": "",
                        "url": "https://devpost.com/software/forkast-l41r0t"
                    },
                    {
                        "title": "BITE-RecipeGram",
                        "description": "Reducing food waste with student-promoted school meals.",
                        "story": "Inspiration: We were inspired by the massive amount of food waste in our school cafeteria, especially from USDA-mandated fruits and vegetables that students are required to take but often throw away. We wanted to create a platform that makes students excited about those ingredients by letting them take ownership\u2014through recipe creation, voting, and feedback\u2014turning waste into engagement.What it does: BITE \u2013 RecipeGram is a student-powered web platform where:\n-Schools post a weekly set of required ingredients.-Students submit creative recipes using those ingredients.-Other students vote on their favorite recipes.-Cafeteria staff use the top-voted recipes to guide meal prep.-Students can also give feedback on portions and hunger levels.-Admins can manage users, reset submissions, and update ingredients with image uploads.-The goal is to reduce food waste, increase student satisfaction, and make cafeteria food smarter and more personalized.How we built it: We built this as a full-stack web app using:-Flask (Python) for the backend-SQLite for lightweight database storage-HTML/Jinja for dynamic templates-Tailwind CSS for sleek and responsive styling-JavaScript for real-time voting functionality-A custom admin panel for managing users, recipes, and ingredients-Secure user authentication with school ID and password-We also added image upload and voting features, and allowed only one account per student.Challenges we ran into: -Handling image uploads securely and ensuring consistent file naming-Preventing users from liking a post more than once-Managing user sessions while allowing admin-specific access-Getting reliable ingredient images without external API limitations-Coordinating all the Flask routes and templates across different user typesAccomplishments that we're proud of: -Built a fully functional, beautiful, and responsive platform from scratch-Integrated a smart recipe voting system with like/unlike functionality-Designed an admin dashboard to manage both users and ingredients-Added visual feedback, error handling, and secure user authentication-Created a platform that could realistically be used in real schools to reduce wasteWhat we learned: -How to structure and scale a Flask app with multiple routes and templates-Best practices for user authentication, validation, and file handling-How to make a clean UI with Tailwind CSS and keep it accessible and responsive-The importance of user testing and debugging for edge cases like double votes or missing imagesWhat's next for BITE-RecipeGram: -Enable students to leave comments on recipes for collaboration-Add a \"Top Recipes of the Month\" leaderboard-Work with local school districts to pilot test in real cafeterias-Add email/password recovery and password hashing-Integrate nutritional info and allergy flags for submitted recipes-Scale beyond schools into community food programs",
                        "github": "https://github.com/PeroroncinoZ/RecipeGram",
                        "url": "https://devpost.com/software/bite-recipegram"
                    },
                    {
                        "title": "FlexPlate",
                        "description": "FlexPlate revolutionizes school cafeterias by personalizing meals, slashing food waste, and tracking impact all with cool, data-driven tech for a sustainable future.",
                        "story": "Inspiration: After doing some research, me and my partner found out that530,000 TONSof food and45 MILLIONgallons of milk are wasted every year from school. We realized through our own experience in school that it was due to the fact that schools automatically give you portions, and it may be too much or too little for someone. This led to the FlexPlateWhat it does: Our platform prototype is an app or website where students can select their portion sizes based off what they want and send their order to the kitchen.How we built it: We used Flask to develop the website and backend through python, and we got help from online resources to learn how to code in HTML for the front end.Challenges we ran into: We are beginners to code, so we had to do a lot of research and get help from our parents to learn how to do this project.Accomplishments that we're proud of: We are very proud of getting through the obstacles and learning how to code.What's next for FlexPlate: We hope to continue working on it and make it a functional app.",
                        "github": "",
                        "url": "https://devpost.com/software/flexplate"
                    },
                    {
                        "title": "Plate Predict",
                        "description": "Saving cafeterias, one plate at a time.",
                        "story": "Inspiration: In our high school experience, we witnessed first hand the wasting of massive amounts of food during lunch time. This is mostly because students get too much food that they do not want to eat, and end up wasting and throwing it away. Since the school cannot reuse the food that another student has touched, they end up losing lots of money and valuble resources that could be allocated to other places. We wanted to find a solution to this, not only because wasting money is bad, but because wasting food is worse.What it does: Our app takes in a student ID and a photo of the student's lunch waste taken by a trash monitor. These are then sent to our backend, which assigns an index to how much food the student wasted and, based off past data, predicts how much food the student will need for the next lunch so that the student doesn't end up wasting food. Additionally, it gives statistics for the whole school based off student data so the school doesn't overproduce food and have to throw it out, wasting money.How we built it: We decided to make a web app using Next.js and Tailwind CSS for the frontend, and Node.js, MongoDB, Flask, and Gemini Requests for the backend. Using AI and machine learning to predict data provied crucial to the development of our project.Challenges we ran into: We ran into multiple challenges in both the front end and the backend. In the front end, we ran into issues with the CORS policy, SSH certificates for camera access, and learning new TypeScript concepts. For the backend, issues with connection to the database and data transfer. We managed to resolve all these issues to get a final working project.Accomplishments that we're proud of: We are proud of fully integrating a database and data transfer to and from the database in this project. We are also proud of learning new frameworks and designs for frontend and expanding our knowledge on user input through cameras.What we learned: We learned that it is important to plan out your project before you actually do it, so that the coding is smooth and effortless. We also learned the importance of collaboration between frontend and backend programmers.What's next for Plate Predict: To expand on this project, we can get a barcode scanner to more easily recieve student ID, more effectively train machine learning to do better predictions, and get our app verified by school districts for real-world use.",
                        "github": "https://github.com/TheSigmaSociety/PlatePredict",
                        "url": "https://devpost.com/software/plate-predict"
                    },
                    {
                        "title": "FlaminGo",
                        "description": "A real-time AI tool that helps users sort trash, recycling, and compost by analyzing waste through webcam or image uploads \u2014 complete with OCR, classification insights, and GPT-powered interpretation.",
                        "story": "Inspiration\ud83d\udd25 Inspiration: Our team was inspired by the overflowing waste bins in school cafeterias \u2014 where compostable materials are often tossed in trash and recyclables go to waste. We realized that most people want to do the right thing, but they\u2019re confused by vague packaging or unclear signage. We set out to build an AI-powered assistant that could help students instantly classify their waste and reduce landfill impact.\ud83e\udde0 What it does\nFlaminGo is a real-time waste classification tool that:Uses computer vision to detect whether an item is biodegradable or not.Extracts text labels from the object using OCR to boost prediction confidence.Leverages GPT-4o to explain the classification and offer insights.Offers both webcam-based and image upload classification.Stores and displays recent classifications, and allows users to query the assistant about trends or errors.\n\ud83d\udee0\ufe0f How we built it\nFrontend: Tailwind CSS, HTML, and vanilla JS for a clean, mobile-friendly UI.Backend: Flask handles image capture, classification, and integration with MongoDB for login/auth.Computer Vision: Trained a CNN model (based on VGG16) to classify waste items into 5 categories.OCR: Used pytesseract to extract any visible text on the object.GPT-4o: Sends recent results (including OCR and images) to OpenAI to interpret user activity.Authentication: Built a secure login/register system with hashed passwords and MongoDB for user sessions.\ud83e\udde9 Challenges we ran into\nManaging realtime webcam input and converting it into usable model input.Handling image hosting vs base64 conversion to support GPT-4o vision input.Cleaning up the dataset to ensure meaningful biodegradable vs non-biodegradable classification.Getting OpenAI\u2019s new API working smoothly with Flask and async vision uploads.Ensuring performance was snappy even with model inference and OCR combined.\ud83c\udfc6 Accomplishments that we're proud of\nCreated a fully functional AI-powered app with vision + language + interactivity in under 48 hours.Integrated OpenAI\u2019s GPT-4o vision API to deliver true assistant-style interpretation.Designed an interface that\u2019s both fun and functional for school environments.Built a secure login system with password hashing and database support.Enabled intelligent conversations between students and the assistant around sustainability.\n\ud83d\udcda What we learned\nHow to deploy vision models in a real-world app.Deepened our understanding of OCR limitations and how GPT can help fill in the gaps.Learned the nuances of OpenAI's latest vision API and how to format prompts effectively.Gained hands-on experience with frontend/backend integration using Flask + Tailwind.Realized the importance of user feedback loops in making AI feel helpful and not confusing.\ud83d\ude80 What's next for FlaminGo\nAdd multi-object detection to support trays with multiple items.Use YOLOv8 for smarter bounding boxes and class prediction.Provide personal stats and insights on how much waste each user diverted.Explore classroom-wide dashboards to gamify sustainable behavior.Deploy the tool on a school tablet kiosk or even as a browser extension near trash bins.What it does: How we built it: Challenges we ran into: Accomplishments that we're proud of: What we learned: What's next for FlaminGo:",
                        "github": "",
                        "url": "https://devpost.com/software/flamingo-v6j3u7"
                    },
                    {
                        "title": "Lunchly",
                        "description": "Recycling and food bank finder",
                        "story": "Inspiration: Our inspiration is the theme for this event, and making something userfriendly for schools.What it does: allows you to view every recycling point and foodbank in a user given areaHow we built it: we built this using git, html, css, and javascriptChallenges we ran into: we struggled with github gists and version controlAccomplishments that we're proud of: we are proud of the current functionality of our systemWhat we learned: we have learned valuable skills such as implementing api's and data basesWhat's next for Lunchly: we plan to improve this and eventually publish this on the app store",
                        "github": "https://github.com/4t0m15/AA-cafeteria-solutions",
                        "url": "https://devpost.com/software/lunchly"
                    },
                    {
                        "title": "Food Frenzy",
                        "description": "In food frenzy, you work as a lunch lady serving hungry children.\u000bProvide them with food they want so that they don\u2019t throw it away.\u000bBe quick, if they get too angry they might start a food fight!\r\n",
                        "story": "Inspiration: We wanted to create a game that would show the people what it's like to serve lunch in a school cafeteria. We wanted it to be fun yet simple, with a strong lesson for the player. We were inspired by games that would have you prepare meals quickly and wanted to make something similar in a school cafeteria environment, but with an additional twist.What it does: In this game, you work as a lunch lady. You need to see what the kids that come up want and bring them the perfect lunch for them. You need to be fast, as more kids are waiting the food fight meter goes up, until the inevitable food fight eventually occurs. You are then provided with information on how this system is very wasteful of food.How we built it: We build this project in the game engine called Godot. For art, we used Aseprite and Piskel, both great software for making pixel art. We used Github to collaborate effectively as a team.Challenges we ran into: Some of the challenges that we ran into were issues with git and merge conflicts, as well as learning how to use new software since some of us have never used Godot or Github before.Accomplishments that we're proud of: We are proud of making a functioning game that had both a charming art style and various features that would make it memorable. We were unsure if we would be able to achieve this impressive feat, but we achieved a lot more than we expected.What we learned: We learned how to use Godot, the game engine we used. Some team members also learned how to create pixel art, use git effectively, and general game design.What's next for Food Frenzy: In the future, we would love to add more advanced way of preparing food and better cut scenes to make the story of the game flow better. We would also wish to make the game more user friendly with more UI and a tutorial.",
                        "github": "https://github.com/TarasAbakumov27/Hackabyte-Game",
                        "url": "https://devpost.com/software/food-frenzy"
                    },
                    {
                        "title": "FreshBay",
                        "description": "An AI-powered platform connecting donors and food banks to reduce waste and fight hunger using smart technology.\r\n\r\n",
                        "story": "Inspiration: Food insecurity affects millions of people, while vast amounts of food go to waste daily. We were inspired by the inefficiencies in the current food distribution system, where food banks struggle with demand forecasting and equitable allocation. Seeing how AI has revolutionized logistics and supply chains, we wanted to apply technology to ensure that food reaches those who need it most.What it does: FreshBay is an AI-powered food redistribution platform that connects food donors, food banks, and food pantries to ensure surplus food is efficiently distributed to those in need. Key functionalities include:AI-Powered Demand Forecasting: Predicts food demand in different regions using demographic and economic data.Sentinel-2 Satellite Imagery Integration: Analyzes agricultural index data to determine food production trends and combine it with statistical models to calculate high-need areas.Smart Matching & Routing: Uses AI to match food donations with food banks and pantries based on perishability, storage capacity, and urgency.Real-Time Inventory Management: Tracks available food supplies and ensures equitable distribution.Community Feedback System: Allows food pantries and individuals to report shortages, delays, and food quality issues to refine the AI model.How we built it: We designed FreshBay as a multi-tier AI-powered food redistribution system that integrates food banks and food pantries seamlessly.AI & ML: Used TensorFlow.js and Scikit-learn to forecast food demand and optimize distribution.Satellite Data Analysis: Integrated Sentinel-2 satellite imagery to assess agricultural productivity and correlate it with food demand trends.Backend: Built with Supabase (PostgreSQL) to manage users, food donations, and real-time inventory tracking.Frontend: Developed using React (TypeScript) for a user-friendly interface.Logistics & Mapping: Integrated Mapbox to optimize delivery routes and visualize food demand trends in real time.Real-Time Updates: Utilized Supabase Realtime to notify food banks, pantries, and volunteers about food availability.Challenges we ran into: Data Collection & Integration: Sourcing accurate hunger data and integrating it with our AI models was a challenge.Logistics Optimization: Balancing real-time food distribution with limited transportation resources required advanced routing solutions.Equity-Based Distribution: Ensuring fairness in food allocation while optimizing efficiency was a complex problem we had to solve with AI.Satellite Data Processing: Leveraging Sentinel-2 imagery to extract meaningful agricultural data and integrate it into our AI models required extensive preprocessing and analysis.Accomplishments that we're proud of: Developed a fully functional AI-powered food distribution system that can scale to real-world implementation.Successfully implemented real-time food matching and routing, significantly reducing food waste.Built an equity-driven food distribution model that prioritizes marginalized communities.Created interactive hunger heatmaps that visualize real-time food insecurity trends.Integrated satellite imagery analysis to enhance food demand prediction and distribution efficiency.What we learned: Throughout the development of FreshBay, we learned about the challenges food banks and pantries face in distributing food efficiently. Understanding social justice aspects, such as food deserts and systemic inequality, helped shape our AI-driven equity-based model. We also gained hands-on experience working with AI/ML demand forecasting, database optimization, and real-time updates for logistics.What's next for FreshBay: Expand Partnerships: Collaborate with local food banks, government agencies, and nonprofits to scale impact.Enhance AI Models: Improve demand forecasting accuracy with more datasets and deep learning models.Introduce Mobile Food Pantries: Deploy pop-up food distribution sites in high-need areas based on AI insights.Public API Development: Allow third-party organizations to integrate FreshBay\u2019s food redistribution model into their systems.Advanced Satellite Data Utilization: Further refine our use of Sentinel-2 data to dynamically adjust food distribution strategies based on crop yield predictions.",
                        "github": "https://github.com/COOLCODERVG/hackabyte",
                        "url": "https://devpost.com/software/freshbay-j4fa68"
                    },
                    {
                        "title": "Food Gaurd",
                        "description": "A 3D Game-Based Approach to Minimizing Food Waste at Schools",
                        "story": "Inspiration: Food Guard was inspired by the real-world challenge of reducing food waste in school cafeterias. Studies have shown that factors such as meal options, plate size, lunch schedule, and cafeteria layout significantly impact food waste. Our goal was to create an interactive and educational simulation where users could experiment with these variables in a 3D cafeteria setting to see firsthand how they influence food waste and make better decisions as cafeteria administrators.What it does: In Food Guard, the user plays as a cafeteria administrator tasked with minimizing food waste. With a budget of $3000, the user must purchase and strategically position lunch tables, waste bins, and food bars (such as salad, fruit, and milk sections) within a 3D cafeteria model. The goal is to arrange these elements in a way that encourages students to waste as little food as possible. Additionally, users can adjust the lunch schedule using a slider to either extend or shorten lunch periods, and modify the size of the students' plates. At the end of the game, the system uses a Chat GPT API to analyze the user's choices and provides an insightful analysis of how their decisions impacted food waste.How we built it: We started by importing assets like lunch tables and waste bins from the Unity Asset Store to create the core elements of the cafeteria. We used Blender to design the structure of the cafeteria and imported textures for a realistic feel. The user\u2019s initial budget is used to buy tables, waste bins, and food bars like salad and fruit bars, with each element having its own properties (such as collision detection). Users can interact with the environment by selecting objects and adjusting their position using WASD controls. To make it even more impactful, we implemented a system where the position of objects influences the amount of food waste. We also integrated a Chat GPT API to analyze the user's layout choices and give feedback on how to reduce food waste.Challenges we ran into: We faced several challenges throughout development. One of the major hurdles was integrating the Chat GPT API, as we encountered compatibility issues between the Sketchfab package (used for the 3D assets) and the GPT API. Additionally, we struggled with version control. We didn\u2019t commit changes to GitHub often enough, which led to difficulties when transferring work between teammates. This sometimes resulted in lost progress when downloading newer versions of the project. Despite these challenges, we pushed through to complete the game.Accomplishments that we're proud of: We\u2019re particularly proud of the way we developed a dynamic system for object placement, ensuring that users couldn\u2019t place tables or waste bins in overlapping positions, forcing them to reconsider their designs. This addition significantly enhanced the realism and interactivity of the simulation. We also made the object selection system more generalizable, which allowed us to easily add new assets like fridges, ovens, and microwaves to enhance the realism of the cafeteria. The integration of the Chat GPT API to analyze user decisions was another key achievement, providing personalized feedback based on the user\u2019s cafeteria setup.What we learned: We learned a lot throughout this process, both technically and conceptually. From a technical perspective, we gained valuable experience with Unity, Blender, and the challenges of API integration. We also learned about the importance of version control and how a lack of proper commits can lead to significant setbacks in collaborative projects. On the conceptual side, we learned a great deal about the various factors that influence food waste in schools and how small changes in cafeteria design can have a significant impact on waste reduction.What's next for Food Gaurd: In the future, we aim to expand Food Guard by adding more cafeteria assets, such as different food options and more customizable elements. We also plan to refine the user feedback system, possibly integrating data analytics to provide users with more detailed insights into their cafeteria designs and how their choices compare to best practices for reducing food waste. We envision turning the game into an even more comprehensive simulation with multiple levels or challenges, and potentially even adding multiplayer features where users can collaborate on optimizing a cafeteria together.",
                        "github": "",
                        "url": "https://devpost.com/software/food-gaurd"
                    },
                    {
                        "title": "PreServe",
                        "description": "Our app lets you pre-order meals while helping schools reduce food waste. Save time, save money, and make a difference- one meal at a time.\u200b",
                        "story": "What Inspired You?: Kiosks at restaurantsHow does it work?: Students can pred-order their lunch/breakfast/meal so that school don't produce excess and waste.How did you make it?: We used Java in Android Studio and combined it with SQLite. We used GtiHub.Struggles and Challenges youran into: Setting up the Android Studio and GitHub took time. Plus, since Siddanth and Vibhas were using Android Studio for the first time while Amogh and Siddanth were using SQLite for the first time, so we took time learning how to use it. Finally, connecting the SQlite to the actual app was very hard.Accomplishments that you are proud of?: We got some on the SQLite connected to the Android Studio. We used Android Studio to make many scripts in java that dealt with SQL databases.What did you learn?: We learned how to use Android Studio, we learned how to use SQLite. We also learned a lot about how versatile Java is. We also learned more about Git, Github, and how to collaborate.What's Next for PreServe?: We will fix the bugs in our code, and we will add the order feature that is currently half developed (the front end is almost ready, the backend needs some work)",
                        "github": "https://github.com/amoghmunikote/hackabyte25.git",
                        "url": "https://devpost.com/software/preserve-o0ldbc"
                    },
                    {
                        "title": "GrubGo",
                        "description": "Ordering food",
                        "story": "Inspiration: Students at my school not getting food :(What it does: Pre-ordering sysHow we built it: DjangoChallenges we ran into: Manipulating ModelsAccomplishments that we're proud of: Successfully manipulating modelsWhat we learned: How to control formsWhat's next for GrubGo: More controls on school",
                        "github": "https://github.com/Water-Ty/GrubGo",
                        "url": "https://devpost.com/software/django-8ovpmi"
                    },
                    {
                        "title": "MenuGenius",
                        "description": "AI app to design school lunch menus based on student feedback.",
                        "story": "Our Story: `We started this project by brainstorming **many** ideas. We narrowed the ideas down and landed on MenuGenius. We faced many challenges, especially with the API's. We built our project on VSCode using HTML, js, and css`.",
                        "github": "",
                        "url": "https://devpost.com/software/menugenius-35j7p4"
                    },
                    {
                        "title": "WastEd",
                        "description": "Solving school cafeteria food waste by aligning incentives.",
                        "story": "(Sorry about the AI summary, we are a bit short on time)Inspiration\nWe wanted to tackle the growing challenge of food waste in schools by merging IoT technology with AI. Our goal was to create a system that not only monitors waste in real time but also offers actionable insights, encouraging more sustainable practices.What it does\nWastEd attaches Raspberry Pis to trash bins, capturing images every 30 seconds via an endpoint (e.g., /latest-image/). Through a user-friendly interface, schools can register, log in, and add bins. Each bin is displayed as a card showing a live image, a food waste score, and key statistics. Additionally, AI analyzes bin snapshots taken every ten minutes, providing detailed counts (like food trays, unfinished burgers, milk cartons, and more) with corresponding emojis and color-coded scores.How we built it\nHardware Integration: Raspberry Pis are installed on trash bins to capture images regularly.Backend Development: We used FastAPI to build a robust backend that serves the bin images and handles requests.User Interface: The admin dashboard allows schools to register, log in, and manage bins effortlessly.AI Integration: An OpenAI API call runs once every ten minutes for each bin, generating detailed food waste statistics.Data Visualization: Snapshot statistics, including images, counts, and overall food scores, are presented in a compact and easy-to-read format.Challenges we ran into\nReal-Time Image Capture: Ensuring that the Raspberry Pis consistently provide up-to-date images.Data Synchronization: Maintaining smooth communication between the hardware and server, especially under varying network conditions.Efficient AI Calls: Balancing the frequency of AI-generated insights to avoid overloading the system while ensuring accurate monitoring.User Interface Design: Creating a dashboard that is both visually appealing and capable of handling a large volume of snapshot data in a compact layout.Accomplishments that we're proud of\nSuccessfully integrating IoT devices with a fast, responsive FastAPI backend.Developing an intuitive interface that empowers schools to actively monitor and address food waste.Seamlessly incorporating AI to extract meaningful insights from periodic bin snapshots.Ensuring consistent color coding and emoji associations for easy, at-a-glance interpretation of food waste severity.What we learned\nThe importance of reliable hardware-software integration in IoT projects.Strategies for optimizing API calls to balance performance with resource limitations.How to design user interfaces that effectively manage and display large datasets in real time.Valuable lessons on using technology to drive sustainability and environmental responsibility.What's next for WastEd\nExpansion: Scaling the system to include more schools and additional types of waste bins.Enhanced AI: Refining our AI algorithms for even more precise waste categorization and predictive insights.Improved Analytics: Developing deeper analytical tools and dashboards to help schools make informed decisions.Community Integration: Exploring partnerships with local waste management services to broaden the impact of our sustainability efforts.",
                        "github": "",
                        "url": "https://devpost.com/software/wasted-5173mx"
                    }
                ]
            ]
        },
        {
            "title": "HackPSU Spring 2025",
            "location": "Business Building",
            "url": "https://hackpsu-spring-2025.devpost.com/",
            "submission_dates": "Mar 29 - 30, 2025",
            "themes": [
                "Beginner Friendly",
                "Education",
                "Open Ended"
            ],
            "organization": "HackPSU",
            "winners": true,
            "projects": [
                [
                    {
                        "title": "Street Fighter: Algorithm Annihilation",
                        "description": "A machine learning-powered version of the classic Street Fighter game.",
                        "story": "Inspiration: Retro games have a timeless charm, but the computer opponent is often limited to fixed, pre-programmed behaviors. We were inspired by the classic Japanese fighting video gameStreet Fighterand decided to put a modern spin on it for Timeless Tech Challenge - adding machine learning to make the game more of a challenge.What it does: Street Fighter: Algorithm Annihilation uses machine learning to understand a player's habits and patterns. We developed a CPU that continuously learns from player actions, adapts its strategies, and evolves over time, making each match more challenging and unpredictable. Repetitive and predictable gameplay is a real-world problem in gaming, leading to player disengagement and shorter game lifespans. Many games rely on scripted behavior, which can feel stale after repeated playthroughs. By implementing machine learning, this project addresses the challenge of keeping players immersed, creating a more dynamic experience that stays challenging and exciting. This approach could redefine game design, making AI-driven experiences that evolve alongside the player, keeping games fresh and enjoyable for longer.How we built it: We built Street Fighter: Algorithm Annihilation using Python.On top of the basic framework, which can be credited to Aaditya Panda, we implemented a machine-learning-based system for tracking player behavior and adapting the CPU opponent. We created the PlayerBehaviorTracker class to collect data on player actions and game states, extracting features such as player and CPU positions, velocity, and relative distance. This data is trains a decision tree model in the MLModel class, which predicts the CPU's next move based on past player behavior. The model is trained, saved, and loaded using pickle, and continuously adapts to player strategies. The extract_features_from_state function converts game states into numerical data for the AI. The system enables the CPU opponent to evolve over time, becoming smarter and more responsive to the player's actions.Challenges we ran into: We encountered challenges in saving the behavior data for different players, as it was crucial to track each player's actions and state accurately. Another significant challenge was ensuring the CPU opponent didn't simply charge towards the player or act predictably. We had to implement more sophisticated logic for the CPU to avoid repetitive movements, allowing it to dodge, block, and strategize based on the player's position and actions, making the game more dynamic and challenging.Accomplishments that we're proud of: We're proud of successfully implementing a machine learning-powered AI for the CPU opponents, which significantly enhanced the gameplay experience. By using reinforcement learning, we were able to create opponents that could learn from the player's actions and adapt their strategies, providing a more challenging and engaging experience.What we learned: Throughout this project, we learned about both machine learning and game development. We gained hands-on experience implementing machine learning algorithms, which gave us a better understanding of concepts like reinforcement learning and supervised learning. In terms of game development, we gained valuable insights into gameplay mechanics and working with graphics. Overall, this project deepened our understanding of how machine learning can be integrated into games, and how thoughtful game design can lead to a more immersive player experience.What's next for Street Fighter: Algorithm Annihilation: We aim to enhance the game's graphics and animations to create a more immersive experience. For example, we can add customization options, power-ups, and different attacks.",
                        "github": "",
                        "url": "https://devpost.com/software/street-fighter-algorithm-annihilation"
                    },
                    {
                        "title": "MedCompass",
                        "description": "Effortless Follow-Ups, Seamless Scheduling!",
                        "story": "Inspiration: MedCompass was born from a simple truth that is recovery doesn\u2019t stop when a patient leaves the hospital. Yet too often, people go home with complex instructions, medications to manage and no clear support. At the same time, nurses and hospital staff are overwhelmed, juggling countless responsibilities. We saw an opportunity to step in and help. MedCompass uses smart, compassionate automation to check in with patients, guide them through recovery and free up medical staff to focus on what matters most delivering quality care.What it does: MedCompass makes patient follow-up simple and stress free. It helps in automating scheduled calls to patients after they\u2019ve been discharged, asks the right questions based on their health condition, and gently checks how they\u2019re doing. Everything is tracked in a clean, easy to use dashboard, giving healthcare teams a clear picture of each patient\u2019s recovery, all without the endless phone calls and manual check-ins.Workflow: Built a dynamic, condition-based call scripting system using multi-collection queries in MongoDB AtlasDeveloped a fully automated follow-up call workflow using Twilio, Flask, and MongoDBCreated an intuitive dashboard for healthcare staff to track patient call history and manage schedulingAchieved seamless integration of frontend, backend, voice APIs, and database in a scalable architecture,What we learned: Twilio Voice API Integration: Successfully integrating Twilio\u2019s voice API to automate and manage patient communication effectively.Scheduling Calls: Implementing a reliable scheduling system to ensure timely follow-ups and patient interactions.Teamwork & Collaboration: Strengthening our ability to work collaboratively, ensuring seamless project execution and problem-solving.,What's next for MedCompass: Personalized One-to-One Conversations: Enhancing patient engagement by introducing more personalized and dynamic interactions tailored to individual needs.Pitching the Software to Healthcare Facilities: Expanding our reach by showcasing MedCompass to leading healthcare organizations, such as MedStar, to demonstrate its value in improving patient care and operational efficiency.,Reference: 1)Boston University Post Discharge Toolkit2)KHAQuality Post Discharge Follow-Up Call Script3)Re-Engineered Discharge (RED) Toolkit",
                        "github": "https://github.com/viny1ic/narad",
                        "url": "https://devpost.com/software/medcompass"
                    },
                    {
                        "title": "I am a gamer, not a robot",
                        "description": "play a game to prove that you are not a robot. we are the captcha she told you not to worry about. ",
                        "story": "Inspiration: Traditional CAPTCHAs are boring, frustrating, and disrupt user experience. We wanted to create a more engaging verification method that turns a tedious task into something enjoyable, while simultaneously gathering valuable training data for AI models. tldr: we hate BS and like games :)What it does: Our solution allows users to play classic arcade games like Space Invaders to verify they're human instead of solving traditional CAPTCHAs. Our solution collect the same data that can be used to sell and collects gameplay data. The gameplay data collected trains AI models, creating a win-win-win situation where verification becomes fun, profitable and productive.How we built it: We built the project using Next.js for the frontend, Phaser.js for the game engine, and implemented 3D models with Three.js. The data is stored in Supabase, with the backend hosted on Railway and model training handled through Modal.Challenges we ran into: The game would not work on the deployed site, only locally, which required extensive debugging. Additionally, integrating the game with Meta Quest presented unique challenges, particularly with WebXR compatibility and ensuring consistent performance in the VR environment while maintaining the core gameplay mechanics.Accomplishments that we're proud of: This is our first project that is VR compatible, allowing users to experience the game verification process in virtual reality through Meta Quest integration. We're also proud of creating a solution that makes CAPTCHA verification genuinely enjoyable. This is also the first real project we have developed together and it was a challenge to get into the flow with each other but we are incredibly proud of how this project turned out.What we learned: We gained extensive experience with modern web technologies, including Three.js for 3D model rendering, WebGL for graphics processing, and responsive design principles. We also improved our skills with Next.js, React components, CSS animations, and implementing mobile-first design approaches. Working with the Phaser.js game engine and integrating VR compatibility taught us valuable lessons about cross-platform development.What's next for I am a gamer, not a robot: We plan to expand our platform with additional game options beyond Space Invaders (like pacman donkey kong and tetris) , implement improved data collection for more sophisticated AI training, and develop a fully-featured API that websites can easily integrate for game-based verification. We're also exploring partnership opportunities with cybersecurity companies to bring our solution to more users.",
                        "github": "https://github.com/Pranav-Karra-3301/game-captcha",
                        "url": "https://devpost.com/software/i-am-a-gamer-not-a-robot"
                    },
                    {
                        "title": "LiDARcade",
                        "description": "LIDARcade: AR-powered arcade action for a better cause!",
                        "story": "LiDARcade \ud83d\udc7e: Breathe new life into classic arcade machines by bringing them into the real world with modern technology! LiDARCade is a refreshingly new way to play your favorite games through the lens of Augmented Reality with the purpose of aiding recovering stroke patients.Inspiration \ud83d\udcad: We took inspiration from the golden age of arcade gaming. More specifically, we sought after the hypnotic pixel patterns of Space Invaders\u2014the simple yet engaging 8-bit action that defined a generation. We asked ourselves: what if we could resurrect these nostalgic experiences but reimagine them through the lens of cutting-edge technology? LiDARcade bridges this gap by transforming ordinary physical spaces into immersive digital playgrounds. By merging the iconic gameplay mechanics of yesteryear with the spatial awareness capabilities of today's LiDAR technology, we're not just paying homage to gaming history\u2014we're evolving it. This fusion of timeless design principles with frontier technology creates an experience that feels both comfortingly familiar and excitingly novel, allowing players to physically move within games that once existed only on flat screens.What It Does \u2699\ufe0f: LiDARcade transforms the classic 2D experience of arcade games like Space Invaders into an immersive three-dimensional reality. While the original game confined aliens to a flat screen with players stationary at an arcade cabinet, our application liberates this gameplay into the physical world through advanced augmented reality.Using LiDAR technology, our app scans and maps real-world environments\u2014whether a hospital room, therapy center, or home living space\u2014and populates them with interactive game elements. Players physically move, aim, and engage with virtual targets that appear throughout their surroundings, creating a dynamic and spatially aware gaming experience that respects and integrates with the physical architecture of their environment.Beyond entertainment, LiDARcade serves a crucial therapeutic purpose. For stroke recovery patients struggling with limited mobility and coordination, traditional rehabilitation exercises can be repetitive and demotivating. Our application gamifies rehabilitation by encouraging natural movements\u2014reaching, pointing, and reacting to stimuli\u2014that directly contribute to rebuilding neural pathways and strengthening affected muscle groups. The game's difficulty automatically adjusts based on performance metrics, providing an optimized challenge level that maintains engagement while preventing frustration.How We Built It \ud83d\udee0\ufe0f: Frontend- \nOur user interface was crafted using SwiftUI to create a seamless and responsive experience. We designed an intuitive arcade-style interface with retro visual aesthetics that evoke nostalgia while remaining accessible for users of all ages. The UI features include:A dynamic scoring system with real-time accuracy feedbackCustom crosshair aiming interface optimized for AR interactionArcade-inspired typography and design elementsAdaptive layouts for different medical environmentsPatient and doctor dashboards for tracking progressEnvironment selection menus for different gameplay scenarios,Backend- \nThe technical foundation of LiDARcade utilizes several frameworks:ARKit integration: Utilized Apple's augmented reality framework to handle spatial tracking, plane detection, and real-world anchoringRealityKit rendering: This provided high-performance 3D content rendering and physics simulation for game elementsLiDAR processing: We implemented algorithms to interpret LiDAR depth data for precise environmental mapping and object classificationNotification system: We built a custom event-driven architecture to handle game state changes and user interactions.Swift concurrent programming: Asynchronous operations manage resource-intensive scanning and rendering processes without affecting gameplay responsiveness.,Challenges We Faced \ud83d\udc7e: Technical Integration Hurdles:Our development approach initially separated frontend and backend responsibilities among team members to maximize parallel productivity. However, this created significant integration challenges when merging the aesthetically-focused UI components with the computationally-intensive AR systems. We had to refactor our notification architecture to ensure seamless connection between these systems.Swift Learning Curve:None of our team members had extensive prior experience with Swift or Apple's development ecosystem. We faced a steep learning curve understanding SwiftUI's declarative paradigm, RealityKit's entity-component system, and ARKit's spatial mapping capabilities simultaneously under hackathon time constraints. Yet, it was necessary for us to learn as we would need to utilize the iPhone 13 Pro's LiDAR capabilities.Device Deployment Complexities:Deploying to physical iOS devices proved particularly challenging due to certificate provisioning, entitlement configurations, and privacy permission requirements specific to camera and LiDAR sensor access. Testing on physical devices was essential since the iOS simulator cannot accurately replicate LiDAR functionality.Spatial Recognition Refinement:Distinguishing between different environmental surfaces and correctly anchoring virtual objects required considerable algorithm refinement to prevent gameplay elements from appearing in physically impossible locations.Accomplishments We're Proud Of \ud83c\udfc6: We're incredibly proud to have created a fully interactive AR experience that seamlessly merges retro gaming nostalgia with cutting-edge spatial computing in just 24 hours. Despite having to simultaneously learn Swift during development, our cross-disciplinary collaboration successfully integrated complex technologies into a cohesive application that's both entertaining and therapeutic. We developed a functional machine learning component that adapts to individual patient capabilities while maintaining an engaging gameplay loop. Most significantly, we've transformed clinical rehabilitation from a tedious necessity into an immersive experience that patients genuinely look forward to\u2014proving that meaningful healthcare innovation can also be delightful and engaging.What We've Learned \ud83e\udd14: This hackathon immersed us in the world of modern AR development and significantly expanded our technical repertoire. We learned Swift's functional programming principles and SwiftUI's declarative syntax while navigating Apple's developer ecosystem. Our team gained hands-on experience with ARKit's spatial anchoring and scene understanding capabilities, particularly in processing and classifying LiDAR point cloud data for real-time environment mapping. Beyond technical skills, we learned valuable lessons about interdisciplinary collaboration\u2014bridging the gap between therapeutic healthcare requirements and engaging user experience design within strict time constraints.What's Next? \ud83d\ude80: Here is a snippet of our future ambitions with LiDARcade:Expanded Game Library:Create a diverse collection of nostalgic arcade games reimagined for AR\u2014from maze-runners like Pac-Man to shooting galleries like Duck Hunt\u2014each thoughtfully adapted to leverage spatial computing and therapeutic movements.Advanced Analytics Engine:Further develop a sophisticated machine learning model that analyzes gameplay metrics to predict patient recovery trajectories, identifies areas needing focused rehabilitation, and recommends personalized therapy regimens.Comprehensive Patient Management System:Continue developing a secure database to track individual player profiles, store performance history, and generate detailed progress reports for healthcare providers with visual analytics dashboards. We currently have a Supabase project shell set up for this instance.Environmental Map Marketplace:Build a library of pre-scanned environments including specialized rehabilitation facilities, common household layouts, and public spaces to accommodate patients in various recovery settings.Multiplayer Rehabilitation Sessions:Enable remote collaborative play between patients and therapists or family members, fostering social connection during recovery while maintaining professional oversight.Adaptive Difficulty Scaling:Refine our difficulty adjustment algorithms to respond dynamically to performance fatigue and frustration indicators, ensuring consistent engagement without overwhelming patients.General UX Updates:Add features such as haptic feedback and sound effects for enhanced user experience.,",
                        "github": "",
                        "url": "https://devpost.com/software/lidarcade"
                    },
                    {
                        "title": "SpatialForge",
                        "description": "In the age of increasingly complex genAI, SpatialForge leverages simple stochastic methods to optimize multi-objective multi-dimensional geospatial tasks.",
                        "story": "Inspiration: We wanted to demonstrate that simple, clever ideas still have their place in a world increasingly focused on complex, powerful, and expensive AI solutions. Markov Chain Monte Carlo Annealing (MCMCA) isn't new or incredibly complex, but it remains powerful and effective.We demonstrate this with two examples.(1) In hydrology, streamflow gauge placement has long been considered more art than science. Our example demonstrates a basin-scale solution for determining the appropriate locations for gauges using quantitative methods rather than subjective judgment.(2)Redistricting is often a politically fraught activity. Using a simple and fair algorithm could both produce more equitable results and remove opportunities for political manipulation, bringing transparency to a process that affects democratic representation.What it does: We've created a generic package that aids in the construction and implementation of MCMCA methods. It works with simple algorithms (like our bullseye animation example) as well as complex implementations. The package features a staging mechanism making it easy to optimize multi-dimensional cases.We built two example applications:How we built it: The package incorporates several high-level optimizations to ensure efficient performance across a variety of applications. One key example is the mathematical modeling of temperature, which leverages a nonlinear exponential function. This approach enables non-linear Gaussian sampling to produce results that are relatively linear in behavior, while avoiding the computational overhead of fully linear optimization. As a result, the scaling remains consistent, providing both accuracy and efficiency in diverse scenarios.For the redistricting application, we obtained precinct shapefiles from Harvard's political database for 2020 along with 2020 census data. We applied a downscaling method that assumes uniform distribution across small precincts and census blocks to produce a square grid mesh of Pennsylvania, which serves as the foundation for our optimization.Challenges we ran into: Time constraints presented our biggest challenge. When an algorithm needs 2 hours to run for testing, development becomes extremely difficult within a 24-hour hackathon timeframe.Some of the major problems came from the complexity of the optimization of redistricting and gage placement, and the need to speed up the algorithm it became clear early on Numba optimization need to be able to run enough simulations which met we only ever knew the state of the single pixel and its neighbors. So, for this means detecting if a pixel is disconnected or in fractal is incredibly difficult. We ended using a two-prong approach of small branching BFS tree of neighboring pixel and a front-end optimization to pixel selection by weight pixels with more neighbor higher.Accomplishments that we're proud of: We were able to demonstrate a better method for placement of river streamflow gauges on a basin scale. Both coming from a hydrology background, we are acutely aware of the importance of streamflow gauges for flood detection, hydropower production and river management. The USGS places roughly 100 new gauges every year, and optimal gauge placement can be the difference between almost a day of warning for floods and just a few hours. Yet methods for placement are often determined by antiquated approaches.Although the redistricting example is not fully optimized to produce ideal results, the current version of the algorithm is working toward an optimal solution and is very efficient given the immensity of the data.What we learned: We developed skills for creating Python packages and integrating animations. We learned to simplify our problem goals and discovered that simple solutions can be powerful when implemented thoughtfully.What's next for SpatialForge: We plan to expand the functionality of the package and further develop our examples. We're considering writing a white paper and pitching our gauge selection implementation to the USGS to improve and inform their placement methodology.Note: Many of our animated gifs couldn't be compressed to under 5 MB, so we'll be showcasing more of our algorithms during the judging expo.",
                        "github": "https://github.com/leoglonz/MCMCAPSUHacks2025",
                        "url": "https://devpost.com/software/spatialforge"
                    },
                    {
                        "title": "VibeVirtuoso",
                        "description": "No gear, no experience, no limits. ",
                        "story": "",
                        "github": "https://github.com/Ishita1110/VibeVirtuoso/",
                        "url": "https://devpost.com/software/vibevirtuoso"
                    },
                    {
                        "title": "Brain Labs",
                        "description": "Building a sustainable future of Artificial Intelligence by making energy-efficient physical learning networks available to everyone through cloud computing",
                        "story": "Tech Stack & Tools: Inspiration \ud83e\udde0: As student researchers, we're inspired by recent advances indecentralized physics-driven learningresearch. We believe systems that can carry outcomputations and learning directly on a physical circuitinstead of relying on massive GPU clusters holds the key to a sustainable AI future. Our goal is to democratize this technology through an accessible cloud platform, making it available to anyone interested in this emerging field.We took inspiration from the work of Dillavou et al. (Demonstration of Decentralized Physics-Driven Learning, Phys. Rev. Applied, 2022), which showed the possibility ofin-situphysical-learning with coupled learning.What It Does \u26a1: Brain Labs is a fully-functionalphysical learning systemconnected to acloud platform:A physical, self-adjusting resistor network acts as the equivalent of a neural networkLearning happens locally by adjusting resistance values via digital potentiometersAuser-friendly cloud platformthat allows users to upload datasets, train models, and conduct inference on our physical learning network based on their data, similar to a GPU cloud,How We Built It \ud83d\udee0\ufe0f: Brain Labs uses atwin-edge network: one network for the free state, and another for the clamped state. This implementation allows us to apply contrastive learning in real physical systems by comparing how the circuit behaves under two boundary conditions.Each edge contains:A pair of digital potentiometers (for the free and clamped states)ComparatorsXOR gates,All updates happen without centralized control, driven by a simple global clock.The system is orchestrated by:Arduinofor interfacing and controlPySerialfor low-level communicationFlaskandVitefor a clean frontend/backend separationNgrokfor remote accessibilityFirebase(optional) for user authentication and experiment logging,Challenges We Encountered \u2699\ufe0f: One challenge that we encountered was that we had to work with inconsistent components. Since we had a limited supply of comparators and had to build a network with four resistor edges, each of which uses a comparator, we had to compensate by using different types of comparators each with a different pinout configuration. This made it difficult to test for errors, especially when working with a fully-integrated network.Additionally, we had to build out an additional Digital to Analog Converter (DAC) circuit since we didn\u2019t have access to an Arduino DUE, which has an inbuilt DAC system.Achievements We\u2019re Proud Of \ud83e\udd47: We are the first people to implement a fully decentralized, physics driven learning network on a cloud platform that allows anyone from anywhere to useWe built a physical learning network of four self-adjusting resistor edges that is able to learn without a processorWe built a web interface that allows non-experts to upload datasets, configure experiments, and visualize learning outcomes.,Insights \ud83d\udca1: The project reaffirmed the potential ofphysical learning networksto be a viable and scalable solution to the growing demands of artificial intelligence.What\u2019s Next \ud83d\ude80: We're currently working towards:Extending Brain Labs to perform more complex machine learning tasks such as multi-class classification and regression.Shifting Brain Labs from breadboard implementation to custom PCB and ultimately IC implementation for increased scalability.,We envision BrainLabs as the foundation fornext-generation neuromorphic hardwarethat is scalable, sustainable, and elegantly simple.Interested? Curious?Check out our full repository, circuit diagrams, and source code to start experimenting withreal, physical machine learning.",
                        "github": "https://github.com/Yannaner/HackPSU2025_SPR",
                        "url": "https://devpost.com/software/brainlabs"
                    },
                    {
                        "title": "Courseflow",
                        "description": "Aligning your personal roadmap with your ambition\u2014helping you shape today and lead tomorrow. ",
                        "story": "\u26a0\ufe0f The Problem: Planning semester courses can be overwhelming, especially for freshman students, primarily due to the limitations of current planning tools and the inconsistent availability of advisors. Tools like the \"What-If report\" feature only provide the courses needed to take, without advising the student when they should complete the course. This \u201cwhen\u201d aspect is important, because certain courses require prerequisites and should be sequenced in appropriate order: the prerequisites before the courses. Additionally, many students encounter barriers to accessing timely guidance from advisors; for instance, one team member faced significant frustration when their advisor was unavailable for an entire year, underscoring a systemic issue in advisor accessibility. This combination of generalized academic plans and inconsistent support leads to increased anxiety as students struggle to meet graduation requirements, ultimately detracting from their college experience and future readiness.\ud83d\udca1 The Solution: Courseflow is a personalized platform allowing students to input unique major and minor combinations, the time they desire to graduate in, and the amount of credits they\u2019d prefer per semester. Our website then outputs a semester by semester plan. Our team used a web scraping technique to aggregate the required courses for engineering majors and minors, and used this data to create the semester by semester plan. By consolidating essential information and providing tailored recommendations, Courseflow empowers students to make informed decisions and take control of their academic paths, reducing stress and maximizing their college experience.\ud83d\udee0\ufe0f Our Technology: We used web scraping, AI-driven processing, and cloud deployment to develop a seamless and accessible academic planning platform. Our integration allows students to effortlessly generate personalized course roadmaps with AI-driven optimization, eliminating the need for time-consuming manual searches across multiple resources. This approach ensures scalability and accessibility, allowing the platform to adapt with various device sizes, from smartphones to desktop computers, providing users with a consistent and responsive experience.BeautifulSoup (Python) \u2013 Used to efficiently extract data on all Penn State majors, minors, and professors from trusted sources like the Penn State Bulletin and RateMyProfessors, ensuring accurate course planning.Gemini API \u2013 Processed the gathered data alongside user preferences to generate personalized academic roadmaps, helping students navigate their unique major and minor combinations with ease.Streamlit \u2013 Developed an interactive web application that allows users to input their academic details and receive a clear semester-by-semester plan in seconds.Koyeb \u2013 Deployed the platform on a scalable cloud hosting service, ensuring seamless performance and accessibility across all devices, from smartphones to desktops.,\ud83d\udea7 Challenges and Solutions: During the development of our program, we faced several significant challenges that tested our technical skills and problem-solving abilities. One major hurdle was integrating the Gemini API, particularly in processing large volumes of data, which required careful optimization for smooth functionality. Additionally, retrieving reviews from RateMyProfessors through the API presented complexities due to outdated APIs, leading us to resort to web scraping instead. This approach, however, introduced its own issues, as BeautifulSoup initially failed to extract valid data. Consequently, we had to pivot to using cURL and other web scraping methods to obtain the necessary information. Finally, getting the domain up and running involved troubleshooting various deployment issues to ensure our website was accessible and operational across all platforms. Overcoming these challenges not only strengthened our team's collaboration but also deepened our understanding of the technologies we employed.\ud83c\udfc6 Achievements that we are proud of: We are proud to have effectively optimized the course scheduling process for students in 24 hours. Traditionally, students spend over an hour piecing together their semester schedules by manually searching through the bulletin, LionPath, and the Gen Ed planner\u2014only to end up with more unanswered questions. Our product curates a full academic plan in just seconds. By instantly providing a structured roadmap for major and minor combinations, we save students countless hours and help them stay on track for graduation with confidence.\u2705 What we learned: Throughout the development of our program, we gained valuable skills and knowledge that significantly enhanced our technical capabilities. First and foremost, we learned essential Git commands, which improved our collaboration and version control as we worked on the project. Additionally, we delved into web scraping techniques using BeautifulSoup, gaining hands-on experience in extracting data from websites, which proved crucial for our data-gathering efforts. We also expanded our skill set by learning to use Bash and cURL, which allowed us to efficiently handle various command-line tasks and troubleshoot issues with web scraping. These experiences not only deepened our understanding of the technologies we used but also fostered a collaborative environment where we could support each other's learning and growth.",
                        "github": "https://github.com/bossbadi-web/hackpsu_sp25",
                        "url": "https://devpost.com/software/courseflow-e6c319"
                    },
                    {
                        "title": "DebbieAI",
                        "description": "The AI software engineer that gets you started.",
                        "story": "Inspiration: A while ago, I learned about a product called Devin \u2014 an AI-powered software engineer that raised over $100M. However, access to Devin costs over $200/month, which inspired me to build my own service: DebbieAI \u2014 a more accessible AI software engineer (for now, mainly focused on building websites).How I built it: The frontend is built with Next.js and TailwindCSS, designed to feel intuitive and responsive for users of all technical levels. I focused on building a clean interface where users can describe their project goals naturally.On the backend, I used Node.js and Express.js to manage project lifecycles \u2014 from receiving prompts to deploying finished applications. When a project is submitted, it goes through a custom-built pipeline that interprets the request, constructs a project scaffold, populates it with relevant code, and packages everything into a Docker container.A reverse proxy dynamically maps container ports to unique project URLs, making each site instantly accessible. Instead of relying solely on an AI model's output, DebbieAI applies logic to organize and verify the generated code, ensuring it's valid and deployable \u2014 like a real engineering assistant.Challenges we ran into: One of the hardest parts was getting Gemini to not just generate code but also apply it properly \u2014 meaning creating a consistent file structure, writing to correct files, and ensuring it was deployable. Ensuring formatting consistency and output structure from Gemini was particularly tricky.Accomplishments that we're proud of: Got the AI to reliably generate full websites end-to-end.Built an automated deployment pipeline with containerized environments.Made it dead simple for non-technical users to spin up working sites using AI.,What we learned: Prompt engineering plays a huge role in AI reliability.Deployment and containerization are just as important as code generation.Even simple UIs require thoughtful UX when powered by AI.,What's next for DebbieAI: Add support for editing and iterating on existing projectsIntegrate more powerful backend scaffolding (e.g., databases, auth)Add custom domain supportKeep improving code quality and formatting consistency,",
                        "github": "https://github.com/Laphatize/debbieai",
                        "url": "https://devpost.com/software/debbieai"
                    },
                    {
                        "title": "Echo",
                        "description": "Echo processes queries in three steps: Breakdown bot creates subquestions, Solver bot answers them, and Breakdown bot verifies the solution before presenting the final answer to the user.",
                        "story": "Inspiration: We were inspired by recognizing limitations in current AI chatbots. While ChatGPT, Gemini, and other large language models are powerful, they can sometimes provide incomplete answers or fail to thoroughly examine complex problems from multiple angles.\nWe identified that human experts often solve problems by breaking them down into components and approaching them systematically. By creating a system that combines multiple AI models in a \"chain of thought\" approach, we're mimicking this human problem-solving process to produce more comprehensive and accurate responses.\nThis approach also addresses common shortcomings like hallucinations or reasoning errors by having multiple models verify and refine each other's work, similar to peer review in academic settings.What it does: Our system implements a sophisticated workflow:Problem Decomposition: The primary chatbot analyzes the user's prompt and breaks it down into distinct subproblems or components.\nDistributed Problem Solving: These subproblems are directed to a secondary AI model (potentially a different LLM or specialized model), which tackles each subproblem independently.\nAnswer Synthesis and Verification: The original chatbot collects all subproblem solutions, evaluates their coherence and correctness, and synthesizes them into a unified response.\nQuality Assurance: Before delivering the final answer, our system performs verification checks to ensure accuracy, completeness, and alignment with the original query.\nRefinement Loop: If inconsistencies or gaps are detected, we have the system iterate through steps 2-4 again to improve the answer quality.This process delivers answers that are more thorough, well-reasoned, and reliable than what a single chatbot could produce on its own.How we built it: Our implementation leverages Streamlit for the frontend interface and Python for backend processing:Frontend Development: We used Streamlit to create a clean, interactive web interface where users can input their prompts and view responses.\nAPI Integration: We developed wrapper functions to interface with multiple LLM APIs (such as OpenAI's API for ChatGPT and Google's API for Gemini).\nPrompt Engineering System: We created a sophisticated prompt engineering framework that structures how queries are broken down, distributed, and reassembled.\nResponse Processing Pipeline: We implemented custom Python functions to handle the parsing, evaluation, and refinement of responses between the different AI models.\nData Flow Management: We built efficient data handling to manage the flow of information between models while maintaining context.Our architecture prioritizes simplicity and modularity, making it easier for us to maintain and potentially expand with additional models in the future.Challenges we ran into: We faced several significant technical hurdles during implementation:Context Management: We struggled with ensuring each AI model retained sufficient context from the original query while focusing on specific subproblems.\nAPI Limitations: We had to work within the rate limits and token constraints of different AI service providers.\nModel Disagreements: We needed to develop resolution strategies when different models provided conflicting answers to subproblems.\nResponse Formatting: Creating a consistent and user-friendly format for final answers was challenging despite varied formats from different AI sources.\nLatency Issues: We had to balance the improved quality of multi-model responses against the increased response time from making sequential API calls.\nPrompt Design: We spent considerable time crafting effective prompts that would guide each model to perform its specific role in the chain of thought process.\nError Handling: We built robust fallback mechanisms for when one model in the chain fails to provide a usable response.These challenges required iterative development and creative problem-solving approaches, ultimately leading us to build a more resilient system.",
                        "github": "https://github.com/sja6039/Echo",
                        "url": "https://devpost.com/software/echo-gfuasy"
                    },
                    {
                        "title": "SkinIQ",
                        "description": "AI-powered skincare analysis and personalized recommendations for healthier skin",
                        "story": "Inspiration: Waking up one day and realizing how challenging it is to find a personalized, effective skincare routine sparked the idea for SkinIQ. I was struggling to clear my ance and always wanted a more detailed knowledge resource telling me what to do with my skin, hence SkinIQ. Our goal was to create an intelligent assistant that simplifies skincare decisions by leveraging AI and web-based recommendations.What it does: Simple as it sounds, IQ for your skin, so you know everything about your skin with a click of a picture!There are four main elements to our app:Skin Type Detection \u2013 Analyzes user inputs to determine if their skin is oily, combination, or dry.Daily Routine Generator \u2013 Suggests a tailored morning and night skincare routine.Skincare Chatbot \u2013 Answers user queries related to skincare concerns, product compatibility, and best practices.Routine Generator with Product Recommendations \u2013 Searches the web to recommend suitable skincare products available for purchase.How we built it: We built it with a variety of elements some of them being: \nPython, Scikit-learn, Supabase, PyTorch, Keras, TensorFlow, Vercel, TypeScript, React, FastAPI, GitHub, OpenCV, Google Auth, Kaggle, and CanvaChallenges we ran into: Data Accuracy: Ensuring reliable product recommendations required filtering misinformation and outdated web sourcesSkin Type Analysis: Developing a model that accurately determines skin type based on user responses was complexScalability: Optimizing API calls and handling multiple user interactions without latency issuesPersonalization: Balancing general skincare guidelines with tailored recommendations for individual needsAccomplishments that we're proud of: Successfully integrated ML Model for multiple disease detection with a connection to an LLM-powered skin type into the skincare routine and informative chatImplemented a web-based product recommendation system that provides real-time skincare suggestionsDeveloped an intuitive, user-friendly website interface that simplifies skincare decision-making and learning about YOUR Skin\u2019s IQWhat we learned: There were plenty of new things we learned in the 24 hours, from DJing (yes we took turns when we needed a break) to how to properly deploying our final product. But jokes aside, the importance of high-quality data in building reliable AI models, web scraping techniques for dynamic product recommendations, the user experience considerations for chatbot interactions in the skincare industry, and the significance of personalization in consumer applications are just some of the knowledge we gained during this 24-hour time frame!What's next for SkinIQ: We are planning to pursue this as a startup and add these features: \nEnhance the AI model to include image-based skin analysis for better accuracyExpand product recommendations to cover a wider range of skin conditions and concernsDevelop a mobile-friendly version of SkinIQ for seamless accessibilityPartner with dermatologists and skincare brands to improve recommendation accuracy and credibility",
                        "github": "https://github.com/Shubh3005/SkinIQ-main-2",
                        "url": "https://devpost.com/software/skiniq-p85tdv"
                    },
                    {
                        "title": "PSU GroupUp",
                        "description": "A Website for Students to connect and organize study group sessions with each other at short notice or in advance. Students can create, join, and manage their own study events.",
                        "story": "Inspiration: We didn't find any easily available solutions for people who want to study in groups but can't.: What it does: Our website is a platform that connects users who prefer studying in groups.: How we built it: We mainly used JavaScript, with libraries like React and the Next.js framework to build our site.: Challenges we ran into: We had many design layout bugs, technical glitches, as well as a few merge conflicts when working on separate machines.: Accomplishments that we're proud of: Building a functional, user-friendly website for a target audience in a short period of time, given our minimal knowledge of the language.: What we learned: We learned a lot about GitHub collaboration, design principles, and teamwork strategies.: What's next for PSU GroupUp: We plan to polish and complete our website, and then provide app support as well.:",
                        "github": "https://github.com/krishnagpanicker/groupstudywebsite.git",
                        "url": "https://devpost.com/software/psu-groupup"
                    },
                    {
                        "title": "PennSL",
                        "description": " This project empowers the deaf community by fostering seamless interactions in educational, professional, and social settings. By making communication accessible to everyone",
                        "story": "",
                        "github": "https://github.com/Devin-M5706/HACKPSUSPR25",
                        "url": "https://devpost.com/software/pennsl"
                    },
                    {
                        "title": "Course Connect",
                        "description": "Stop wasting hours deciphering Penn State's course requirements! Course Connect streamlines academic planning. Upload your transcript, and get personalized schedules and a 4-year path tailored to you",
                        "story": "The Problem: Navigating the academic journey at Penn State can be daunting. Students spend countless hours cross-referencing information between the LionPath interface, RateMyProfessor reviews, dense University Bulletins, and recommended major pathways. This fragmented process is time-consuming, stressful, and often leads to poor course selections, potentially impacting GPA, graduation timelines, and overall college experience. There's currently no unified, intelligent solution to simplify course planning and scheduling tailored to individual Penn State students.The Solution: Introducing CourseConnect, a streamlined platform designed to revolutionize how Penn State students plan their academic careers. CourseConnect aims to eliminate the research grind by providing personalized course recommendations based on a student's actual academic record. By simply uploading their transcript, students receive tailored suggestions incorporating factors like course difficulty, professor ratings (via RateMyProfessor data), Gen Ed requirements, and major/minor progress. The result is an intuitive interface that presents not just the next semester's schedule, but a potential four-year path, empowering students to make informed decisions effortlessly.Our Technology: CourseConnect uses Google's Gemini API for accurately extracting completed courses, credits, and grades from uploaded PDF transcripts, understanding complex degree requirements based on scraped Bulletin information and major pathways, and generating optimal course suggestions considering prerequisites, student preferences (time of day, online/in-person), interests (for Gen Eds), and professor quality metrics.Our backend infrastructure involves scraping Penn State's course scheduler, University Bulletins, and recommended major pathways. This data, combined with user-specific information and preferences gathered during onboarding, feeds into the Gemini API. User data and scraped information will be managed via a RESTful API, connecting to our backend services. The frontend aims to deliver a clean, user-friendly web interface, potentially built with [mention framework like React, Vue, etc. if decided, otherwise keep general].Our API allows for user session management (/session), transcript upload (/user/transcript), preference setting (/user/preferences), retrieving user info (/user), fetching the recommended semester schedule (/schedule), and getting a full 4-year path (/path).Challenges and Solutions: We ran into many challenges creating this, the biggest one was overestimating Gemini's context window and not being able to feed it all of our data. We overcame this by carefully hand selecting the most important data and only giving it what it needs. Another challenge was scraping and maintaining up-to-date information from various Penn State sources (LionPath structure, Bulletins, RMP) is a significant challenge due to potential website changes and anti-scraping measures. To address this we developed scraping scripts with error handling, potentially incorporating manual verification steps, and designing a flexible database schema to accommodate data variations.Making effective prompts for the Gemini API to accurately interpret academic transcripts and complex degree rules required significant iteration and testing. We had to implement a rigorous testing cycle for prompt variations, focusing on edge cases and ensuring the structured output meets our application's needs. Merging data from transcripts, scraped sources, user preferences, and real-time availability required careful system design. We developed a clear data flow and robust backend logic to sort these diverse inputs into actionable recommendations.Lastly, balancing the MVP (transcript import, basic recommendations) with the ideal feature set (RMP integration, schedule view, full onboarding) within a hackathon timeline is demanding. We had to prioritize core functionality (MVP) first, building a solid foundation before adding advanced features.Accessibility: We aimed to build Course Connect with accessibility in mind from the start. This includes adhering to WCAG guidelines for web content, ensuring proper color contrast, logical content flow, sufficient font sizes, and keyboard navigability. Our goal is to create an interface usable by all students, including those using screen readers or other assistive technologies.Accomplishments that we're proud of:: Successfully designing the core concept and architecture for Course ConnectDeveloping the initial API structure for user management, transcript upload, and recommendation retrievalSetting up the initial framework for scraping essential Penn State academic dataExploring and integrating the Google Gemini API for the core recommendation logicDefining a clear and user-centric onboarding process,What we learned: The intricacies of Penn State's academic data landscape and the challenges involved in unifying itEffective techniques for interacting with large language models like Google Gemini for specific, structured tasksThe importance of robust data scraping strategies and error handlingHow to design a scalable backend API to support a complex recommendation engineThe value of prioritizing MVP features under tight deadlines while keeping the larger vision in mind,Operationalization (Future Plans): Post-hackathon, we envision expanding Course Connect significantly:Full Feature Implementation: Build out the ideal features, including direct RateMyProfessor scraping and integration, Gen Ed recommendations based on the interest survey, and a visual calendar view for proposed schedulesAlgorithm Refinement: Continuously improve the Gemini-powered recommendation engine based on user feedback and evolving course dataEnhanced User Profiles: Allow users to fine-tune preferences, track degree progress visually, and explore different major/minor scenariosUI/UX Polish: Iterate on the user interface to ensure it's intuitive, visually appealing, and highly efficientExpansion: Adapt the platform to support students at other universities, requiring modular data source integrationReal-time Data: Explore possibilities for integrating real-time course availability from LionPath (if feasible and permitted),",
                        "github": "https://github.com/tgmstudios/course-connect",
                        "url": "https://devpost.com/software/course-connect-tymz8p"
                    },
                    {
                        "title": "Auto Swarm",
                        "description": "\u201cI am developing a system for autonomous drone swarms that self-navigate, adapt, and collaborate in real time \u2014 enabling rapid response in complex or denied environments.\u201d ",
                        "story": "Inspiration\nTraditional drone delivery systems are limited by central control, GPS dependency, and poor adaptability in unpredictable environments. I wanted to design a resilient, scalable solution that mimics how nature solves complexity \u2014 with autonomous swarms.What it does\nAuto Swarm is a decentralized drone delivery system where each drone communicates with others via a mesh network. The swarm self-organizes, adapts to obstacles, and can deliver payloads without GPS, enabling operations in GPS-denied zones like forests, warzones, or disaster areas.How I built it\nUsed ESP32-based boards with mesh communication to create a lightweight mesh network. Each drone is equipped with a Pixhawk flight controller, VL53L0X distance sensor, an MPU6050 for orientation, and custom algorithms for swarm behavior. I incrementally tested flight logic, inter-drone communication, and real-time coordination.Challenges I ran intoAchieving reliable mesh communication in dynamic environmentsStable Connection to telemetry moduleFine-tuning motor control and PID stability without external positioningPower management and ensuring consistent sensor feedback mid-flightDebugging swarm logic in real-time was tricky without a centralized system,Accomplishments that I proud of\n_ Built two functional unitsBuilt a fully functional swarm communication protocolAchieved a stable test flight on a single drone,What I learnedSwarm intelligence requires simplicity in individual agents but robust inter-agent logicReal-time sensor fusion is critical to maintaining flight stability,What's next for Auto SwarmTrain reinforcement learning models for emergent swarm behaviorsPartner with defense, disaster response, or logistics organizations for field testing,",
                        "github": "https://github.com/Adim5465/Auto_Swarm_2",
                        "url": "https://devpost.com/software/auto-swarm"
                    },
                    {
                        "title": "Wallstreetbets Analyst",
                        "description": "Our program analyzes WallStreetBets posts from the past month, creates stock trades based on sentiment, and compares their performance to the S&P 500, offering insights for smarter investments.",
                        "story": "Inspiration: Initial IdeaOur initial idea had to do with analyzing posts from popular politicians to see if their sentiment followed the actual stock value, and how much money it would have madeUpdated IdeaDue to Twitter API costs, we switched to a well-known Reddit community, WallStreetBets, which is known for risky, high-reward stock picks, aggressive trading strategies, and a rebellious, meme-driven culture that challenges traditional Wall Street norms.What it does: Our program first collected all the posts from the past month on the WSB (WallStreetBets) Reddit community, with information such as the title, body text, upvotes, date posted, and top 10 comments to analyze if the post's sentiment was supported by the community. Then we used Gemini API to extract the stock ticker and post sentiment to see which stocks were to be bought and which to be shorted. Next, we ran a simulation through our data to simulate a portfolio for the WSB community in the pasts month. We also created a website to hold this information, with the WSB portfolio being compared to the S&P 500. This website also hosts a chatbot that can give stock recommendations based on our analysis of each post's sentiment.How we built it: We used Gemini to analyze post sentiment and for the chatbot. We used flask for the backend of the website and react for the frontend. The simulation was done using Python.Challenges we ran into: We ran into problems with different ways to parse the tickers and sentiment from the post. We tried using regex and nlp but decided to use Gemini because those methods were not very good. The front-end implementation and simulation implementation were very difficult as well.Accomplishments that we're proud of: We are proud of utilizing the Gemini API for post-analysis and the chatbot. We are also proud of being able to complete this project in 24 hours and have a functional backend and frontend as well.What we learned: We learned how to use flask, react. We also learned how to use the Gemini API for analyzing post sentiment.What's next for Wallstreetbets Analyst: We are currently working on the final touches for the website and finishing up the backend/frontend",
                        "github": "https://github.com/georgejiang2/WSB",
                        "url": "https://devpost.com/software/wallstreetbets-analyst"
                    },
                    {
                        "title": "Nittany Bites",
                        "description": "A web-based platform where Penn State dining locations and cafes can list surplus food at a discounted rate. Students can browse, reserve, and pick up meals before they go to waste.\r\n",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/nittany-bites"
                    },
                    {
                        "title": "Dumb Ways to Kill a Mosquito",
                        "description": "\"Dumb Ways to Kill a Mosquito\" is a game about finding the best ways in each scenario to squash mosquitos, blends click-drag mechanics, silly physics, and fast action.",
                        "story": "Inspiration: We were inspired by the classic flash games we all loved growing up, bringing back that nostalgic, fast-paced fun. We mixed in the creativity of games like Magic Cat Academy and the chaos of Rock-Paper-Scissors for a unique twist.What it does: \"Dumb Ways to Kill a Mosquito\" combines quirky click-and-drag mechanics, fun physics, and dynamic challenges. Players draw, battle mosquitoes in Rock-Paper-Scissors, and solve puzzles in hilarious, chaotic ways.How we built it: We used Unity to bring the game to life, leveraging its physics engine for the interactive elements and combining different gameplay styles (clicking, dragging, drawing) to create a smooth, fun experience.Challenges we ran into: Balancing the physics and keeping the gameplay fluid was tricky. We had to fine-tune the interactions so they felt satisfying but not overly complicated. Additionally, creating a unique yet familiar experience was a challenge.Accomplishments that we're proud of: We managed to capture the spirit of those old-school flash games while adding a fresh spin. The combination of mechanics\u2014drawing, clicking, and Rock-Paper-Scissors\u2014turned out to be more fun than we imagined.What we learned: We learned the true value of friendship\u2014this project really tested our bonds and our willingness to stay awake through late-night dev sessions. It\u2019s all about teamwork, resilience, and caffeine!What's next for Dumb Ways to Kill a Mosquito: We\u2019re looking to expand the levels, introduce new mechanics, and add even more wacky ways to tackle those pesky mosquitoes. We\u2019re also considering a mobile release to reach a wider audience.",
                        "github": "https://github.com/WillNguyen228/Button-Game-Project.git",
                        "url": "https://devpost.com/software/dumb-ways-to-kill-a-mosquito"
                    },
                    {
                        "title": "Path indicator",
                        "description": "A stuff that is useful in showing the road situation on your path. Including peak times to work, road slippery, unexpected drunk or addicted people and even criminal information locations. ",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/path-indicator"
                    },
                    {
                        "title": "Timely",
                        "description": "AI powered chat based scheduling application",
                        "story": "",
                        "github": "https://github.com/JaeMinBird/timely",
                        "url": "https://devpost.com/software/timely-6begip"
                    },
                    {
                        "title": "CKD Prediction",
                        "description": "This project utilizes artificial intelligence and ensemble learning techniques to create an accessible, web-based tool for early prediction of CKD by users through entered lab values. ",
                        "story": "",
                        "github": "https://github.com/srdtrfztguzh/PSU-2025.git",
                        "url": "https://devpost.com/software/ckd-prediction"
                    },
                    {
                        "title": "Hyperweb",
                        "description": "Hyperweb is a website that allows you to create your own websites without any previous coding knowledge.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/hyperweb"
                    },
                    {
                        "title": "ShopTheLook",
                        "description": "ShopTheLook turns your vibe into a complete outfit you can shop instantly\u2014no styling stress, no endless searching. Just say the mood, and let Gemini-powered AI handle the rest.\r\n",
                        "story": "",
                        "github": "https://github.com/akashbaidya015/shopthelook",
                        "url": "https://devpost.com/software/shopthelook-2funvs"
                    },
                    {
                        "title": "ClauseSense",
                        "description": "ClauseSense ingests legal docs, retrieves key clauses using advanced AI retrieval, and transforms complex legal language into clear, actionable reports for smarter decisions.",
                        "story": "As students navigating hackathons, internships, and early-stage startups, we constantly ran into dense legal documents\u2014NDAs, term sheets, and incorporation papers\u2014that were difficult to interpret without legal help.\nWe thought: what if we could build an AI agent that reads legal docs for you, finds what matters, and explains it clearly\u2014instantly? That's how ClauseSense was born.It is an AI-powered legal document analyzer that ingests legal files, retrieves relevant clauses using semantic search, and simplifies complex legal language into actionable, plain-English reports.Document Ingestion:\nWe used python-docx to extract text from model legal documents (Voting Agreement, SPA, IRA, etc.).Clause Segmentation:\nUsing regex and heuristics based on legal structure (e.g., \"WHEREAS\", \"Section\"), we split long documents into clause-sized chunks.Vectorization & Indexing (RAG):We embedded all clauses using sentence-transformers (all-MiniLM-L6-v2) and indexed them in FAISS.User queries are embedded and matched against this index to retrieve semantically similar clauses.Simplifier Agent:\nEach retrieved clause is passed to a T5-based summarizer, which simplifies complex legal language into understandable text.FastAPI Backend:\nWe wrapped the logic into an API endpoint (/report) that accepts queries and returns legal reports.Frontend Interface:\nWe built a lightweight HTML/JavaScript UI that interacts with the API, lets users type legal questions, and displays the resulting reports.Challenges we ran into: Model Accuracy:\nPretrained models often hallucinated or oversimplified legal clauses. We plan to fine-tune the summarizer on domain-specific datasets for better results.Document Parsing Complexity:\nLegal documents don\u2019t follow consistent formatting. Segmenting them cleanly without breaking context was tricky.CORS and Frontend Integration:\nConnecting a local HTML interface to our FastAPI backend required handling CORS issues and proper request formattingClauseSense can take any legal document (in .docx format), extract clauses, semantically retrieve the most relevant ones for a user\u2019s query, and rewrite them in simpler language\u2014all within seconds.",
                        "github": "https://github.com/ams11271/hackpsu_ai_agents.git",
                        "url": "https://devpost.com/software/clausesense"
                    }
                ],
                [
                    {
                        "title": "Apartment_Finder",
                        "description": "Apartment Finder",
                        "story": "Inspiration: We have had some issues in the past when searching for apartments in State College, so we wanted to make something to make the process easier.What it does: Our apartment finder lets the user search for apartments with a simple text prompt which then returns recommended apartments.How we built it: We used React and Firebase, with help from AI.Challenges we ran into: Data collection was quite difficult, and we also had trouble with the Gemini API, which we were able to fix.Accomplishments that we're proud of: We are very happy with integrating Gemini into our project and having a practical website.What we learned: We gained a lot of useful experience with Firebase.What's next for Apartment_Finder: We could add more apartment data and implement the option to search for subleases.",
                        "github": "https://github.com/Hellothereobi123/SAT_Hackpsu_2025",
                        "url": "https://devpost.com/software/apartment_finder"
                    },
                    {
                        "title": "QuestLog",
                        "description": "\"QuestLog transforms your daily tasks and goals into an engaging adventure. By combining AI-powered journaling with gamification, it turns your personal development journey into an epic quest.\"",
                        "story": "Inspiration: The group wanted to encourage people to go outside more, and encourage people to make the most out of otherwise mundane tasks. Side quests were built to be fun and recommend positive life style choices.What it does: Analyzes journal entries using ChatGPT to generate personalized quests\nCreates both main quests (primary goals) and side quests (smaller tasks)\nProvides AI-generated insights and suggestions based on user progress\nUses a \"DATA FRAGMENTS\" reward system to gamify task completion\nBuilds personalized schedules based on user preferences and task priorities\nAdapts quest difficulty and suggestions based on user performanceHow we built it: Frontend: Next.js with React for a modern, responsive UI\nBackend: Firebase for real-time data storage and authentication\nAI Integration: OpenAI's GPT-4 for intelligent analysis and quest generation\nStyling: Tailwind CSS for consistent, modern design\nState Management: React hooks for efficient data handling\nAPI Routes: Next.js API routes for secure AI communicationChallenges we ran into: Fine-tuning prompt engineering for consistent JSON responsesAccomplishments that we're proud of: Created a scalable Firebase data structure\nDeveloped an engaging and intuitive interfaceWhat we learned: Importance of proper error handling in AI integrations\nHow to make AI suggestions feel personal and relevantWhat's next for QuestLog: Mobile app development\nSocial features for quest sharing and collaboration",
                        "github": "",
                        "url": "https://devpost.com/software/questlog-cwpmtb"
                    },
                    {
                        "title": "commenTrix",
                        "description": "A dashboard for youtube creators to learn more about their audience through sentiment analysis on the comment over their videos. ",
                        "story": "Inspiration: With the rise of user-generated content across platforms like YouTube, comment sections have become a vital space for feedback, discussion, and community building\u2014but they are often cluttered with toxicity, irrelevant chatter, or overwhelming volume. commenTrix was born out of the need to create a semantic dashboard that helps users, content creators, and moderators better understand the emotional tone, relevance, and thematic structure of their comment sections using modern NLP techniques.What it does: commenTrix is a semantic analysis dashboard designed for YouTube comments. It automatically performs:Ternary Sentiment Classification (positive, negative, neutral)Emotion Detection (e.g., joy, anger, fear, surprise)Aspect-Based Sentiment Analysis to identify sentiments linked to specific topics or featuresToxicity Detection to highlight harmful or offensive contentAll insights are visualized in an interactive frontend that enables users to filter, explore, and make sense of their comment ecosystem efficiently.How we built it: We designed commenTrix with a modular architecture:Backend (Python): Built using FastAPI and integrated with NLP models from Hugging Face Transformers for sentiment, emotion, and aspect extraction.Frontend (React.js): Displays the dashboard with charts, filters, and keyword highlights using Styled Components and Recharts.Database: Comment data is stored and queried through a Cloudflare-hosted PostgreSQL setup.YouTube API: Fetches comments and metadata using video links provided by users.Multi-task NLP pipeline: We trained and fine-tuned models to run multiple analyses simultaneously with optimized performance.Challenges we ran into: Balancing model performance and inference speed for large-scale comment sectionsIntegrating multi-task learning pipelines without excessive latencyParsing and cleaning YouTube comment data, which often contains slang, emojis, and mixed languagesEnsuring frontend reactivity and scalability for dynamic filtering and displayAccomplishments that we're proud of: Successfully integrated multiple NLP tasks into a unified, multi-task inference modelBuilt a complete end-to-end system with both backend APIs and a clean frontend dashboardCreated a solution that can scale across video comment sections of any size with meaningful analyticsWhat we learned: The power of multi-task NLP models in reducing resource overheadBest practices in building scalable frontend-backend pipelinesReal-world challenges of working with user-generated data and making it interpretableHow to use cloud services and deployment tools like Cloudflare and Vercel for robust architectureWhat's next for commenTrix: Add support for multilingual comment analysisImprove real-time comment monitoring for creators and moderatorsLaunch a Chrome extension for on-page semantic insightsExpand to other platforms like Twitter and RedditOffer custom model fine-tuning for niche domains (e.g., education, finance, gaming)",
                        "github": "https://github.com/Swayam-Bansal/commenTrix",
                        "url": "https://devpost.com/software/commentrix"
                    },
                    {
                        "title": "EcoSplit",
                        "description": "Split bills. Share impact. Go green together.",
                        "story": "",
                        "github": "https://github.com/yadavallimodak/HackPSUEcoSplitProj.git",
                        "url": "https://devpost.com/software/ecosplit"
                    },
                    {
                        "title": "CrewUp: A group movie recommender",
                        "description": "CrewUp: group movie recs. Create accounts, set prefs, get tailored picks. KMeans clusters group tastes; TMDB API fetches movie info. SQLite stores data. Enjoy seamless movie nights!",
                        "story": "",
                        "github": "https://github.com/Hawlll/movie_recommender_system",
                        "url": "https://devpost.com/software/crewup-a-group-movie-recommender"
                    },
                    {
                        "title": "Pomen",
                        "description": "AI-powered. Scam-free. Your trusted car repair assistant.\r\n",
                        "story": "",
                        "github": "https://github.com/faieweldan/pomen-ai2",
                        "url": "https://devpost.com/software/pomen"
                    },
                    {
                        "title": "Nittany Exchange",
                        "description": "Nitanny Exchange is a website where Penn State students can more reliably purchase items from other students.",
                        "story": "",
                        "github": "https://github.com/ykp5138/Hackthon-Spring-2025",
                        "url": "https://devpost.com/software/nittany-exchange"
                    },
                    {
                        "title": "MyMuscle",
                        "description": "Your AI-powered fitness coach in your pocket \u2014 track your form, count reps, get real-time feedback, and crush your goals with personalized workouts.",
                        "story": "",
                        "github": "https://github.com/mantavya0807/fitness-pose-coach",
                        "url": "https://devpost.com/software/mymuscle"
                    },
                    {
                        "title": "TypingUpTomorrow",
                        "description": "We are TypingUpTomorrow, an interactive typing app meant to help ESL students' learn how to type in English.",
                        "story": "The digital divide is real, and we've seen its impact firsthand in our diverse community. Many students struggle with technology, putting them at a disadvantage in their education. We wanted to create a tool that makes learning accessible, helping ESL students not just type, but type with confidence.TypingUpTomorrow is an AI-powered typing tool designed for ESL students. Using age and grade level as inputs, our system generates personalized typing prompts that align with real-world learning standards. The interface prioritizes accessibility with features like high-contrast visuals and a strikethrough function for mistakes.Through analytics, the system identifies common errors and adapts future prompts to reinforce challenging letters. After three completed exercises, users receive detailed insights on their words per minute and their most inconsistent keystrokes, empowering them to track and improve their skills.Our system follows a structured pipeline:User Input (age/grade) \u2192 GeminiAPI \u2192 Adaptive Prompt Generation \u2192 Personalized Difficulty ScalingWe used GeminiAPI to generate tailored prompts based on the user\u2019s grade level, ensuring that exercises progress in complexity alongside the learner\u2019s skills.Connecting the backend with the frontend for seamless communicationHandling HTTP requests efficiently,We successfully implemented features and technologies that were entirely new to us, broadening our development skills along the way.How to build withReact and Flask, both of which were new to usEffectiveteam collaborationusing a shared GitHub repositoryHow to divide work into smaller, focused teams to streamline development,Expanding support formultiple languagesAddingaudio feedbackfor visually impaired usersEnhancing theprompt generatorto create more personalized and challenging exercises,",
                        "github": "https://github.com/AdjacentRhyme67/hackpsu2025",
                        "url": "https://devpost.com/software/rajinikanth-project"
                    },
                    {
                        "title": "CommonGround",
                        "description": "L",
                        "story": "Inspiration: CommonGround was born out of a shared challenge among our team members\u2014feeling hesitant to attend campus events because we didn\u2019t know anyone in the hosting organizations. We wanted to bridge this gap by creating an app that helps students connect others within the same major or that have similar interests who are attending events across different organizations. Our goal was to foster a stronger sense of community at Penn State.What it does: CommonGround allows students to connect with others attending campus events, even if they are from different organizations. The app displays attendees based on shared majors and interests, making it easier for users to find familiar faces and make meaningful connections before and during events.How we built it: Challenges we ran into: Accomplishments that we're proud of: What we learned: How to connect a Next.js frontend with a Python backend.The importance of clear documentation and debugging in web development.The lifecycle of app development\u2014from ideation to prototyping and implementation.What's next for CommonGround:",
                        "github": "https://github.com/AdithyaBadrinath/HackPSUSp2025",
                        "url": "https://devpost.com/software/common-ground-wgady2"
                    },
                    {
                        "title": "Sync Lab",
                        "description": "A collaborative work environment designed for business and schools.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/sync-lab"
                    },
                    {
                        "title": "GestureAI",
                        "description": "Train, Bind, Predict. GestureAI allows you to use your webcam, a chronically underused part of every computer, to bind hand gestures to any keyboard & mouse action.",
                        "story": "Inspiration: As big fans of video games, our ideas came from quite an unexpected place, online streamers. We saw these streamers playing games with the weirdest possible controls, i.e. beating games with a guitar, and thought how cool it would be to create something new ourselves and push the limits of what's possible.What it does: Our website allows users to train their own machine learning models to recognize specific hands gestures. After binding them to keyboard or mouse inputs, the user can then control their computer using these custom gestures, completely hands-off. This opens up a ton of opportunities to both play game in a more unorthodox manner and perform more productive tasks such as controlling presentation slides at a distance.How we built it: We utilized multiple python libraries for this projects, including:pyAutoguifor keyboard & mouse controlTensorflowfor machine learning/model creationFlaskfor backend web developmentMediapipefor providing hand segmentation data,Challenges we ran into: It was also our first hackathon and first experience with tensorflow and machine learning. One of the biggest challenges that we've faced was porting the cli code into a web development framework.Accomplishments that we're proud of: We're very proud of how smooth the experience of training and prediction is for the user. This was our first hackathon and we are all very proud of the progress we made, especially since we used so many new technologies.What we learned: We've learned how to create tensorflow models using data from a different machine learning model.What's next for GestureAI: The next step is to allow for binding gestures to a macro instead of just a single action.",
                        "github": "https://github.com/LennyWei/HackPSU/",
                        "url": "https://devpost.com/software/gestureai-8yz120"
                    },
                    {
                        "title": "ReWryte",
                        "description": "ReWryte streamlines AI-powered editing, making writing and reviewing documents faster and more efficient.",
                        "story": "",
                        "github": "https://github.com/emipalmer/hackpsu-project",
                        "url": "https://devpost.com/software/rewryte"
                    },
                    {
                        "title": "Wander AI",
                        "description": "Your AI Travel Assistant",
                        "story": "Wander AI: The Story Behind the Innovation\nThe Spark of Inspiration \ud83d\udd25\nIt all started with a simple yet frustrating experience\u2014planning a trip. Scrolling through endless travel blogs, juggling multiple tabs for flights, hotels, and itineraries, and trying to fit everything into a budget felt overwhelming. Everyone loves traveling, but the planning process? Not so much. That\u2019s when the idea hit: What if AI could handle the stress of planning, making travel seamless and personalized?The Vision \ud83c\udf0d\nWe envisioned Wander AI as more than just another travel app\u2014it would be a smart, AI-powered travel assistant that understands your interests, budget, and preferences to craft the perfect trip. Whether you\u2019re searching for a destination, need the best flights and hotels, or want a fully planned itinerary, Wander AI would handle it all.From Idea to Execution \ud83d\udca1\u2192\ud83d\udcbb\nWith the hackathon as our battleground, we brought Wander AI to life. The challenge? Making AI truly intuitive for travelers. We broke it down into three seamless steps:The AI Travel Assistant \ud83e\udd16 \u2013 Using the Google Gemini API, we created a chatbot that helps users decide on a destination based on their interests. If they already have one in mind, the chatbot gathers budget and activity preferences to create a tailored itinerary.Flight & Hotel Finder \u2708\ufe0f\ud83c\udfe8 \u2013 Once a destination is chosen, we scrape the Gemini API response to find the best flights and hotels using the Google Flights API & Google Hotels API/Serp AI.Itinerary Generator \ud83d\udcc5 \u2013 Finally, based on the user\u2019s duration and interests, Wander AI leverages Gemini API again to generate a detailed, day-by-day itinerary.The Tech That Powers the Magic \ud83d\udee0\ufe0f\nBuilding Wander AI required an efficient and scalable tech stack:Frontend: React, HTML, CSS \u2013 for a clean and intuitive user interface.Backend: Node.js, Python \u2013 to process API calls and handle data.APIs: Google Gemini API, Google Flights API, Google Hotels API, Serp AI \u2013 for AI-powered recommendations and real-time travel data.Challenges & Breakthroughs \u26a1\nLike any ambitious project, we faced hurdles:Ensuring AI-generated responses were accurate and relevant. We fine-tuned prompts and handled JSON parsing efficiently.Integrating multiple APIs smoothly. We optimized data retrieval to ensure real-time responses.Creating an engaging and user-friendly experience. We focused on intuitive UI/UX design to make trip planning effortless.The Future of Wander AI \ud83d\ude80\nThis is just the beginning. In the future, we aim to:\n\u2705 Add real-time price tracking for flights and hotels.\n\u2705 Integrate user accounts for personalized trip histories.\n\u2705 Offer AI-generated travel blogs and local experience suggestions.\n\u2705 Partner with travel companies to enhance recommendations.Why Wander AI? \ud83c\udf1f\nWander AI is more than a hackathon project\u2014it\u2019s a step toward the future of travel. By blending AI with human curiosity, we\u2019re making trip planning stress-free, smart, and personalized.The world is waiting. Let Wander AI take you there. \u2708\ufe0f\ud83c\udf0d",
                        "github": "https://github.com/Keshav-kh/WANDER-AI",
                        "url": "https://devpost.com/software/wander-ai"
                    },
                    {
                        "title": "Hangman ",
                        "description": "Guess smart. Hang smooth. A sleek and responsive take on the classic word-guessing challenge.",
                        "story": "Inspiration: The classic game of Hangman has always been a simple yet engaging way to challenge vocabulary and deduction skills. I wanted to reimagine this childhood favorite with a modern, smooth, and responsive design, giving it a fresh feel while keeping its timeless gameplay. The goal was to create a lightweight word-guessing game that could work across devices, with intuitive UI and satisfying feedback.How we built it: HTML5, CSS3 (with TailwindCSS) for layout and styling\nJavaScript (React) for interactivity and game logic\nVite for fast development and hot reloading\nResponsive design principles to ensure the game works smoothly on desktop and mobile\nA clean and user-friendly interface with animations and transitions for a modern feelChallenges we ran into: Balancing minimalism with engaging visuals, making the game look polished without overwhelming the user\nManaging edge cases like repeated guesses or unexpected input\nEnsuring the game was fully responsive and intuitive on smaller screens\nKeeping the code clean and efficient while allowing for future expansions (like difficulty levels or categories)What we learned: How to break down a game into small, reusable components\nManaging state effectively for gameplay (e.g., tracking guesses, word completion, game over)\nCreating smooth user interactions using CSS animations and transitions\nUsing modular code for scalability and easier maintenance\nDesigning UI/UX that's both functional and visually appealing",
                        "github": "",
                        "url": "https://devpost.com/software/hangman-yf9bq1"
                    },
                    {
                        "title": "InSync",
                        "description": "Your AI-powered coding assistant for real-time fixes, smarter debugging, and seamless coding!",
                        "story": "Inspiration: There are many AI-powered code assistantsextensions available for VSCode, but most lack real-time debugging capabilities. Additionally, they often provide inaccurate recommendations. Wanted to create a tool that ensures precise, real-time assistance while enhancing the overall coding experience.What It Does: InSyncis an AI-powered VSCode extension that offers:Multilingual Support\u2013 Debugs and assists with all programming languages.Real-Time Debugging\u2013 Provides line-by-line error detection and corrections as you code.Guilt-Free Hints\u2013 Users get Hints when clicked \"Request Help\" instead of completely relying on AI codeSession Timer\u2013 Tracks coding time for productivity monitoring.Reference Suggestions\u2013 Recommends relevant documentation and resources,How We Built It: Technologies Used:NPM, Node.js, and the Gemini API for real-time analysis and recommendations.,Challenges We Faced: Ensuring real-time performance without lag.Limited API Response due to the free versionImproving the accuracy and relevance of AI-generated suggestions.Designing an intuitive and user-friendly interface.,Accomplishments We're Proud Of: \u2728It provides real-time, line-by-line debugging!\u2728What We Learned: Optimizing AI-generated recommendations for better accuracy.Enhancing real-time interactions in a VSCode extension.Streamlining user experience for seamless integration into the coding workflow.,What's Next for InSync: Voice Command Integration\u2013 Enable hands-free coding assistance.Code Snippets Library\u2013 Allow users to save and quickly insert frequently used code patterns.,",
                        "github": "https://github.com/Sahithya2003/InSync.git",
                        "url": "https://devpost.com/software/insync-wm85v2"
                    },
                    {
                        "title": "Simplify",
                        "description": "A groundbreaking tool for the simplification of legal documents, particularly terms of service. ",
                        "story": "",
                        "github": "https://github.com/SeanGunn-ter/Simplify",
                        "url": "https://devpost.com/software/simplify-k6qfin"
                    },
                    {
                        "title": "BareBonez",
                        "description": "Never forget what matters\u2014your words, remembered.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/barebonez"
                    },
                    {
                        "title": "CyberScribe",
                        "description": "CyberScribe turns images into PDFs and audio with a neon flair. Fast, stylish, and accessible\u2014snap, convert, listen in seconds.",
                        "story": "Inspiration\nCyberScribe was sparked by the need for a futuristic, accessible tool to convert images into usable formats, blending neon aesthetics with practical innovation.What it does\nCyberScribe transforms images into PDFs and audio, offering a dyslexia-friendly output and a sleek interface for quick, stylish conversions.How we built it\nWe built it with Flask, Azure Computer Vision for text extraction, FPDF for PDFs, gTTS for audio, and a neon-themed HTML/CSS frontend.Challenges we ran into\nWe tackled file upload issues, font integration hiccups, and Git push errors, refining our debugging and teamwork skills along the way.Accomplishments that we're proud of\nWe\u2019re proud of creating a visually stunning, functional app with real-time image previews and seamless PDF/audio output.What we learned\nWe learned Flask routing, Azure API integration, responsive design, and Git version control\u2014skills that leveled up our dev game.What's next for CyberScribe\nNext, we\u2019ll add multi-language support, enhance audio customization, and deploy it online for broader access.",
                        "github": "https://github.com/sang773/panthers.docx",
                        "url": "https://devpost.com/software/cyberscribe"
                    },
                    {
                        "title": "Verdict Academy",
                        "description": "The next revolution in Higher Education",
                        "story": "",
                        "github": "https://github.com/amartya16proxtx/Verdict-Academy",
                        "url": "https://devpost.com/software/verdict-academy"
                    },
                    {
                        "title": "GMB-Vocab",
                        "description": "This intelligent vocabulary flashcard web app helps users understand how a word is used in a sentence. We also provide mini-quizzes to help with memorizing.",
                        "story": "",
                        "github": "https://github.com/shauangel/GMB-Vocab.git",
                        "url": "https://devpost.com/software/gmb-vocab"
                    },
                    {
                        "title": "Hack-Match",
                        "description": "Idea: A next generation group meeting web application.",
                        "story": "",
                        "github": "https://github.com/Skylarking212/Merge-A-Tron",
                        "url": "https://devpost.com/software/hack-match"
                    },
                    {
                        "title": "TBD",
                        "description": "TBD",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/large-language-defender"
                    },
                    {
                        "title": "Devscan",
                        "description": "AI-Powered Hackathon Project Analysis",
                        "story": "Inspiration: During hackathon judging, evaluators face a challenge: they need to quickly understand and assess dozens of complex projects within a limited timeframe. We wanted to create a tool that would help judges, mentors, and other hackathon participants efficiently analyze projects by extracting key technical insights and presenting them in an accessible format.What It Does: DevScan is a web application that uses AI to analyze hackathon projects from DevPost and their associated GitHub repositories. It provides:Automated Technical Analysis: Extracts key features, implementation details, and complexity assessmentsProject Scoring: Evaluates projects based on technical merit, complexity, and feature completenessAdvanced Search: Find projects by keywords, technologies, or specific criteriaVisual Insights: Clean, intuitive UI that presents technical information in digestible formats,To start just replace the \"https://devpost.com\" => \"https://devscan.club\" and view the amazing analysisHow We Built It: DevScan is built with a modern tech stack focused on performance and scalability:Next.js 14: For server-side rendering and optimal performanceReact 19: For building the responsive UI componentsMongoDB: For storing project data and analysis resultsGoogle Gemini AI: For natural language processing and technical analysisGitHub API: For accessing repository content and code analysisTailwindCSS: For beautiful, responsive styling,The architecture follows a modular approach with clear separation of concerns:Server components for data fetching and initial renderingClient components for interactivityAPI endpoints for external integrationsDedicated services for analysis operations,Challenges We Ran Into: Rate Limiting: Managing API rate limits across GitHub and Gemini APIData Extraction: Reliably extracting structured data from unstructured project descriptions*Performance Optimization *: Ensuring fast search and analysis operations even with complex dataSearch Relevance: Creating a search algorithm that returns truly relevant resultsHydration Errors: Resolving React hydration mismatches between server and client rendering,Accomplishments That We're Proud Of: Built a comprehensive project search system with filtering and paginationCreated a sophisticated AI analysis pipeline that extracts meaningful technical insightsImplemented a responsive, accessible UI that works across device sizesAchieved high performance with server-side rendering and optimized database queriesDeveloped comprehensive documentation for future contributors,What We Learned: Advanced Next.js patterns for mixing server and client componentsTechniques for efficient MongoDB text searching and indexingStrategies for optimizing large language model promptsMethods for handling asynchronous operations in React applicationsBest practices for TypeScript error handling and type safety,What's Next for DevScan: Machine Learning Model: Train a custom model on hackathon project data to improve analysis accuracyTeam Analysis: Add functionality to analyze team composition and contributionsReal-time Collaboration: Enable judges to share notes and evaluationsIntegration with MLH: Connect directly with Major League Hacking's platformMobile App: Develop a companion mobile application for on-the-go analysis,Try It Out: Check out our project atdevscan.cluband explore the source code onGitHub.",
                        "github": "https://github.com/BeLazy167/Devscan",
                        "url": "https://devpost.com/software/devscan"
                    }
                ],
                [
                    {
                        "title": "xyloweft",
                        "description": "Using Google's gemini model, xyloweft aims to assist with students' learning journey with engaging VR visualizations. Recording the words, xyloweft creates an unique, engaging playground for learning.",
                        "story": "Inspiration: From studying PHY 211, PHY 212, and EMECH 212 at Penn State, we noticed many students struggle with the course materials because they couldn't visualize words, equations, and diagrams on paper. To help ourselves and our fellow students engage and understand those challenging course materials better, we wanted to build a program that provides a comprehensive visualization of what the bland words and equation mean in real life.What it does: Using Gemini's API, Houdini, and Meta Quest 3, the program takes live audio inputs from the user and generates, modifies, and/or interacts with objects simultaneously in the virtual playground.How we built it: We first gathered the API keys from Gemini and tested how we can optimize the output with different prompts. At the same time, two of us were working on setting up the headset and the virtual playground. Finally, we intertwine one section with another so that our product flows smoothly upon demand.Challenges we ran into: From the basic localization of whisper(we forgot to install ffmpeg and hence have to wait for 8 minute to transcribe a 5 second audio) to the design of structures (couldn't save json file for unknown reason, cost us 2 hours), we've overcome what we've run into, and here we are. Our favourite bug that we encountered would be one of the AI literally returned us \"negative 5\" when we asked for some axis stuff. lovely result, though, didn't expect to be able to come this far in 24 hours.Accomplishments that we're proud of: In general, we are proud of the fact that we went from knowing absolutely nothing about VR to finishing this project. We are proud of our determination and our ability to tackle various issues on the journey.What we learned: In general, we have learned a lot about how LLM models and VR environment works in real life. More importantly, we have learned to debug and problem-solve as young engineers. Behind the scenes, we have learned about different tools and modules programmers incorporate to improve the performance, like CUDA and ffmpeg for example. For the backend, we have learned more about the pricing, usage of LLM as well as the importance of prompt engineering. For the front end, we have learned more about how to better synchronize with the backend and manage data for better presentation.What's next for xyloweft: After this Sunday, we will continue working on this project to ultimately launch an app for the program. Going forward, we will seek opportunities to collaborate with research professors and industry professionals to enhance the functionality and overall performance of the program.",
                        "github": "https://github.com/noxusCastrator/xyloweft",
                        "url": "https://devpost.com/software/xyloweft"
                    },
                    {
                        "title": "InFlow",
                        "description": "InFlow is an AI-powered planner that breaks down tasks and boosts focus with smart Pomodoro flow. Built to help students stay on task, stay in flow. ML meets timeless productivity.",
                        "story": "Inspiration: We were inspired by the common feeling of overwhelm students face when trying to start big assignments. Many of us have stared at a blank screen, unsure how to begin or break down what we need to do. Traditional planners and productivity tools assume we already know how to plan, prioritize, and stay focused \u2014 but many students need support with that part too. We wanted to build something that bridges the gap between intention and execution: a tool that meets students where they are and guides them, one clear step at a time.What it does: InFlow is an AI-powered productivity assistant that helps students:Generate a structured, multi-day to-do list from a single promptRegenerate that plan if it doesn\u2019t suit their needsUse a built-in Pomodoro timer to stay focused and avoid burnoutClick a \ud83d\udca1 lightbulb button on each task for tips, tricks, and an explanation of why that step mattersTrack their progress visually, all within a calming dual-pane interface\nThe result: students can go from\u201cI don\u2019t know where to start\u201dto\u201cI know exactly what to do next.\u201d,How we built it: We built InFlow using:HTML, CSS, and JavaScript for a clean, responsive frontendOpenAI\u2019s GPT model to generate task breakdowns and lightbulb tipsPrompt engineering to tailor task generation and explanationsA custom task parsing engine that adjusts task duration and urgency across multiple daysBuilt-in Pomodoro timer and progress bar logic using vanilla JSEverything runs client-side for a seamless experience with no login or backend required.,Challenges we ran into: Prompt tuning: it took time to get OpenAI to return task lists that felt human, structured, and relevant across different prompt types.Designing a plan regeneration system that doesn\u2019t feel frustrating or repetitiveKeeping the UI minimalist while packing in meaningful functionality \u2014 balancing flexibility with simplicityAligning the visual layout to make the checklist, date headers, and timer feel cohesive and intuitive,Accomplishments that we're proud of: We created a polished, functional prototype in under 24 hours \u2014 complete with real-time AI integration.The lightbulb feature is something we haven\u2019t seen in other planning tools \u2014 it gives students not just structure, but insight.,What we learned: Simplicity is powerful \u2014 the most helpful features are often the ones that feel invisible.Prompt design is an art. A small change in wording made the difference between a vague output and a clear, actionable task list.Not all students need the same structure \u2014 flexibility and regeneration make InFlow feel personal and empowering.A clean UI goes a long way in keeping users engaged and not overwhelmed.,What's next for InFlow: User accounts and saved progress, so students can come back to their plans across devicesTask difficulty sliders to adjust cognitive load (easy \u2192 deep work)More lightbulb types (motivation boosts, checklist templates, calming reminders)Voice input for accessibilityA campus rollout model \u2014 partnering with wellness centers to distribute InFlow to students who need support managing their mental workload,",
                        "github": "https://github.com/sarahshahrir/hackpsu25",
                        "url": "https://devpost.com/software/to-do-1a6opt"
                    },
                    {
                        "title": "FormFriend",
                        "description": "New country, endless forms, confusing language? FormFriend translates, simplifies, and guides you, so you can enroll, apply, and access what matters. Let\u2019s make paperwork painless, one form at a time.",
                        "story": "",
                        "github": "https://github.com/rishitap25/HACK25",
                        "url": "https://devpost.com/software/formfriend"
                    },
                    {
                        "title": "Food Flow",
                        "description": "Food Flow is a volunteer app created to reduce food waste and food insecurity. Restaurants, organizations individuals that make a surplus of food can have this food be delivered to those who need it.",
                        "story": "Inspiration: The inspiration of this project was that we wanted to assist those struggling with food insecurity while also reducing food waste.What it does: It connects individuals, restaurants, or organizations that have surplus food to those in need.How we built it: We made an initial UI design, Attempted to turn that design into a rough program with a google maps API and then made a revised UI design with additional features such as a second UI for the driver's view.Challenges we ran into: Implementing the Google maps API and associated custom style sheet in order to get real-time restaurant location data was a struggle of ours, and costed us a significant amount of time in the programming process. We were able to get real-time restaurant names, locations, and data, but we were unable to pull meal-related data associated with the correct restaurants. Instead of implementing and pulling from the official Google maps API like we would likely use for a long-term project, we elected to create demo restaurants using a similar API in order to demonstrate all of the functionalities we created. We also had trouble connecting the databases between the Customer and the Driver sides of our application, which we eventually remedied by adding a temporary array when the file is launched.Accomplishments that we're proud of: We were able to implement the Google maps API to work with the example restaurants and the Geo-location function.What we learned: We learned a lot about teamwork and adapting to each other work styles. Making a website UI and using an array.What's next for Food Flow: We definitely would like to expand this website and add more functionality, especially implementing a live Google maps API. Creating a persistent MongoDB database to hold the orders as well.",
                        "github": "",
                        "url": "https://devpost.com/software/food-flow"
                    },
                    {
                        "title": "Omelody",
                        "description": "Our goal was to make a site\u00a0that put its users in slightly randomized chat rooms based on their imported playlist. ",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/omelody"
                    },
                    {
                        "title": "T.I.M.E",
                        "description": "T.I.ME an interactive chatroom features Einstein and Turing as well as Monroe conducting private conversations with students to make their historical figures come alive through personalized dialogue.",
                        "story": "Inspiration: We found the direct AI-based communication with historical figures to be incredibly interesting. The possibility to engage in relaxed conversations with Albert Einstein Marilyn Monroe and Alan Turing enables direct encounters with their distinctive personality traits. Our objective was to offer an interactive experience that allows users to conduct natural conversations with historical figures.What it does: T.I.M.E (Talkative Interactive Multi-agent Experience) enables users to have normal conversations with AI-controlled historical figures with different characters and distinctive information knowledge. The application implements controlled system instructions together with protective measures to uphold character consistency throughout interactive experiences that mimic natural human behavior.How we built it: T.I.M.E received its development through Python and Flask combined with Gemni Modesl to establish conversational functionality. Through Flask the system processes API demands from the client interface which leads the dialogue execution with agent management to sustain discussion coherence. The system integrates through REST APIs which provides immediate chat functionality to end users. Our system uses multi-threading methods to maintain high performance which allows simultaneous agent-consumer communication.Challenges we ran into: Our main difficulty included sustaining character consistency and stopping the agents from impersonating one another. The system displayed unintended character deviation in specific circumstances together with improper prefix insertion and incorrect responses. The system faced these problems through our deployment of extensive validation protocols and response purification mechanisms that enhanced prompts and created detailed logging mechanisms because of our debugging needs.Our team needed to develop smart approaches to executing conversational agents and asynchronous request management to maintain system performance.Accomplishments that we're proud of: The team is most proud of their achievement in creating natural dialogue with historical personalities that retains their authentic vocal identity and individual character traits. The security guard mechanisms in place successfully stopped character impersonation which created believable and engaging communication sessions. Our team achieved success by deploying multi-threaded conversation management which allowed both participants to maintain seamless communication without any delay.What we learned: Through this project, we obtained comprehensive knowledge about managing the complexities of conversational AI that involved understanding prompt engineering combined with precise definition of conversational limitations. Our research into Flask showed us how to manage threading functions as well as asynchronous tasks and delivered foundational knowledge about insuring AI-generated content quality.What's next for T.I.M.E: The future development will add more fictional and historical personalities to the platform together with improved agent scalability features while investigating possible multimodal input options including images and audio. Our system development plans include letting users personalize the conversational agents for an improved tailored interactive experience.",
                        "github": "https://github.com/priyanshudey11/T.I.M.E-Machine/tree/main",
                        "url": "https://devpost.com/software/t-i-m-e"
                    },
                    {
                        "title": "PudiAI",
                        "description": "Customizable AI assistant, easy modularity. Can be PResident Bendapudi or Danny Devito",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/pudiai"
                    },
                    {
                        "title": "wagebait",
                        "description": "Ever participated in intellectual gambling?",
                        "story": "\ud83d\udca1 Inspiration: We were inspired byKahoot\u2019sfast-paced trivia gameplay and the strategic depth ofpoker, so we decided\u2014why not combine them? The result is a trivia game that rewards not only what you know, but how confidently and strategically you play.\ud83d\ude80 What It Does: This interactive trivia platform allows users to create custom games with personalized categories and invite others to join using a unique game code. It adds acompetitive, poker-inspired twistto traditional trivia\u2014players place bets across multiple rounds before answering each question.\ud83c\udfae Game Overview: wagebaitblends the energetic buzz of Kahoot with the strategic tension of poker. With custom trivia categories, multiplayer functionality, and a three-round betting mechanic, the game challenges players to balanceconfidence, timing, and speed.\ud83d\udd79\ufe0f How It Works: The host creates a game by selecting or customizing trivia categories.Other players join using aunique join code.,Each question unfolds overthree suspenseful rounds\u2014just like poker\u2014with more information revealed each time:\ud83c\udccf Round 1: Topic RevealPlayers see the general topic of the question.\u2192 Players canbet,call, orfoldusing in-game currency.\ud83e\udde0 Round 2: Question RevealThe full question is revealed,but not the answer choices.\u2192 Another round of betting begins.\u2753 Round 3: Options RevealAll answer choices are shown.\u2192 Final bets are placed before the action starts.,Players who are still \u201cin\u201d after Round 3 must answer asquickly and accuratelyas possible from their own devices.The total betting pot issplit among all correct responders.\u2192 Thefasteryou answer correctly, thelargeryour share of the winnings.\ud83e\udde0 Why It\u2019s Different: This isn't just trivia\u2014it\u2019strivia with stakes. By combining knowledge with poker-style decision-making,wagebaitrewards:Strategic risk-takingConfident playQuick reflexesSmart timing,Fold too early and you could miss easy points. Stay too long and risk losing your bet.\ud83d\udee0\ufe0f How We Built It: We used the following stack:Framework:Next.jsHosting:VercelDatabase & Realtime:Supabasefor user data, game state, and real-time updates,\ud83e\udde9 Challenges We Ran Into: Handlingreal-time gameplaywas new territory for us. Ensuring that multiple players could join, place bets, and receive question updatesin syncwas both complex and rewarding.\ud83c\udfc6 Accomplishments We're Proud Of: Built a really close to fully functional multiplayer trivia game with real-time playDesigned and almost implemented athree-phase betting systemLearned to work with Supabase\u2019s real-time subscriptions and Next.js routingCreated a unique and engaging game thatfeels fresh and fun,",
                        "github": "https://github.com/Tythran/wagebait",
                        "url": "https://devpost.com/software/wagebait"
                    },
                    {
                        "title": "SymptomSense",
                        "description": "A simple, intuitive web assistant that helps users understand their symptoms, check urgency levels, discover nearby clinics, and get medication info\u2014all in one place.",
                        "story": "Inspiration: As international students, we have personally experienced these challenges. For example, when we feel unwell, we usually need to visit the University Health Services (UHS) on campus. However, even reaching UHS involves multiple steps: logging into the website, navigating the system, and booking an appointment \u2014 all of which can be overwhelming when we\u2019re already not feeling well. The biggest challenge often comes when we are required to choose a department based on our symptoms before booking a visit. For many of us, it\u2019s difficult to accurately judge what kind of care we need, leading to confusion and delays in getting help.We have made up our mind to develop a platform whereby not only international students but also elderly individuals and people who may not fully understand the symptoms they are experiencing can properly take care of their health. To solve these problems, our platform would provide users with initial guidance based on their symptoms, helping them better understand their condition and navigate their next steps. It will offer a one-stop-shop for pharmaceutical advice, symptom diagnosis, and suggestions for the nearest reliable hospitals or clinics, allowing users to make informed health decisions quickly and confidently.This will ensure that everyone, regardless of age or background, receives appropriate care at the appropriate time, avoiding unnecessary confusion and saving valuable time during stressful situationsWhat it does: Our platform provides users with an empowerment through a thorough health checkup based on their submitted symptoms. When the users input what they have as symptoms, the system generates an extensive output that includes:How we built it: Our application was coded in TypeScript as the main programming language, and the development environment utilized was Visual Studio Code. Because of the AI-driven nature of the application, we implemented the Gemini API to obtain responses for various features including:Challenges we faced: During the development of our platform, we faced some challenges which required innovative solutions and collaboration.Things that we're proud of accomplishing: Throughout the development process of SymptomSense, we enjoyed a few major accomplishments which are a testament to our technical ability and hard work. We were able to combine several APIs, such as Google Maps and OpenFDA, and calibrated AI responses through the Gemini API to provide users with feedback on symptoms, medication, and next steps. Our focus on user-centered design paid off in a simple-to-use interface, and MongoDB gave us secure and successful management of user data.What we learned: We learned API integration, AI output prompt engineering, and user interface design with the user at the forefront through building SymptomSense. We learned more about database management with MongoDB and version control by working with GitHub.What is next for SymptomSense.: To take SymptomSense forward, we intend to introduce a feature that creates a PDF health report based on symptom information input by the user and AI-based suggestions. The user can have this report sent securely to a health professional at their own will, enabling users to get expert medical guidance from home. For this feature to function, however, there would have to be positive user authorization and consent in order to ensure that sensitive health information is shared appropriately. Further, to secure user information and prevent potential violations, we would ensure full compliance with HIPAA standards, adhering to best practices of data security and protection.",
                        "github": "https://github.com/9916bernard/health_assistance_app",
                        "url": "https://devpost.com/software/symptomsense"
                    },
                    {
                        "title": "SoundTrack - Your Music, Your Moments. Mapped",
                        "description": "Ever forgot the name of a song you heard outside class? Ever wondered where you listen to music the most? Ever wanted to reminisce with your favorite playlist and videos? Dont worry- SoundTracks ",
                        "story": "Inspiration: We've all had that moment\u2014walking through a city or driving with the windows down\u2014when a song hits just right, and you instantly associate it with that time and place. But later, when you try to recall what song it was or where you were, it's a blur.SoundTrackwas built to capture those moments. It saves your Spotify listening history along with your location, so your music becomes a living map of memories. Whether you're reliving a summer trip or just want to know what your \u201csad playlist\u201d was on a rainy Tuesday night, SoundTrack has you covered.What it does: SoundTrack is a location-aware Spotify desktop player that:Maps your music: Every song you listen to gets logged with geolocation and pinned on an interactive map.Rewind Recap: Generates a personalized \u201cSpotify Rewind\u201d with your top songs, artists, and albums. You can upload custom media to enhance the nostalgia.Music Roast: Uses ChatGPT to generate a hilarious roast of your music taste.Multimedia Playlists: Add photos and videos that play alongside songs, bringing your playlists to life.Full Player Interface: Built-in playback controls and song browsing, all within the app.,How we built it: We used a wide range of tools, libraries, and APIs to bring SoundTrack to life:spotipy: For Spotify API integration (authentication, track history, playback, etc.)sqlite3: Lightweight local database to store listening data and location pins.geocoder: To fetch real-time location when a song plays.folium: To build interactive maps that display listening history.openai(via custom moduleRoast_test.py): To generate witty, AI-powered music roasts using ChatGPT.,tkinter: For the desktop user interface (cross-platform and simple to deploy).tooltip.py: Custom module to add helpful tooltips to UI elements.PIL (Pillow)+requests+BytesIO: To load, process, and display album art and user-uploaded images.threading+time: For smooth background polling and async UI updates.,Roast_test.py: Handles API calls to ChatGPT for generating roast content.rewing.py: Aggregates play history and builds Spotify Rewind summaries.tooltip.py: Adds hover-based tooltips to the interface.,Challenges we ran into: Integrating Spotify\u2019s playback SDK with a desktop GUI required a lot of trial and error.Handling geolocation reliably across different devices and locations while protecting user privacy.Making sure all data (songs, locations, images) sync correctly in real-time.Keeping the UI responsive while polling Spotify\u2019s API in the background.Prompt engineering to make the ChatGPT roast feature consistently funny and relevant.,Accomplishments that we're proud of: Creating a fully functional multimedia Spotify player from scratch.Visualizing listening history in a way that's both personal and nostalgic.Seamless integration of music, maps, and memories.Building an AI roast generator that actually makes people laugh\u2014and sometimes cry.Keeping the app lightweight, local, and privacy-respecting.,What we learned: How to integrate multiple APIs and libraries cohesively.Using OpenAI's ChatGPT for creative features like roasts.How to manage media and location data effectively in a music app.Designing a fun yet functional user interface withtkinter.Just how emotional and powerful music memories can be when they're captured visually.,What's next for SoundTrack: Launching amobile versionfor on-the-go location tagging and memory logging.Addingcollaborative memory mapsso friends can share music memories.Smarter AI recaps: ChatGPT-generatedstoriesorsummariesof your listening year.Offline mode for capturing data without a connection, then syncing later.Public playlists with memory overlays\u2014like interactive audio diaries.,",
                        "github": "https://github.com/Dhasvanth-mg/SoundTrack.py",
                        "url": "https://devpost.com/software/soundtrack-4vb5tf"
                    },
                    {
                        "title": "CampusConnect",
                        "description": "A place to receive information on the latest events on campus!",
                        "story": "Inspiration: As a Penn State student, keeping up with campus events is a challenge! Clubs rely on social media like GroupMe and Instagram, which can easily flood your feed, leading to missed opportunities. We've all had that moment of discovering an amazing event\u2014only to realize it already happened. That\u2019s why we created CampusConnect\u2014a seamless way to keep student life organized beyond the classroom.What it does: CampusConnect consolidates all local clubs and events in State College into one easy-to-navigate platform. Users can browse events, add them to their dashboard, and categorize them for a personalized experience. The app integrates with calendars, allowing users to track their interests effortlessly. Plus, with a built-in chatbot, users can ask about upcoming events\u2014like the next basketball game\u2014and instantly add them to their schedule.How we built it: We built a sleek, professional-looking front end! For many of us, this was our first hackathon or our first real programming project. Yet, we crafted an intuitive UI that makes it easy for users to explore and organize events.Challenges we ran into: One of the major challenges was back-end development. With little experience in this area, we spent hours learning Selenium for web scraping but eventually pivoted to SeatGeek\u2019s API for event data. When integration proved difficult, we created placeholder events for future API implementation. Despite the challenges, we embraced the learning curve and gained valuable insights into back-end development and APIs.Accomplishments that we're proud of: We built a sleek, professional-looking front end! For many of us, this was our first hackathon or our first real programming project. Yet, we crafted an intuitive UI that makes it easy for users to explore and organize events.What we learned: Diving into back-end development was a major learning experience for us. We started with little to no knowledge in this area, but throughout this hackathon, we gained a solid understanding of APIs, web scraping, database management, and server-side logic. Learning Selenium for web scraping introduced us to automation, while working with SeatGeek\u2019s API taught us how to fetch and integrate external data into our platform. We also explored authentication, data structuring, and the challenges of handling dynamic content.Beyond technical skills, we learned the importance of adaptability\u2014when something didn\u2019t work, we quickly pivoted and found new solutions. Collaboration was also key, as we divided tasks, helped each other debug, and kept pushing forward despite roadblocks. This experience gave us a strong foundation in back-end development and boosted our confidence to tackle even bigger projects in the future!What's next for CampusConnect: We're doubling down on back-end development to enhance functionality and integrating a smart chatbot for a more interactive experience. Stay tuned\u2014CampusConnect is just getting started!",
                        "github": "https://github.com/CatHub11/CampusConnect",
                        "url": "https://devpost.com/software/campusconnect-1v3fdc"
                    },
                    {
                        "title": "Fair Share",
                        "description": "Tired of tracking expenses with your roommates/friends whenever you go shopping? Try out Fair Share so you never lose a dime.",
                        "story": "Inspiration: I live with four other roommates and we often go but groceries together. However, it is always a frustrating whenever we want to split the final spending. Therefore, we decided to make an app that can possibly solves this problem as it probably also impacted many other university students.What it does: This app allows users to add \"friends\" inside the mobile app. After adding all your \"friends\", the users can input spendings and decide how they want to split the spending. They can choose who they want to split the expense with. After inputing spendings, users would be able to see who owes who money individually. The system also deals with the case: \nFriend A owes B 20 dollars, while Friend B owes A 15 dollars -> Friend A owes B 5 dollars.\nUsers can also tapped on the user cards of their \"friends\" to see the detailed transactory history.How we built it: We decided to use Google Flutter to develop a local mobile app as it allows us to build an app for both IOS and Android simultaneously.Challenges we ran into: The biggest challenge that we've ran into would be designing the UI. The time we spent in designing UI exceeds our expectations tremendously. We both are not professional in art design, and designing the UI with code really frustrates us.Accomplishments that we're proud of: We are proud of the fact that the code works better than we thought. Although the app has a lot of spaces for improvements, it still finishes its jobs.What we learned: Through this hackathon experience, we can both agree that we have way more understanding about Flutter and app design. We've been wanting to make a real mobile app for a long time and we only had a little experiences in this field. Due to this event, we are able to dive into the field of mobile app development and acquire the basic knowledge for Flutter design.What's next for Fair Share: As the app works way better than we imagined, we are planning to discover more possibilities and opportunities in this app while enhencing our personal coding skills in the future. If everything works as intended, we might also upload the app on to google play and app store.",
                        "github": "",
                        "url": "https://devpost.com/software/fair-share-copzj4"
                    },
                    {
                        "title": "PetVitals",
                        "description": "Between all your daily obligations, giving pets the needed care can be overwhelming. With PetVitals\u2019 AI veterinary assistant, vet finder, and more, you will never stress over your pet\u2019s health again.",
                        "story": "Inspiration: As pet lovers, we all know the worry of having a new pet. We don't want anyone to stress about taking care of their animals.One of our team members also owns an African pygmy hedgehog\u2014an animal in which few veterinarians have expertise. We understand the importance of access to species-specific information when it comes to pet health and care.How it works: PetVitals boasts a variety of features: first and foremost, it contains an AI veterinary assistant where users can inquire about their pets\u2019 injuries or illnesses. Using the user\u2019s query and any images or medical history provided, this assistant helps find potential causes of the pets\u2019 ailments and whether they need medical attention. If so, the app will provide information on nearby vet clinics. On the other hand, if users find it difficult to know when to feed their pets, the PetVitals app can help keep track of what is best for them.How we built it: For the backend, we leveraged Gemini to create the AI assistant. We also used Google Places API and geolocation to find the vets closest to any given user. For the frontend we built the IOS app using SwiftUI, which communicated with the backend through our Render.com cloud server. The frontend then interpreted and displayed the json files for both the Gemini and Google Places frameworks within the app.Challenges we faced: One of our most significant challenges was connecting the server-side to the client-side through Render.com; although some of our team members have experience connecting backends to frontends on a website, connecting it to an IOS app was just different enough to create a few extra difficulties. It was especially difficult to test the connection at the stage right before we got it working. This led to hardship while trying to get Swift to interpret the information from the backend and display it accurately as well.We also initially had some small troubles working with Google Gemini. Finding the best prompt to get the information we desired took a good chunk of our allotted time.Accomplishments: Our greatest accomplishment is the app itself. From ideation to creation, it proved to be a challenging yet fun task. All of our hard work has led to a product that we can confidently say we are proud to have created.What we learned: We learned how to host an app on the cloud and integrate Swift with backend Python files. We also learned how to work with Gemini AI and its prompts. Ultimately, building a large-scale app was a new yet fun learning experience for all of us.What's next for PetVitals: Adding compatibility for Android is a must. We want to make the app accessible to as many people as possible. Furthermore, we want to implement a feature where the app will send an email when you need to immunize your pets and include an option to send all data collected by the diagnosis to your chosen clinic.",
                        "github": "https://github.com/AlexanderMcGreevy/PetAid",
                        "url": "https://devpost.com/software/petvitals"
                    },
                    {
                        "title": "NFS-R (Network File System Reimagined (in Rust))",
                        "description": "The purpose of this project is to reimagine NFS with security in mind using a memory safe programming language, Rust. This will be a re-build from the bottom up. ",
                        "story": "",
                        "github": "https://github.com/Mauzy0x00/nfs_r",
                        "url": "https://devpost.com/software/nfs-r-network-file-system-reimagined-in-rust"
                    },
                    {
                        "title": "Space Goggles",
                        "description": "A way for everyone to see. This app takes in point cloud data and identifies objects within the given PCD file.",
                        "story": "Inspiration: We all thought that the ICDS challenge was very interesting and a new topic for all of us.What it does: Takes in PCD (Point Cloud Data) and detects items/objects given the data by running through a trained model and is displayed on a web app, showing the point cloud visualization as well as boxes representing identified objects.How we built it: We started with research into libraries, frameworks, models, different vision LLMs, and picked a tech stack we believed would be best suited for the task. We built out the front-end with React and Bootstrap, developed our backend using the Django web framework and numerous libraries to create visualizations, organize data, make API calls, etc.Challenges we ran into: Early on we were unclear about the goal of the challenge and where to begin, so we started off on a slow foot. As we started getting into it, one member solely was able to find a viable library for detecting objects, but he was the only one that was able to get it running without errors. Many of us were having trouble keeping up with all the imports and requirements, causing many halts in development time.Accomplishments that we're proud of: We took on a difficult and new challenge head-on, creating a functional website that seamlessly displays our data in an organized manner.What we learned: We learned how staying up over 24 hours makes your code suffers, how frustrating errors can be, and how developing on all MacOS, Windows, and Linux can cause some problems. We also learned how to effectively split up work, to enjoy the big wins when the code compiles correctly, and how fast a product can come to life when even just students work together.What's next for Space Goggles: In the future we will add compatibility and support for item detection for phone lidar, image, and video processing. The end goal would to be able to have a camera or lidar sensor on a pair of glasses or headwear that can help those that are blind or with poor vision to identify what is around them.",
                        "github": "https://github.com/ErnieCoding/hackpsu-icds",
                        "url": "https://devpost.com/software/gang-gang-nzjf46"
                    },
                    {
                        "title": "MajorMatch",
                        "description": "Connect with like-minded students who share your academic interests and personality traits.",
                        "story": "Inspiration: We were inspired by how social media over the past few years have greatly enhanced the quality of connections, especially throughout school. We planned to make this process even more efficient and make friends along the way.What it does: MajorMatch is a website that intelligently creates a system for likeminded students to be able to connect with one another. It has \"big\" chatrooms for colleges and majors but also \"small\" chatrooms for closely connected individuals.How we built it: We built it as a React app with a backend for Mongodb and another backend for Gemini APIWhat's next for MajorMatch: We plan to expand our userbase and refine our algorithms",
                        "github": "https://github.com/PrestoshTJ/majormatch",
                        "url": "https://devpost.com/software/majormatch"
                    },
                    {
                        "title": "A.I.D.E.",
                        "description": "A.I.D.E. (Adaptive Intelligent Department Efficiency) is an AI-powered assistant designed to streamline customer-facing interactions at front desks.",
                        "story": "Inspiration: A.I.D.E. (Adaptive Intelligent Department Efficiency) was inspired by real-world frustration. My teammate and I both experienced firsthand the limitations of customer-facing roles during our time working at Georgia State University's Recreation Center. Every time a patron asked us a question, we found ourselves hurriedly sifting through stacks of manuals and documents, struggling to quickly pinpoint accurate information. We knew there had to be a better, tech-driven solution.What it does: A.I.D.E. empowers front desk employees by providing rapid and accurate responses to customer inquiries. It utilizes Retrieval-Augmented Generation (RAG) to quickly search through uploaded departmental documents and deliver precise answers within seconds. This dramatically enhances response accuracy and reduces wait times, improving the overall customer experience.A unique feature of A.I.D.E. is its \"Add Scenario\" button, allowing users to instantly incorporate new queries into the system, continually improving its accuracy and adaptability.How we built itHow we built it: We developed A.I.D.E. using:Flask: For creating the web application framework.OpenAI API: For advanced natural language processing and understanding.Retrieval-Augmented Generation (RAG): To efficiently search and retrieve relevant information from documents.\nOur solution supports multiple file formats including PDFs, DOCX, and plain text, ensuring versatility and ease of integration in various departments.Challenges we ran into: One major challenge was effectively integrating RAG with diverse document formats. Handling the variety and complexity of real-world departmental documents required us to use different modules like Py2PDF etc.Accomplishments that we're proud of: We're particularly proud of:Successfully implementing the innovative \"Add Scenario\" feature, making the tool increasingly adaptive.Achieving swift and accurate information retrieval, significantly cutting down response times.Creating a user-friendly interface that can be easily adopted by non-technical users.What we learned: Throughout the project, we gained valuable insights into:Advanced natural language processing techniques.Handling and indexing large, varied datasets effectively.Developing user-centric features and interfaces tailored to real-world needs.What's next for A.I.D.E.: Future plans include:1.) Expanding support to additional file types and multimedia formats.2.) Enabling encryption to enhance data security.3.) Developing A.I.D.E. as a fully web-hosted application for broader accessibility.4.) Strengthening resilience against prompt injection attacks.5.) Deploying A.I.D.E. in real-world environments to gather user feedback for continuous improvement.Through A.I.D.E., we aim to continuously empower front desk staff to deliver exceptional customer service efficiently and effectively.",
                        "github": "https://github.com/Vihaan-Dhaka/HackPSU10project",
                        "url": "https://devpost.com/software/a-i-d-e"
                    },
                    {
                        "title": "CreaStream",
                        "description": "Track your creatine and water intake. View insights about saturation.",
                        "story": "Inspiration: There aren't any creatine trackers on the market that offer a comprehensive view of your saturation. If you take a week off, or want to take a lower dose there is no way of knowing how efficacious your supplementation is.Creatine can also be detrimental to your kidney if not paired with adequate water intake. We integrate this with water tracking features and have a great-looking front end.What it does: This app is a creatine and water intake tracker with a unique saturation tracking feature. Here\u2019s how it works:\nCreatine Saturation Tracking \u2013 The app estimates how much creatine is stored in your muscles based on your daily intake. Since muscles can only hold a certain amount of creatine, this helps users avoid wasting supplements, ensures they're optimizing their intake, and gives them a better understanding of their supplement status.Daily Creatine Intake Log \u2013 Users can log how much creatine they consume each day, whether they're in a loading phase (higher intake for rapid saturation) or a maintenance phase (smaller daily doses to sustain levels).Water Intake Tracking \u2013 Because creatine increases water retention in muscle cells, proper hydration is crucial. The app encourages users to drink enough water based on their creatine consumption and overall hydration needs.Metrics - The user can view a chart visualizing their creatine saturation or daily hydration status over the past 4 weeks. They can see key metrics about the types of drinks they consume most, how consistently they log/hit their creatine and water goals. Features like streaks and total logged days keep the user invested and engaged, rewarding prolonged use.User-friendly interface \u2013 The app is designed with a visually appealing and responsive front end, making it easy to track progress and understand saturation levels at a glance.Full-Stack - The back-end of the app is built off of supabase. The user is able to make an account, sign in, and all of their data is stored in our hosted databases. All of their data is safe, we leverage PostgreSQL's row level security.The app is a production ready MVP.How we built it: We built this using React Native w/ Expo Router and plenty of other packages for components, styling etc. The app has a supabase backend for database and user authentication (powered by PostgreSQL).Challenges we ran into: We ran into plenty of bugs, both big and small, along the way. The trickiest part was actually the project setup! Since we had so many different technologies we had a slow start. React Native doesn't have too many capable, customizable, and pretty out of the box chart libraries, unlike React, so that was difficult to implement. Typescript was a challenge, adding a type for everything has huge advantages but can be tedious.Accomplishments that we're proud of: We are very happy that we essentially completed a full-stack app in the 24/hr time period. Our app is both functional and polished. It solves a problem we face daily so its an app we will both be using. Every technology we worked with was new to us and we managed to learn and implement them effectively.What we learned: We learned how quickly we can build things if we collaborate closely. We also became quite competent in a range of industry standard tools and technologies. Finally we learned how far pure grit, and 5 Celsius can take you.What's next for CreaStream: We plan to publish this to the app and add monetization features such as ads and a pro plan.",
                        "github": "",
                        "url": "https://devpost.com/software/creastream"
                    },
                    {
                        "title": "Smart_Camera",
                        "description": "We wanted to try to improve something old with something new. We decided to try to incorporate Gen ai with security camera systems to help improve identifying individuals with little information. ",
                        "story": "Inspiration: Small businesses often get the small end of the stick when it comes to security. We wanted to create something that a small business could leverage to catch thieves even if their face isn't fully seen. This brings security to a larger mass of people, offering them more security, and freedom from thinking that they won't catch the perpetrator.What it does: Our project uses a webcam to detect full or partial faces. Once a face is detected, a picture is taken. Users can select this picture from the GUI and place a black box around part of the face that is blocked. This image is fed to our GenAI model and where it recreates the face from the facial parts left on the image.How we built it: Used pytorch to make a UNET model to generate a complete face given a partial face image.\nThe camera used deepface to detect partial/full faces. Took a picture from the camera and zoomed in on the face.\nWe used tkinter to make a GUI to select an image with a partially blocked face. The GUI also allows the user to make a black box around the face. This image is fed to the GenAI where it recreates the face with what is left from the image.Challenges we ran into: Initially, we wanted to use a Raspberry pi and a usb camera to collect our video feed and take picture of faces. We ran into a problem with uploading our images to MongoDB and opted to use our laptops instead.Accomplishments that we're proud of: The GenAI model is very accurate in recreating faces with parts left in the image. The model also took 7 hours to train, so we were very pleased with how it turned out.What we learned: We learned how to create a UNET model using pytorch, how to create a GUI using tkinter, and how to detect faces with deepface.What's next for Smart_Camera: Improve the GenAI model capability in up-scaling and improve the GUI useability.",
                        "github": "https://github.com/kieranlaverty/hackpsu_S25",
                        "url": "https://devpost.com/software/smart_camera"
                    },
                    {
                        "title": "Work it Out",
                        "description": "AI-generated workouts with real-time pose tracking. Get feedback, count reps, and stay on form\u2014no gym or trainer needed.",
                        "story": "\ud83d\ude80 Inspiration: As students constantly battling stress, screen time, and irregular routines, we\u2019ve realized staying active ishard. One of our team members recently started going to the gym but struggled with motivation, form, and structure. The question became:Most fitness apps throw random workouts at you without feedback or correction. We wanted something smarter\u2014an AI-powered experience thatunderstands your goals, tracks your form, and grows with you.\ud83d\udee0\ufe0f What We Built: Work-it-outis a full-stack workout assistant that:\ud83d\udcc5 Generatespersonalized weekly workout plansbased on your goals.\ud83c\udfa5 Tracks your reps in real time using yourwebcam + pose detection.\ud83e\udde0 Classifies exercises like squats, push-ups, bench press usingMediaPipe + TensorFlow.\ud83d\udd18 Launches the correct form-tracking script with a single click\u2014no setup, no terminals.,It\u2019s a personal coach, rep counter, and workout planner\u2014all in one app.\ud83d\udca1 What We Learned: How to bridge theReact/Next.js frontendwithFlask + Python-based pose tracking.How to launchMediaPipe pose estimation in real-timefor form classification and rep counting.How to handlescript execution through Flask, dynamic data routing, and smooth UI feedback.How to design a frictionless experience that feels native\u2014despite working across two languages.,\ud83d\ude05 Challenges: IntegratingMediaPipe\u2019s real-time detectionwith webcam feeds and counting reps accurately.Dynamically mapping GPT-generated exercises (like \u201cIncline Bench Press\u201d) to Python scripts.Ensuring each.pyscript launched correctly without freezing the backend.Dealing withCORS,Windows subprocesses, andpath resolutionacross systems.,\u2705 Outcome: We created an AI workout generator with real-time rep tracking that runslocally, launches withone click, and works out of the box. No CLI. No setup. Just results.For students like us\u2014and millions of others trying to stay active\u2014it\u2019s one less excuse and one more reason to show up.",
                        "github": "https://github.com/Gotey/work-it-out",
                        "url": "https://devpost.com/software/work-it-out-2oi1h6"
                    },
                    {
                        "title": "Employr",
                        "description": "A web application that allows recruiters to doomscroll through potential job applicants like a dating app",
                        "story": "Inspiration: We were inspired by the addictive nature of swiping on dating apps, and the difficulty of finding a job as a computer science major. This would allow recruiters to see a larger volume of applicants quickly, and ensure a human monitor to AI resume checks.What it does: We take input from applicants for their resume data, and we show that data to recruitersHow we built it: We used HTML, CSS, and Typescript to make the frontend, and Express.js for the backend, with a database of resumes in MongoDB AtlasChallenges we ran into: No one on our team had much experience with backend, making mongoDB and express a struggleAccomplishments that we're proud of: We made a website & finished a project!What we learned: We learned about Express and MongoDB, and many of our teammates have never used HTML or CSS.What's next for Employr: Traditional dating apps are done on mobile rather than as a web app, we may want to port this to mobile. We want to have a better algorithm for recommending applicants to recruiters",
                        "github": "https://github.com/rmedcraft/HackPSU-Spring2025",
                        "url": "https://devpost.com/software/employr"
                    },
                    {
                        "title": "Blizzard Warning",
                        "description": ".",
                        "story": "Inspiration: Recent ICE raids have caused fear and uncertainty in vulnerable communities. We wanted to create a tool that aggregates and visualizes this information in real-time, helping people stay informed and safe. Our goal was to turn scattered, often hard-to-find news into accessible, actionable data.What it does: Blizzard Warning scrapes news sources for reports of ICE raids, uses Gemini AI to extract key details like date, location, description, and then formats them into structured JSON. A web app then displays this data in two ways:Sidebar: Cards summarizing each raid for quick browsing.Interactive Map: Google Maps markers show raid locations\u2014clicking them reveals descriptions.,How we built it: Our website primarily used React and TailwindCSS  in TypeScript for front-end. Then Node.js and express in TypeScript for back-end. We first scraped the web for links related to ICE raids, then scraped those links for all text, then used Gemini AI to parse out and format the information as a JSON file. Then used this formatted data to list out recent raids and place markers on the interactive Google Map.Challenges we ran into: As this was our first time ever webscraping, it took us few hours to implementAccomplishments that we're proud of: We built a fully automated pipeline from scraping to formatting to visualization.What we learned: We learned how to work with Node and Express to implement the backend portion in TypeScript and pipeline data to client side.What's next for Blizzard Warning: Scraping social media for recent raids because social media can give us access to more recent and authentic data.",
                        "github": "https://github.com/rkamron/ice-raid-map-final",
                        "url": "https://devpost.com/software/blizzard-warning"
                    },
                    {
                        "title": "CollabaMap",
                        "description": "You Had To Be There",
                        "story": "Inspiration: We always kept missing events on campus we didn't even know about. We looked at Waze and SnapChat's Snapmap for inspiration. We like how people can submit their own markers like cop locations in Waze and we like the community aspect of Snapmaps, so we decided to combine them.What it does: CollabaMap is an interactive mapping project that brings the entire local community together to shout out all things in life. With how busy modern living is, things always fly under the radar. With CollabaMap anyone and everyone can chart out events in their life. With just a simple click of a button, an event like a party or stand can be seen by the entire community. Then, other people can contribute, either upvoting or downvoting events based on whether they like the event or if it's not as exciting as they expected. We provide a simple and effective way to contribute real-time events to the people around us.How we built it: The front end is built using Google Maps React JS API. The back end is FastAPI with MongoDB as the database.Challenges we ran into: We had a lot of challenges getting the jsx file to compile, as we kept adding components into one file, creating this huge file that was very hard to debug.Accomplishments that we're proud of: We are very proud of how stateful the map is with the events, it can update across multiple clients and is even mobile friendly!What we learned: We learned to split up our jsx files into separate files, each one with its own web component, especially now that we know how hard it is to debug one large file. We also learned how to use FastAPI which worked out really well.What's next for Collabamap: We'd like to implement more social media type features such as likes and the ability to post images associated with an event. We would also like to create an LLM that analyzes the images uploaded to create a badge that verifies that the event actually exists. Another stretch goal we had was to create a decay function that slowly got rid of less popular events/landmarks.",
                        "github": "",
                        "url": "https://devpost.com/software/collabamap"
                    },
                    {
                        "title": "PeerPulse",
                        "description": "PeerPulse utilizes efficient distributed computing power using peer connectivity as an alternative to expensive hardware scaling.",
                        "story": "Inspiration: We at PeerPulse were inspired to develop our app because of high barrier to entry that deep learning and other computationally expensive tasks bring to the tech atmosphere. Hardware is becoming more important by the day, and students and many personal developers do not have the capital to invest in enterprise-level hardware that is necessary to keep up in today's day in age. However, most people have access to plenty of consumer grade hardware that should help with this problem.What it does: PeerPulse establishes a centralized network where clients can come to donate their computing power. Clients can join this network and run specific functions, allowing computationally heavy tasks to be allocated to each client evenly, maximizing parallel processing power. These kinds of tasks are usually expected to be ran by a GPU, as they can handle large amounts of simultaneous calculations and data interpretation, but our solution receives similar results.How we built it: To preserve the retro vibe for the 10th anniversary, the frontend was built entirely with a terminal user interface and written in C++ with ncurses. We also utilized python for the client script and frontend sockets. We used tailscale to help create our distributed network.Challenges we ran into: The biggest challenge was managing the retro vibe with the modern features of our application. We decided to create a socket server to manage the sending and receiving of data, but this created various problems for us that we had to solve. Additionally, using a multithreaded process environment made deadlocks and other synchronization errors quite common. Additionally, we had initially planned to use raspberry pi's as a simple source of computing power, but we quickly realized that there was no way to connect them to a monitor, so we had to use 2 laptops checked out of Patee Library, for which we could not install any software that required a restart (linux, WSL, VMware, etc. ).Accomplishments that we're proud of: Our ability to create a ncurses frontend despite not having much experience with terminal UIs is something that we are proud of. Additionally, some of our team members didn't have much experience with low level C programming, but we were able to pull together and learn whatever we needed to to get the project done.What we learned: Even though our team members had different skill sets, we learned how to utilize each person's strengths to the overall advantage of our team. Working in a heavy time crunch meant that we needed as much out of everyone as possible, so our more experienced members were able to guide those who didn't have much experience with the codebase into making meaningful contributions.What's next for PeerPulse: PeerPulse recognizes the inherent preference for GPUs, but the average student does not have access to a GPU that can be utilized for deep learning. Therefore, the next step for PeerPulse is to extend functionality towards other devices, like consoles and iOT embedded systems, which can truly maximize the efficiency of PeerPulse for the future.",
                        "github": "https://github.com/dumrich/PeerPulse.git",
                        "url": "https://devpost.com/software/peerpulse"
                    }
                ],
                [
                    {
                        "title": "QR GENRATOR",
                        "description": "A qr genrator backend code using python\r\n",
                        "story": "",
                        "github": "https://github.com/AKS-2005/QR.git",
                        "url": "https://devpost.com/software/qr-genrator"
                    },
                    {
                        "title": "Tuk Tuk",
                        "description": "Trusted, affordable carpooling for college communities. \r\nRide safe, save money, and network on the go!",
                        "story": "Inspiration: Finding safe, affordable travel options as a college student can be challenging.Ride-sharing apps are often expensive and unreliable, public transportation is either lacking, poorly maintained, or inconsistent, and hitching a ride with strangers comes with safety risks. We wanted to create a trusted, college-exclusive ride-sharing platform where students, faculty, and staff can carpool with verified members of their college community\u2014making travel cheaper, safer, and more social while also reviving the carpooling culture and contributing to environmental sustainability. Plus, Tuk Tuk isn\u2019t just about commuting; it\u2019s a networking opportunity on wheels.Nowadays, everyone is looking for new ways to connect, and we bring it right to your doorstep!What It Does: Tuk Tuk lets you:Post ride details at your convenience\u2014whether you\u2019re traveling and have empty seats, or want to split fuel costs\u2014with information like destination, date, and time given beforehand.Search for and join rides with verified students and staff, ensuring you travel with someone from your own college community. User details are available, ensuring safety and informed ride choices.Enjoy a cost-effective, secure, and community-driven ride-sharing experience while also networking on the go. Who says commuting can\u2019t be productive? We make travel social and environmentally friendly!Redefine the outdated carpooling system by eliminating its flaws\u2014we\u2019ve tackled the key problems of trust, safety, and coordination, revamping the entire idea to make ride-sharing smarter, safer, and more efficient.Introduce networking as a core feature, turning every ride into an opportunity to connect with like-minded peers. Whether it\u2019s making new friends, finding study partners, or expanding professional connections, Tuk Tuk integrates community-building into daily commutes.,Tracks it Addresses:: EntrepreneurshipTuk Tuk transforms a hackathon project into a scalable startup by addressing key gaps in modern commuting\u2014high ride-sharing costs, safety concerns, and lack of community-driven solutions. With a university-backed authentication system, a freemium adoption model, and strategic partnerships with colleges, we ensure both trust and sustainability while promoting affordable and eco-friendly travel for students.10th Anniversary: Timeless TechCarpooling peaked in the 1970s but declined due to a lack of structure and trust. Tuk Tuk revives this lost trend by leveraging technology-driven ride-matching, verified authentication, and built-in networking opportunities. By blending past insights with modern innovation, we redefine carpooling for today\u2019s students, making travel safe, social, and efficient while pushing the boundaries beyond mere imitation.How We Built It: Frontend: HTML, CSS, React for an intuitive and user-friendly interface.Backend: Flask (Python) for handling ride management and other core functionalities.Database: SQLite for lightweight and efficient storage of user and ride details.Authentication: Not implemented yet, but future plans include Supabase or Microsoft API integration for secure college credential verification.,Challenges We Ran Into: Ensuring Security: Preventing unauthorized access from users without active college credentials, ensuring only verified students and staff can use the platform.Getting Colleges on Board: Convincing universities to recognize Tuk Tuk as a trusted solution for student transportation and networking.User Adoption: Encouraging students to shift from traditional ride-sharing apps to a more community-driven and affordable alternative.,Accomplishments That We're Proud Of: Successfully building an easy-to-use interface for posting, sharing, and searching for rides.Creating a fully functional web app designed for secure, college-exclusive ride-sharing.Enabling students to connect with their peers, enhancing the sense of college community, and promoting networking on the go.Providing a way for students to earn extra income while offering affordable transportation options to others.,What We Learned: The importance of trust in community-driven ride-sharing and how to make it a core feature.How networking and social connections can be seamlessly integrated into everyday activities like commuting.Redefining networking on the go\u2014who knew rides could be both productive and budget-friendly?,What's Next for Tuk Tuk: Implementing secure authentication through university credentials (Supabase/Microsoft API).Expanding to more universities across the U.S. and popularizing the concept.Launching a mobile app for seamless, on-the-go ride management.,Join Tuk Tuk today and experience a safer, more affordable, and community-driven way to travel!",
                        "github": "https://github.com/blackspider-ops/Hackpsu_part2",
                        "url": "https://devpost.com/software/tuk-tuk-lowz06"
                    },
                    {
                        "title": "Pixelity",
                        "description": "Self-hosted solution to all your photo storage needs; all on one phone.",
                        "story": "Inspiration: How many of you have an old smart phone lying around? Phones that can't seem to keep up with modern requirements. Pixelity allows you to make good use of it with a single click.What it does: Pixelity allows you to sign up and use your old phone as a network attached storage, like your own personal cloud. This allows the user to backup all their pictures privately on their own old phone. Our project requires no setup, it is ready out of the box and is easy for laymen to use.How we built it: We built the web interface with React+Typescript+Next.js and hosted it on Vercel. The mobile app is made using the React Native library, based off of our web interface.Challenges we ran into: Initially we decided to use the Go+HTMX stack to make our project. The main issue we ran into was the fact the go mobile, which would have allowed us to compile to apk was deprecated. Due to which we had to pivot to using Next.js and React Native with Vercel hosting. There was also an issue with the POST method in the React Native app which proved to be rather finicky.Accomplishments that we're proud of: Given the limited time we had, the team really pulled out all the stops. We were able to have a fully functioning MVP at the end of the hackathon which did more than what was our initial prediction. Also React Native was a new technology for all of us. We found it a joy to work with and are proud of what we have achieved with it.What we learned: We learnt that a careful study should be performed of the the tech stack before getting bogged down in development. This would help us to determine if our stack will let us achieve our projects goals efficientlyWhat's next for Pixelity: We had 2 main directions in mind that we can explore in the future.",
                        "github": "https://github.com/lakhia13/pixelity-app/tree/main",
                        "url": "https://devpost.com/software/pixelity"
                    },
                    {
                        "title": "Home Inventory System",
                        "description": "A way to keep track of your home pantry",
                        "story": "",
                        "github": "https://github.com/icy-strong/hackpsusp25",
                        "url": "https://devpost.com/software/home-inventory-system"
                    }
                ]
            ]
        },
        {
            "title": "Lumination",
            "location": "No Context",
            "url": "https://lumination.devpost.com/",
            "submission_dates": "Mar 28 - 30, 2025",
            "themes": [
                "Blockchain",
                "Machine Learning/AI",
                "Music/Art"
            ],
            "organization": "Luma AI",
            "winners": false,
            "projects": []
        },
        {
            "title": "WAI DataQuest ",
            "location": "London, Canada",
            "url": "https://wai-dataquest.devpost.com/",
            "submission_dates": "Mar 29 - 30, 2025",
            "themes": [
                "Beginner Friendly"
            ],
            "organization": "Western Artificial Intelligence",
            "winners": false,
            "projects": [
                [
                    {
                        "title": "Meal Snap",
                        "description": "Our app uses a photo of your fridge to identify ingredients with a YOLO v12 model and suggests recipes based on what you have. Save time and cook smarter without guessing or extra trips to the store!",
                        "story": "Inspiration: We were inspired by the common struggle of deciding what to cook with what\u2019s already in the fridge. We wanted to make meal planning easier, reduce food waste, and save time by offering a solution that instantly suggests recipes based on available ingredients.What it does: Our app lets users take a photo of their fridge, identifies the ingredients inside using a YOLO v8 object detection model, and suggests recipes based on those ingredients. It helps reduce food waste, save time, and cook meals without needing extra trips to the store.How we built it: We built the app using a machine learning YOLO v8 model for object detection, combined with a large language model (LLM) to generate recipe suggestions based on the detected ingredients. The backend handles image processing, ingredient identification, and recipe suggestion logic, while the front-end is designed for an easy and intuitive user experience.Challenges we ran into: Tuning the object detection model to accurately identify a wide range of ingredients.Integrating the machine learning model with the natural language processing system to generate relevant and practical recipes.Ensuring the app works seamlessly across different devices with varying camera qualities.Fine-tuning the app to handle diverse kitchen setups and ingredient variations.,Accomplishments that we're proud of: Successfully integrating the YOLO v12 object detection model for precise ingredient identification.Creating a user-friendly interface that\u2019s quick and intuitive for anyone to use.Offering an AI-powered solution that saves time and reduces food waste.,What we learned: Combination of machine learning with natural language processing for real-world applications.Fine-tuning an object detection model for real-world environments,What's next for DataQuest 2025: Expanding the database of ingredients and recipes to include more international dishes and dietary preferences.",
                        "github": "https://github.com/ompatel-24/Meal-Snap/",
                        "url": "https://devpost.com/software/dataquest-2025"
                    },
                    {
                        "title": "StudiaVida",
                        "description": "Our tool uses AI to turn unstructured academic documents like syllabi and assignment PDFs to a calendar. It parses deadlines, estimates assignment difficulty, and generates study schedules. ",
                        "story": "",
                        "github": "https://github.com/VidaLucia/DataQuest2025",
                        "url": "https://devpost.com/software/studiavida"
                    },
                    {
                        "title": "Dataquest 2025",
                        "description": "An automated tool that transforms your resume and a job post URL into a ready-to-send, custom cover letter.",
                        "story": "Inspiration: Software jobs in this day are very competitive, which leads to aspiring software developers to apply to as many jobs as possible, but to make unique and high quality cover letters for these applications takes time and effort, which can be better used to apply for more jobs or to better self skills and prepare for potential interviews.What it does: What the app does is generate a cover letter.  The user will have to provide a resume, a linkedin job posting url, any extra details they wish to add, and select a template, and that will be sent to a LLM which will generate a latex document for the cover letter.  The user can then edit this document, and when they are happy with the result, they are able to download the document, where the app will compile the latex, and download a pdf of the cover letterHow we built it: We build it using OpenAI's api model, a backend made in python utilising Flask, and a frontend made in Javascript utilising ReactChallenges we ran into: Some challenges we ran into was having trouble with the compilation of latex file, which was caused by some import issues which we solveAccomplishments that we're proud of: This app is an accomplishment we are very proud of as it work as is intended and has the functionalities we wanted.  We are proud of how this app turned out, and we may plan on deploying it in the future.What we learned: Some things we learned were how to set up a flask backend, as well as just some project structureWhat's next for Dataquest 2025:",
                        "github": "https://github.com/raginggoosedev/dataquest-2025",
                        "url": "https://devpost.com/software/dataquest-2025-ngpof5"
                    },
                    {
                        "title": "Palendr",
                        "description": "We make your goals achievable ",
                        "story": "\ud83d\udc4b Inspiration: We all have goals \u2014 whether it's learning a new skill, launching a side hustle, or finally getting in shape. But turning intention into action? That's the hard part. Existing planners ask for too much structure, and calendars don\u2019t think for you.We wanted something smarter. Something that planswithyou.That\u2019s wherePalendrcomes in.\ud83d\udca1 What It Does: Palendris an AI-powered planning tool that takes yourgoal, yourcommitment level, and yourpreferred format, and turns that into a realistic, personalized timeline. Think of it as a hybrid between a planner and a strategist \ud83e\udde0\ud83d\udcc5\ud83d\udcdd You input:Yourgoal(e.g., \"Learn Python\", \"Write a screenplay\")Yourcommitment level(Low, Medium, High)Youroutput style(Calendar, Checklist, Timeline Summary),\ud83e\udd16 Palendr does the rest:Breaks down the goal into actionable stepsDistributes them based on your chosen paceOutputs a clear roadmap \u2014 no guesswork, no overload,\ud83e\udde0 How We Built It: Frontend:Built with React + Tailwind for a clean and fast UIBackend:Node.js + Express for input handling and output generationAI Logic:OpenAI\u2019s GPT model breaks down goals into subtasksScheduling Engine:A custom lightweight allocator that maps tasks over a timeline based on pacing,\ud83d\ude80 Challenges We Ran Into: Interpreting vague goals like \"Get fit\" or \"Be more productive\" and translating them into actionable stepsMaking sure our pacing engine feltrealistic, not roboticKeeping the UI clean while still offering meaningful customization,\ud83c\udfc1 Accomplishments We're Proud Of: A fully working MVP that feels lightweightandintelligentSmooth user flow: input to schedule in under 60 secondsGetting \u201cPalendr\u201d to feel more like a tool than a chore,\ud83d\udd1c What's Next: \ud83d\udcc6 Google Calendar integration\ud83d\udd01 Feedback-based re-planning (missed a step? We adjust!)\ud83d\udcca Goal analytics & habit tracking\ud83d\udde3\ufe0f Natural language goal clarification,\u2728 Final Thoughts: Palendrisn't just another planner \u2014 it's a smarter way toget where you want to go.Whether you're grinding hard or taking it slow, it adapts to your flow.\ud83c\udfafYour goals, on your terms.",
                        "github": "",
                        "url": "https://devpost.com/software/palendr"
                    },
                    {
                        "title": "TapIn",
                        "description": "eat, play, stay...tapped",
                        "story": "\u2728 Inspiration: What\u2019s the first place you turn to for travel recommendations? For Gen Zs, it's not Google or Yelp. 74% of them check TikTok \u2014 part of 1.58 billion daily users across 154 countries. But with the abundance of content and endless scrolling, it's hard to find direct suggestions without feeling overwhelmed or exhausted.We wanted to streamline that discovery process. What if you could tap into TikTok\u2019s authentic, visual experiences \u2014 but see them on a map?\ud83d\udccd What it does: TapIn combines the functionality of interactive maps with the power of TikTok\u2019s firsthand content.\nUsers simply enter a location and choose a category \u2014 food, nature, shopping, or stay \u2014 and TapIn displays TikToks relevant to that area directly on the map as clickable red pins.Each red dot opens up a curated TikTok that gives users an instant peek into that spot, so they can make decisions visually, confidently, and quickly. Users can then click into the TikToks and be redirected to the app where they can save the posts to their own collections.\u2699\ufe0f Challenges we ran into: AI Model IssuesOur original idea was to utilize OpenAI\u2019s API and use a ChatGPT wrapper that extracts TikToks based on what the user is looking for. The AI should also be able to extract the location of where the TikTok is recommended. The issue we realized mid-way through coding the backend was that OpenAI is unable to access TikToks directly; thus, we had no access to direct TikTok links. As a result, we had to come up with other solutions. After hours of brainstorming and thoughtful consideration, we found out another AI, Perplexity (known as a research AI), can easily access sent links and further information other APIs were not able to access.Prompt EngineeringGetting good results from the AI wasn\u2019t just about asking questions \u2014 it required crafting the perfect prompt. We spent a lot of time testing, refining, and iterating to make sure our queries returned complete, location-rich responses that we could actually use.A fully functional and polished interface.Seamless 3D map integration with embedded TikTok pins.Color-coded pins by category\u2013 Enhance the map experience by assigning distinct colors to each category (e.g., food, shopping, nature, stay), allowing users to visually filter and explore at a glance.Sponsored Pins\u2013 Introduce sponsored or promoted pins for restaurants, attractions, or businesses that want to reach travelers directly. These pins would be subtly marked and could include exclusive deals or featured content.Verified Experiences\u2013 Highlight trusted or high-quality recommendations with a badge system to differentiate between authentic content and ads.",
                        "github": "https://github.com/kevinli5371/TapIn",
                        "url": "https://devpost.com/software/tapin-uf6bxy"
                    },
                    {
                        "title": "TextGem",
                        "description": "Text to Generative AI",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/textgem"
                    },
                    {
                        "title": "SmartPantry",
                        "description": "Smart Pantry scans your fridge and pantry to generate recipes instantly\u2014no waste, no stress. Scan. Generate. Cook.",
                        "story": "Inspiration: We wanted to solve the everyday problem of \"What do I cook?\" maximizing what people already have at home. Smart Pantry was born to make meal planning effortless, budget-friendly, and sustainable.What it does: Smart Pantry scans your fridge and pantry to recognize ingredients and generates personalized recipes based on what\u2019s available. It also helps you manage your inventory by tracking what you have, notifying you of expiring items, and suggesting meals to minimize waste.How we built it: We made use of AI for image recognition to identify items in your fridgeWe scraped a recipe website and compared the recipe with the items in the inventory and only displayed recipes with ingredients you have,Challenges we ran into: Ensuring accurate ingredient recognition from imagesHandling edge cases like missing ingredients,Accomplishments that we're proud of: Successfully implementing real-time ingredient scanningBuilding an intuitive and visually appealing user interface,",
                        "github": "https://github.com/NichoIas-L/SmartPantry",
                        "url": "https://devpost.com/software/smartpantry-3hrozd"
                    },
                    {
                        "title": "Visionary",
                        "description": "Visionary is a web application that leverages real-time camera technology to detect objects instantly, providing users with immediate feedback and enhancing their interaction with the environment.",
                        "story": "",
                        "github": "https://github.com/IanPTan/dataquest25",
                        "url": "https://devpost.com/software/visionary-m0tary"
                    },
                    {
                        "title": "Cook Smart",
                        "description": "Dinner decisions? Done. More cooking, less thinking.",
                        "story": "Inspiration: You know that feeling when you stare into your fridge like it\u2019s gonna magically tell you what to make? That\u2019s the problem. Everyone's busy, and decision fatigue is real. We wanted to create something that takes the mental load off and makes cooking fun and easy again.What It Does: Cook Smart uses AI to recommend meals based on what\u2019s in your pantry. Just input your ingredients, and it generates recipes tailored to what you have. No more wasted food, no more endless scrolling through recipes, just practical, delicious meals in minutes.How We Built It: We leveraged machine learning algorithms to analyze ingredient compatibility, nutritional value, and user preferences. Our backend uses a Python-based recommendation engine while the frontend provides a seamless user experience with React. We also integrated a dynamic recipe database and personalized suggestions using past cooking patterns.Challenges We Ran Into: Ingredient data can be unpredictable - people don\u2019t always log items accurately. Building a robust system that handles substitutions and suggests smart alternatives was tricky.Ensuring the AI understands regional cuisines and dietary preferences took extra fine-tuning.Creating a smooth and intuitive user interface that minimized friction was a constant focus.Accomplishments That We're Proud Of: Successfully built an AI that doesn\u2019t just suggest random recipes, but actually learns user preferences and uses what the user currently has in their inventory.Reduced food waste by helping users cook with what they already have.Integrated real-time nutritional analysis to promote healthier eating.Built an intuitive interface that makes decision-making effortless.What We Learned\nSimplicity wins. Users want quick answers, not a thousand options.Personalization is everything \u2014 the AI had to learn tastes over time to add real value.Data hygiene matters. Clean input = smarter outputs.Feedback loops are key. We constantly gathered insights to fine-tune suggestions.What's Next for Cook Smart\nVoice Integration: Imagine asking, \"Hey Cook Smart, what can I make for dinner?\" and getting an instant response.Smart Shopping Lists: Generate a list of missing ingredients for suggested recipes.Partnerships: Collaborate with grocery delivery services to fill gaps in ingredients.Expanded Recipe Database: More regional and cultural dishes for a wider variety of tastes.Nutrition Insights: Provide meal recommendations based on dietary goals and restrictions.",
                        "github": "",
                        "url": "https://devpost.com/software/cook-smart"
                    }
                ]
            ]
        },
        {
            "title": "Hack The Herd 2025",
            "location": "A131, Science Center",
            "url": "https://hacktheherd25.devpost.com/",
            "submission_dates": "Mar 29 - 30, 2025",
            "themes": [
                "Machine Learning/AI",
                "Productivity",
                "Social Good"
            ],
            "organization": "AI in Liberal Arts at Amherst College",
            "winners": true,
            "projects": [
                [
                    {
                        "title": "Amherst Lost & Found",
                        "description": "Each year, campuses gather tons of lost items\u2014many go to landfills. Our Django-based Amherst Lost & Found app uses AI and maps to reunite items with owners, cutting waste and helping the planet.",
                        "story": "\ud83e\udde5 The Beginning: A Lost Coat, Some Water Bottles, and an Idea\nIt all started when I lost my coat and water bottles.I searched everywhere\u2014dorm lounges, classrooms, dining halls\u2014but had no luck. There was no organized system to check where lost items were, no clear way to report something missing, and no map or image to help identify things. It was frustrating.That frustration sparked an idea:\nWhat if I could build a better lost and found system\u2014one that used AI, maps, and filters to help students find what they lost, fast?\ud83e\udde0 Lessons Learned\nThis project taught me everything\u2014seriously. From coming up with the idea to deploying it online, I had to learn a wide range of skills:\ud83d\udca1 Idea Generation: Identifying a real problem and thinking about how tech could solve it.\ud83e\uddf1 Django Framework: Creating models, views, templates, and routes for a functioning web app.\ud83d\uddbc\ufe0f AI + Image Recognition: Integrating a model to auto-detect the type of item from uploaded images.\ud83d\uddfa\ufe0f Mapping with Leaflet.js: Displaying where items were found and dropped off.\ud83c\udfa8 User Interface Design: Making the app clean, accessible, and easy to navigate.\ud83d\udd27 How I Built It\nBackend: Django (Python)Frontend: HTML, CSS, Bootstrap, JavaScriptDatabase: SQLiteImage Recognition: TensorFlow with MobileNetMap Integration: Leaflet.js + OpenStreetMapKey Features\nImage upload and automatic item classificationMap with location pins for where items were foundFilters by category, date, and locationItem list with detailed info (photo, found location, drop-off point, time)\ud83d\udea7 Challenges\nConnecting AI to Django: Making the image recognition work smoothly inside a Django project was new territory.UI/UX: I had to constantly ask, Would someone who just lost their coat know how to use this?Database Design: Getting the structure right for filtering and search was a puzzle.\ud83c\udf1f Looking Back\nLosing my coat was annoying\u2014but it led to something cool.\nNow, I\u2019ve built something that could help other students avoid that same frustration. That feels like a win.",
                        "github": "https://github.com/Tetsuya787/LostandFound",
                        "url": "https://devpost.com/software/amherst-lost-found"
                    },
                    {
                        "title": "EcoDash",
                        "description": "A browser extension that tracks your digital carbon footprint in real time. Visualize your impact with live CO\u2082 emissions data\u2014helping you understand the hidden environmental cost of digital life.",
                        "story": "EcoDash was born from a stark realization: digital activities generate more CO\u2082 annually than the entire aviation industry. Website emissions arise from the energy consumed to power data centers, servers, and the network infrastructure that hosts and transmits website content, ultimately leading to carbon dioxide (CO2) emissions. While working remotely during the pandemic, we noticed how much time we spent on energy-intensive platforms like YouTube and Zoom. This sparked the question: Could AI help users understand and reduce their digital carbon footprint without sacrificing productivity? The project aims to bridge the gap between abstract sustainability goals and actionable personal insights.EcoDash is a Chrome extension that:Tracks CO\u2082 emissions from digital activities (streaming, video calls, social media)Visualizes savings potential through interactive charts and real-time metricsAnalyzes usage patterns using AI to identify high-impact opportunitiesRecommends optimizations like enabling data-saving modes, off-peak streaming, or audio alternativesProvides platform-specific strategies (e.g., \"Switch Spotify to \u2018Normal\u2019 quality (40% savings) that balance user experience with sustainability,Data Layer:: Developed a tracking system using API to monitor active tabs and service usage\nCreated a CO\u2082 calculation engine with platform-specific emission rates (e.g., YouTube: 0.9g CO\u2082/min)AI Integration:: Engineered dynamic prompts for API to generate non-obvious optimizations\nImplemented strict output formatting to ensure actionable recommendations\nAdded fallback mechanisms when API calls failUI/UX:: Designed a minimal dashboard with dark/light mode support\nUsed Chart.js for emissions breakdowns\nPrioritized clarity in displaying AI-generated adviceBalancing Accuracy and Usability: Estimating CO\u2082 emissions for diverse digital services required reconciling conflicting industry data.\nAI Prompt Engineering: Early versions produced generic advice like \"watch less YouTube.\" Refining prompts to suggest optimizations rather than reductions took multiple iterations.Purpose-Driven AI: The system avoids guilt-based messaging, instead offering more actionable strategies than baseline sustainability tools.\nProven Impact: In testing, users reduced digital emissions by 12-18% without reported productivity loss.\nTechnical Innovation: Developed a novel method to correlate browser activity with energy consumption models from the Shift Project\u2019s research.Coding Languages: Most of us were inexperienced but learned enough HTML, CSS, JavaScript, etc., to complete the project.\nAI usage: None of us had implemented AI systems.Accuracy: Extracting and using platform-specific user settings data for higher quality and more updated recommendations.\nExpanded Coverage: Adding gaming platforms (Steam, Xbox Cloud) and more websites.\nCollaborative Features: Team-level emissions tracking for campuses and workplaces.\nPredictive AI: Forecasting future emissions based on usage trends and suggesting preemptive adjustments.",
                        "github": "https://github.com/Jiahuai28/EcoDash",
                        "url": "https://devpost.com/software/ecodash-eqj7hc"
                    },
                    {
                        "title": "Disposify",
                        "description": "Hear the difference! AI waste sorting uses sounds to reward good behavior & correct mistakes. Reduce contamination! ",
                        "story": "",
                        "github": "https://github.com/tdl321/Recognise.ai",
                        "url": "https://devpost.com/software/waste-detection-95kjlp"
                    },
                    {
                        "title": "\u201cBusted!\" - incorrect waste classification warning system",
                        "description": "Introducing Busted; When you throw a plastic container into the compost bin, our program captures you at the right moment, displaying your guilty face on a screen---until you move the trash correctly.",
                        "story": "Inspiration: At Amherst, whenever we try to trash something, we find every trashcan containing incorrect items inside. We were wondering, \"Why are students not caring enough to put the trash in the correct bin?\" When non-compostable items, such as plastics, metals, or synthetic materials, are put into compost bins, they contaminate the compost pile, making the entire pile unusable for agricultural purposes or soil enrichment. And even when biodegradable plates are put in landfills, they decompose in landfills anaerobically, producing methane and contributing to climate change. So, we came up with a fun and promotional idea of raising students' awareness of these trash throwing out actions, by warning them when they incorrectly sort waste.What it does: Our system, \"Busted\", detects incorrectly sorted items in each trash bin. When a student throws a plastic container into the compost bin, it captures the student's picture at the right moment, displaying on a screen above the bins for all to see. It might not go away until the next person comes and makes the same mistake! If students do not want their guilty face to be shown to everyone, they should make sure to put the trash in the right bin.How we built it: This model scans images at regular intervals from a live IP camera feed obtained through a smartphone. When an item is incorrectly placed, a photo of the person is taken with the laptop\u2019s webcam, and displayed on our Django-based web app. In a real implementation, the first camera would be positioned inside of the waste bin facing downward, and the other camera would be positioned above the waste bin, near the screen.\nWhile our project focuses specifically on compost due to time constraints, with use of larger datasets this could be implemented for compost, recycling, and landfill bins.Challenges we ran into: We were initially fine-tuning a ViT(Vision Transformer) model in Hugging Face, to sort the items with higher accuracy at Amherst. We collected more than 150 pictures of compostable plates and containers to train the model; however, even with four repeated trials, the biases were occurring due to disproportionate data. So, we pivoted to using the existing API for the MVP.https://colab.research.google.com/drive/1IDQ6ETNUEPT7-cthSCTORd8XuiXDIyx9?usp=sharinghttps://huggingface.co/rubysmac/compostable-plates-classifier-v0/tree/main,We also tried three other languages/frameworks to develop the front-end to find the optimal way to connect with two cameras and ML model we were using.Accomplishments that we're proud of: We are proud of putting in an amazing effort into this project in a team of three. All of our teammates stayed up all night in the classroom for the entire duration of this hackathon. We had a passion to study new things and apply them to our project.What we learned: We learned how to make a web app using Django, and also how to fine-tune the model in Hugging Face.What's next for \u201cBusted!\" - incorrect waste classification warning system: We would like to update the model into fine-tuned model.Video Demo Link (YouTube): https://youtube.com/shorts/xmZbeYUmmoE?feature=shareInitial Version Website Interface Link: https://af37c701-77b0-4820-8b57-a09051da850a-00-158jrm4g4vwqs.riker.replit.dev/",
                        "github": "",
                        "url": "https://devpost.com/software/busted-incorrect-waste-classification-warning-system"
                    },
                    {
                        "title": "Transit Carbon Emissions",
                        "description": "An app/website that lets users compare carbon emissions of different transport methods ",
                        "story": "",
                        "github": "https://github.com/mirandayang80/HacktheHerd2025.git",
                        "url": "https://devpost.com/software/transit-carbon-emissions"
                    },
                    {
                        "title": "Foodback",
                        "description": "Empower smarter eating with AI! Our planner leverages Amherst's daily menus and leftover photos to help students effortlessly cut food waste by 15%, boosting health, happiness, and sustainability.\r\n\r\n",
                        "story": "",
                        "github": "https://github.com/Mireinstein/Food-Waste-Management-System/tree/main/food-waste-app",
                        "url": "https://devpost.com/software/foodback"
                    }
                ]
            ]
        },
        {
            "title": "GREENHAX",
            "location": "Online",
            "url": "https://greenhax.devpost.com/",
            "submission_dates": "Mar 28 - 30, 2025",
            "themes": [
                "Open Ended",
                "Productivity",
                "Social Good"
            ],
            "organization": "IBM",
            "winners": false,
            "projects": []
        },
        {
            "title": "DataWhiz 2025",
            "location": "Richardson, TX",
            "url": "https://datawhiz-2025.devpost.com/",
            "submission_dates": "Mar 28 - 30, 2025",
            "themes": [
                "Databases",
                "Education",
                "Machine Learning/AI"
            ],
            "organization": "EnVision UTD",
            "winners": false,
            "projects": [
                [
                    {
                        "title": "DataViz - SEG",
                        "description": "By analyzing OSHA injury data with AI and NLP, we uncover root causes like pinch points and PPE gaps, delivering targeted safety strategies to prevent hand injuries in manufacturing workplaces.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/dataviz-seg"
                    },
                    {
                        "title": "Strategic Expansion the Kimberly Clark Statement ",
                        "description": "decision-makers in logistics, procurement, or sustainability seeking data-backed efficiency gains",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/strategic-expansion-the-kimberly-clark-statement"
                    },
                    {
                        "title": "Kimberly Clark",
                        "description": "To overcome resource availability challenges, we prioritize access to a skilled workforce, tackle labor shortages, and strategically position facilities near key transportation and supply hubs",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/kimberly-clark"
                    },
                    {
                        "title": "Uncovering Patterns in HandRelated Injuries in Manufacturing",
                        "description": "The objective of this study is to analyze OSHA's Severe Injury Report dataset to identify prevailing patterns and root causes of hand-related injuries within the manufacturing industry.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/uncovering-patterns-in-handrelated-injuries-in-manufacturing"
                    },
                    {
                        "title": "Accio Insights",
                        "description": "Leveraging OSHA data , Python, Tableau and NLP, we identify root causes of hand injuries in manufacturing to help SEG drive smarter, targeted, and preventive safety interventions.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/empowering-safety-excellence-uncovering-handinjury-patterns"
                    },
                    {
                        "title": "RiskVision for Safety Excellence Group",
                        "description": "Empowering manufacturing with data-driven insights to prevent hand injuries, enhance safety protocols, and ensure a safer workplace for all.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/riskvision-for-safety-excellence-group"
                    },
                    {
                        "title": "Forecast wizards",
                        "description": "AI-powered injury analysis tool that helps SEG identify root causes, assess severity, and prioritize safety actions by company, event type, and location.\r\n\r\n",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/forecast-wizards"
                    },
                    {
                        "title": "Finding the causes of injuries events",
                        "description": "In this project, we try to find out the factors of injuries in each industry, manufacturing for example in this current version, and propose solutions or advice for enhancing safety in. ",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/change-it-later-qepf9x"
                    },
                    {
                        "title": "Axis Of Insight",
                        "description": "We use public data and machine learning to find the best locations for Kimberly-Clark\u2019s next tissue mills\u2014maximizing demand coverage while minimizing logistics cost.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/axis-of-insight"
                    },
                    {
                        "title": "VizVibe",
                        "description": "Data Visualization on Kimberly-Clark\u2019s Problem Statement",
                        "story": "Inspiration: Team workWhat it does: VisualizationHow we built it: Tools using KnimeChallenges we ran into: Data cleaningAccomplishments that we're proud of: Workflow and VisualizationWhat we learned: New tools and team workWhat's next for VizVibe: More opportunities to work on real time data",
                        "github": "",
                        "url": "https://devpost.com/software/vizvibe"
                    },
                    {
                        "title": "DataBuddies - kimberly Clark ",
                        "description": "\"Optimizing Kimberly-Clark\u2019s U.S. Expansion: Data-Driven Mill Locations to Slash Logistics Costs & Boost Market Reach\"",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/databuddies-kimberly-clark"
                    },
                    {
                        "title": "404: Insights Not Found A Safety Intelligence Dashboard  ",
                        "description": "\"We turned 20,000+ OSHA injury reports into AI-driven safety insights. Using NLP, we decoded how and why hand injuries happen \u2014 so companies can stop them before they do.\"",
                        "story": "\ud83d\ude80 What Inspired Us\nHand injuries are among the most common and severe incidents in manufacturing, often resulting in amputations, fractures, or long-term disability. While OSHA collects thousands of injury reports, most insights remain hidden inside unstructured text fields. We wanted to unlock those hidden patterns and transform safety data into actionable intelligence using the power of AI and NLP.\ud83d\udee0\ufe0f How We Built It\nData Source: We used OSHA\u2019s Severe Injury Reports dataset, filtering for Manufacturing industries and incidents involving hands, fingers, or thumbs.Data Cleaning (Python):\nCleaned and filtered 20,000+ records using pandas and datetime. Missing values were handled, and date columns were standardized.NLP & AI (spaCy + Rule-Based Logic):Used rule-based keyword tagging to detect likely injury causes (e.g., \"unguarded equipment\", \"slipping\", \"machine saws\").Used spaCy NER to extract entities like machines, tools, and affected body parts from narrative text.Extracted top keywords from narratives using CountVectorizer to find recurring terms and themes.Data Enrichment:\nAdded new columns like Likely Cause, Cleaned Source, Narrative Entities, and Month.Dashboarding (Power BI):\nBuilt an interactive dashboard with slicers, AI-tagged causes, KPI cards, timeline analysis, and raw narrative visibility.\ud83c\udfaf What We Learned\nHow to apply Natural Language Processing (NLP) to messy real-world safety dataHow simple rule-based AI can bring huge clarity to unstructured incident reportsThe importance of visual storytelling in turning data into decision-ready insightsHow to design a user-friendly dashboard that speaks to both technical and non-technical stakeholders\ud83e\uddd7 Challenges We Faced\nThe narrative text was unstructured, inconsistent, and often noisy \u2014 requiring creative keyword engineering.Many causes were not labeled in the dataset, so we had to build our own tagging logic for AI-based categorization.Choosing the right visuals and layout in Power BI to balance detail with clarity.Making the NLP pipeline fast and understandable within a limited timeframe.\u2705 Impact\nThis dashboard gives safety managers, analysts, and manufacturing leaders a clear view of what\u2019s causing hand injuries, where they\u2019re happening, and how to reduce them \u2014 potentially saving lives and millions in workplace injury costs.",
                        "github": "",
                        "url": "https://devpost.com/software/a-safety-intelligence-dashboard-for-seg"
                    },
                    {
                        "title": "Powered Prevention",
                        "description": "Powered Prevention Dashboards combines advanced NLP and visual analytics spotlighting patterns behind manufacturing hand injuries, transforming insights into proactive safety strategies.",
                        "story": "How I built it\nI cleaned the data, filtered for manufacturing hand injuries, and used AI to find patterns in injury descriptions. Then, we built a clear, interactive dashboard in Power BI to show trends, causes, and hotspots.\nIt was a good challenge working with unstructured text data and turning it into useful categories. Making the dashboard both insightful and easy to use also took time and testing.Through this hackathon I have learned how AI and data visuals can turn raw reports into real-world safety improvements. I also learned the importance of clear, user-friendly design.As a next advancement, I plan to add predictive features and expand beyond hand injuries. The goal is to build a full safety analytics tool that helps prevent injuries across all types of work.",
                        "github": "",
                        "url": "https://devpost.com/software/powered-prevention"
                    },
                    {
                        "title": "Datavengers",
                        "description": "We've created an automated tool to analyze workplace injuries. and help organizations design better awareness campaigns and make data-backed decisions to reduce injuries before they happen.",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/ai-driven-insights-for-injury-prevention"
                    },
                    {
                        "title": "Mamba Mentality: New Paper Mill Locations For KC",
                        "description": "We used Python, Maps API & Tableau to find Sheboygan, WI as a top KC mill site with 47% savings and $3.5M logistics cost\u2014smart, strategic, and data-driven.",
                        "story": "Project Story: Optimizing Tissue Mill Locations for Kimberly-Clark\nOur project began with identifying the top 20 U.S. metropolitan areas based on Kimberly-Clark\u2019s shipping priorities. Instead of analyzing demand at a general county level, we enriched our model using census data specific to metropolitan regions\u2014capturing deeper insights on population density, household size, income, and age distributions.To structure the analysis, we divided the country into five demand clusters, weighted by population and regional spread. From these, we advanced with three priority clusters based on strategic gaps and supply chain needs.We then developed a demand index per cluster using demographic variables to quantify tissue product demand. With the help of the Google Maps API, we calculated freight distances from hundreds of counties to the top 20 metros and Kimberly-Clark\u2019s existing converting plants.Following this, we narrowed down candidate counties by analyzing not just logistics but also industrial zoning availability, independence from current mill regions, and regional expansion potential.For Utah, we went a step further and conducted land parcel detection to identify areas meeting the ideal characteristics for a tissue mill\u2014flat topography, highway access, zoning compliance, and future scalability.This multi-layered approach allowed us to recommend three optimal mill sites\u2014each backed by data, grounded in strategy, and built for Kimberly-Clark\u2019s next phase of growth.",
                        "github": "",
                        "url": "https://devpost.com/software/logistical-analysis-of-new-paper-mill-locations"
                    },
                    {
                        "title": "- VizStuds - Safety Excellence Group Analysis",
                        "description": "Ergonomic Insights to Prevent Machine Related Hand Injuries",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/safety-excellence-group-analysis-vizstuds"
                    },
                    {
                        "title": "Nishudhanaa",
                        "description": "Predicting Crime, Protecting Austin: Data-Driven Forecasts for Safer Communities",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/nishudhanaa"
                    },
                    {
                        "title": "kimberly clarke problem statement",
                        "description": "We strategically leverage US Census data to select optimal mill locations, minimizing logistics costs and enabling sustainable operations close to major metropolitan markets across the United States. ",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/kimberly-clarke-problem-statement"
                    }
                ]
            ]
        },
        {
            "title": "DataQuest",
            "location": "London, Canada",
            "url": "https://dataquestai.devpost.com/",
            "submission_dates": "Mar 29 - 30, 2025",
            "themes": [
                "Databases",
                "Machine Learning/AI",
                "Open Ended"
            ],
            "organization": "Western Artificial Intelligence",
            "winners": false,
            "projects": []
        },
        {
            "title": "CodeQuest",
            "location": "Austin Elementary School",
            "url": "https://codequest-24508.devpost.com/",
            "submission_dates": "Mar 29, 2025",
            "themes": [
                "Beginner Friendly",
                "Gaming",
                "Open Ended"
            ],
            "organization": "tomorrows youth empowerment",
            "winners": false,
            "projects": []
        },
        {
            "title": "Innovate, Create, WIN",
            "location": "Student Union Building",
            "url": "https://innovate-create-win.devpost.com/",
            "submission_dates": "Mar 28 - 29, 2025",
            "themes": [
                "Education",
                "Machine Learning/AI",
                "Social Good"
            ],
            "organization": "biola",
            "winners": false,
            "projects": []
        },
        {
            "title": "Code I/O 2025",
            "location": "Ohio State University - Dreese Labs",
            "url": "https://code-i-o-2025.devpost.com/",
            "submission_dates": "Mar 29, 2025",
            "themes": [
                "Beginner Friendly",
                "Social Good",
                "Education"
            ],
            "organization": "Youth for STEM Equity",
            "winners": false,
            "projects": [
                [
                    {
                        "title": "Zombie Math Thing",
                        "description": "with alex liu",
                        "story": "Inspiration: Other Games such as blooket and gimkit: What it does: Teaches young kids how to do young math: How we built it: using scratch made by mit: Challenges we ran into Time, wasn't that good: Accomplishments that we're proud of: We made an alright game: What we learned: How to code ish: What's next for Zombie Math Thing : More code:",
                        "github": "",
                        "url": "https://devpost.com/software/zombie-math-thing"
                    },
                    {
                        "title": "hack project",
                        "description": "help the purple ball out the maze!",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/hack-project"
                    },
                    {
                        "title": "Fun Fact Quizzes",
                        "description": "A new, innovative way to do a quiz.",
                        "story": "Inspiration: Our inspiration was how some online quizzes are so boring, and they teach you only one thing.What it does: Fun Fact Quizzes teaches you about 3-4 subjects, ELA, Math, Social Studies, and Science.How we built it: We build Fun Fact Quizzes via HTML, and a discovery of more HTML Tags.Challenges we ran into: One challenge we ran into was to make the buttons work, at first they did nothing. But after a few attempts, we got it.Accomplishments that we're proud of: We love how we got a wide variety of questions, and fixing our roadblocks.What we learned: We learned how to how  use HTML code in real-life.What's next for Fun Fact Quizzes: We will continue adding more things, like grade groups, and more questions, and a better UI.",
                        "github": "",
                        "url": "https://devpost.com/software/betterquizzerz"
                    },
                    {
                        "title": "Maze Game",
                        "description": "This maze game represents a fun, educational experience where you try to help Cat find his way home!",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/blog-post-1"
                    },
                    {
                        "title": "The History Behind the Statue of Liberty",
                        "description": "Historical monument in New York City. ",
                        "story": "Inspiration: We went to New York recently and saw the Statue of Liberty. We were fascinated by the structure and wanted to learn more about it.What it does: It teaches people the history of the Statue of Liberty, including how it was made, how it came to the United States, and how it got its name.How we built it: We used replit, the html coding software. We used the previous Celebrity Flight Bio page as our page, creating new codes for our own projectChallenges we ran into: Spacing everything out to make it look neat and organized.Accomplishments that we're proud of: The text we wrote, changing the colors, and make the headings pop out from the informational text.What we learned: We learned more about the Statue of Liberty and how to a code an article.",
                        "github": "",
                        "url": "https://devpost.com/software/the-history-behind-the-statue-of-liberty"
                    },
                    {
                        "title": "Crystal Clicker",
                        "description": "Click the crystals to learn about them!",
                        "story": "Inspiration: Cookie ClickerWhat it does: Click on the crystals to get informationHow we built it: ScratchChallenges we ran into: End ScreenAccomplishments that we're proud of: TimerWhat we learned: How to create timerWhat's next for Crystal Clicker: Create end screen",
                        "github": "",
                        "url": "https://devpost.com/software/crystal-clicker"
                    },
                    {
                        "title": "Weird Facts Platformer",
                        "description": "This is an obby/platformer that contains some fun facts in each slide. Red is toxic lava and cyan is invisability! Have fun.",
                        "story": "Inspiration: I've played many platformers on games like Roblox and Geometry Dash, so I thought, \"Why not make one myself?\".What it does: It provides some fun facts to people who play it, plus it increases your skills in keyboarding.How we built it: Using Scratch, we used the codes to put things together and enabled the platformer content! We used shapes, designs, codes, and with help from others.Challenges we ran into: When it comes to making things educational, it didn't really turn out well because it was hard to add more things to something that is finished. But we managed to make it into an actual working platformer.Accomplishments that we're proud of: It looks pretty decent and good. We were also creative with the facts and designs.What we learned: Directions and motion coding, and as well as the weird facts haha!What's next for Weird Facts Platformer: Probably more of these types of platformers!",
                        "github": "",
                        "url": "https://devpost.com/software/weird-facts-platformer"
                    },
                    {
                        "title": "Atom merger",
                        "description": "A game were you merge atoms to find atoms.",
                        "story": "I did this because I have always loved science. This game was basically meant for kids to explore and find different ways atoms can combine. It was a relatively easy project, but it was hard to try and find a way to look if multiple items were in a list. I am really proud of this project, and generally what it can teach students. I mentioned that it was hard to find if multiple of the same item was in a list, so I was able to look it up and solve that problem. I hope to add more molecules in the future!",
                        "github": "",
                        "url": "https://devpost.com/software/atom-merger"
                    },
                    {
                        "title": "Quiz",
                        "description": "Science and Geography Quiz by coding in Python",
                        "story": "Inspiration: trivia games online: What it does: Helps you study for geography and science: Made a class called Quiz; made lists and for loops: Challenges we ran into: Everything: Accomplishments that we're proud of: Completing the code that was 110 LINES: What we learned: Using \"in\" in the loop: What's next for Quiz: More questions:",
                        "github": "",
                        "url": "https://devpost.com/software/quiz-8zm9u4"
                    },
                    {
                        "title": "Why code: Animated",
                        "description": "A short and basic animation that used all the resources available resourcefully during production. It features 2 characters discussing why you would want to code your own game",
                        "story": "Inspiration: We were inspired to do this because we belived that an animation could be created with the limited resoruces we hadWhat it does: It teaches students about why someone would want to code their own gameHow we built it: We built the converstion by useing wait blocks when the other person was talking, and talking blocks when they were.Challenges we ran into: The most improtant resource, time, was very restricted.Accomplishments that we're proud of: We enjoyed the design of the charcter on the write, Gregory.What we learned: We learned that we can make something nice, even with limited resourcesWhat's next for Why code: Animated: Perhaps a continuation would be important, as the animation ends on a cliffhanger",
                        "github": "",
                        "url": "https://devpost.com/software/why-code-animated"
                    },
                    {
                        "title": "Coin Clicker",
                        "description": "Coin Clicker is a wonderful game that involves gaining coins with puzzling math problems that occur every so often to encourage the gamer to build their brain muscles",
                        "story": "Inspiration: We love moneyWhat it does: Lets you gain more money (not really)How we built it: Love, Hope, and a dreamChallenges we ran into: Coding, cooperation, off topic resourcesAccomplishments that we're proud of: Capitalist gainsWhat we learned: How to code a little betterWhat's next for Coin Clicker: Dollar Clicker",
                        "github": "",
                        "url": "https://devpost.com/software/coin-clicker"
                    },
                    {
                        "title": "Rat from Cat Math problem if score = 5",
                        "description": "Rat getting chased by cat and if score is 5 summons a math problem",
                        "story": "Inspiration Not sure just some idea I have thought about: What it does Each time the rat touches a watermelon it increases the score by 1 and when it hits the score 5 it summons a random 1-100 addition math problem answer varies from 2-200: How we built it I built it using scratch and its simple features using the mouse the rat can follow the cursor and run away from the cat: Challenges we ran into Trying to make the math problem itself: Accomplishments that we're proud of that we basically finished the product: What we learned I learned that you should use your time wisely: What's next for Rat from Cat Math problem if score = 5 maybe multiplication problems:",
                        "github": "",
                        "url": "https://devpost.com/software/rat-from-cat-math-problem-if-score-5"
                    },
                    {
                        "title": "Math Pong Coding Project",
                        "description": "Math Pong - The Way to Have Fun And Learn",
                        "story": "we were inspired by the game pong. we built it from scratch and we used sprites\nwe faced challenges such as getting the ball to bounce and the sprites to move",
                        "github": "",
                        "url": "https://devpost.com/software/math-pong-coding-project"
                    },
                    {
                        "title": "1x1",
                        "description": "it is 1x1",
                        "story": "Inspiration is math: What it does: How we built it: Challenges we ran into: Accomplishments that we're proud of: What we learned: What's next for 1x1:",
                        "github": "",
                        "url": "https://devpost.com/software/1x1"
                    },
                    {
                        "title": "MATH QUIZ",
                        "description": "This can help you with your math and is very pretty!",
                        "story": "Inspiration: We love math so this is what we did.What it does: It challenges your brain.How we built it: Using HTML code.Challenges we ran into: Not knowing what to do.Accomplishments that we're proud of: How pretty it is and finishing it.What we learned: How to better use HTML code.What's next for MATH QUIZ: Harder questions.",
                        "github": "",
                        "url": "https://devpost.com/software/math-quiz-yn2kp4"
                    },
                    {
                        "title": "Coding",
                        "description": "We made  project on scratch and it is very fun",
                        "story": "I was inspired by most of the computer games I play on the computer and I like to play them so I made one.",
                        "github": "",
                        "url": "https://devpost.com/software/coding-qn83dv"
                    },
                    {
                        "title": "fall",
                        "description": "fun",
                        "story": "",
                        "github": "",
                        "url": "https://devpost.com/software/fall-pcr19a"
                    }
                ]
            ]
        }
    ]
}